TP	Hi all! __eou__	User

TP	I don't know if this is the right place but I need some advice on how to train a classifier using a data set with multiple feature types: text, integers, floats, dates any idea? __eou__	User

TP	My first approach was to convert each feature to binary, example: feature 1 -> "the word w_i is in column j?" , feature 2 -> "the value in column j is greater than 10?", feature 3 -> "the month in the date of column j was abril?", etc. by the way, Oliver Grisel: Hi! :D __eou__	User

TP	This message is without content. I wonder if anyone hangs out here. __eou__	User

TP	@mac2bua, I just saw your message. It's probably better to ask specific question with running code snippets on toy example data on stackoverflow. You might be interested in having a look at sklearn_pandas for feature extraction from heterogeneously typed columns. FeaturesUnion might also be helpful. __eou__	User

TP	@ogrisel Thanks a lot for the advice! Fortunately I found this very useful blog: http://bit.ly/1mPEEhH and I decided to start using feature union and pipelines. The result was very good! :smile:  I know that you are a member of the core team of scikit learn so I'd like to thank you, sklearn is so awesome!.  Just one more question: Is there any easy way to get the name of the features?  So often I needed to print out the names in order to understand the relationship between them and the different classes. I wrote some code that gather the information of the different vocabularies in the case of the textual or categorical features and the name of the column in the case of the numerical ones.  Sorry for my bad english! __eou__	User

TP	No, not in general unfortunately: most of the time the data will get internally converted to numpy arrays for efficiency and code simplicity reasons  and even if the column were named in the original representation (e.g. a DataFrame) that information is lost along the way. __eou__	User

TP	hey __eou__	User

TP	hi @amueller __eou__	User

TP	Thanks for starting on the reviews :) __eou__	User

TP	Hi __eou__	User

TP	I want to do the `check_array` empty data first __eou__	User
TP	I'm with family this week and so not super active, but I hope we can work towards the release next week :)__eou__	Agent
TP	sure. There is another check_array pr that might be conflicting__eou__	Agent
TP	and that I think is very important__eou__	Agent
TP	the dtype=object one__eou__	Agent
TP	yeah, I got side-tracked to work on clusterlib the past 2 weeks, I hope I will rebalance effort to sklearn next week as well__eou__	User
TP	what is the issue number of dtype=object?__eou__	User
TP	https://github.com/scikit-learn/scikit-learn/pull/4057__eou__	Agent
TP	tks__eou__	User

TP	Most of the bug-fixes are isolated to one estimator. So I think they are important for the release, but shouldn't interact with other issues so much. The ones that are more API-ish are the dtype=object one (https://github.com/scikit-learn/scikit-learn/pull/4057), the clustering / pipeline one https://github.com/scikit-learn/scikit-learn/pull/4064 and the input validation one https://github.com/scikit-learn/scikit-learn/pull/4136 __eou__	User
TP	ok__eou__	Agent
TP	the last two interact somewhat. not sure how clear that is from the PR but the main thing in the input validation is that it extends common tests to estimators that don't inherit from clustering, classification, regression or transformation mixins__eou__	User
TP	I'll try but I'm not sure I'll finish writing the pipeline integration tests today__eou__	User
TP	oh and going toward the release, we need to fix the bugs and regressions in isotonic regression :-/ I don't think a fix exists yet__eou__	User
TP	yeah__eou__	Agent
TP	the slinear side effects?__eou__	Agent
TP	I saw it broke the calibration PR__eou__	Agent
TP	too many open tabs, my firefox is so slow at times...__eou__	Agent

TP	it did? I didn't see that. haha I know the open tabs issue. Well the ``slinear`` broke some cases of ``fit`` and ``fit_transform`` not doing the same thing. Maybe it broke other things, too. Bugs that are not regressions are that fit and fit_transform were not consistent before in the case of ties, and that having sample_weight=0 in multiple places can lead to infinite loops in the isotonic regression code __eou__	User
TP	that's bad__eou__	Agent
TP	I agree. I proposed a fix in one of the issues. Maybe just remove the implementation of ``fit_transform``, that is use the naive fit.transform, and mask out sample_weight=0. That should get rid of the worst bugs.__eou__	User
TP	but doesn't solve all issues with tie-breaking, I think.__eou__	User
TP	some discussion here: https://github.com/scikit-learn/scikit-learn/issues/2507 and in the issues linked at the bottom__eou__	User
TP	So that was a list of the "hard" issues / PRs. If you want I can also give you a list of the simple bug fixes lol__eou__	User
TP	what are you working on with clusterlib btw?__eou__	User

TP	I want to implement the `concurrent.futures` API. That includes porting cloudpickle.py to Python 3 or implemeting something similar with dill. The goal is to be able to use SGE / SLURM clusters easily, without having to write bash or boilerplate python scripts. There is also cloudpi.pe to watch in the same space. going grab some lunch, see you later I agree ... I don't understand what you mean by finite targets integers & categorical labels? __eou__	User

TP	Args. I am slightly confused by the current API requirements in pipeline. If ``fit_transform`` accepts ``y=None`` we don't require ``transform`` to accept a ``y=None``. That is somewhat inconsistent and weird, I think... Huh ok transform is never passed ``y`` at all... never mind... __eou__	User

TP	@amueller I am reviewing #4057 In another PR I suggested in a comment to add support for the `dtype=[np.float64, np.float32]` idiom. but I don't remember which PR ;) I think the dtype='numeric' is a fine, loose default but I would like to check with the other PR if we also need the `dtype=<list of accepted dtypes idiom>` for specific cases as well. do you remember which PR it was? __eou__	User

TP	Ok found it: it was collapsed in: https://github.com/scikit-learn/scikit-learn/pull/4136#discussion_r24176562 __eou__	User

TP	sorry didn't watch the chat yeah so a dtype list for check_array would be nice, and then we could get rid of ``as_float_array`` __eou__	User

TP	"unfriend all multi-output estimators on facebook." :) __eou__	User

TP	There was only slight frustration on my part. I'm not sure if the ``y_numeric`` is a good workaround, because it somehow stipulates that by default "y" are arbitrary objects aka classification labels btw, there are not really that many tests on ``y``.  Many estimators just used ``np.asarray(y)``... we should probably test for finite targets for regression __eou__	User

TP	I meant for regression, where the target is a float __eou__	User

TP	Hey all! :) @ogrisel You might want to set up gitter to get live activity feeds on the right! Choose github and travis under integrations ( under settings ) __eou__	User

TP	@ragv done! thanks for the tip __eou__	User

TP	It seems that I broke travis but I cannot reach the report page. Running the tests locally. __eou__	User

TP	Ok fixing the broken tests ATM __eou__	User

TP	Scikit learn has crossed 5000 stars on github :beer: __eou__	User

TP	hey hey ogrisel, are you around? __eou__	User

TP	Hi @amueller sorry I was offline for the past couple of days as I had to attend local events. Today I will be working on clusterlib with a colleague. tomorrow I will be available to review PRs on sklearn __eou__	User

TP	Alright. Next week I'll be at strata, though, and won't have much time. We should settle on a timeline for release. Sounds good :) __eou__	User

TP	What about setting the objective of cutting the first beta on Friday Feb 27. Then one beta every two weeks. And release end of March? __eou__	User

TP	@ogrisel Would you be interested in setting up http://landscape.io and landscape bot? #3888 __eou__	User

TP	Refer [this](https://landscape.io/github/ragv/scikit-learn) for live preview of our repo and [this](https://github.com/hugovk/word-tools/pull/7#issuecomment-67533284) for an example comment by the bot. A general suggestion... I feel it would be more helpful to debug test results if we replaced docstrings for tests by comment...  Output when a docstring is present for a test ``` Tests all estimators which support partial_fit ... ok Tests all estimators which support partial_fit ... ok Tests all estimators which support partial_fit ... ok ``` Output when no docstring is present for a test ``` test_common.test_partial_fit('Perceptron', <class 'sklearn.linear_model.perceptron.Perceptron'>) ... ok test_common.test_partial_fit('Perceptron', <class 'sklearn.linear_model.perceptron.Perceptron'>) ... ok test_common.test_partial_fit('Perceptron', <class 'sklearn.linear_model.perceptron.Perceptron'>) ... ok test_common.test_partial_fit('SGDClassifier', <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>) ... ok ``` __eou__	User

TP	Trying out gitter from irc __eou__	User

TP	@ragv thanks for the invite __eou__	User

TP	I agree about the test docstring issue. __eou__	User

TP	@ogrisel would it be worth to have a pr for that? Or should I just create an issue and refer that when any related code is changed? Ah! thanks ;) that was helpful! Sorry should have googled a bit before asking... __eou__	User

TP	Explaining the pbm in an issue is a good idea. Then a bunch of small PRs related to that issue if people have no objection. __eou__	User
TP	Ok! thanks!__eou__	Agent

TP	Alternatively we could use this on travis: https://pypi.python.org/pypi/nose-ignore-docstring __eou__	User

TP	and in the makefile, I'd say don't we even have a nose config file in the repo? :) oh we do need to install a package, I thought it was a build-in option __eou__	User

TP	Im about to send a PR for that ;) and yea `setup.cfg` yea It will go in `continuous-integration/install.sh` after `pip install nose` I'll take care of the setup... __eou__	User

TP	@amueller How do you feel about https://github.com/finnp/gitter-irc-bot ? It look really very simple and could cleanly sync irc and gitter... __eou__	User

TP	sounds ok but I don't have too many cycles to look into it. you can do it and see if anyone complains about noise (which is unlikely) __eou__	User

TP	okay! thanks! :) __eou__	User

TP	+1 __eou__	User

TP	hey hey. I'm kinda back, though somehow in Dallas. And I only have ~400 unread github notifications __eou__	User

TP	... @amueller how did you enjoy strata? __eou__	User

TP	It was pretty awesome :) met a lot of interesting people. Are you bug-crunching this week? I have to do some organisational stuff for GSOC but the rest of the week should be free for release things. __eou__	User

TP	I was having a look at the FDR PR and trying to understand why the actual FDR is always lower than alpha. I am working on something else tonight. Will resume tomorrow. I think I have an idea. __eou__	User

TP	> And I only have ~400 unread github notifications  haha :p __eou__	User

TP	How do I make sure the cython sources are compiled when I rebuild for testing? ( related to #4288 ) ? __eou__	User

TP	cython path/to/file.pyx __eou__	User
TP	Thanks! :)__eou__	Agent

TP	@amueller here I mean __eou__	User

TP	so much sense sorry about that btw what time is it in Paris? Are you in Paris? __eou__	User

TP	yes I am in paris it's almost midnight I am rebasing #3945 then going to bed __eou__	User

TP	sweet, thanks. tomorrow I will be in the office earlier. Thanks for working late :) __eou__	User

TP	For some reason I tagged the old SVC.sparse_decision_function PR with 0.16, but not my new one. That doesn't make sense. Do you think I should tag my PR or untag the old? __eou__	User

TP	Actually I was primarily working on dl-machine this evening :) > For some reason I tagged the old SVC.sparse_decision_function PR with 0.16, but not my new one. That doesn't make sense. Do you think I should tag my PR or untag the old?  I am not sure which is which https://github.com/deeplearningparis/dl-machine __eou__	User

TP	dl-machine? __eou__	User

TP	rephrasing my question: should the sparse decision function be tagged for 0.16 or not (i.e. is it a priority to go into the release) ah, I see. Are you working with torch or theano primarily? __eou__	User

TP	setting up tutorial material for a workshop on theano, but adding support for torch was easy so I added it. +1 for including the sparse decision function fix __eou__	User
TP	cool__eou__	Agent
TP	ok__eou__	Agent
TP	good night, see you tomorrow!__eou__	User

TP	that means 23 PRs left for the release lol __eou__	User
TP	:)__eou__	Agent

TP	have a good night. Hope we make some more progress on the PRs this week __eou__	User
TP	Even if we don't fix them all prior to friday (the first beta) it's fine. As long as we only include fixes in the 0.16.X branch once the beta is cut.__eou__	Agent
TP	thanks__eou__	Agent

TP	good night :) __eou__	User

TP	anything in particular that you would like me to review? __eou__	User

TP	Did you have a look at this one yet: https://github.com/scikit-learn/scikit-learn/pull/4192 I haven't really followed what is happening with the pos_label etc in the metrics __eou__	User

TP	Hey :) sorry to interrupt you... do you feel we could do the data indep. cv part first and later do the renaming? __eou__	User

TP	There are two roads we could go down: either doing both together, but using the fact to have a deprecation path for it. That would mean copying a lot of code for a while. OR doing them separately. If we do them separately, I would do the CV objects first. Also, I wouldn't do either for the 0.16 release, I think. Actually, I think the code duplication thing is not necessary, so we should just focus on the cv objects @ogrisel green button on https://github.com/scikit-learn/scikit-learn/pull/4249 (sign flip) ? __eou__	User

TP	Okay! so I'll salvage #4294 to do the CV related fixes alone? __eou__	User
TP	yeah that sounds like a good idea__eou__	Agent
TP	thanks! :)__eou__	User
TP	haha yea ;)__eou__	User
TP	That was fast ;)  What issue are you currently working on? :)__eou__	User

TP	Just one more minor thing... what is the expected range of scores ( w.r.t #4295 ) ( could it by any chance be >= 40? ) __eou__	User

TP	the sigmoid should be between 0 and 1 as you plotted, and the votes shouldn't be larger than the number of classes, right? __eou__	User

TP	if the values for `sum_of_confidences` ( previously `scores` ) exceed 34 we get 1 which is not desirable... i.e `expit(34)` is 1... with 0.5 it extends upto 73... or am I too paranoid? perhaps `scores` do not extend  upto 34? sorry its at 37.... and yea in my system `expit(37) == 0`... would you mind confirming the same pl? __eou__	User
TP	you mean == 1__eou__	Agent
TP	yes :P sorry again...__eou__	User

TP	sorry, I don't follow. scores can be arbitrary high. You say that it actually reaches 1? that seems a bit odd I see... maybe then we need to do 0.9 * expit(scores) ? which would be just another magic value.... I commented on the issue and pinged @mblondel np. you are right, that is an issue __eou__	User

TP	In [9]: expit(37) Out[9]: array(0.9999999999999999)  In [10]: expit(38) Out[10]: array(1.0) __eou__	User

TP	Perhaps we could scale to `[0, 1]` before mapping it using sigmoid? ( it would be a tad slower ) __eou__	User
TP	that is also weird ;)__eou__	Agent

TP	took me only 3 days to catch up with 7 days of sklearn notifications.... now for some actual work __eou__	User

TP	I have to do some pystruct stuff now because I commited to some research projects. Other than that, I'm working on getting as many PRs as possible that are tagged with 0.16 in, and writing as many bug-fixes for 0.16 tagged issues as possible mostly reviewing currently __eou__	User

TP	@ogrisel this one should also be simple: https://github.com/scikit-learn/scikit-learn/pull/4082 __eou__	User

TP	morning __eou__	User

TP	morning :) __eou__	User

TP	or afternoon ;)_ __eou__	User

TP	are you working on the isotonic stuff right now? Anything I should review? ah, you are doing https://github.com/scikit-learn/scikit-learn/pull/4082, cool :) __eou__	User

TP	yes #4082, I am resolving conflicts (both syntactic and logic), almost done. running the full test suite __eou__	User
TP	cool__eou__	Agent
TP	previously I was trying to understand what's wrong with the isotonic transform (interp1d with kind='slinear')__eou__	User
TP	the bug you found (the infinite loop with zero sample weight) is something entirely different__eou__	User
TP	yeah maybe that was a bad idea. but it was already broken before__eou__	Agent
TP	both are important bugs we should make a priority to fix IMHO.__eou__	User
TP	I know the bugs are unrelated, but I thought the fix might be. I was thinking about removing the implementation of partial fit, which would make it easier to fix both bugs, I think__eou__	Agent
TP	I don't understand the details of any of the 2 bugs at the moment, feel free to investigate on your side__eou__	User
TP	Are you sure this is related to partial_fit?__eou__	User
TP	args I meant fit_transform__eou__	Agent
TP	err fit_predict?__eou__	Agent

TP	I kind of understand the infinite loop. there is a while x != y and if both are NAN then it never finishes. The easiest way to solve the problem is to remove data points with zero sample weight. But if you do that, you can not implement fit_predict the way it currently is. The other way would be to take care of zero sample weight in the Cython code, but I don't understand that part 100% fixed https://github.com/scikit-learn/scikit-learn/pull/4189 ok. I'll try to look into it more today. Now my tests are failing the same way yours are doing. But I had different results on my different laptop this one is on 0.13.3 bye which ones in particular? For the release only the tagged issues are important imho __eou__	User

TP	ok, but this seem completely independent from the bug found in transform by mjbommard in #4185 __eou__	User

TP	when I last checked, I had the impression that some of the inconsistencies come from the way that fit_transform is implemented. If it is indeed only an error in transform, then they are unrelated. I'll double check. it does look like fit_transform works correctly, never mind then it seems different versions of scipy give different results __eou__	User

TP	which version of scipy are you running and how the test fail in your case ? I need to run catch my shuttle. not sure I will be able to work on that further today but will catch up tomorrow bye __eou__	User

TP	what is the difference between "predict" and "transform" in isotonic regression? ah, there is none. Isn't that slightly confusing? __eou__	User

TP	It is, mayve we could deprecate transform in favor of predict. __eou__	User
TP	fyi https://github.com/scikit-learn/scikit-learn/pull/4285 should be good__eou__	Agent
TP	I had some weird behavior with the interp1d but I need to do some NYU stuff before I can investigate further__eou__	Agent

TP	@amueller can I take up the newly tagged easy issues, assuming this will help u release the 0.16 beta cut or should I leave those for a new contributor instead? __eou__	User

TP	I had meant #4298, #4296, #4292... but I just realized I had one unfinished 0.16 tagged PR - #4225... I'll work on it instead :) __eou__	User

TP	you can leave the three issues for new people, or you can go for them. But they are really low priority. Finishing up the other PRs you already have open might be better for us at the moment. __eou__	User

TP	yea :) thanks! __eou__	User

TP	@ogrisel what is your plan for the day? I didn't have much time on the weekend unfortunately :-/ __eou__	User

TP	no pbm. I just pushed #4317 to tackle the remaining docstring and test issues w.r.t. radius_neighbors. Please feel free to have a look. I will now catch up on your work on isotonic regression. I think it looks good from a first look at it. Will test it a bit more. That might make it possible to remove the random tie breaking in the calibration code. __eou__	User

TP	The behavior is different from what it was before, but I think this is the only way to make fit().transform() and fit_transform() be the same. I'll head t othe office now and look at #4317 once I'm ther __eou__	User

TP	Alright. I will be offline for 1h (commute back home from saclay) but should get back online afterwards. __eou__	User
TP	damn time difference ;)__eou__	Agent

TP	It's also because I have to commute early in the morning and in the afternoon to take a shuttle bus that avoids most of the Paris rush hour traffic :) I would not wake up at 6:30am naturally otherwise... I a good paper to read on hyperparam search on the way back home :) it seems only useful when you can use a big compute cluster for a week though. Still it looks interesting. It means that we might be able to use the future MLP model for hyperparam search instead of trying to fix the GP to make them efficient ;) __eou__	User

TP	the one kyle sent around? Jasper Snoek will give a talk at NYU on Friday :) __eou__	User
TP	yes__eou__	Agent

TP	well spearmint uses the sklearn random forests for hyperparameter search l) so we could also use that ;) __eou__	User

TP	yes but apparently it does not reach the quality of the solution of bayes optimization with GPs or NNs. Nor does TPEs, at least in the benchmarks run in this paper. anyway let's focus back on the release :) __eou__	User

TP	btw, for https://github.com/scikit-learn/scikit-learn/issues/2274 currently the random projections are pretty loud __eou__	User

TP	#4318 __eou__	User

TP	wait, which is the paper you read on the train? not this one, right? http://arxiv.org/pdf/1502.03492v2.pdf I guess this one: http://arxiv.org/pdf/1502.05700.pdf sweet thanks :) __eou__	User

TP	do we have a link to the pdf docs on the website? I think we don't :-/ where should it go? __eou__	User

TP	I read the one by Snoek, but also read the other 2 weeks ago I think. > do we have a link to the pdf docs on the website? I think we don't :pensive: where should it go? no we don't, we don't build it on a regular basis but we could. It should be possible to upload it as part of the build process. I made a mistake: train_idx, validation_idx = train_test_split(np.arange(n_samples), test_size=0.2, random_state=0) missing the np.arange I thought about k-means where predict is possible but this is not always the case, for instance when clustering is done on a precompute distance or similarity matrix I am testing it now the isotonic __eou__	User

TP	after this the latex build should be relatively clean: https://github.com/scikit-learn/scikit-learn/pull/4320 anything you want me to review / work on? __eou__	User

TP	Is there a way to make `GridSearchCV` act without the CV part ( do parameters `cv=None` or `refit` help in this context )? __eou__	User

TP	You can precompute a single test train split: train_idx, validation_idx = train_test_split(n_samples, test_size=0.2, random_state=0) cv = [(train_idx, validation_idx)] GridSearchCV(model, cv=cv).fit(X, y) __eou__	User
TP	Ahh thanks! :)__eou__	Agent

TP	I am using your code in a comment at #4301... __eou__	User

TP	it might be interesting to have the ability to not split into training and test data for unsupervised algorithms. :) __eou__	User

TP	what does setting cv to None do currently? Perhaps we could set cv to -1 for that behaviour? __eou__	User

TP	@amueller unsupervised scores can still overfit __eou__	User

TP	it depends on the context. For clustering, where you can not even evaluate on non-training data, it totally makes sense. I'm looking into the sphinx thing but I'm not entirely clear on how to achieve this. I don't know where to hook it in. Did you have time to look at the isotonic stuff? __eou__	User
TP	indeed__eou__	Agent

TP	@amueller for #4301, do you think the metrics might be useful? __eou__	User
TP	possibly. I didn't have much time to look at them yet__eou__	Agent
TP	this is not really a priority as I want to get stuff clean and fixed for the release first__eou__	Agent
TP	Okay... I'll just leave a comment there... take a look if you happen to find time after the release related work :)__eou__	User

TP	+1 for not working on not introducing any new feature that has not already received a significant amount of reviews. Rephrasing: I meant not working on PR introducing new features unless it has already been reviewed extensively. __eou__	User
TP	Sure :)__eou__	Agent
TP	no, no site change for the beta. But it's good to fix it anyway :)__eou__	User
TP	verbose tests and maybe decide on #4309 because it has a slight API impact.__eou__	User
TP	and off-course the isotonic  and radius PRs that should be ready to merge very soon now.__eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/4313 is part of the kneighbors_graph fix where you can press the green button. I agree that this should be in the beta __eou__	User

TP	haha I merged it before reading your message here ;) __eou__	User
TP	haha I just saw it and thought you read my message ;)__eou__	Agent
TP	anything I should look at now? btw, does beta mean uploading docs?__eou__	Agent
TP	probably not I guess?__eou__	Agent

TP	so the plan is uploading a non-default pipy package and writing a mail? and wheels? Maybe then I should focus on the very verbose tests ^^ As I said above, if you have anything on your wishlist, let me know __eou__	User
TP	yes__eou__	Agent

TP	it's midnight here, I think I will call it a day on my side see you tomorrow. Tomorrow morning I will work to prepare a team meeting on probability calibration and meeting in the afternoon. Should be online after that. __eou__	User

TP	well but for those we need another reviewer, right? have a good night and see you tomorrow __eou__	User
TP	yes we need other reviewers for those.__eou__	Agent
TP	good night__eou__	Agent

TP	looks like you got a lot of reviewing done already, sweet :) care to weight in on the class_weight discussion? __eou__	User

TP	we really need someone else for reviews :-/ __eou__	User

TP	@agramfort are you around? __eou__	User

TP	there is another bugfix here @ogrisel  https://github.com/scikit-learn/scikit-learn/pull/4326 I merged the RBM fix and docs after Kyle +1'ed thanks woah getting late again on your side ;) 10PRs left I think oh, wrong filter. 14PRs __eou__	User

TP	Done. 15 left. 15 PRs left. __eou__	User

TP	the agglomerative clustering one I am not sure about. I don't know what the canonical meaning of alpha is. https://github.com/scikit-learn/scikit-learn/pull/3758 __eou__	User

TP	@amueller for the partial_fit tests PR do you think there is still a lot of testing code that could be reduced/removed? __eou__	User

TP	I'm not really sure __eou__	User

TP	@ogrisel are you there and do you have time to talk about an outline for the webcast? __eou__	User

TP	@amueller  Sorry I was feeling sick yesterday night and I did not go online. __eou__	User

TP	I started thinking about it, we can do: - general intro to ML in Python with scikit-learn - couple of words on the project organization and the team of contributors - highlight of some new stuff in 0.16: improvement on speed or scalability of some methods such as:    - IncrementalPCA    - clustering (I am preparing a notebook comparing DBSCAN, MiniBatchKMeans, KMeans and Birch on 100k blobish 2D points to simulate someone working to extract Point of Interests from raw geo location data (e.g. GSM phone records) - new Approximate Nearest Neighbors search with LSHForest Then for the future we could talk about TSNE and algorithmic improvements under review in the pipeline I forgot, I also wanted to say a couple of words about probability calibration sounds good to me I am half feeling better, really weird some hours I feel sick and then completely fine for a while that's weird I played a bit with clustering on 1e5 points in 2D http://nbviewer.ipython.org/github/ogrisel/notebooks/blob/master/sklearn_demos/Large%20Scale%202D%20Clustering.ipynb DBSCAN is 3x faster than 0.15.2 MB KMeans is still the fastest although it does not deal with outliers as DBSCAN does Birch core set extraction seems to works great but the final affinity clustering pass is not as scalable as the core set extraction If it's ready soon, +1 for a backport to the 0.16.X branch. __eou__	User
TP	ok__eou__	Agent
TP	maybe we can do: beta tomorrow, continue backports of fixes and target 0.16 in 2 or 3 weeks. And if we fix additional important bugs, 0.16.1 in a month and half__eou__	User
TP	I will be in PyCon at that point but you can do the 0.16.1 with the help of others if I am not responsive enough at that point.__eou__	User
TP	nope__eou__	User

TP	Right, calibration is not on my list.  Is it in whatsnew? Oh, and a title Hope you are feeling better today __eou__	User

TP	News from scikit-learn 0.16 and soon-to-be gems for the next release. ?? __eou__	User

TP	cool :) and there are still faster DBSCAN variants in the PRs there is a pretty new PR on that I think __eou__	User

TP	what's on todays agenda for the release? or what was for you ;) no progress on isotonic I see would you mind having a look at https://github.com/scikit-learn/scikit-learn/issues/4324 go for it. It shouldn't interfere with anything. __eou__	User
TP	ok__eou__	Agent
TP	yeah, sounds good.__eou__	User
TP	 we should really release more regularly.__eou__	User
TP	:) sweet__eou__	User
TP	yeah I saw. cool :)__eou__	User
TP	alright. you work soo much later than I do.... Have a good night and see you tomorrow__eou__	User

TP	I guess I start reviewing the non-release PRs now. Or is there anything to review left? Delaying the beta even more seems a bit silly to me, but the only other option is to merge the remaining urgent fixes with fewer reviews Alright. I don't think I'll make it this year. you are not coming to scipy are you? __eou__	User

TP	I am having a look at the DBSCAN. Let's release the beta tomorrow. Whatever the review state of the remaining PRs. we will do backports for the fixes afterwards for the second beta. __eou__	User

TP	you want a second beta? Alright.... __eou__	User
TP	maybe not__eou__	Agent
TP	it depends how stable we find the beta__eou__	Agent
TP	I would like to merge this https://github.com/scikit-learn/scikit-learn/pull/4028 before cutting the beta branch__eou__	Agent
TP	Do you think we should get this one in soon: https://github.com/scikit-learn/scikit-learn/pull/4228__eou__	User

TP	I was planning to attend ICML as a free-styler because is just 1h by train __eou__	User

TP	Hey :) could you help me with #4228 ? I don't understand a few things... When testing you said I could use multilabel data right? but which classifier do I test against? (or if you might have time could you take over?) __eou__	User
TP	Try a tree? Or use OneVsRestClassifer(LogisticRegression).__eou__	Agent
TP	I'll have a look soon.__eou__	Agent
TP	will grid search propagate the parameters from `OneVsRestClassifier` to `LogisticRegression`?__eou__	User
TP	and thanks :)__eou__	User
TP	probably with ``estimator__``. But I'd rather go with a tree, which should work__eou__	Agent

TP	okay I'll push in sometime... pl see if I am in the right direction or kindly take over... sorry for having kept you waiting over that :) A small doubt do you use multiple monitors? ;) __eou__	User

TP	no why? __eou__	User

TP	Just curious how @amueller could be online and commenting at the same time :O I am online sometimes while I work since I use vim in a transparent guake terminal over the gitter chat window... :) __eou__	User
TP	no transparency here. I technically have a laptop monitor and a large one, but in the office I currently only use the large one. Still context-switching too much.__eou__	Agent
TP	I have the gitter tab just always open__eou__	Agent
TP	Ah :D :)__eou__	User

TP	are you working on https://github.com/scikit-learn/scikit-learn/pull/4228 now or should I go for it? I only have like 2 hours of work left today, though ;) __eou__	User

TP	Please go for it :) :P Most ests don't seem to support sparse y... :) __eou__	User

TP	__eou__	User

TP	maybe... but with dense y it works? __eou__	User

TP	Even after changing to `_num_samples(y)` the error seems to persist :| __eou__	User
TP	ok let me look at it tomorrow__eou__	Agent
TP	thanks!! :)__eou__	User

TP	I am working on the polishing of the no-shuffle DBSCAN running the tests and should merge soon. I have not read the reference. Do you? I meant "have you?" not "do you?" Great! Alright, I will call it a day. See you tomorrow! ++ __eou__	User

TP	I'm thinking about the class_weight heuristic __eou__	User

TP	skimmed. But it makes more sense. I'll post a regression test in a second. __eou__	User

TP	@amueller , just so you're aware, when I was messing with how to structure the forests' class_weight implementations, I noticed float-point numerical differences in some little toy test datasets between a couple of different implementations that differed only in where or how i was multiplying, say sample weight and bootstrap weights. I'd expect that the change in #4347 will definitely change the way small floating point differences are evaluated. This obviously has a big effect in trees as magnitude doesn't matter, just gini improvement... So the outputs using class_weight between implementations will likely be different for forests, probably trees. Not that this really matters as they are both entering @ 0.16 Long story short, I think SVMs and Linear Models could potentially change a little bit with #4347, though not as bad as trees I'm guessing. I'm not sure what "promise" is made to users about reproducibility between scikit-learn versions... __eou__	User

TP	Unfortunately we cannot promise much for those cases. It would be interesting to check the actual impact of the correct heuristic on some datasets though. __eou__	User

TP	@amueller @ogrisel could we use `0.5 - np.finfo(float).resolution` for #4295 (With the assumption that, it wouldn't be considered a magic no. since 0.5 is a standard symmetric scaling factor?) to scale it to `(-0.5, 0.5)`? __eou__	User

TP	@ogrisel fair enough. I'll try to find some time this weekend to run some experiments. I agree that the new implementation is more intuitive, just throwing it out there. __eou__	User

TP	Thanks __eou__	User

TP	@amueller shall we try to cut the 0.16.X branch? __eou__	User

TP	@ogrisel feel free. do we want to backport the isotonic stuff then? Or take @mjbommar's +1? @mjbommar's fix is not in a PR... __eou__	User

TP	Let's do the isotonic merge now I think it's tested enought both by the isotonic tests and the probability calibration tests to be correct. __eou__	User
TP	I feel the same way__eou__	Agent
TP	do you want to press the green button or should I do the rebase dance?__eou__	Agent
TP	I pressed the green button.__eou__	User

TP	ok. and merge the other fix on top? It has my +1 and yours __eou__	User
TP	Let's create the PR ourselves from his branch :)__eou__	Agent
TP	Should I create PR or just merge?__eou__	User
TP	I guess we want to run travis once__eou__	User
TP	Yes better create the PR from his branch to get travis to run it.__eou__	Agent
TP	I'm on it__eou__	User
TP	actually it's not possible to create the PR directly from his branch as we need to get rid of the past commits:__eou__	Agent
TP	https://github.com/scikit-learn/scikit-learn/compare/master...mjbommar:issue-4297-infinite-isotonic__eou__	Agent
TP	that are redundant with your own PR__eou__	Agent
TP	I let you do the cherry-pick__eou__	Agent

TP	I did a rebase thing https://github.com/scikit-learn/scikit-learn/pull/4352 __eou__	User

TP	should we wait for feedback on https://github.com/scikit-learn/scikit-learn/pull/4322 and then backport it? warnings in the beta seem not that bad __eou__	User

TP	yes +1 for waiting for this one I have to go now, sorry for leaving you alone. __eou__	User

TP	@amueller unfortunately I won't have time to cut the branch tonight. I have to go now. Feel free to do it, push a commit with the 0.16b1 version number (that follows PEP-440) and push a tag with pointing to it. That should get the CI workers to build the wheels. If everything goes well we should be able to push the release to PyPI (after testing on https://testpypi.python.org/pypi first) using wheelhouse-uploader. I will be busy tomorrow (we organize a deep learning workshop in Paris) and on sunday I should be mostly offline. I can push the release on PyPI on Monday if appveyor works well. travis is slow today... I would like to wait for appveyor and MacPython : https://github.com/MacPython/wiki/wiki/Spinning-wheels wrong copy and paste I meant: https://github.com/MacPython/scikit-learn-wheels I changed the master to follow PEP-440, so it's 0.16.dev0 at the moment It should move to 0.17.dev0 once the 0.16.X branch has been cut. I granted you push rights to https://github.com/MacPython/scikit-learn-wheels __eou__	User

TP	Ok, then I'll do it this afternoon. Mail to mailing list after pypi push? Or wait for appveyor? Are there instructions on the wheel builts in the docs? Ok, wasn't sure if we do that after the beta or the release, but makes sense if we branch. thanks __eou__	User

TP	what version number do you leave on master after doing the beta? Still 0.16-dev? __eou__	User

TP	once the scikit-learn 0.16b1 tag has been pushed to our github repo, we have to update the git submodules of https://github.com/MacPython/scikit-learn-wheels to point to it to get the travis bots of that repo to build all the Mac OSX wheels for that release. __eou__	User
TP	ok__eou__	Agent

TP	merge this on green then branching? https://github.com/scikit-learn/scikit-learn/pull/4352 __eou__	User
TP	yes__eou__	Agent

TP	Andy could you quickly take a look at #4228 to see if the test to be satisfied is correct. I'm using that as an objective for that PR! (I'll iterate different fixes till that test is satisfied...) __eou__	User

TP	I've reopened #4228 as #4354 since #4228 won't reopen (more precisely, github won't track branches against which closed PRs were raised... So #4228 remains at 0+0- even after pushing the new commit to that branch and hence won't reopen as it brings about no change) Sorry for wasting an issue no.! :/ __eou__	User

TP	Sorry, as I'll have to take care of the beta now, so I won't have time to look into the issue __eou__	User
TP	okay no problem :)__eou__	Agent

TP	@ogrisel so if we don't build /upload the docs on beta, what should the links in the navigation of the "documentation" page be? I guess we have to fix them later.... because now there will not be any 0.16 documentation online __eou__	User

TP	pushed the tag, updated the MacPython wheels. __eou__	User

TP	No there won't be any 0.16 website, before the 0.16 final release but I think this is fine. dev points to 0.17.dev0 and stable points to 0.15.2 next time we upload the website we should remove the version for the link to the development documentation number from the documentation menu. __eou__	User

TP	Is there a nice cython guide that I could use? (Related to #4354 for `_tree.pyx` to make it support sparse y) __eou__	User

TP	@amueller I tried to install the macwheels in a new venv and it works: ``` (sklearn-0.16b1)0 [~]$ pip install -f http://wheels.scipy.org/ --pre numpy scipy scikit-learn Downloading/unpacking numpy   http://wheels.scipy.org/ uses an insecure transport scheme (http). Consider using https if wheels.scipy.org has it available   Downloading numpy-1.9.2-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.7MB): 3.7MB downloaded   Storing download in cache at ./.pip/cache/https%3A%2F%2Fpypi.python.org%2Fpackages%2Fcp34%2Fn%2Fnumpy%2Fnumpy-1.9.2-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl Downloading/unpacking scipy   http://wheels.scipy.org/ uses an insecure transport scheme (http). Consider using https if wheels.scipy.org has it available   Downloading scipy-0.15.1-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (19.0MB): 19.0MB downloaded   Storing download in cache at ./.pip/cache/https%3A%2F%2Fpypi.python.org%2Fpackages%2F3.4%2Fs%2Fscipy%2Fscipy-0.15.1-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl Downloading/unpacking scikit-learn   http://wheels.scipy.org/ uses an insecure transport scheme (http). Consider using https if wheels.scipy.org has it available   Downloading scikit_learn-0.16_git-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.8MB): 4.8MB downloaded   Storing download in cache at ./.pip/cache/http%3A%2F%2Fwheels.scipy.org%2Fscikit_learn-0.16_git-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl Installing collected packages: numpy, scipy, scikit-learn Successfully installed numpy scipy scikit-learn Cleaning up... (sklearn-0.16b1)0 [~]$ python -c "import sklearn; print(sklearn.__version__)" 0.16-git ``` However the version number is not good Actually it's my bad, it picked up the wrong wheel (because of the old version  number 0.16-git that is considered more recent than 0.16.b1) We won't have the problem when we upload on pypi as we won't upload that 0.16-git wheel If I fix the version of scikit-learn, it works: ``` (sklearn-0.16b1)0 [~]$ python -c "import sklearn; print(sklearn.__version__)" 0.16-git (sklearn-0.16b1)0 [~]$ pip uninstall -y scikit-learn Uninstalling scikit-learn:   Successfully uninstalled scikit-learn (sklearn-0.16b1)0 [~]$ pip install -f http://wheels.scipy.org scikit-learn==0.16b1 Downloading/unpacking scikit-learn==0.16b1   http://wheels.scipy.org uses an insecure transport scheme (http). Consider using https if wheels.scipy.org has it available   Downloading scikit_learn-0.16b1-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (5.0MB): 5.0MB downloaded   Storing download in cache at ./.pip/cache/http%3A%2F%2Fwheels.scipy.org%2Fscikit_learn-0.16b1-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl Installing collected packages: scikit-learn Successfully installed scikit-learn Cleaning up... (sklearn-0.16b1)0 [~]$ python -c "import sklearn; print(sklearn.__version__)" 0.16b1 ``` I am launching the tests now All tests pass with the 0.16b1 wheel on my OSX laptop On windows on the other hand, the wheels were not uploaded because the tests are broken: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/history They have been broken for a while and I had not noticed yes I see it thanks! pushed let's wait for appveyor to build the wheels __eou__	User

TP	__eou__	User

TP	I created this to track it: #4358 __eou__	User

TP	I pushed a fix for master (hopefully) Apparently you did not push the 0.16.X branch to the repo. I cannot cherry-pick the fix (c23d4f31a3fcd7b24e46d46df6f2d274a839b201) to get it into the 0.16.X branch. __eou__	User

TP	huh, pretty sure I pushed the 0.16.X branch. the tag is in the branch hum ok guess I didn't push it. I pushed with --tags, apparently that only pushes the taggs __eou__	User

TP	so which branch is this commit in? 09dc09a1e9d9088c2cb783c818980f5509d77a11 fixed it, I think __eou__	User

TP	@amueller I fixed #4358 it was a missing `random_state`, do you want me to do the backport? __eou__	User
TP	yeah, go ahead. the branch should be there now.__eou__	Agent

TP	btw, where is the continuous integration on appveyor? I'm sure there is a badge for that so we can see when it breaks __eou__	User
TP	https://ci.appveyor.com/project/sklearn-ci/scikit-learn/__eou__	Agent
TP	Ok, I'll check back tomorrow, not gonna do anything today I think__eou__	User

TP	I have to go offline now. I might get back online tomorrow evening Paris time. If we are both online at that point we can push the release to PyPI together. Otherwise on monday. If the windows wheels are all generated correctly and you feel confident to use the upload-all command of the wheelhouse-uploader tool, feel free to do it before that. ok have a nice WE then :) __eou__	User
TP	you too!__eou__	Agent

TP	I think I would need to login to get the badge url __eou__	User

TP	well we can use shields: https://img.shields.io/appveyor/ci/sklearn-ci/scikit-learn/master.svg __eou__	User

TP	spectral embedding doesn't accept lists... Should that be considered an issue? __eou__	User

TP	the function or the class? __eou__	User

TP	the `fit` method I had meant __eou__	User

TP	are you sure? That should be a common test. It might be that the common test doesn't cover clustering yet. Then this should be fixed. __eou__	User
TP	Yes `SpectralEmbedding().fit([[1, 2]])` doesn't work... Ok I'll fix it and add tests for that...__eou__	Agent

TP	Now that we have appveyor tests, there is an appveyor integration for gitter that we could configure! Oh! I didn't know that :) __eou__	User

TP	we've had appveyor tests for a long time ;) we just didn't look at them __eou__	User

TP	what needs to be done for integration? __eou__	User

TP	Try Settings --> Integrations --> appveyor. Use this webhook https://webhooks.gitter.im/e/0dc8e57cd38105aeb1b4 :) @ragv wonders who added the webhook :O `/me` messages look too large in gitter :/ @ogrisel Seems to have added that from the git blame... I think he would have integrated it already (why aren't we getting notifications then?) Hmm... __eou__	User

TP	BTW there seems to be a chrome app for gitter for linux users - https://chrome.google.com/webstore/detail/gitter/ldhcdmnhbafhckhidlhdbeekpifobpdc __eou__	User
TP	landscape.io does not seem to be very mature yet. but could be cool and carl is pretty responsive__eou__	Agent

TP	hello! yes, seems there are still several kinks to work out in the PR comparison logic working on it! __eou__	User

TP	haha didn't know you were here. Well your help is much appreciated! :) comment whereever you like __eou__	User

TP	@ragv just sent me an invite to the chatroom, probably to stop me cluttering up PR comments :) __eou__	User

TP	@carlio Not at all :) I added you so we could ping and trouble you whenever we hit a snag with landscape :p :P Thanks for the nice work! __eou__	User

TP	@carlio so does a push to a PR trigger a restart? I realize there would be some delay, though on the master branch landscape.io was pretty fast. Coveralls takes like a day to visit a PR if ever __eou__	User

TP	@amueller Yes, Landscape gets a GitHub push hook to trigger the check, so it should start within a few seconds. It's a bit wobbly right now. It also polls every 5 minutes or so (legacy behaviour for before I set up the push hooks to trigger on PRs) __eou__	User

TP	Does this (push to trigger) work for the old PRs too? __eou__	User
TP	kinda, if they get a new pull request than yes__eou__	Agent
TP	er, sorry__eou__	Agent
TP	if they get a new commit__eou__	Agent
TP	Thats awesome! I'll try out ;)__eou__	User

TP	This one didn't get a report for half an hour: https://github.com/scikit-learn/scikit-learn/pull/4381 still pending or something up there? slow travis today :-/ maybe we should ask around if someone wants to pay 250 USD / month for more travis power ^^ __eou__	User

TP	I sort of broke it a bit but it's running now __eou__	User

TP	 __eou__	User

TP	@amueller I think you can manually restart it... In the travis build page there is an option to log in, once logged in you should get the option for restarting it... incase you didn't know that already... __eou__	User

TP	@ragv I know I can restart travis. The problem with travis is not that it is not restarting, the problem is that is is queueing. __eou__	User
TP	Ah... okay! Sorry for the noise!__eou__	Agent

TP	closing issues as non-issues is fun. anyhow, I gotta run. ttyl btw, which time zone are you on @ragv? __eou__	User

TP	See ya! I am in India GMT+05:30 :D __eou__	User

TP	Just throwing in a minor suggestion! We could perhaps have another CI build and test in parallel... like codeship / wercker... Let me know your opinion about that when you get online again!! both seem to be very easy to setup! __eou__	User
TP	(I have nothing to do with this repository but I'm a big fan of circle-ci)__eou__	Agent
TP	Thanks for the suggestion! :)__eou__	User

TP	@ogrisel why b2? any major issues? __eou__	User

TP	no, it's just that if we keep on backporting stuff to the 0.16.X branch, I don't want the artifacts generated by the CI bots to be named 0.16b1 to avoid confusion. __eou__	User
TP	ok__eou__	Agent

TP	but the artifacts are only generated for tags, right? __eou__	User

TP	Our current configuration on appveyor generates them continuously for everything and they get uploaded on to http://windows-wheels.scikit-learn.org . But we never documented that URL publicly until now. It's just useful to help us automate the releases. What I would like to have is all tags get uploaded to http://wheels.scipy.org (both OSX and windows wheels) while development wheels (without tags) get uploaded to a separate container (e.g. http://wheels-dev.scikit-learn.og or something). but never found the time to do so so far. __eou__	User

TP	BTW, I fixed the wheelhouse-uploader bug under Python 2. If you want to try again: ``` pip install -U wheelhouse-uploader cd /path/to/scikit-learn python setup.py fetch_artifacts ``` __eou__	User

TP	today is my meeting day, don't expect much ;) __eou__	User

TP	Hi @amueller, I fixed a bug in @larsmans PR #4157 to optimize DBSCAN. It works great in my experiment. If we merge it quickly, I would like to get it in 0.16. __eou__	User

TP	Fine with me :) __eou__	User

TP	since `svd_flip` is public, do you also want `_deterministic_vector_sign_flip` to be made public too, or I'll leave it as such? __eou__	User

TP	keep it private svd_flip should probably not have been private in the first place. BTW it's probably better to have this kind of specific technical discussions in the comment threads of the issue itself rather than on gitter __eou__	User

TP	Thanks! and yes sure! __eou__	User

TP	we screwed up with the GSOC registration. I didn't remember you had to fill in melange and the form :-/ __eou__	User

TP	Can anyone explain to me how to implement Affinity Propogation in Map/Reduce? http://www.chinacloud.cn/upload/2015-01/15011111364805.pdf I don't really understand the paper and the English is pretty much well... chinese :P __eou__	User

TP	scikit-learn is not a community of mapreduce users so I doubt you will get good feedback on here. You might rather ask on a discussion forum with spark or mahout users. I am not sure that using the hadoop mareduce API is a good thing for iterative machine learning anyway. It's probably better to build upon higher level parallel construct like allreduce or the spark API in general. __eou__	User

TP	The thing is that I have a huge dataset and I don't want to use K-Means since that means I have to guess the optimal number of clusters each time __eou__	User
TP	Also if you still want to ask questions about the paper on a discussion forum, you should ask specific questions, otherwise you probably won't get any interesting answer.__eou__	Agent
TP	I have no idea where to start__eou__	User
TP	How do I adapt the existing algorithm to be parallel?__eou__	User
TP	I'm not much of an ML guy but I'm trying to learn__eou__	User

TP	Use minibatch kmeans on a subset with init_size=int(1e4) and batch_size=int(1e3) __eou__	User

TP	if you are trying to learn, start with smaller datasets (e.g. a random subset) where algo run fast. To learn stuff you will have to fail many times to learn from your mistakes. If each failures take days of cluster programming and execution you will learn slowly :) By working on a subset that fits in memory on your laptop you will learn much faster. __eou__	User
TP	But working on a subset of the data is meaningless__eou__	Agent
TP	I can't even work on 10,000 nodes__eou__	Agent
TP	Also your clustering will fail not because of the algorithm but because of the way you extract features and fail to normalize them, see for instance: https://www.youtube.com/watch?v=TC5cKYBZAeI__eou__	User
TP	> I can't even work on 10,000 nodes  What are "nodes"?__eou__	User
TP	> But working on a subset of the data is meaningless  I am not sure about that. If you are "not much of an ML guy", start with a smaller / simpler problem first.__eou__	User
TP	I asked our ML guy. Working with 10,000 data points is meaningless in my context__eou__	Agent
TP	nodes = data points in the matrix__eou__	Agent
TP	minibatch kmeans can work with millions of high dimensional points, as long as they fit in memory__eou__	User
TP	init_size is just a parameter for the subset used to initialize the centroids__eou__	User
TP	But I still have to optimize the number of clusters__eou__	Agent
TP	you will have to do that anyway, whatever the algorithm__eou__	User
TP	there is always at least one hyperparameter that controls directly or indirectly the number of clusters__eou__	User
TP	As far as I understand AF figures that out__eou__	Agent
TP	it does something by default that might or might not reflect what you expect to be a "good" number of clusters__eou__	User

TP	I'm working with geolocation data (x, y) I think that eucelidian distances is a good choice __eou__	User
TP	but hyperparameters such as "preference" in the scikit-learn implementation of AP will impact the clustering outcome__eou__	Agent
TP	200 milion__eou__	User
TP	x & y are coordinates__eou__	User
TP	GPS degrees__eou__	User

TP	yes +1 for euclidean distance for geo data. How many samples? __eou__	User

TP	If you use scikit-learn master you might want to try DBSCAN on a 1M subset with eps the distance in meters of two points that are close enough to be considered part of a common cluster (assuming x and y are meters as well) the implementation of DBSCAN in sklearn 0.15.2 will be much to slow. > x & y are coordinates  I understand, but which unit? meters, km, miles, GPS degrees? they decide how much GPS degrees should be considered close points. Start with lower values to generate smaller clusters. Birch is probably a good candidate as well if you want to compress your 200M points into a smaller summary dataset. __eou__	User

TP	affinity propagation is not a great method imho. people rarely use it in practice I think, in particular not on such large data dbscan or birch or any other method that selects the number of clusters are much likelier candidates otherwise we keep duplicating fixes __eou__	User

TP	btw @ogrisel if you have the time, it would be awesome if you could work through some of the MRG + 1 PRs. There is a ton of them __eou__	User
TP	But it's probably harder to use correctly.__eou__	Agent
TP	@amueller alright.__eou__	Agent

TP	It's 200M unique points That's why I was looking at map/reduce __eou__	User

TP	``` >>> 200e6 * 2 * 8 / 1e9 3.2 ```  3.2GB of double precision floats => it fits in memory minibatch kmeans and birch can eat it yes AF is not scalable __eou__	User

TP	AF raises MemoryError __eou__	User

TP	Unless you're able to run it on multiple nodes which makes it scalable __eou__	User

TP	I am pretty sure that minibatch kmeans will converge in less than an hour __eou__	User
TP	@omerzimp why are you so set on AF?__eou__	Agent
TP	try on 1M first, look at the results (plot the clusters of on map)__eou__	User
TP	Because I like automatic things :P__eou__	Agent
TP	and sometimes you have "natural" clusters at differrent scales (nested clusters)__eou__	User
TP	in your data__eou__	User
TP	so there is no "true" / "good" number of clusters__eou__	User

TP	it is a lot less automatic than any of the other methods it is really hard to tune __eou__	User
TP	k=10 for KMeans is as automatic as preference=median for AP__eou__	Agent

TP	And I really want to learn to apply ML on M/R __eou__	User
TP	it's not automatic, it's lying to you__eou__	Agent

TP	all clustering algorithms have parameters that influence the number of clusters. some are explicit, like k-means, some are implicit, as in mean-shift, dbscan and birch. __eou__	User
TP	exactly__eou__	Agent
TP	only AF has the most non-intuitive parametrization of the implicit assumptions__eou__	User
TP	yeah I think it is rare that you have a single scale. either there are clusters on multiple levels or none at all ;)__eou__	User

TP	That's why the presented AF M/R is hierarchical __eou__	User

TP	the good number depends on what kind of application you want to use the result of the clustering for __eou__	User
TP	there is also AgglomerativeClustering, which is much faster and easier to understand than AF__eou__	Agent
TP	and provides a hierarchical clustering__eou__	Agent
TP	no, but for 200M points x 2D it's probably overkill to use a cluster__eou__	User
TP	but you can use the spark MLlib clustering implementations if you really want to use a cluster__eou__	User
TP	well ok if your point is to understand how to implement AF on MR then go for that. I just don't see why you would want to do that, as AF is a crappy algorithm.__eou__	Agent
TP	There are a also papers about implementing good clustering algorithms on MR__eou__	Agent

TP	AF also has a hierarchical version but not in sklearn just saying yeh? Because I have a paper on it that explains (somewhat) how to implement it in M/R which is something that I want to be able to understand and implement The number will increase and we have other jobs executing with even more data __eou__	User

TP	but is probably not scalable to 200MB unless you impose neighbors constraints which will be similar to DBSCAN or Birch __eou__	User
TP	I am just asking why you want AF.__eou__	Agent
TP	even if you run it on multiple nodes?__eou__	Agent

TP	do not focus on MR, they are better ways to distribute machine learning on a cluster: spark and the H20 runtime http://mahout.apache.org/ : even mahout is moving away from mapreduce: 25 April 2014 - Goodbye MapReduce  The Mahout community decided to move its codebase onto modern data processing systems that offer a richer programming model and more efficient execution than Hadoop MapReduce. Mahout will therefore reject new MapReduce algorithm implementations from now on. We will however keep our widely used MapReduce algorithms in the codebase and maintain them.  We are building our future implementations on top of a DSL for linear algebraic operations which has been developed over the last months. Programs written in this DSL are automatically optimized and executed in parallel on Apache Spark.  Furthermore, there is an experimental contribution undergoing which aims to integrate the h20 platform into Mahout. __eou__	User

TP	We're using Druid over here And the jobs are written in python. Not java __eou__	User

TP	@amueller I merged @lesteve's PR with doc fixes. I have to catch my bus. Will probably reconnect later tonight. spark and h20 are JVM stuff as well. But it's a detail in practice. What matters is how the memory is used, how the algo scales and can it be efficiently be distributed on a cluster or not. __eou__	User
TP	I know spark__eou__	Agent
TP	I haven't written anything that uses it yet__eou__	Agent

TP	We got disco and druid over here __eou__	User
TP	@ogrisel cool :) there are sooo many reviews to be done....__eou__	Agent
TP	yes__eou__	Agent
TP	There's much better python support there__eou__	User

TP	I'm trying AgglomerativeClustering on 10k points to see what happens in comparance to K-MEANS __eou__	User

TP	@ogrisel Do you know a Python library that is similar to mahout? __eou__	User
TP	btw @ogrisel if we merge one Pr every day, we will still be going at Chrismas (if no-one opens any more).__eou__	Agent
TP	omerzimp there is none. Pyspark is the closest, I'd think. there are also python interfaces for h2o and graphlab/dato__eou__	Agent

TP	I already have Druid (druid.io) and pydruid I'm talking about a batched implementation of ML algos in Python that integrates with tools like spark or druid I can adapt spark to druid __eou__	User

TP	@ogrisel I tried AgglomerativeClustering and I can't find where to fetch the cluster centers from __eou__	User
TP	np.unique(clustering.labels_)__eou__	Agent
TP	I only get the cluster numbers__eou__	User
TP	``` In [34]: np.unique(ac.labels_) Out[34]:  array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,        68, 69]) ```__eou__	User
TP	k means has an attribute that contains the cluster centers__eou__	User
TP	@amueller Can you explain what I'm looking at right now?__eou__	User

TP	`ac.labels_` gives you the cluster membeship of each sample (row) in the matrix `X` with shape `(n_samples, n_features)`, in your case `n_features==2` `AgglomerativeClustering` has no notion of center. The clusters can have non-convex (e.g. folded) shapes. Same for `DBSCAN` (and `Birch` to some extent). (Minibatch) k-means makes the convex clusters assumption, hence the cluster centers are meaningful and computed by default. I agree. __eou__	User

TP	Although whatever the clustering algorithm you can always compute the center of mass of any cluster a posteriori with basic numpy operations: ```python center_of_cluster_42 = X[ac.labels_ == 42].mean(axis=0) ``` assuming `X` is a numpy array. __eou__	User

TP	Btw, AF also has no notion of centers ;) yeah definitely __eou__	User

TP	hurray, thanks for merging the OMP fix @ogrisel. I'm quite confident that it is the right fix, but only time will actually tell ^^ __eou__	User

TP	It's no big deal if it fails though. We can still re-add the travis skip as a temporary fix. __eou__	User

TP	I was confused by all the commits showing up twice in my notifications, but just realized they are for the two branches ^^ Merging this one soon will probably also help: #4370 not sure if it needs two reviews, it is just removing deprecated stuff Just don't backport it ;) btw, did we set a date for 0.16? I guess 0.16.0 __eou__	User

TP	Hey which do you feel is better? `if n is not None` or `if n`? __eou__	User

TP	they are different. If you want to ask "if n is not None" you should do that. the second one will also be false if n is 0 __eou__	User

TP	Ah! explicit is always better as usual :) thanks!! __eou__	User

TP	@ogrisel @amueller could you kindly take a look at this comment - https://github.com/scikit-learn/scikit-learn/pull/4294#issuecomment-83123205 - This stands in the way of completion of the rest of the PR... __eou__	User

TP	No official date for 0.16.0 but maybe we could target next thursday? __eou__	User

TP	fine with me @ragv currently the PR doesn't yet use the new method in the GridSearchCV etc, right? Isn't that the big thing to do for that PR? I agree. lets do next thursday (the 26th) have you fixed cross_val_score? and make sure to keep them backward compatible with custom CV objects people might have written I also commented on you question in the issue __eou__	User

TP	from the 30th of March to April 1st we have a team retreat at work so I won't be able to work on the release. Then there is the oreilly webcast & PyData Paris and it would be great to have it released at that time. __eou__	User

TP	@amueller Oh! I haven't touched `grid_search.py` :/ Thanks! I'll work on that for now :) __eou__	User

TP	Not yet! Thanks a lot!! :) and okay sure! ( Sorry for pestering :p ) __eou__	User

TP	@amueller off to the movie theater to watch citizenfour. Will resume PR reviews tomorrow morning a bit, otherwise Friday. See you! __eou__	User
TP	thanks, that's awesome! have fun!__eou__	Agent
TP	I'll try to figure out the test that crashes on all kind of weird combinations of python and numpy__eou__	Agent

TP	@amueller I was also chanting while standing on one leg, and waving burning sage around during the test passes, not sure if that's relevant. __eou__	User
TP	:feelsgood:__eou__	Agent

TP	thanks for your extensive investigation, this one is .... interesting /play bezos damn, would have been to good if that worked __eou__	User

TP	the selection of sounds supported by campfire is great: http://www.emoji-cheat-sheet.com/ __eou__	User

TP	haha. yeah no problem. i can investigate more, but having two systems that build without failing is nice too __eou__	User
TP	Well if you want, you can try to track down what are the changes in python and numpy that cause this to fail ;)__eou__	Agent
TP	I have to work on organizational stuff for the rest of the day, so I can't track this down much further at the moment__eou__	Agent
TP	i was skimming through python's change log and nothing stood out. will take a more in depth glance at both this evening.__eou__	User

TP	just if you have nothing better to do ;) I'm not sure about how to move forward. It seems strange that it works with the much older version on travis and not with the newer versions. What you could do is create a virtualenv with the travis 2.6 version of python and numpy and see if that also fails for you __eou__	User
TP	ill try to do that on the linux box tonight.__eou__	Agent
TP	thanks__eou__	User

TP	btw @ogrisel is there a reason not to enable OS X with travis? __eou__	User

TP	to enable OSX on travis we would need to adapt our CI script, but it should be doable. __eou__	User

TP	how do I plot AgglomerativeClustering correctly? __eou__	User

TP	@omerzimp are you looking for dendograms? https://github.com/scikit-learn/scikit-learn/pull/3464 or what do you want to plot? __eou__	User

TP	I have a scatter plot I want to show where the clusters are and color them according to the number of data points in a cluster yes __eou__	User

TP	what do you mean by where the clusters are? A common way to plot a clustering is to plot he points and color by cluster membership __eou__	User
TP	That's exactly what I want but I want to rate them in green shades, yellow shades and red shades according to the number of data points in a cluster__eou__	Agent
TP	or so what is the question? how to get the right colors?__eou__	User
TP	I don't know how to get the number of data points in a cluster__eou__	Agent
TP	you can get the number of clusters by np.bincount(agg.labels_)__eou__	User
TP	I need the number of datapoints in a cluster__eou__	Agent
TP	Yep that works__eou__	Agent
TP	why does it work__eou__	Agent

TP	http://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.html __eou__	User

TP	and a label is the cluster number in this case? thanks. It's pretty accurate How come the results are very similar to K-MEANS? Also I still don't understand where the cluster centers are Ok that's good to know How do I check if a value would end up in a cluster? I have predict in K-MEANS but not in agglomerative clustering __eou__	User

TP	there are no cluster centers in agglomerative clustering __eou__	User

TP	@amueller Is there a parallel version of agglomerative clustering? I see that K-MEANS has one that uses joblib __eou__	User

TP	the k-means uses joblib just to parallelize the restarts. there is no parallel version of agglomerative clustering, but it should be pretty fast the way the clustering is constructed there is not really a way to predict in agglomerative clustering, but you could assign clusters to new points using nearest neighbors for example this seems to be somewhat tricky... which version? they talk about the locally oriented version which is described in a different paper yes but they don't say how __eou__	User

TP	I've seen papers about parallelizing them @amueller See http://www.cs.cornell.edu/~kb/publications/irt08.pdf Clause 3.3 but doable right? Both are parallelizable according to 3.3 You have to look at other papers __eou__	User
TP	I could but I have other things to do. and without looking at it, how should I say if it is doable?__eou__	Agent

TP	@amueller ... you around? should i also update `y_numeric` for https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/kernel_ridge.py#L144 and https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/stochastic_gradient.py#L868 ? Both are explicitly regressors. there's a couple others that use fit between classifiers and regressors too... gbm, adaboost, bagging... Not sure I want to touch them for 0.16 though, would be more involved than a +1/-1 change presumably all of the above get through tests somehow though... __eou__	User

TP	nm. i copied these questions to the PR. you can comment there if you like, but i think those do not call the offending np function __eou__	User

TP	if the tests pass, I don't think we need to do anything __eou__	User

TP	thanks for all the merges @ogrisel :) as a thank you, I pinged you in 4 other issues ^^ __eou__	User

TP	K. Well #4422 should be good then. It passes all tests locally on 1.8.0 __eou__	User
TP	thanks, looks good and I marked for backport__eou__	Agent

TP	that will save us some eror reports on the new release! __eou__	User
TP	excellent__eou__	Agent

TP	Hey :) @ogrisel @amueller Could I take up "Multiple metric support for cross-validation and gridsearches" as my GSoC project...? Sorry for the delay in my proposal... Final semester takes up a lot of my time :| I chose that since so far no one has proposed for the same and also I am working on a related PR... __eou__	User

TP	yes, that would be good and could also tie into finishing up the cross-validation stuff you are working on btw does anyone remember a PR adding more git rebase docs to the developer documentation? I though someone did it but I can't find it any more docs for the contributors on how to do git magic, like rebasing, not merging in master, and squashing we were talking about that somewhere (ML? issues?) and I though someone made PR. but maybe that was just wishful thinking apparently today is minor doc fix day. third push to master in a row. trevorstephens: psource should give you the source in ipython __eou__	User

TP	"git rebase docs" you mean contributor docs?? __eou__	User

TP	@ogrisel in a recent PR you posted "only 202 PRs to go". Now it's 227. GSoC I guess ;) And that even with pretty aggressive merging... __eou__	User

TP	(shamefully) Thats me :p Joel had posted an issue asking for a better contributor docs... I am still doing that :| btw I did update wiki with a neat tree outlining the structure of our improved contributors guide... https://github.com/scikit-learn/scikit-learn/wiki/Contributors-Guide __eou__	User

TP	Something I spend way too much time doing when writing new stuff, or figuring out what's happening in an existing method (and perhaps there's already an easy way that this dum dum doesn't know)... But would adding the actual source code file in which a given developer func is located to the sklearn.utils.x functions' docstrings be helpful? (only for non-public functions in utils I think) __eou__	User

TP	that is magical... thx! but doesnt seem to tell the file it's in. :-/ __eou__	User

TP	You could also do a `git grep "def function_name"` right? Thats the easiest I think! :) __eou__	User

TP	for the file you can use function.__file__ that is ``function.__file__`` __eou__	User

TP	__file__ didnt work, but func.__code__ seems to get what i want. thanks! that is `func.__code__` __eou__	User

TP	it might be that __file__ only works for modules, not functions psource should give you the same as file err code __eou__	User

TP	Given that there is a huge interest among students in learning about ML, do you think it would be within the scope of/beneficial to skl to have all the exercises and/or concepts, from a good quality book (ESL / PRML / Murphy) or an academic course like NG's CS229 (not the less rigorous coursera version), implemented using sklearn? Or perhaps we could instead enhance our tutorials and examples, to be a self study guide to learn about ML? I was planning to include this in my GSoC proposal... does it seem like an useful idea? (Also is this a correct place to discuss this or the Mailing list might be better for such questions?) Or would it be better if I simply add more examples? __eou__	User

TP	Yes please! I really want to learn ML and it's pretty hard as it is An extensive tutorial would be lovely. __eou__	User

TP	Thanks for the response!! Lets wait for a confirmation from the core devs, since any additional code does come with its own maintenance cost... They would be the best at judging if that additional maintenance is worth the benefit it offers :) __eou__	User

TP	> Also is this a correct place to discuss this or the Mailing list might be better for such questions?  NM! I've copied this to the ML, so as to reach a wider audience :) Please let me know your views there! __eou__	User

TP	@ragv +1 for a GSoC on cross-validation improvements. I can volunteer to be a mentor on that one. __eou__	User

TP	Thanks!! :D would you be able to take a look at my proposal at https://github.com/scikit-learn/scikit-learn/wiki/GSoC-2015-Proposal:-Multiple-metric-support-for-CV-and-grid_search-and-other-general-improvements and let me know your views? __eou__	User
TP	will do.__eou__	Agent
TP	Thanks! :)__eou__	User

TP	@ragv I don't know about NG's course, but for the books, sklearn is useless for the exercises, right? They are mostly mathematical proofs. How would sklearn help with that? __eou__	User

TP	@ragv I think we should focus your GSoC on improving cross-val, writing a generic tutorial to ML is interesting but too long to do in addition to a GSoC for the cross-val, we could have the data-independent refactoring and then for the second part of the GSoC experiment with high level helper API to do out-of-core cross-validation for models that support partial_fit I have personally not started to thing about what such an API would look like but I think this is an important use case multiple metrics is interesting as well __eou__	User

TP	I thought we wanted to do multiple metrics? __eou__	User

TP	btw have you looked at any of the gsoc proposals yet? __eou__	User
TP	but https://github.com/scikit-learn/scikit-learn/pull/4294 should definitely be part of the GSoC__eou__	Agent
TP	just https://github.com/scikit-learn/scikit-learn/wiki/GSoC-2015-Proposal:-Multiple-metric-support-for-CV-and-grid_search-and-other-general-improvements__eou__	Agent

TP	I guess I need to review them all... __eou__	User

TP	btw, @ogrisel I thought about doing the split of the ``utils`` module into the private and public modules, that people have been talking about for ever. What do you think about that? __eou__	User

TP	Hey all, I have noticed [here](http://sourceforge.net/p/scikit-learn/mailman/message/24403926/) and [here](http://mail.scipy.org/pipermail/scipy-dev/2010-January/013709.html) that there was once a genetic algorithm module in scikits.learn, it appears to have been removed mostly due to code rot, maybe API differences too, but does anyone know if there is an underlying general issue with genetic algorithms that are not scikit-learn friendly? For context, I am writing a symbolic regression class that constrains genetic programming's flexibility to the main use cases found in scikit-learn (regression, classification, transformation) and sticks with the existing scikit-learn API style. So just wanted to check in to ensure I'm not going off the deep end as I'm nearing a functional regressor already... Though still a fair ways from a PR :smile: __eou__	User

TP	@ogrisel green button on https://github.com/scikit-learn/scikit-learn/pull/4432/files to avoid merge conflicts? __eou__	User

TP	@trevorstephens genetic algorithms are probably going off the mariana trench. what is your application of symbolic regression? __eou__	User

TP	haha. what do you mean by application? where would it be used? __eou__	User

TP	its just a regressor, except that the final result is expressed as a non-linear equation. could also be used for automated feature extraction in a supervised transformer __eou__	User

TP	basic idea is to take an initial sample of random equations and apply genetic operations such as mutation, reproduction, etc to the fittest individuals in a population. the equations are expressed like LISP trees... well im just using python lists, but similar structure __eou__	User

TP	@amueller about the split of utils I have no strong opinions. I am +1 on introducing new private utils with a `_` prefix. __eou__	User
TP	Ok you are just building a regressor. By application I mean: does this ever work better than a random forest? Also, is that what people currently use in symbolic regression? And does that work better than greedy expansions of variables in a linear model?__eou__	Agent
TP	@ogrisel would you still have them in the ``.utils`` module? I thought about introducing ``._utils`` and just moving (backward-compatible) all the non-public API there?__eou__	Agent
TP	but maybe I should rather spend my time on reviewing the LDA and GP PRs  or finishing the MLP.__eou__	Agent
TP	Btw, for the MLP, I'm not 100% certain on how to do nesterovs momentum. I find the paper was not very clearly written, but maybe I was just tired__eou__	Agent
TP	https://github.com/scikit-learn/scikit-learn/pull/4436 just passed__eou__	Agent

TP	ive seen people using symbolic regression in combination with stacking on kaggle (the higgs boson comp was memorable) to extract new features that helped their gbm's latch onto some very interesting segments of the feature space. there is a lot of research into competitive results though i dont know if they have been compared to RFs "Also, is that what people currently use in symbolic regression?", do you mean LISP trees? yes, it is very common. basically a flattened tree representation __eou__	User

TP	@amueller about the MLP PR, I have implemented an adaptative schedule for the learning rate here: https://github.com/ogrisel/scikit-learn/commit/3c4cc13b39fdb6a91be44a0977766e16d45ed5dc . It seems to be very useful in practice, although Ilya Sutskever recommends to use a validation set instead of the training score. Using a validation set is complicated from an API point of view though. Nesterov momentum is not that complicated. Have you read:  http://www.cs.toronto.edu/~fritz/absps/momentum.pdf ? it's mostly the ordering of the update. @amueller this experimental notebook might help http://nbviewer.ipython.org/github/ogrisel/notebooks/blob/master/Gradient.ipynb __eou__	User

TP	also I am currently experimenting with a from scratch implementation in theano to have a comparision point (and more NN experience): http://nbviewer.ipython.org/github/ogrisel/notebooks/blob/master/representations/MNIST%20experiments.ipynb about the utils stuff it does not seem to be particularly high priority to me but feel free to work on that if you find it important yourself. but it's utility would have to be proven first. So better implement that in a dedicated python package first. I have to go now, I will review the other sections and give you finer feedback tomorrow __eou__	User

TP	I read the momentum pdf. but I'll have a look at your notebook. I feel we should merge this asap as it has been lying around for so long. adding monitoring etc can always be done afterwards and the use of a validation set would be great but is a whole other can of worms @trevorstephens sorry for being ambiguous. I meant are people using genetic programming for symbolic regression. __eou__	User

TP	I am not sure we should merge that early. The optimizer is clearly suboptimal, the one in my branch seems to be much more robust but changes the meaning of the 'constant' hyperparameter. I think it's very important to have a robust hyperparameter by default. But I want to focus on the 0.16 release first. @trevorstephens encourage you to implement that as a third party project first. If it proves empirically useful then we might consider it for inclusion in the future. But Genetic Programming stuff is culturally very different from the sklearn spirit. Especially we do not want to have a generic Evoluationary Algorithm module as part of sklearn as the API would not be suited for it. A black box Genetic programming solver hidden in a SymbolicRegressor estimator might be interesting though. __eou__	User

TP	@ogrisel is there much to do for 0.16? @ogrisel is there anything to be done for the EML branch? It has no +1 yet. this one would be nice: https://github.com/scikit-learn/scikit-learn/pull/4350 __eou__	User

TP	@amueller @ogrisel Thanks for the reviews!! Based on your comments I'll replace that section of my prop. with a more significant work > They are mostly mathematical proofs. How would sklearn help with that?  I was thinking more on the lines of http://www-bcf.usc.edu/~gareth/ISL/code.html.. But like you said for the practical part of it, we do have some nice books! Perhaps out of GSoC, it would  be better if I simply cleaned up existing examples and added a few more perhaps! __eou__	User

TP	@amueller , yes, it is the 'classic' use of GP. __eou__	User

TP	@ogrisel . "A black box Genetic programming solver hidden in a SymbolicRegressor estimator might be interesting though." Yep. That's exactly what I'm going after. Though understand if you don't want it in the package yet, makes perfect sense. FWIW, here's the (did I mention super-early) code: https://github.com/trevorstephens/scikit-learn/blob/genetic_programming/sklearn/genetic.py __eou__	User

TP	running is as simple as `gp = SymbolicRegressor(random_state=415)`  `gp.fit(diabetes.data, diabetes.target)`  `print gp.program_` __eou__	User

TP	@amueller I am working on pinpointing numpy version issues for #3747 at the moment ho you merged it already. Did you do the backport as well ? I am not sure about the 1.9 version comparison in the test I am testing with numpy 1.8.0 Ok the tests pass with 1.8 as well :) @trevorstephens would be great to extract that as a new library. You can write some sklearn compat tests with stuff in`sklearn.utils.estimator_checks`. you can make it easy to install by writing a simple setup.py. As you don't need compiled extensions, you don't need the complicated numpy.distutils, you can just use the regular distutils or setuptools. you could name your project symbolicregressor or something symregressor __eou__	User

TP	billie-gene has a nice ring to it :smile: __eou__	User

TP	@ragv I would move generic tutorial out of the scope of your GSoC but you can expand the documentation item of your GSoC proposal to include a tutorial on how to do proper cross-validation: how to select the CV strategy for a given problem, how to use stratification, how to check that the i.i.d. assumption is not violated... __eou__	User

TP	thanks for the advice as well @ogrisel __eou__	User

TP	@ogrisel Thanks for the suggestion! On it :) Do the other sections look okay...? Okay! bye :) __eou__	User

TP	@ogrisel the thing about the mlp is, that as long as it is a PR, we will not get feedback about it from anyone. But once it is merged, people will use it, and people will sent PRs. I don't think we need to "nail it" before it is merged. The naive end-user will only use it after the release __eou__	User

TP	That seems like a good idea... More after merging to master, it becomes easier for anyone to contribute to it by a normal PR rather than a PR to a PR... (for whatever my comment is worth) +1 from me for that! :) also, if it is not considered convoluted, we could raise a `BetaModule` warning like `DeprecationWarning` to warn users not to use it in production code? __eou__	User

TP	well, if it is the dev version, people should know __eou__	User

TP	I have the common tests on OneClassSVM (I think) hanging randomly on my box. no good. __eou__	User

TP	@amueller Should #1674 be closed in favor of #1626, since @mblondel has been positive about the same in his last comment? __eou__	User

TP	which comment? __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/issues/1674#issuecomment-13982364 __eou__	User
TP	well he said he understood my idea__eou__	Agent
TP	not that it solves his issue / replaces his__eou__	Agent
TP	I should reread my proposal__eou__	Agent
TP	It has been two years (sic!)__eou__	Agent

TP	Well.. I thought of including it in my proposal ;) I feel this is a tough one and will take easily around 1.5 months? (Is that adequate amount of time that could be allocated for the same?) __eou__	User
TP	I would have to re-read but it tries to address some of the points that I mentioned in my email__eou__	Agent
TP	Yea :) please take your time... I'll just put it tentatively as 1.5 months for now... but you'd know better, given my previous work and my 8hr a day schedule... :)__eou__	User
TP	:)__eou__	Agent

TP	the main question would be how to pass it the validation set so to make it pipeline compatible and GridSearchCV compatible __eou__	User

TP	any GSOC applicant looking for stuff to do can check: https://github.com/scikit-learn/scikit-learn/issues/4442#issuecomment-85683185 (ragv you have enough contribs and don't need to worry about it ;) not that help would not be appreciated, but I think you have enough stuff to do __eou__	User

TP	LOL okay :D I am a bit busy this month due to my academics moreover, thats why I am not able to spend enough time on scikit-learn  :cry: Sure! I haven't yet... I was fine tuning it a bit.. sorry :) __eou__	User
TP	I think it's better to put it on melange early so that we can comment on it and read each others comments__eou__	Agent
TP	you can still edit it afterwards__eou__	Agent
TP	I am doing that now... sorry for the delay :)__eou__	User

TP	@ragv have you put you proposal on melange? I cannot find it if so please edit the title to add "scikit-learn" in it __eou__	User

TP	@ogrisel Done! but the formatting is awefully bad... A little more description is needed too... I'll fix both ASAP :) Is "path algorithms" a good name to mean all cv related objects? __eou__	User
TP	no__eou__	Agent
TP	Does this seem okay? - "scikit-learn: Enhance cross validation and related modules."__eou__	User

TP	wait, depends on what you mean by cv related objects It applies to LinearModelCV objects __eou__	User

TP	cross_validation / learning_curve / model_selection etc... oh.. sorry then I'll call them CV objects simply :) __eou__	User
TP	anyhow, I commented on melange that I think it is not a good name__eou__	Agent
TP	I would just call it model selection and evaluation maybe__eou__	Agent
TP	Thanks :D__eou__	User
TP	Okay :)__eou__	User

TP	I think you will have a great proposal. I haven't read your most recent version, I just wanted to reiterate that you should try to focus, in particular provide concrete deliverables sound more appropriate than the current title. sorry if my feedback today is a bit short, I am feeling sick and wanted to at least briefly comment on all submission s __eou__	User

TP	Not an issue at all! Feel free to comment whenever you feel like :) And more over I got the basic idea and hence can work out my proposal according to your guidelines :) Also take care :) Sorry to trouble you at the last min... HI5 :p __eou__	User

TP	haha well we just started to review now so it is our fault ;) __eou__	User

TP	+1 on the new title and focus __eou__	User

TP	I at least gave some cursory comments on all proposals. I think all the mentors should review at least one that they are interested in. @ogrisel do you want to put your name on one or more proposals on the list (more than one meaning one of these will be selected) __eou__	User

TP	@amueller you mean in the google doc or on melange? __eou__	User
TP	google doc__eou__	Agent
TP	@ogrisel do you think this is a good idea: https://github.com/scikit-learn/scikit-learn/issues/4445 we could ask cangermueller to do it, he didn't have much opportunity to prove his coding skills yet, I think__eou__	Agent
TP	heading over to facebook for free drinks now ^^__eou__	Agent
TP	ttyl__eou__	Agent

TP	cheers! __eou__	User

TP	ok I did a pass on the GSoC proposals. I need to go to sleep now. Tomorrow will be very busy with meetings and meetups. I will probably be back online on Thursday morning to resume work on GSoC reviews and sklearn 0.16 release. __eou__	User

TP	I might cherry-pick some more things today, I have to check __eou__	User

TP	@amueller do you think we should probably fine tune landscape settings to avoid reporting not-so-useful fixes that are probably ignored... Thing is without it raising red flags only for important issues that a core dev would normally frown upon (like PEP8, imports, the docstring format), I am afraid this will eventually be ignored like the coveralls bot. :/ WDYT? __eou__	User

TP	the main problem is that it reports new problems that are not related to the code at all. __eou__	User
TP	Ah... We'll then just wait for it to get better ;)__eou__	Agent

TP	maybe ;) __eou__	User

TP	@ogrisel do you think there is a chance @GaelVaroquaux will review and / or put his name next to some of the proposals? It would be nice to have tentative mentors. __eou__	User

TP	@ogrisel I backported some doc fixes and the faster polynomial features __eou__	User

TP	Will I be able to edit my prop after 27th? :p __eou__	User

TP	I don't think so. the mentors might but you should not count on it ;) vim ;) I should probably switch at some point __eou__	User

TP	Okay! Would you be able to take a look at the revised prop and leave your comments? :) __eou__	User
TP	tomorrow. It's 21h in NYC__eou__	Agent
TP	Oh!! Sorry :P how are you today btw?__eou__	User

TP	melange's formatting is very bad :neutral_face:  The wiki pages look neat compared to melage ... __eou__	User

TP	You could copy paste, I think __eou__	User

TP	still sick, meh __eou__	User

TP	Thats bad :/ tc! Hey @xuewei4d :) I tried but its still very bad :( This site was useful for me tho :D - http://markable.in/editor/ I edited entirely in md and copy pasted the html code into melange :P haha... subl seems to be better in so many ways... hmm I guess time to move away from vim... __eou__	User
TP	noooo don't do it ;)__eou__	Agent

TP	GSOC 2016: melange This is what I did. I use sublime to preview markdown, then copy paste html page __eou__	User

TP	Thanks atleast I got one person saying that :O Every other person I meet recommends subl or pycharm... I love vim but hate them ranting about their shiny editors :p What do you use? :) __eou__	User

TP	"i've been using vim for three years, but that's only because i cant figure out how to exit" __eou__	User

TP	@trevorstephens you released gplearn? nice! __eou__	User

TP	you should create an `examples/` folder with a sample script on some regression dataset from scikit-learn (e.g. load_boston) and another on a synthetic data where you know the symbolic ground truth __eou__	User

TP	@amueller I am checking the documentation of the 0.16.X branch. It looks good. Will run trough the release checklist next. let me know if you find any release blocker in the mean time. Thanks ok, thanks about the version number, shall we use 0.16.0 or 0.16? Last time we did 0.15.0. But I used 0.16b1 instead of 0.16.0b1 I think we should do 0.16.0 for lexicographical sorting of folders and stuff like that @amueller shall we do the `git log <last_release>.. | git shortlog -s -n` report for this release, or not? as you said earlier it, this vanity contest might encourage contributors not to squash their PR by default (and it does not reflect review work). __eou__	User

TP	I think that everyone in this room should read http://www.amazon.com/Scaling-Machine-Learning-Distributed-Approaches/dp/0521192242 This is f***ing awesome! __eou__	User

TP	np __eou__	User

TP	I didn't know about it. But it seems a bit dated (2011) @ogrisel no blockers that I know of. Anything you want me to work on? scheduling the release for the day before the GSoC deadline was a genious move on our part ^^ this needs backporting I think: https://github.com/scikit-learn/scikit-learn/pull/4448 __eou__	User

TP	@amueller I'm sure there is a new edition somewhere __eou__	User
TP	@omerzimp I wouldn't expect that.__eou__	Agent

TP	Hi @ogrisel . Just reserved the name on Pypi and am getting the hang of all the code review tools last few days. Still a long way to go before I'm satisfied enough to release it. Definitely need to add a lot of tests (coverage at 4% haha), documentation, examples, etc etc. But it'll get there I'd plan on some practical examples like boston, and then another like x^3 + x^2 + x + 1 or something, maybe in two dims Maybe I'll try to write some docs for sklearn on the process of setting up your own <plugin>learn or something So long as it remains stable. A lot of thought right now going into how I'll support various versions of sklearn. I may need to grab a copy of a lot of the tests and utils to keep from being harassed by bug reports Since those don't go through deprication cycles so far as I know But yeah, that looks like a good start @ragv __eou__	User

TP	First step would be to build at a nice testing framework (#3810) I feel :) __eou__	User

TP	@ogrisel I just did the backport __eou__	User

TP	I'm -0. We could do alphabetical order? Is 0.16.0 according to the pep thing? I think 0.16.0b1 was not, so we didn't do it. I think 0.16.0 is good The whatsnew already mentioned the contributors next to the changes. Maybe that's enough vanity? That encourages cool things ;) FUCK again broke the SVC trying to rebase on master. That is really finnicky __eou__	User

TP	@ogrisel can you do me a favor and physically force @GaelVaroquaux to review #4189 so we can merge it? I only pick him because no-one else is in physical proximity ;) why is fabian so high up in the commits? oh I think my git log was wrong __eou__	User

TP	>  physically force @GaelVaroquaux to review #4189  Haha :laughing: __eou__	User

TP	@ogrisel any parts of the release you want me to do? update the links in the website? built the website? damn I though #4326 was in __eou__	User

TP	all 0.16b1 0.16.0b1 0.16 0.16.0 are valid according to pep 440 +1 for not including the stats I had to leave the office early (going to a meetup to try and hire graduating students to work on sklearn next year) ... ok for 0.16.0 then Let's merge #4189: it's mostly a fix, no new API, no hyperparams, no documentation or example to update. It should not be controversial. git log 0.15.2..0.16.X | git shortlog -s -n if you want the stats @amueller I let you do the cherry-pick for https://github.com/scikit-learn/scikit-learn/pull/4189/commits into 0.16.X to fix your ranking for 0.16.0 commit number :) argl actually the cherry-pick of #4189 is not trivial. We probably also need to bacport #4326 too but this one is not trivial either... I have to go to the meetup soon, I won't have time to do the #4189 backport today. __eou__	User
TP	I'll do it, I can release afterwards.__eou__	Agent
TP	@ragv let me do another pass on your proposal__eou__	User

TP	+1 0.16.0 __eou__	User

TP	@ogrisel @amueller Any final comments on my revised prop? I made it a bit ambitious but I think it is doable with full time involvement. WDYT? __eou__	User
TP	not there? let me do it. Then I will change the version number and start fixing the links in the doc.__eou__	Agent
TP	@ogrisel I'm back__eou__	Agent
TP	just out for lunch__eou__	Agent
TP	@ragv I'll do it in a bit__eou__	Agent

TP	Thanks... sorry for hijacking into your 0.16 discussions... :) __eou__	User

TP	no pbm, it's our fault to have scheduled to do the release on the same week as the GSoC deadline maybe it is in then I had conflicts that looked non trivial when picking #4189 so it did a git reset --hard on my sandbox __eou__	User

TP	#4326 was before branching even __eou__	User

TP	@ogrisel it was the docstring stuff by @ragv that caused the cherry-pick error. I backported it pushed to 0.16.X should be fine now __eou__	User

TP	Yayy I got 50 commits :P __eou__	User
TP	@ogrisel what is the status now? do you want to do it or should I? Also, should we have a "highlights" section in whatsnew?__eou__	Agent
TP	maybe not__eou__	Agent
TP	next we should push the 0.16.0 version, trigger the buildbot, and move the docs, right?__eou__	Agent
TP	are you working on any of these?__eou__	Agent

TP	@ragv I would move the out-of-core cross-val as a low priority optional in case everything else is implemented first. I doubt we will have the time to implement everything else but one never knows :) @amueller would be great to have a highlight section. __eou__	User

TP	hum, https://github.com/scikit-learn/scikit-learn/pull/4420 would have been nice, but without a review by a PLS person I'm doubtful __eou__	User
TP	if you feel like it__eou__	Agent
TP	ok__eou__	User
TP	@ogrisel I'll make a quick PR so you can review__eou__	User

TP	 __eou__	User

TP	other than that, version change to 0.16.0, update the links in the menus for the doc, put the 0.16.0 tag and push it, update the submodule in the MacPython repo to get travis build wheels for OSX, wait for appveyor to do the same for windows. Then let me know, I will manually test install on my mac and a windows VM __eou__	User

TP	@ogrisel Okay! Thanks for the review.... :) Should I perhaps concentrate on gen cv and early stopping for the month of July? Except for the month of July, I believe I have framed a satisfactory (to myself :p) timeline. It is the month of July I am not quite sure, what I will get to work on that could be useful for sk and also be feasible for me... @ogrisel thanks :D __eou__	User
TP	thanks__eou__	Agent

TP	my presentation will start soon, then there will be beers so I won't be of any use tonight. but if appveyor and macpython / travis are all green, I can do the manual tests tomorrow morning and build the doc to rsync the new version. @ragv don't worry too much, I think there is a lot of testing /  benchmarking / polishing / documentation cleaning to do __eou__	User
TP	can you have a brief look at the commit https://github.com/amueller/scikit-learn/commit/dbe3cd33145677cdae9bdbcbef0c3791bf94bca3 @ogrisel ?__eou__	Agent
TP	@ogrisel Thanks I'll just remove ooc... pl let me know if you feel I should do any more modifications to my prop :)__eou__	Agent
TP	@amueller maybe put a word on the input validation stuff: "Improved and more consistent error messages when fitting estimators on invalid input data."__eou__	User
TP	LSH is Locality-sensitive hashing__eou__	User
TP	not local-sensitive hashing.__eou__	User
TP	Maybe we should explicitly mention DBSCAN for the speed improvements__eou__	User
TP	"speed improvements, notably DBSCAN, ..."__eou__	User
TP	or maybe the phrasing is bad__eou__	User
TP	0.15__eou__	User
TP	but it has been improved: the utils were there in 0.15 but they were not consistently used throughout the code base.__eou__	User

TP	https://github.com/amueller/scikit-learn/commit/dbe3cd33145677cdae9bdbcbef0c3791bf94bca3 ok __eou__	User

TP	Maybe reword as "Scalable approximate nearest neighbors search with Locality-sensitive hashing forests" the auto-tuning part could still be improved :) __eou__	User

TP	input validation was already mostly in 0.14, right? that's what I ment yeah true __eou__	User

TP	remember the work you did in test_common over the past 6 months :P ? __eou__	User
TP	- Improved error messages and better validation when using malformed input data.__eou__	Agent
TP	great phrasing :+1:__eou__	User

TP	what I did when in test_common is mostly a blur ;) I remember touching every single file during the last sprint when removing check_arrays lol __eou__	User
TP	:)__eou__	Agent

TP	DBSCAN mentioning jay or nay? maybe also "Many speed improvements, reduced memory requirements, bug-fixes and better default settings." __eou__	User
TP	as you wish.__eou__	Agent
TP	I think many users are interested in DBSCAN__eou__	Agent
TP	@ragv I think your proposal is fine__eou__	Agent
TP	https://github.com/amueller/scikit-learn/commit/4aaaaae3ecdea54f5bd6d073a712efc389668529__eou__	User
TP	Looks great to me__eou__	Agent

TP	Don't forget to cherry-pick the paragraph both in master and 0.16. __eou__	User
TP	ok. I'll push it, do the docs stuff and push the docs. I think we can do this now.__eou__	Agent
TP	whatsnew changes will go to both branches__eou__	Agent

TP	@ogrisel have you looked at Vinayak's proposal by any chance? __eou__	User

TP	Hi, I am wondering how many slots does scikit-learn have this year, if it is not a top secret? I remember the number is 4 in 2014. __eou__	User

TP	https://wiki.python.org/moin/SummerOfCode/FrequentlyAskedQuestions#How_many_slots_does_python_get.3F__How_many_does_project_.24x_get.3F we currently have 5 mentors registered (if no one else registered) which means we would get a max of 5 slots __eou__	User

TP	OK. Thanks! Hope we have 5. __eou__	User

TP	Hello everyone! Just got an invitation to join the chat here. Any final bit of advice on my proposal? __eou__	User
TP	@amueller__eou__	Agent
TP	^__eou__	Agent
TP	Sorry wrong key presses :worried:__eou__	Agent

TP	? __eou__	User

TP	hi @bryandeng I have to look at your proposal again. Sorry, we are a bit busy today, we are also doing a perfectly timed release today The work-load seemed a bit light. The proposal on the idea page was not super fleshed out, unfortunately Maybe you can give a bit more background on the improvements for the existing algorithms. currently it is only half a sentence. Do you have any other ideas in the semi-supervised scope? __eou__	User
TP	Got it.__eou__	Agent

TP	I think I can implement more than one semi-supervised algorithms, candidates of which can be discussed in the community bonding period. __eou__	User

TP	I know it is pretty late now, but it would be great if you could propose some. In the other semi-supervised proposal I mentioned transductive SVMs. I'm not entirely sure they are the best way to go, but at least they are a common benchmark. The more your proposal shows you are familiar with the material and already looked into it, the better __eou__	User

TP	OK. My original intention to write like this is that I want to serve the needs of the community to the greatest extent. Since what algorithms we most want in sklearn.semi_supervised hasn't been fully discussed in the community, I decided not to show personal preferences. Now I'll pick some algorithms according to my understandings. __eou__	User

TP	I understand your motivation, and it would have been better if we could have had a broader discussion previously on the mailing list. Unfortunately, mostly because of lack of time of the developers, that didn't happen. For your proposal to be strong, you do need to say more than you currently say, though. __eou__	User

TP	@ogrisel right, I forgot to fix the menu. Is there a good way to upload just the html so you don't have to optimize the pngs again? __eou__	User

TP	I was about to find a way to not optimize the png again :) I tested on OSX, now on testing on windows and the sdist from linux in a docker container in // __eou__	User

TP	Well I can rebuild the docs in the folder with the optimized png once I get into the office if you want to wait for an hour __eou__	User

TP	I already fetched them local with rsync actually not locally, to some rackspace VM but this is the same __eou__	User
TP	good__eou__	Agent
TP	I just decided to stay at home, I'm too sick__eou__	Agent

TP	do you need help with anything? Otherwise I might turn around and sleep on __eou__	User
TP	I think I can do it all.__eou__	Agent

TP	let me know when you tweet so I can retweet ;) __eou__	User

TP	I have pbm testing under windows as apparently the latest scipy from christoph gohlke under Python 3.4 is broken. will try on python 2.7 instead ok for the GSoC do you think we need to do anything else in the short term? We can still enable the edit mode on the GSoC proposals that students on a case by case basis for 2 weeks after today's deadline if we need to. yes I'll put some feedback on the semi-supervised proposal (directly as comment on melange) thanks can you do it? I don't remember by nickserv passwd so I cannot do op stuff anymore __eou__	User

TP	ok. well it would be good to reread the final versions. I did that only with like two of them. But we can do that later __eou__	User

TP	@ogrisel I'm still in the process of adding some stuff. Will be finished in 40 minutes. __eou__	User
TP	ok great__eou__	Agent

TP	My proposal in the google-melange has unnecessary html marks, like underscores. I think it comes from google-melange, since I did not add them at all. I also submitted a dropbox link to a PDF version, just in case. __eou__	User

TP	@xuewei4d Can you try to clean up the formatting? __eou__	User
TP	Yeah, I am on it.__eou__	Agent

TP	OK. Now it's clean. Copy/paste the html source code instead of paste into the melange own editor __eou__	User
TP	ok great__eou__	Agent

TP	@amueller let's the tweeting storm start! __eou__	User

TP	Yayy 0.16 :beers: __eou__	User
TP	:beers: :__eou__	Agent
TP	We should probably change the gitter channel status like some do in IRC?__eou__	User
TP	I meant here in gitter but I'll try the same for IRC too :)__eou__	User
TP	[![blob](https://files.gitter.im/scikit-learn/scikit-learn/g3A7/thumb/blob.png)](https://files.gitter.im/scikit-learn/scikit-learn/g3A7/blob)__eou__	User

TP	:beers: __eou__	User

TP	It says I need to be channel op to do that... How do I become one? __eou__	User
TP	you need to find another op :)__eou__	Agent
TP	which I think I used to be but cannot find my account info anymore__eou__	Agent
TP	yea on it ;)__eou__	User
TP	there might be others__eou__	Agent
TP	maybe @NelleV and @amueller are op__eou__	Agent
TP	ops__eou__	Agent
TP	not 100% sure though__eou__	Agent

TP	@NelleV is around I'll ask her :) (her right? :p) __eou__	User

TP	`scikit-learn 0.16 is out! http://scikit-learn.org/0.16/whats_new.html... Try out the same with "pip install scikit-learn==0.16"` Would be good? __eou__	User
TP	`pip install -U scikit-learn` would do__eou__	Agent
TP	@ragv yes it's "her"__eou__	Agent
TP	Ah :) BTW Only could do it for gitter (by double clicking on the channel status) ;)__eou__	User
TP	no problem__eou__	Agent

TP	:beers: __eou__	User

TP	I think I'm the only IRC op ;) __eou__	User

TP	@ogrisel the oreilly people asked for the slides for the webcast on Monday night ^^ we should probably talk about the content __eou__	User
TP	yes__eou__	Agent
TP	which topics do you want to cover?__eou__	Agent

TP	I am generic intro slides in there: https://speakerdeck.com/ogrisel/machine-learning-in-python-with-scikit-learn-1 but they overlap a lot with the slides I presented for the 0.15 release. and do not cover the new stuff __eou__	User

TP	I can work on sunday on new slides to present stuff like LSH Forest, DBSCAN and / or Birch we can use google docs to work collaboratively on a new deck those slides are in keynote, but feel free to do as many screen grabs as you want I think we should prepare a slide on t-SNE too Hope you will get better tomorrow is going to get complicated for me. But sunday I should be able to work on it for some reasonable amount of time. __eou__	User

TP	I feel super sick, I hope I'll be well enough tomorrow to work on new slides. But I also have to work on slides for a talk I give on Monday :-/ google docs seems like a good idea __eou__	User

TP	I'll try to make some slides on T-SNE, LDA and the GP, ok? We also need slide for Birch, the logistic path algorithm (?) and calibartion do we want to talk about MLPs and do you already have slides for that? __eou__	User
TP	I forgot about calibration, I can do that indeed__eou__	Agent
TP	no good slide for MLPs but would indeed say a couple of words about that__eou__	Agent
TP	I can review the logistic stuff__eou__	Agent
TP	I can also do the calibration. I have no idea about Birch. Do you?__eou__	User
TP	I think I should go out and find a doctor now__eou__	User

TP	a bit yes ok take care __eou__	User

TP	Thanks for all the work on releasing 0.16 guys! It is much appreciated! :beers: __eou__	User

TP	@ogrisel Any reviews for #4362? :) __eou__	User

TP	There is an unrelated [test failure](https://travis-ci.org/scikit-learn/scikit-learn/jobs/56248127) in Python 3.4 `:/ ` __eou__	User

TP	[Using code formatted smiley in gitter looks good `:D`] __eou__	User

TP	@xuewei4d You could also use commit headers like `ENH` `FIX` `TST` `COSMIT` `MAINT` etc... to describe your commits `:)` just a minor suggestion... feel free to ignore `:)` __eou__	User

TP	Sure. __eou__	User

TP	COSMIT means cosmetic? __eou__	User

TP	Yeah `:)` __eou__	User

TP	I think @ogrisel is busy today and I'm sick and need to make slides :-/ I re-triggered travis for you though __eou__	User

TP	Thanks! and take care `:)` __eou__	User

TP	@ogrisel are you around? __eou__	User

TP	yes are you feeling better? __eou__	User

TP	yeah, somewhat. So do we do only slides, no notebooks? __eou__	User
TP	apparently yes, this is a constraint of their platform apparently__eou__	Agent
TP	ok. and use google presentation?__eou__	User

TP	actually it's possible to have a screen share for live demo but it needs flash and or java I would not count on it too much we can use google doc and export the slides as PDF I think let's try I sent you an invite __eou__	User
TP	thanks__eou__	Agent
TP	That sounds like a good strategy__eou__	Agent

TP	@amueller I think I am ok on my side for the slides. __eou__	User

TP	@amueller I let you send the slides to yasmina and ben once you are happy with them __eou__	User

TP	@ogrisel around? __eou__	User

TP	@ragv yes __eou__	User

TP	> If you don't think it is horrible, I think I'll leave it as is  @amueller I only see a single slide with bullet bullet points for incremental PCA. Is it this slide you are talking about or do you have a figure for IncrementalPCA? __eou__	User

TP	@ogrisel I don't have a figure for IncrementalPCA. I couldn't produce a good one I could have added a code block showing partial fit, but I felt that added little __eou__	User

TP	no pbm I think it's fine like this there is a lot of material to cover already __eou__	User
TP	I agree__eou__	Agent

TP	@ogrisel I saw you online and thought of pestering you to review #3907 `;) :p` __eou__	User

TP	@ragv  I am too tired to do it now, I put that on my todo list for tomorrow morning. Don't hesitate to ping me again if I fail to deliver :) __eou__	User

TP	Thanks :D > Don't hesitate to ping me again if I fail to deliver  don't worry about that `:p` __eou__	User

TP	sprinters here? __eou__	User

TP	I feel excluded :-/ __eou__	User

TP	hi @amueller sorry I was in the metro to get back home for the webcast there are still people at the sprint but I think nobody is on gitter maybe @NelleV is on IRC __eou__	User

TP	@amueller the call starts in 3 mins right? I wanted to make sure that I am not off by one hour because of the summer time stuff. __eou__	User

TP	@ogrisel you are on time :) at least the same time I think it is __eou__	User

TP	I will call with my phone __eou__	User

TP	good! :) __eou__	User

TP	sprinter? Is there any event recently? So many new PRs ... __eou__	User

TP	yes there was a short one day sprint in Paris today (it should be over now). There was ~15 people in the room when I was there earlier this afternoon. __eou__	User

TP	Glad to see these new contributors. :) __eou__	User

TP	Wow looking forward to  participating in one!!! :D :D __eou__	User

TP	ragv you'll be at the epi-center of sprints soon, I heard :) I home I can come to paris for the next one, will be awesome! __eou__	User

TP	Yeaa :D Thats awesome... I'm really looking forward to that :) __eou__	User

TP	this looks pretty bad: https://github.com/scikit-learn/scikit-learn/pull/4507 how about we do a bug-fix release when this is done with the fix to isotonic, my CCA fix, and this? __eou__	User

TP	@ogrisel where was your mlp notebook again? __eou__	User

TP	@ogrisel it looks like the current partial_fit goes over the data multiple times. That seems odd. __eou__	User

TP	+1 for a 0.16.1 release with such bad bug fixes. I created the milestone. However I have to work on my talks for PyCon now. Maybe I can help for the 0.16.1 release during the sprints next week. __eou__	User

TP	The k-means fortran layout bug might need another fix and we should check if the bug appears elsewhere, too.... __eou__	User

TP	yes we could do a new common test for that, leveraging @ragv's `assert_same_model` utility. __eou__	User

TP	or we could just use ``fit_predict``.... Ok, I'll try to use his branch and see if that works... __eou__	User

TP	`fit_predict` or `fit_transform` would do as well. BTW, I had forgotten to upload the 0.16.0 artifacts to sourceforge, this is fixed. __eou__	User
TP	oh damn, I didn't think of that.__eou__	Agent
TP	btw, can you ping gael on gsoc?__eou__	Agent
TP	we need to move forward__eou__	Agent
TP	to get the right number of slots__eou__	Agent

TP	I think Gael is offline till tomorrow. I agree. __eou__	User

TP	ok __eou__	User

TP	ok back to my PyCon preparation __eou__	User

TP	@ogrisel I cherry picked some stuff in the 0.16.X branch, I'll work on the kmeans and then I think this should be good to go. The isotonic is a major concern for me as we introduced something that seems to be pretty broken. fixed should have been #4535 I'd like to think that #4535 wasn't as bad before I introduced ``check_array`` and that I broke it and that scikit-learn wasn't always i that state... ok I was just being silly. There are no breakages with fortran ordering, at least not with default settings. __eou__	User

TP	@amueller in your latest comment on #3907, you link to #3907 itself. __eou__	User

TP	the k-means fix in #4531 seems good.... __eou__	User

TP	I don't quite understand #4507 and #4531. What's the problem there? __eou__	User

TP	The problem is that the results on fortran-ordered data are garbage with precompute_distances=False results should be independent of the memory layout of the data. If you run his code, the cluster-assignments are [0, 0, 1] which is clearly garbage, as the two last points are identical favourite bug of the day: a pandas dataframe with a "dtype" column __eou__	User

TP	I see. Thanks! __eou__	User

TP	haha __eou__	User

TP	@amueller Just saw your comment in Melange. __eou__	User

TP	I did some investigation several days ago. Zhu has a nice tutorial giving introduction on how to implement S3VMs:  http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf It introduces 5 implementations, of which SVM^light has corresponding implementation in C: http://svmlight.joachims.org/ . We may write a wrapper for it. __eou__	User

TP	I can also consult the author of libSVM, Professor Lin, to get some suggestions. __eou__	User

TP	Don't worry about asking the libSVM author. We can't use SVM^light because of licensing issues, also we don't want to wrap more c code. @bryandeng sorry for the late reply. Don't worry too much about it, I think your proposal is quite good already. __eou__	User

TP	A review on #4541 would be cool, as that might be an additional nice fix for 0.16.1 __eou__	User

TP	[not the weird .dtype column part, the part where having dataframes with dtype object inside doesn't get correctly cast] __eou__	User

TP	@amueller I'm glad that you like it. And sorry for being inactive on Github these days. I'm writing a term paper (Hausarbeit) and playing with deep neural networks. I'll come back to scikit-learn this week. It's still during semester break so I have pretty much time. __eou__	User

TP	cool :) yeah being a bit more active would be great. You are welcome to help us with the MLP if you like ;) does anyone know how to disable the comments by coverall? __eou__	User

TP	[![Untitled.png](https://files.gitter.im/scikit-learn/scikit-learn/hYei/thumb/Untitled.png)](https://files.gitter.im/scikit-learn/scikit-learn/hYei/Untitled.png) should show up at the bottom of https://coveralls.io/r/scikit-learn/scikit-learn for the owner(s) __eou__	User

TP	assuming you mean PR bot comments @amueller ... Otherwise check the notifications tab to turn off emails, etc. which i think is personalized per user.. https://coveralls.io/r/scikit-learn/scikit-learn/notifications/email __eou__	User

TP	I don't have that showing up at the bottom, maybe because I'm not an owner. Which is weird since I'm a repo owner yeah I logged in __eou__	User

TP	That's strange. Are you logged into coveralls? It uses your GH login but I don't think it automatically logs you in __eou__	User

TP	maybe this? https://github.com/lemurheavy/coveralls-public/issues/199 __eou__	User

TP	ie visit https://coveralls.io/refresh?private=true ...? __eou__	User

TP	or this if you dont want coveralls to know about private repos perhaps: https://developer.github.com/v3/orgs/members/#publicize-a-users-membership though im pretty sure you're a public owner of skl. \ __eou__	User

TP	Won't this fail with an attribute error if there are no empty clusters? https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cluster/_k_means.pyx#L289  __eou__	User

TP	@omerzimp the loops goes over the empty clusters. If there are none, it never gets executed __eou__	User

TP	@amueller I noticed it a while after I sent that message Why not just nest it in the if? __eou__	User

TP	@amueller Here's a PR that does just that https://github.com/scikit-learn/scikit-learn/pull/4570 Also am I missing something in https://github.com/scikit-learn/scikit-learn/pull/4568 ? __eou__	User

TP	How good is the test coverage? I'm not sure if I broke something or not __eou__	User

TP	@ogrisel when do the sprints start? Any specific plans? @omerzimp #4568 supposed to be a speed improvement? issparse is just an instance check, so pretty neglible compared to everything else. __eou__	User

TP	Yeh but it's something :P I contribute what I can __eou__	User

TP	I would call this premature / overoptimizing. It makes the code a bit longer and a bit harder to read, but doesn't improve anything really. __eou__	User
TP	Feel free to close it__eou__	Agent

TP	How neglegable is it when you have a lot of data points? __eou__	User

TP	Hi @amueller, sorry I was busy chatting in the real life, I had not checked the gitter room yet So the sprints have just started today, we are 4 people at the sklearn table at the moment. We are digging down easy fix issues for first time contributors __eou__	User

TP	> I contribute what I can  But you have to keep in mind that the goal of the project is to stay maintainable and therefore code simplicity is an asset. We need to find the right trade-offs between simplicity and performance. __eou__	User

TP	@ogrisel  ok I'll keep an eye on the tracker. @omerzimp the check is independent of the dataset size __eou__	User

TP	@ogrisel btw if anyone at the sprint wants a brain-teaser, I recommend this one: https://github.com/scikit-learn/scikit-learn/pull/4435 no machine learning required ^^ __eou__	User

TP	I think we should do 0.16.1 soon. This is the only one I'd like to get in that hasn't been merged: https://github.com/scikit-learn/scikit-learn/pull/4541 ok just got merged. I'll need to do two backports and rewrite whatsnew, then I think we are in good shape. I'm just doing it __eou__	User
TP	ok__eou__	Agent

TP	I agree. Do you want me to backport 4541 ? __eou__	User

TP	should I also backport your astype fix? __eou__	User

TP	@ogrisel astype fix backport yes / no? otherwise tagging now alright :) __eou__	User

TP	I don't think it fixes any user bug in itself. Let's tag __eou__	User

TP	gah, forgot to add an entry to the "news" on the website... __eou__	User
TP	:)__eou__	Agent

TP	appveyor will have to do another 1h build :) __eou__	User
TP	ok, fixed. I'll update the mac build thing, I guess, and then we check the sdists...__eou__	Agent
TP	and build the website and minimize the PNGs ... fun :)__eou__	Agent

TP	macpython should be building __eou__	User

TP	great __eou__	User

TP	is it possible to unqueue on appveyor? it'll take 8 hours at least to run through the queue __eou__	User
TP	yes I just did that__eou__	Agent
TP	I will send you the password to the github account to log in and cancel / restart builds__eou__	Agent
TP	in your mail box__eou__	Agent
TP	when unqueuing stuff, it better to first unqueue (cancel button) the from the most recent to the most ancient queued job__eou__	Agent
TP	why is that better?__eou__	User
TP	off to lunch? oh, right, you are in my timezone now ^^__eou__	User
TP	oh cool. png's are optimized, too. So I'll upload the docs, then__eou__	User
TP	I'll err... foward-port (?) the whatsnew to 0.17__eou__	User

TP	otherwise you get the next ancient job in state running, and canceling running jobs takes more time than queued jobs BTW, if you are interested in all of this windows build I gave a talk at pycon on that: https://twitter.com/ogrisel/status/587326055171694594 off to lunch, see you later! It's great to have this release out this week. Thanks so much! __eou__	User

TP	@amueller the windows build is up: http://windows-wheels.scikit-learn.org/ the OSX wheels are up as well: http://wheels.scipy.org/ Do you want me to do the upload to PyPI and sourceforge or do you want to do it? __eou__	User

TP	The docs are up. Can you do pypi upload please? how do you upload the wheels btw? or maybe you do it ;) I still get the "unknown url type error" __eou__	User

TP	you can either use upload_all or the twine command __eou__	User
TP	maybe I'll try upload_all__eou__	Agent
TP	Have you updated wheelhouse-uploader ?__eou__	User
TP	oh, right__eou__	Agent
TP	now it works__eou__	Agent

TP	it works with python 2.7 on a new virtualenv were I just pip installed wheelhouse-uploader __eou__	User
TP	yeah It does work. Did you upload already?__eou__	Agent
TP	yes__eou__	User
TP	I just did, with twine__eou__	User
TP	to avoid TLS man in the middle attacks :)__eou__	User
TP	ok. what is twine?__eou__	Agent
TP	I'm just drafting the ann mail, ok?__eou__	Agent
TP	https://pypi.python.org/pypi/twine__eou__	User
TP	ok please go a ahead__eou__	User
TP	for the ANN__eou__	User
TP	and the tweet__eou__	User

TP	done ah, twine makes sense ^^ __eou__	User

TP	ok __eou__	User

TP	@amueller is your LGTM still valid in light of the latest changes to #4590? I uploaded the release to sourceforge as well __eou__	User

TP	thanks __eou__	User

TP	@agramfort https://github.com/scikit-learn/scikit-learn/pull/4550 @agramfort this one would be cool: https://github.com/scikit-learn/scikit-learn/pull/4534 https://github.com/scikit-learn/scikit-learn/pull/4526 https://github.com/scikit-learn/scikit-learn/pull/4467 https://github.com/scikit-learn/scikit-learn/pull/4365 __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/issues/3855 that last one is about changing the number of samples __eou__	User

TP	Hey guys, I know this is slightly off topic, but is there any recommendation regarding structured/sequence learning libraries in Python? I have been using crfsuite but unforunately it's only first order and not very good for named entity recognition The Stanford NER Java lib is crazy huge/unwieldy -_- nm looks like @amueller has ChainCRF in PyStruct! I hope it's fast. Please be fast. __eou__	User

TP	even has great docs =) __eou__	User

TP	PyStruct and seqlearn are two projects maintained by scikit-learn developers (@amueller and @larsmans respectively). __eou__	User

TP	@ldqc the docs are new and work in progress ;) pystruct implements maximum margin learning and perceptron, seqlearn implements HMMs and perceptron,  I think. So both don't implement maximum likelihood CRFs __eou__	User

TP	looks like i can't use sparse matrices for yours atm __eou__	User

TP	@lqdc unfortunately yes. hopefully soon(ish) __eou__	User

TP	Hello guys! I'm having problems with a django application that uses a random forest classifier to classify items. The error that I'm receiving says: [ 'Thread' object has no attribute '_children' ] and googling it leads to http://stackoverflow.com/questions/9749875/strange-error-while-starting-threads-inside-django-application On the other hand, the exact line where the application throws the error is "clf.predict_proba(items)" and I'm not using threads at all but I set n_jobs=-1 in the initialization of the classifier so I wonder if this error could be related to joblib (it uses ThreadPool in the method "  call  " of the class Parallel). Any idea? Can I modify the n_jobs variable in order avoid parallelism? Sorry! I wasn't able to find the backticks in my spanish keyboard :/ __eou__	User

TP	@mac2bua please create a new stackoverflow question with a minimalistic reproduction script that generates its own test data. __eou__	User

TP	Done! http://stackoverflow.com/questions/29852680/thread-object-has-no-attribute-children-django-scikit-learn __eou__	User

TP	answered. Please include the full traceback to make it possible for us to fix the real cause of the problem. __eou__	User

TP	Thanks @ogrisel the error seems to be fixed after I set the n_jobs parameter to 1. Here is the complete traceback: ``` File "/home/cristian/env/local/lib/python2.7/site-packages/django/core/handlers/base.py" in get_response line 111.                     response = wrapped_callback(request, *callback_args, **callback_kwargs) File "/home/cristian/env/local/lib/python2.7/site-packages/django/views/decorators/csrf.py" in wrapped_view line 57.         return view_func(*args, **kwargs) File "/home/cristian/env/local/lib/python2.7/site-packages/django/views/generic/base.py" in view line 69.             return self.dispatch(request, *args, **kwargs) File "/home/cristian/env/local/lib/python2.7/site-packages/rest_framework/views.py" in dispatch line 452.             response = self.handle_exception(exc) File "/home/cristian/env/local/lib/python2.7/site-packages/rest_framework/views.py" in dispatch line 449.             response = handler(request, *args, **kwargs) File "/home/cristian/env/local/lib/python2.7/site-packages/rest_framework/decorators.py" in handler line 50.             return func(*args, **kwargs) File "/home/cristian/filters/classifiers/views.py" in classify_item line 70.    y_pred = clf.predict_proba(pd.DataFrame(item_dict)) File "/home/cristian/env/local/lib/python2.7/site-packages/sklearn/pipeline.py" in predict_proba line 159.         return self.steps[-1][-1].predict_proba(Xt) File "/home/cristian/env/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py" in predict_proba line 468.             for i in range(n_jobs)) File "/home/cristian/env/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py" in __call__ line 568.             self._pool = ThreadPool(n_jobs) File "/usr/lib/python2.7/multiprocessing/pool.py" in __init__ line 685.         Pool.__init__(self, processes, initializer, initargs) File "/usr/lib/python2.7/multiprocessing/pool.py" in __init__ line 136.         self._repopulate_pool() File "/usr/lib/python2.7/multiprocessing/pool.py" in _repopulate_pool line 199.             w.start() File "/usr/lib/python2.7/multiprocessing/dummy/__init__.py" in start line 73.         self._parent._children[self] = None  Exception Type: AttributeError at /items/ Exception Value: 'Thread' object has no attribute '_children' ``` __eou__	User
TP	Please edit the stackoverlow report instead.__eou__	Agent
TP	Also quoting is done with backticks, not forward ticks :)__eou__	Agent

TP	I've just added the traceback to stackoverflow! __eou__	User

TP	finalllllly all my exams got over :D been inactive for too long :/ full time scikit-learn from today B) __eou__	User

TP	sweet :) __eou__	User

TP	welcome back @ragv, unfortunately myself I will be busy for the next couple of weeks. (strata in london / then offline for vacation / the moving to a new flat). __eou__	User

TP	sure :) no problem :) I'll take care of all the pending works for now :)) and I do have a lot of pending stuff :O __eou__	User

TP	heyyy I got in GSoC ;) Thanks a lot @amueller @ogrisel @jnothman and @MechCoder   :) Congrats to @xeuwei4d and @barmaley-exe too :) __eou__	User

TP	Congrats @ragv! __eou__	User
TP	Thanks a lot Vinayak :)__eou__	Agent

TP	and to @xuewei4d and @Barmaley-exe! __eou__	User

TP	hurray :) __eou__	User

TP	Thanks! It will be a great summer! __eou__	User

TP	@ogrisel can I get a +1 on #4371? __eou__	User

TP	Greetings! I've done a pure Python implementation of the k-modes algorithm for clustering categorical data: https://github.com/nicodv/kmodes It's modeled after the k-means code in sklearn. Any interest in this? __eou__	User

TP	Congratulations! @ragv @xuewei4d @Barmaley-exe Though not accepted into GSoC :worried: , I'll continue working on the tasks proposed. __eou__	User

TP	great, thanks :) __eou__	User

TP	I'm sorry you didn't make it, we didn't have enough mentors for all the projects. __eou__	User

TP	@nicodv we could link to your code in related projects, but I think we decided against including kmodes in the past because it was very slow. is that not the case? I forgot the details. __eou__	User

TP	@amueller In pure Python, it's almost exactly an order of magnitude slower than k-means. But it scales roughly the same as k-means in terms of number of points (but worse in terms of n_clusters and dimensions) __eou__	User

TP	It's less that an order of magnitude, actually. Let's say 5 to 10 times slower. __eou__	User

TP	wait, I think i confused it with k-mediod. Sorry, never mind. medians is just the l1 variant. that should be pretty easy to add to the current code, I think. __eou__	User

TP	how do you compute the ndim medians in l1 space? __eou__	User

TP	per-coordinate median, right? argh, ok, got double confused, then. __eou__	User

TP	from k-medians Wikipedia: "The median is computed in each single dimension in the Manhattan-distance formulation of the k-medians problem" but k-modes, my code, does not use Euclidean space because it clusters categorical variables so, quite different from k-medians or k-medoids there's k-modes, for which all is assumed categorical; and there's k-prototypes (this combines k-modes and k-means), which receives X as a list of 2 arrays, one for numerical and one for categorical variables __eou__	User

TP	Feel free to sent a PR to include it as related project. We don't really have anything for categorical variables at the moment, and we haven't really figured out the API. How do you denote which variables are categorical and what are the inputs? Or are just all features assumed to be categorical? __eou__	User

TP	ok. So this list would mess with the sklearn api a lot, for cross-validation tries to sample along the first axis. __eou__	User
TP	another option would be to take a single X, and a "is_categorical" variable that specifies which columns are categorical__eou__	Agent

TP	yeah, that is what I think we would like to do for the forests, but I'm not sure. It is a bit awkward that they are float then, but not a big deal I guess using pandas dataframes would also be an option, maybe not in scikit-learn though. __eou__	User

TP	anyone a quick +1 for #4526 ? should be an easy fix __eou__	User

TP	I'm always complaining about the many github notifications, and I just sent 74 for #3306 ^^ I'm glad I don't get them in my inbox __eou__	User

TP	Hey @amueller ... Just a FYI, ran your check_estimator API on my package. It works for the most part. Though I raise a copied version of the NotFittedError and get a fail since it isn't explicitly using the scikit-learn version (I wanted to support 0.15.2 as well so exported a bunch of functions to reside in gplearn.skutils.etc). __eou__	User

TP	Thanks for the feedback. Is there a reason your NotFittedError doesn't inherit from the sklearn one? Well I guess actually we wanted people to be able to provide compatible code without needing to rely on sklearn.... hum... @trevorstephens maybe I'm slow, how is that related to the remark in the parentheses? __eou__	User

TP	NotFittedError was new in 0.16.0 I think, so I'd fail tests on my own Travis builds based on 0.15.2 if I tried to inherit from sklearn I guess And to guard against other changes to the non-public API, I grabbed a few key utils modules from 0.16.0 and stuffed them into a folder in my project I could potentially wrap all the important stuff in try blocks, like the fixes module in sklearn. But that seems like a lot of work given how interconnected some of the utils are __eou__	User

TP	yeah no, don't do that. Actually we should aim at allowing people to pass tests without inheriting from sklearn. __eou__	User

TP	@trevorstephens gael and I think we shouldn't check for NotFittedError, but a public one. I'm not sure if ValueError or AttributeError yeah, probably. __eou__	User

TP	that'd work. mine is a direct copy of the scikitlearn error. probably other's are using older code bases where I think a ValueError was directly raised when not fitted actually maybe not... https://github.com/scikit-learn/scikit-learn/pull/4029/files shows a mix of both were in the code base. perhaps check for either of them. i think checking from both is overkill and makes an unnecessary burden on people who want to work with sklearn, but not require it. __eou__	User

TP	Hi! Any tips regarding GradientBoostingClassifier vs. class imbalance?  Contrary to e.g. LogisticRegression, there is no class_weight parameter.  Without doing anything, I get high precision but tiny recall. I guess I could just pre-process the dataset manually (adding copies of the less represented class) but that feels dirty.  Surely there's a better way? __eou__	User

TP	@pasky , you could use the function `compute_sample_weight` found at https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/class_weight.py#L68-L166 like so:  from sklearn.utils.class_weight import compute_sample_weight # "auto" class weights inversely proportional to class frequencies sample_weight = compute_sample_weight("auto", y) # dict to define them yourself with {class_symbol: weight} sample_weight = compute_sample_weight({1: 2, 2: 1}, y) ... then just feed that as sample_weight to the fit method geeze. apparently python comments are taken to mean bold... but you get the idea :smile: it's a private util function, so not really advertised. its used under the hood in 0.16.0's random forest and decision tree classifiers. __eou__	User

TP	Oh, that sounds pretty nice! Thanks, I'll try that out. I completely forgot about the option to also pass the sample weights to fit() method instead of specifying class weights in constructor. __eou__	User

TP	Yep, it's kind of sort of an indirect method to oversample/undersample. __eou__	User

TP	we should probably add class_weights to the gradient boosting classifier __eou__	User

TP	@amueller ... #4215 is just waiting on a #4347 merge so that there isnt more refactoring on your part required. unless you want me to address the comments and add more work to your pile?? im good either way :smile: __eou__	User

TP	... it would effect gbm, as well as the  adaboost and bagging meta-ensembles __eou__	User

TP	oh, I forgot that PR. #4215 that is. My PR needs work to deprecate the class_weight in trees as you know. I should do that, but I didn't really get any reviews yet :-/ __eou__	User

TP	that's why I figured I'd wait. Boosting has the subsample option too __eou__	User

TP	@trevorstephens  ```python try:     from sklearn.utils.validation import NotFittedError except ImportError:     # backward compat for scikit-learn < 0.16.0     class NotFittedError(Exception):         pass ```  should work. __eou__	User

TP	@ogrisel what do you think of #4661 ? __eou__	User

TP	after three years, I'm apparently still confused by scale_C vs n_samples @ogrisel if you have time, your input on #4597 would be nice. __eou__	User

TP	Thanks @ogrisel , that's what I was thinking of, though I have several other scikit-learn utils that I've included in my package in order to avoid dependency hell-fire. Only a few of them are actually used so it might not be that heavy, I guess, if I was to pick out only the ones that matter. Maybe once I get 0.1.0 released I'd think about making it more sensible on the utils. @amueller .. I think you know the appropriate emoji for dependency hell-fire :smile: __eou__	User

TP	haha __eou__	User

TP	And believe me, I have been there, more than once ;) __eou__	User

TP	@ogrisel the #4597 issue is not really with pandas, it is with Cython, which doesn't support read-only buffers. So my question was more: why does joblib produce a read-only buffer there? ah, I haven't read your comment there, sorry __eou__	User

TP	@amueller indeed I read some more discussions in this thread: https://mail.python.org/pipermail/cython-devel/2013-February/003384.html and then replied the following: https://github.com/pydata/pandas/issues/10043#issuecomment-99227037 __eou__	User
TP	makes sense, but we can also not wait for the fix in pandas.__eou__	Agent
TP	@ogrisel so I think we should maybe just copy the DF in the "rare" situation that happens and throw a warning that people should use .values to avoid the copy?__eou__	Agent

TP	Calling `.values` will just force a copy if am not mistaken. The "real" fix can only be done in pandas by using the ndarray type instead of a typed memoryview (or even better in Cython by adding support for readonly typed memoryviews). __eou__	User

TP	I don't think values will force a copy always. It is not writeable here, so I don't think a copy was made. Well the real fix is clearly in cython but we have to be backward compatible, so we need to do a workaround now matter what __eou__	User

TP	Indeed `.values` does not copy for single block data frames. but it does for multiple blocks dfs __eou__	User

TP	 can you reproduce the error with multiple block dfs? I posted my try at #4597 __eou__	User

TP	I cannot reproduce the error without manually assigning a readonly memmap to `df._data.blocks[0].values` or replicating what the joblib.pool.MemmapingPool class does (that is replacing np.ndarray instances by np.memmap instances at pickling time). Let me work on fixing the joblib.load with `mmap_mode='r'` on structures that have numpy arrays with dtype=object inside. in joblib master Ok going to bed now. Tomorrow is second day of strata london. I gave a tutorial today. Tomorrow I will just attend the conf and maybe work on sklearn issues if the talks are boring. see you __eou__	User

TP	@ogrisel do you know anything about the issue in #4421 ? some warnings are not raised / caught .... __eou__	User

TP	cool. say hi to everybody from me ;) __eou__	User

TP	@ragv you had a PR to include some help on git to the dev docs, right? __eou__	User

TP	@ogrisel do you have time for #4362 ? __eou__	User

TP	@rvraghav93 @ragv I know you don't have much time, but if you could point me again to where you did the improvement of the contrib docs, I could maybe work on that. I think having some more comments on git would really help some people. __eou__	User

TP	AFAIK I think the intended framework was just put on the wiki for now - https://github.com/scikit-learn/scikit-learn/wiki/Contributors-Guide __eou__	User

TP	@amueller Sorry for the delayed response... as @trevorstephens pointed out that was the proposed structure... I'd love your comments on the same :) __eou__	User

TP	it looks good __eou__	User

TP	@rvraghav93 @amueller oh that looks really good. One thing I noticed though is that there's overlap between "API design" and our current "rolling your own estimator", and users do get confused about what they can and can't do in `__init__`, and why things fail afterwards. Do you have any ideas on how to make it better? __eou__	User

TP	We could use ideas from the existing page and make it into a single page or a section "Custom designed estimators" under "API design"... BTW I think I'll start a PR for the same and it will be easier for you to comment and advice me... :) __eou__	User

TP	I was just wondering why we can't add line-comments to github wiki pages :) __eou__	User
TP	haha :D I'll make it easier by raising a PR ;) But incase you want to comment to the wiki pages... You could just edit it and add a comment "(@vene: This should be removed)" etc... at the particular place and make it bold... I'll take note of the same :)__eou__	Agent

TP	@vene I have wondered that myself from time to time ;) __eou__	User

TP	@bdholt1 just showed up on IRC and said he's back :) __eou__	User

TP	hi all! __eou__	User
TP	hey :)__eou__	Agent

TP	My favourite GSOC students, @rvraghav93 @xuewei4d @Barmaley-exe how are things? Reading up on your projects? ;) __eou__	User

TP	@bdholt1 oops haha __eou__	User

TP	what i meant to say, was, @amueller .. sorry, i noticed your tag on #4732 today and then your #4711 comment asking for a new issue. Rather crazy week, will see if I can solve it though. and hi @bdholt1  :smile: __eou__	User

TP	Yes. I am currently reading the Murphy's paper 'Fitting a Conditional Gaussian Distribution'. That's the paper described three kinds of co-variance matrix in current  GMM implementation. But it seems a little harder to understand in terms of notations. @amueller __eou__	User

TP	@amueller yeah ;) :P working on completing my exiting PRs :) __eou__	User

TP	@rvraghav93 cool :) __eou__	User

TP	@xuewei4d have you talked with your mentors yet? it would be great to get a first blog post from both of you ;)_ __eou__	User

TP	Not yet. I will email them soon. I have published a post several days ago, which is an introduction to my project and myself. I am trying to derive the updating functions of VBGMM with other three kinds of covariance matrix, sphere, diag and tied following PRML. __eou__	User

TP	Xuewei4d: did you mail your blog post to the mailing list? maybe I overlooked it, sorry xuewei4d: that sounds great. Make sure you have your mentors looped in so they know what is going on! btw @ogrisel are you around? I think it is good practice to post each blog post to the mailing list for maximum visibility I have to admit I didn't rss subscribe, and I don't usually use RSS, so i am likely to miss it if you don't post ;) @rvraghav93 btw why did you change your github handle? @rvraghav93 also: for wrapping up the partial_fit testing, I think the most important part is to simplify _compare_attributes __eou__	User

TP	Ohh, I will post it into the mailing list. Sorry. __eou__	User

TP	any one here familiar enough with LDA to do a final review? master is broken with a heisenfailure I don't like it __eou__	User

TP	anyone who has an opinion on how back-links from the references to the user-guide should look like should speak here or be forever silent: https://github.com/scikit-learn/scikit-learn/pull/4723 __eou__	User

TP	@amueller Sorry for the confusion... I just merged my github profiles ragv and the old one rvraghav93 into one to sync with my email rvraghav93@gmail.com ... :) I earlier created ragv as a temporary profile...  And yea I just pushed it ;) could you pl take a look? :) __eou__	User
TP	ok, I will. not sure I have time to go through all of it now__eou__	Agent
TP	Sure :)__eou__	User

TP	@amueller the heisenfailure (the nans in solve_triangular called by OMP) only happens on scipy 0.9.0 from ubuntu precise right? What about skipping that test if scipy is too old? __eou__	User

TP	yeah, but we still support scipy 0.9.0, right? I tried reproducing but failed. I fear it is something silly like last time, where there was an if on the scipy version __eou__	User

TP	Have you tried to reproduce with ubuntu 12.04? __eou__	User
TP	not yet__eou__	Agent
TP	ok I can try tomorrow if you don't do it in the mean time.__eou__	User
TP	Need to run now, see you later.__eou__	User

TP	ok. I think I have a vagrant box, I'll have a look btw if you had time to look at the mlp branch, this would be great nice! When are you in NYC? Let's have a beer! next week? ttyl! __eou__	User

TP	@rvraghav93 could you maybe start with a first blog post on your plans for the summer? __eou__	User

TP	@amueller @rvraghav93 Maybe we could find the time to sync and discuss an outline of the blog post. It's also a good opportunity to think about the upcoming steps. __eou__	User

TP	Sure __eou__	User

TP	BTW, I think it's good to be aware of each other's time zones, so we can plan better. I'm currently on Central European (Summer) Time until Sunday, and then New York time. (We can take this to a private chat too if you want.) __eou__	User
TP	where are you going vlad? Will you be around?__eou__	Agent

TP	I will actually be in New York for about a week, if everything goes well. I have to apply for a Chinese visa. Afterwards it's back to boring Ithaca. __eou__	User

TP	Currently in Florence for WWW Very beautiful city! __eou__	User

TP	Sure! Yes, Monday-Friday I think. Gotta run for a while, talk to you later! __eou__	User

TP	@amueller I just PM-ed @vene about the blog post :) Will make one before 20th and share the link :) __eou__	User
TP	great!__eou__	Agent

TP	@tomdlt btw we have this thing here were we hang out and chat ;) __eou__	User

TP	@amueller I think I understand part of the pbm for the _gram_omp failure on old scipy although I cannot reproduce it in ubuntu 12.04 docker container with scipy 0.9.0. Will submit a PR. I think we can merge. @amueller #4743 __eou__	User

TP	@ogrisel thanks! I tried vagrant but didn't reproduce __eou__	User

TP	damn, yeah I'm stupid of not having seen that @ogrisel __eou__	User

TP	Hopefully, this will fix the issue. But travis is so slow nowadays. I opened #4749 to tackle travis speed on the longer term. __eou__	User
TP	pretty sure that will fix the issue__eou__	Agent

TP	#4749 would be really, really nice. I did not know travis was starting to support that - awesome! __eou__	User

TP	@ogrisel so do you definitely want to do 0.16.2? I'm not sure. Maybe if we have the jaccard fix? Btw, @ogrisel if you have time reviews of these should be quick: #4741 #4739 #4714 maybe we should have added some more whatsnew entries __eou__	User

TP	This is not necessarily for sklearn (more for say mne-python, nilearn, sklearn-theano, etc): would the change mentioned in the blog post referenced in #4749 make certain data-intense (but fast) test cases possible with datasets stored in S3? __eou__	User

TP	depends on how large, right? I guess we don't know which datacenter, but even between datacenters I think ec2 is pretty fast __eou__	User

TP	ah yes - I thought that data was represented in several locations and always near to where you need it, but that is probably wishful thinking. Cost-wise,  as far as I understand data transfer from S3 to EC2 machines is free, so that shouldn't be a hindrance. Then the only question is whether practically out of these containers one can 'see' S3 as one can from EC2. __eou__	User

TP	probably not without some kind of credentials or something - you would have to authorize the EC2 instance that it is "OK" to access your s3 data __eou__	User

TP	well that is the same for all s3 access, right? ec2 machines are no different from any other machines on the internet wrt to s3 access iirc @ogrisel do you think we can ask intel for an MKL licence for building our wheels? __eou__	User

TP	I am thinking more if your data was in a private S3 bucket - in the case where people are accessing your data publicly it would be fine though it would cost $$ but if you wanted to only allow data access for,  say, nightly tests on master branch to cut down cost, you would need some kind of authorization. So if Amazon gives free S3 to open source (or Travis), that would work. But otherwise I don't think we can rely on it without a bill. __eou__	User

TP	I just realized that if I wanted to test the neural nets with mkl I have to wait until we release and continuum builds it for me... that seems slightly silly... __eou__	User

TP	hum, thinking about it again, how do we build this on travis? That uses MKL, right? __eou__	User

TP	Yes I think one of the anaconda builds on Travis is MKL - do you have the academic version of Anaconda? It is free for you and would allow a conda env with MKL if I remember right __eou__	User

TP	yeah I do. but what blas does sklearn link against? anaconda doesn't come with the library to link against, right? __eou__	User

TP	The academic version does you have to register with .edu address - not sure how Travis has it but I am sure they have some license from Continuum __eou__	User

TP	interesting, I have to check. I thought I had the academic license but I ran into trouble when trying to built sklearn using it @ogrisel would you mind reviewing the mlp? I haven't added the early stopping yet, but mostly because I didn't see a case where it helped Also, I'm not sure how much the adaptive learning rate helps yeah it would be great if you could help do you know what the right gain is in the initialization for the different non-linearities? I tested against validation data currently using warm-start. but what I am implementing right now is doing a split inside fit __eou__	User

TP	>  Yes I think one of the anaconda builds on Travis is MKL - do you have the academic version of Anaconda? It is free for you and would allow a conda env with MKL if I remember right  on travis we just use the one month evaluation period of MKL in anaconda. As a travis build tend to last less than one month, it works :) sklearn probably does not build against MKL on travis with anaconda, but the included numpy and scipy packages are linked against it. __eou__	User

TP	SGD is unreasonably effective ;) and I am surprised you didn't find a case where early stopping helped. But how are you testing against validation data inside sklearn API? I have been hacking my own code to take fit(X, y, valid_X=None, valid_y=None) I really should help review that code but NIPS is looming :( __eou__	User

TP	what do you mean by gain? initializations are mostly (in my exp) about how tricky it is to get the right settings of the optimizer/avoid gradient explosion or vanishing glorot init is normally the "right thing" for basically all nets, though I hear orthogonal is good too. I don't know that you should expect a gain in performance on many tasks due to init settings, but you will certainly tell when you want to apply a net to a brand new dataset __eou__	User

TP	you need to multiply the 1/sqrt(fan_in + fan_out) by a constant that is dependent on whether you do relu, tanh or sigm in lasagna that is called the gain factor __eou__	User

TP	Ah ok. I usually go by this, page 15 http://arxiv.org/pdf/1206.5533v2.pdf 4 * for  sigm, tanh 1 * for relu should be sqrt(6/fan_in + fan_out) though I guess you could pull out the 6 and adjust the "gain" to compensate. I just stick to the script __eou__	User

TP	Sanders said gaussian / uniform doesn't make a difference and I trust his judgement @kastnerkyle thanks I think I'll go with the script __eou__	User

TP	Well, this is weird: https://travis-ci.org/scikit-learn/scikit-learn/jobs/64079397 . cross_val_score, GridSearchCV and SVC and the iris dataset should all be deterministic. __eou__	User

TP	indeed weird __eou__	User

TP	maybe some corrupted memory on the travis host? we should keep this failure in mind if it ever happen a second time __eou__	User

TP	corrupted memory sounds unlikely btw, why does pip install on OS X do a compile? https://github.com/scikit-learn/scikit-learn/issues/4766 when are the wheels used? fair enough __eou__	User

TP	@rvraghav93 I just started reading your blog post. I wouldn't argue with google search hits, as they are google-bubble dependent. Yours and mine are very likely biased heavily towards sklearn. @rvraghav93 end of may is pretty soon, btw. What is the status of the data-dependend CV iterators?  and why is the SVM infamous? ok thanks. ping me once you incorporated vlads comments __eou__	User

TP	Vlad advised me on a few improvements  and I am working on it too :D __eou__	User

TP	BTW this is my blog url rvraghav93.blogspot.in (so the rest of the people could take a look too) infamous was a wrong usage :/ correcting it :) And I am working on it... The final consensus was that the code must be duplicated right... one with the deprecations and one without... Will push soon ;) sure :) btw I intentionally aimed it to be biased towards scikit... I wanted to give a view where in the end user might see what they get out of my work... like Vlad had advised in the email... Is that okay? __eou__	User

TP	What I meant is that if you use google, google knows you contribute to scikit-learn, so it will show scikit-learn results on top. if someone else reads your blog and clicks the links, but they are an R user, it might show them links to R libraries, and no scikit-learn anywhere __eou__	User

TP	I just checked for "cross validation" using an online proxy server... our documentation page for cross validation still ranks at 3 ;) actually 2 if you consider the first 2 wikipedia pages as one :) __eou__	User

TP	woah my google cred is improving __eou__	User
TP	:beers:__eou__	Agent

TP	and may end as in without counting code reviews and revisions which I could do in parallel with the next goal :) __eou__	User
TP	hehe ok__eou__	Agent
TP	loooool http://en.wikipedia.org/wiki/Andreas_M%C3%BCller__eou__	Agent

TP	nice :) It appears you have been there for more than a year http://en.wikipedia.org/w/index.php?title=Andreas_M%C3%BCller&type=revision&diff=597996618&oldid=573147763 time to fill in that page ;) __eou__	User

TP	awesome ;) we are filling it up :P __eou__	User
TP	lol__eou__	Agent
TP	I'm not that interesting__eou__	Agent

TP	Hey @amueller , regarding #4767 , just remembered your #4347 now... do you think I should pause work there until that merges? Do you think it will be merged? Also applies to #4215 I suppose... I guess both of these PRs would not necessarily require deprecation since they would not be in a public release, but would be more work for you. Should I press on with @vmichel 's PR comments, or put both on hold pending renewed class_weight naming/implementation? Reviewing these, I begin to feel that I am a one-trick pony :smiley: __eou__	User

TP	#4347 should be merged. There were no real reviews yet, though. I'm pretty sure you have more tricks up your leave, though @trevorstephens ;) I'm not sure, I would love some reviews for #4347. I'll rebase now. Maybe @ogrisel has time to have a look? __eou__	User

TP	Well I'll leave #4215 alone for now then. If PassAgg gets merged, it should be pretty simple to change it over in your PR I guess. Unless you want me to advertise #4347 there? I'm fine either way. __eou__	User

TP	@vene @amueller @ogrisel Hey this is the updated blog post incorporating all your suggestions... I am still a newbie at ML and might have made a few stupid mistakes... Please take a look at the post and feel free to point out if there are any :) http://rvraghav93.blogspot.com/2015/05/gsoc-2015-with-python-software.html __eou__	User

TP	@rvraghav93 doesn't Wei Xue work on Gaussian Mixture Models? I thought that bayesian hyperoptimization didn't make it into accepted projects __eou__	User

TP	yeah, that is correct __eou__	User

TP	I'll have a look at the blog post probably tomorrow, I am sprinting with @pprett and @GaelVaroquaux today https://github.com/scikit-learn/scikit-learn/issues/3560 __eou__	User

TP	There is a sprint in boston! (like, today) if anyone here wants to join like @llllllllll lol that is the worst name __eou__	User

TP	@GaelVaroquaux do you want to merge this? https://github.com/scikit-learn/scikit-learn/pull/4785 __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/issues/4784 __eou__	User

TP	@Barmaley-exe thanks! :) @xuewei4d apologies for the same ;) __eou__	User

TP	@amueller @llllllllll worst name, or *best* name? :smile: __eou__	User

TP	I think you could go a level further @l1Ill11IIlll111III just totally destroy people with certain fonts __eou__	User

TP	Hi, @rvraghav93 I think you need to update the RSS feed of your blog in Terri's website, (link)[https://github.com/terriko/gsoc/blob/master/blog-aggregator-configs/config2015.ini] __eou__	User

TP	@xuewei4d wow, it's been a while since I've seen an .ini file! Those are great. For users who learn by hacking around in IPython, it was so easy to miss the User Guide completely __eou__	User

TP	haha @vene @llllllllll thinks it is the best. I just feel it is hard to type ;) __eou__	User

TP	@rvraghav93 which PR are you currently working on? I think the data independend CV should have priority. __eou__	User

TP	@rvraghav93 in the blog post, "improvise" should probably be "improve" __eou__	User

TP	@amueller good thing Gitter has autocompletion __eou__	User

TP	Can I ask a quick joblib question here? https://github.com/joblib/joblib/blame/master/doc/memory.rst#L187 it says r+ and w+ "will propagate the changes to disk". Shouldn't that be "will NOT propagate"? __eou__	User

TP	ping @ogrisel ? __eou__	User

TP	I am fascinated time and again by which PRs and issues get attention yeah I mean it is neat, but like 6 core devs in a day? __eou__	User

TP	What do you mean? The CallableVectorizer? *CallableTransformer? __eou__	User

TP	I think it's because all of us hacked up one at some point or another __eou__	User
TP	yeah probably__eou__	Agent
TP	but the FeatureUnion thing didn't get as much attention, I think__eou__	Agent

TP	I am unreasonably happy about the backlinks ok __eou__	User

TP	well but from IPython you don't get a clickable link, only a useless reference, right? __eou__	User
TP	good point... but at least you know it exists__eou__	Agent
TP	true__eou__	User

TP	@amueller @ogrisel I think I need more time to write down all equations of VBGMM and DPGMM. Currently I have those for full and diag covariance. https://www.dropbox.com/s/8hlbb7dlwllwcry/VBGMM.pdf?dl=0 __eou__	User

TP	so you are missing spherical? Shouldn't that be easiest? I'll try to have a look tomorrow, but it would be great if @ogrisel and loic could have a look __eou__	User

TP	Well, I read through  PRML and equations for full covariance. Since there are some repeated routines, I choose to do them in order of 'full', 'diag', 'sphere' and 'tied'. since 'diag' share some similarities with 'full' __eou__	User

TP	@rvraghav93 "digits" is not the same dataset as MNIST, it is much smaller. in your blog post, you mention mnist but link to a digits example. @rvraghav93 why do you use alpha when plotting points? @rvraghav93 I feel this sentence is unclear: "Even when the model is optimized with the constrain of maximizing the score based upon the test set, there is still a chance of overfitting as the information about the test set can leak into the model and hence the model could be optimized for the test set alone."  it would be more explicit to say the information leaks via the selection of hyperparameters. @rvraghav93 and cross-validation does not entirely overcome this. Overfitting to cross-validation is harder than overfitting to a single test set, but it is still possible, which is why people do nested cross-validation. @rvraghav93 grid.best_estimator_.score(X_test, y_test) is also not great btw. You can just use grid.score @rvraghav93 talking about gamma=0 for the SVM is also a bit weird. This is an odd way that we used to select the default, which is 1. / n_features ,I think. __eou__	User

TP	"which is why people do nested cross-validation" I think that's where @rvraghav93 is trying to lead to. __eou__	User
TP	Thanks, @amueller . The text in the draft is kind of vague, but I think you could get ideas just looking at equations.__eou__	Agent
TP	Hope you don't mind.__eou__	Agent

TP	> Can I ask a quick joblib question here? https://github.com/joblib/joblib/blame/master/doc/memory.rst#L187 it says r+ and w+ "will propagate the changes to disk". Shouldn't that be "will NOT propagate"?  Not it is correct. Both `r+` and `w+` open the file in read-write mode. `r` is read-only. The difference between `r+` and `w+` is that `w+` will delete any existing file before creating a new one from scratch. __eou__	User

TP	@xuewei4d will have a look thanks. Another thing we could try to address during your GSoC is the ability to add a `partial_fit` function for incremental / out-of-core fitting of (classical) GMM, for instance http://arxiv.org/abs/0712.4273. Of course the priority is fixing issues on the current code base. I can currently familiarizing myself with this part of the code base that I don't know well enough to comment on the open PRs / issues. __eou__	User

TP	Thanks @ogrisel . I just finished the derivations for VBGMM. __eou__	User

TP	@xuewei4d Thanks!! I just updated the same :) @amueller thanks for the feedback will update my blog post accordingly :) I think Terri should merge the PR then only it will refresh :) __eou__	User

TP	great, but the webpage http://terri.toybox.ca/python-soc/ seems not updated yet. __eou__	User

TP	@ogrisel Would the partial_fit version be EM style, or SGD? I think EM style is "easier" from a convergence perspective but I am not sure how you would do it in minibatch fashion Though I think David Cournapeau had some good papers on it he showed at PyCon I have the links somewhere __eou__	User

TP	I think we should focus on fixing what we have before implementing new algorithms __eou__	User

TP	I agree with @amueller.  May I ask what is EM style @kastnerkyle ? __eou__	User

TP	@xuewei4d I think he meant that in order to do partial_fit (update model rather than retrain it) you can just do several Expectation Maximization iterations from the point you've stopped at previously Though I don't get SGD part of it. I'm not sure of how severe expenses of a single EM iteration are, but I don't think they're so huge that we can't afford even a minibatch iteration __eou__	User

TP	Expectation-maximization style instead of gradient descent by minibatches. I am weaker in EM than SGD approaches, but I have done SGD style learning for GMMs in the very recent past. This is why I asked - trying to gauge what I would need to read :) Also, +1 to @amueller comment. Nothing can really happen until there is something that is well documented and understood by several people is there to experiment with/on __eou__	User

TP	there was once a PR that summarized all classifiers, I think. Does anyone know where that went? __eou__	User

TP	Hey @amueller Would it not be better to have `model_selection/validation.py` instead of `validate.py`? __eou__	User
TP	yeah sounds good__eou__	Agent
TP	thanks!__eou__	User

TP	@kastnerkyle  I was thinking of online EM as described in http://arxiv.org/abs/0712.4273, but if you have good references for SGD for GMM, that's interesting too. I agree fixing existing stuff is the priority over implementing new incremental solvers but I also think that incremental solvers would make GMMs more practically useful so it would be good to review the literature on that topic. __eou__	User

TP	You know me - partial fit for all the things! But this link looks good - I will read up once I get spare time. __eou__	User

TP	@xuewei4d how different are your derivations from http://scikit-learn.org/dev/modules/dp-derivation.html ? David recommended to read http://leon.bottou.org/publications/pdf/online-1998.pdf as an intro to some of the concept of the online EM paper. __eou__	User

TP	lol. Actually, the doc that the link points is for DPGMM, not VBGMM I am trying to figure out that Blei's paper for DPGMM now. @ogrisel __eou__	User

TP	does anyone have opinions on the heterogeneous feature union interface? 3886 #3886 ? __eou__	User

TP	hi Andy __eou__	User

TP	I also had thought about what you just proposed `(estimator, column_name, weight)` but I think it's not self-documenting enough __eou__	User

TP	and also it makes it hard if you don't need weights. Pass explicit `None`s? ugly __eou__	User
TP	well you could just leave them out, that could be supported.__eou__	Agent
TP	but still ugly__eou__	Agent

TP	I'd something like `dict(name='date', estimator=DateExtractor(), column='timestamp', weight=0.5)` but without a dict namedtuple maybe? I never used them, let me check __eou__	User

TP	it'd be nice to also support `('date', DateExtractor(), column='timestamp')`, it'd be almost like a pipeline with optional arguments is that even doable in Python? __eou__	User

TP	with namedtuples right? but it would be super cumbersome for the user because they need to import this particular named tuple class __eou__	User

TP	if only there existed anonymous namedtuples like ad-hoc, you just write (a, b, d='something'). Something like a function call argument list, but without the function __eou__	User

TP	I think if we want a default argument for weight, we need to define a custom class. But for the user that doesn't look much different from a namedtuple how do you mean anonymous? hehe yeah that's not possible in Python because () calls the tuple constructor I guess we could monkey-patch it :P the other option would be a syntax more like the make_stuff helpers but that would require kwargs __eou__	User

TP	I feel dirty just having this conversation __eou__	User
TP	haha__eou__	Agent

TP	ColumnTransformer(some_name=(CountVectorizer(), column_name, weight), some_name2=(OneHotEncoder(), column_name, weight)) still wouldn't document what the weight is, though I think I'm going with ColumnTransformer({'some_name': (CountVectorizer(), column_name), 'some_name2': (OneHotEncoder(), column_name)}, weights=[weight1, weight2]) err  ColumnTransformer({'some_name': (CountVectorizer(), column_name), 'some_name2': (OneHotEncoder(), column_name)}, weights={'some_name':weight1, 'some_name2':weight2}) __eou__	User

TP	I wish they could all be grouped in the same place __eou__	User

TP	a quick question. Where should I add deprecation warnings in GMM? Is that good adding warning  just after ```class GMM```, (not within any function) __eou__	User
TP	this is a mix of grouping by row and grouping by column__eou__	Agent

TP	the other extreme would be ColumnTransformer(names=['a', 'b', 'c'], estimators=[CountVectorizer()] * 3, columns=['title', 'content', 'comments'], weights=[1, 1, 2]) hey, what if for ColumnTransformer we drop the names completely and use the column as the name? __eou__	User

TP	then you can't grid-search if you have multiple transformers on the same column hm if we change the data structure of how we store the transformers from FeatureUnion, we'll have a lot of code duplication :-/ __eou__	User
TP	correct__eou__	Agent
TP	@xuewei4d maybe in `__init__`?__eou__	Agent
TP	I can't find any examples__eou__	Agent
TP	You can look at the rename of LDA to LinearDiscriminantAnalysis. well actually there it was in the file. for GMM the init would be good__eou__	User

TP	that's how Ward was deprecated looks god good https://github.com/scikit-learn/scikit-learn/pull/4370/files#diff-d7365adec1b76c4ff63051e4d1dd32b0L932 why? __eou__	User

TP	hm if we use dicts we also need to sort them every time we use them lol __eou__	User

TP	updated #3886 because dictionaries have undefined sorting, and if we iterate over them in transform we might get them in a different order then in fit i.e. the features would be shuffled __eou__	User

TP	but aren't they ever only accessed by key? ahhh, I know what you mean now __eou__	User

TP	yes, this is probably the reason why Pipeline and FeatureUnion aren't {name: Estimator()} dicts, but lists of tuples sorting isn't good, we really want to keep the order the user gives. Otherwise there'll be a world of confusion Now, the question is:  `ColumnTransformer([('name', (Est(), 'col')), ...]), transformer_weights=...)` vs. `ColumnTransformer([('name', Est(), 'col'), ...]), transformer_weights=...)` __eou__	User

TP	I think you can use OrderedDict from collections also. I like the flat one personally __eou__	User

TP	@kastnerkyle yes, but then the user needs to do `ColumnTransformer(OrderedDict(...))` __eou__	User

TP	Was thinking you could do d= OrderedDict(); d.items = user_d.items internally, though I have no idea if this would work. Probably more trouble than it is worth maybe sorting is just as easy. But I like the flat list version better er list of tuple __eou__	User

TP	Pipeline is not a dict because it has a sequence, FeatureUnion is not a dict becaues I'm stupid. Look at the issue, I went with dict. why do you think sorting isn't good? because people don't know which indices correspond to what? damn I just spend two hours on that. __eou__	User

TP	Well if you put it that way __eou__	User

TP	It's not a lot different from classes_ being sorted. Which also is a bit confusing for the user. But it is cleaner. It would be good to support OrderedDicts if e user so wishes __eou__	User

TP	@xuewei4d wouldn't you happen to have a PDF version of PRML? I am working at home today and I left my hardcover copy in our lab. I don't remember... __eou__	User

TP	 Yes, I have __eou__	User

TP	https://www.dropbox.com/s/7u13hvokr1lh2fa/Pattern%20Recognition%20and%20Machine%20Learning.pdf?dl=0 __eou__	User
TP	thanks :)__eou__	Agent

TP	:smile: Let me know if there is any problem in the derivation draft. __eou__	User

TP	yeah sorry, I am slow to review your work. I have been busy helping a colleague with a nips submission this week. The deadline is tonight. I hope to be more responsive next week. __eou__	User

TP	@rvraghav93 any update on the generalized CV? It would really be great if you'd report more regularly so we can follow what is happening. __eou__	User

TP	Sory sory! :( will update that soon...!! I am unable to pass all the tests :/ BTW can I go ahead and add the exceptions module or should I wait for a reply from Lars (#4309)? __eou__	User

TP	even if tests are failing, you can still push __eou__	User
TP	Okay :)__eou__	Agent
TP	I am not saying "work harder" but "communicate more" ;)__eou__	User
TP	making tests pass is not always easy, and it is hard for me or @vene to help if we don't see the code__eou__	User
TP	do you need the exceptions module for the cv pr?__eou__	User

TP	Yes sure... apologies for the lack of regular communications :) and yea it would be cleaner to put those in exceptions... coz we are already duplicating a lot of code it would be great if we could group those together into the exceptions module... (not a big issue though) __eou__	User
TP	feel free to do it__eou__	Agent
TP	Thanks :)__eou__	User

TP	@vene @ogrisel do you remember the estimator summary PR? I think it was by @mblondel ? is there a more readable alternative to mgrid with complex numbers? this seems like the most horrifying hack. but how do I solve my one-hot-encoding problem now? Should I change OneHotEncoder to work with strings? __eou__	User
TP	That really should work__eou__	Agent
TP	I can already hear gael shouting ^^__eou__	User

TP	maybe I'm imagining things... __eou__	User

TP	@amueller what are you referencing re: mgrid? is it in any scikit-learn code? __eou__	User

TP	I just noticed that the problem setting in the current derivation of DPMM is simpler than mine. You could find it in my new blog post http://xuewei4d.github.io/2015/06/05/gsoc-week2-vbgmm-and-gmm-api.html. @amueller @ogrisel @lesteve. __eou__	User

TP	@vene yes, in the examples. np.mgrid[1:2:10j] means "do ten steps from 1 to 2" it is a multi-dimensional version of linspace without the j it means "step" so mgrid implements a multi-dimensional arange if the third argument is real, and a multi-dimensional linspace if it is complex. not sure if :rage4: or :trollface: what should the allowed input types for OneHotEncoder be? currently it is integer arrays __eou__	User

TP	So there is no function for this without a slice api? __eou__	User

TP	Thanks for the wrap-up @xuewei4d! I will have a deeper look at it next week. __eou__	User
TP	@vene that is what I was asking__eou__	Agent
TP	and the slicing API with complex number that do not mean steps seems a really awkward construct__eou__	Agent
TP	isinstance(sparse.csr_matrix([[1, 2]]), containers.Mapping) == True. # that is all__eou__	Agent
TP	gitter is weird__eou__	Agent
TP	will come back to you to discuss that on the relevant github issues next week__eou__	User
TP	BTW thanks for the effort to typeset the derivation in latex with clear formatting, this is well appreciated.__eou__	User

TP	@xuewei4d about the API issues, we need to make sure that the score API (in particular the shape) is not conflicting with the score method of other models in scikit-learn in particular models that are not density estimators. I think we would have a `density` or `log_density` methods to have more explicit names and avoid conflicts __eou__	User

TP	 isinstance(sparse.csr_matrix([[1, 3], [5, 3]]), containers.Mapping) == True. # that is all __eou__	User

TP	@xuewei4d indeed, that is pretty cool hum, if I want to one-hot encode ['paris', 'paris', 'london', 'new york'] I have to use ``LabelBinarizer``, not ``OneHotEncoder``. That seems odd. And if it is ['paris', 'paris', 'london'] I only get a single column.... __eou__	User

TP	@amueller the mapping thing is horrifying __eou__	User
TP	more or less horrifying than the complex slicing?__eou__	Agent

TP	Ok so I want to do an example for the heterogeneous feature union / ColumnTransformer that processes `` ['paris', 'paris', 'london', 'new york'] `` as a categorical variable. Is this really not possible with scikit-learn? __eou__	User

TP	Less. CSR is naturally a dict-like structure __eou__	User
TP	;)__eou__	Agent

TP	We could code a gael-bot for gutter __eou__	User

TP	Anything that can be turned into a set and indexed? __eou__	User
TP	sorry, I didn't get that__eou__	Agent

TP	list of strings should work too __eou__	User

TP	and how about lists of lists of strings? ;) __eou__	User

TP	@amueller what is the use case? I'd just use CountVectorizer without a tokenizer there :D In your example above, `['paris', 'paris', 'london', 'new york']` is the entire column (4 values) or a single value? if the former, OneHotEncoder would work, right? @amueller try this: ``` x = [['paris', 'paris', 'london'], ['london', 'nyc']] CountVectorizer(analyzer=lambda x: x).fit_transform(x).todense() ``` I think OneHotEncoder should support it what I meant yesterday is that it can support anything that's an object as long as you can assign integers to different objects you encounter __eou__	User

TP	the entire column four samples it is just one categorical value OneHotEncoder only works on integers true. It seems odd to me, though. Is that the interface we want? and I feel like I also want to support data that has one column that is city and one column that is color. But maybe ColumnTransformer is for that ok but what is the type / shape of X? Would you support this for X being a list of arbitrary objects? Or for X being an 2d array of arbitrary objects? __eou__	User

TP	LabelBinarizer kind of works, but is not really made for this usecase and doesn't really have a transformer interface __eou__	User

TP	this works: ``` In [12]:  x = [['paris'], ['paris'], ['london'], ['new york']] CountVectorizer(analyzer=lambda x: x).fit_transform(x).toarray()  Out[12]: array([[0, 0, 1],        [0, 0, 1],        [1, 0, 0],        [0, 1, 0]]) ``` __eou__	User

TP	I'd say list (or 1d array) of arbitrary objects why 2d array? for city and color? once you have more than one field of categorical variables, you could encode them as dict(city='new york', color='red') in which case dictvectorizer works, right? or encode them as dict(cities=['nyc', 'paris'], colors=['red', 'yellow']) and then ColumnTransformer + OneHotEncoder should work __eou__	User

TP	for your example you could do ``` X = {'post_content': ["long string 1", "long string 2"...],         'metadata': [{location='nyc', category='misc'}, {location='paris', category='programming'}...] ``` and ColumnTransformer(dict(post_content=CountVectorizer(), metadata=DictVectorizer())) __eou__	User

TP	I feel the nested dicts are ugly. I just updated the example but something is wrong :-/ ah how do you do the code highlighting again? right __eou__	User

TP	```python x = ['paris', 'paris', 'london', 'new york'] CountVectorizer(analyzer=lambda x: [x]).fit_transform(x).toarray() ``` is what I want __eou__	User

TP	backticks __eou__	User

TP	that works, but the analyzer lambda is super opaque idea: why not deprecate DictVectorizer and create a CategoricalVectorizer with the same interface except if you pass it a 2d array, it would treat each column as a different implicit key so then you can do `['paris', 'paris', 'new york', 'london']` as well as `[['paris, 'red'], ['paris', 'green'], ...]` or that __eou__	User

TP	hm... maybe just add that? I added a somewhat interesting example to the pr __eou__	User
TP	yes but then it's no longer a **dict** vectorizer :)__eou__	Agent
TP	I meant just adding a different class with a different interface__eou__	User

TP	IThe example is very nice __eou__	User

TP	thanks :) __eou__	User

TP	Why do we have the `_check_cv` function? It is called by `check_cv` which does nothing additionally? Am I missing something? https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cross_validation.py#L1431 __eou__	User

TP	It should be removed... I opened a PR #4829 for that... Please review @vene @amueller ! :) and @ogrisel __eou__	User

TP	@amueller can `sklearn.utils.testing.ignore_warnings` ignore *specific types of warnings*? the code looks odd __eou__	User

TP	(this is in reference to #4824) __eou__	User

TP	so have we decided to just never use the words "extreme learning machine" in docstrings/documentation? __eou__	User

TP	That is my opinion. Though it is weird cause that is what people call it. So I am of 2 minds at once But the paper citation stuff is really pretty shady so that is tough too. __eou__	User

TP	Do we care about being googlable for this? __eou__	User

TP	Just a general question... Is there a way to track changes in a particular module and get notified if that gets merged? (w.r.t #4294) (ref [comment](https://github.com/scikit-learn/scikit-learn/pull/4829#issuecomment-109952865)) Ah its easier to just check the history of the file ;) sorry for the noise! __eou__	User

TP	@kastnerkyle btw I'm going to seriously start on the tutorials now, I was pretty busy with other things before @vene I think we should use the word Extreme Learning Machine in the narrative and maybe as a remark in the docstrings, but not as the name. Like "This can be used to implement so-called Extreme Learning Machines" or something like that __eou__	User

TP	That's probably wise. __eou__	User

TP	Also, scikit-learn should have the power to change the way researchers call algorithms. Maybe our naming decision will fix this mess a bit. __eou__	User

TP	I've seen researchers calling svms SMO because that's what the solver class is called in Weka __eou__	User

TP	omg __eou__	User

TP	I remember Fabian had started wrapping liblinear's CD solver. Now that SAG is joining the family of solvers available for logistic regression, we might be able to simplify the codebase by picking that up assuming it matches the interface of scipy optimizers https://github.com/fabianp/pytron/ (question triggered by @amueller's [comment](https://github.com/scikit-learn/scikit-learn/pull/4738#issuecomment-108592200)) __eou__	User

TP	@kastnerkyle when do you have time to discuss the tutorial? Do you think we should put exercises after each sections or do "morning lecture, morning exercises, afternoon lecture, afternoon exercises"? Maybe multiple short breaks would be good? four hours is such a long time. Maybe a four 45 blocks with 15 minutes exercises? Not sure. __eou__	User

TP	Hi, I am almost done the derivation draft. I fixed some typos and errors, and completed the DP part. What is left is the lower bound and predictive distribution for   two cases. I think I could finish it tomorrow. The difference from current derivation includes, the initial parameters and the updating functions for Wishart distribution. The rest of them are the same. I hope I could find out the reason of the latter. With regards to the new names of DPGMM and VBGMM, I think these two names are not suitable, just like someone calls SVM as SMO. Actually, the models are Bayesian GMM, Dirichlet Process Bayesian GMM (DPGMM is often used) respectively. Both of them are solved by variational inference. In other words, VBGMM is not a good name. The new names, I think, should have the meaning of 'Bayesian GMM solved by VB', 'DP(B)GMM solved by VB'. __eou__	User

TP	@tw991 the two PRs with the mlp code are #3939 and #3204. #3204 contains adagrad @xuewei4d I agree wrt VB. It should be "bayesian GMM" or something similar __eou__	User

TP	@amueller I have time. Just started my internship today so I should be able to take a look at it now :) I think 45 min blocks + excercises is good but we should also budget for questions *during*. And hopefully we will not be only 2 who can help when questions arise - maybe some advanced sklearn users in audience can help too Though I am expecting more question during advanced section for sure. We could do that in 30/30 blocks if necessary. Also the first day tutorial might inform us of mods to make for second day __eou__	User

TP	@kastnerkyle cool. I basically just copied the 2013 as a start, but I want to do some reorganization. I'm not entirely certain about the structure and the cluster stuff I am now going through the lectures again and I'll draft a new toc tomorrow. I think having a bit more intro about what ML is in the beginning would be good, and I'll try to recycle some of my diagrams Actually the current notebooks are pretty close to the toc in the proposal, with a bit of reorganization @kastnerkyle is there any part you'd like to work on particularly so that we don't conflict? Also, when do you have time or a beer in the city? I think I'd like to work on the intro for now. __eou__	User

TP	I am planning right now to pop in over the weekend. So if you have a day or time which is good I am game. I am probably stronger talking about PCA, GP and friends than random forests, SVM and the like if we are gonna go into theoretical aspects. But if not then I have no preference. Also I am terrible at the text preprocessor stuff. But I think it is important DictVectorizer etc. Might be a crucial thing for examples. Raw text -> classification ala your troll detection model __eou__	User

TP	I don't think we will do much theory, and I don't think we should include GPs. I want to talk a bit more about details about some of the commonly used supervised models. I think Sunday would work for me. __eou__	User

TP	OK cool. I will let you know later in the week what is up but I can try to be in NY on Sunday. What part are you in? __eou__	User

TP	I'm in east village but I'm pretty free on Sunday, so I could travel a bit __eou__	User

TP	That sounds good to me. I will email you and set up something. I will be the country kid in the Big Apple ;) __eou__	User

TP	@kastnerkyle one thing  I noticed about the notebooks is that they are pretty text-heavy. I'm not sure how I feel about it. I guess it depends on whether you think of it more as a presentation or as an "in your own time" thing. While someone is speaking, the text seems more distracting than helpful. I'm not sure about it, though. __eou__	User

TP	I agree. The text is useful "after the fact". But our presentation should basically be the content contained in that text IMO, so we can focus on the pretty pictures We could make a stripped down version of all, with just images. Or just scroll so that images are the focus __eou__	User

TP	So, you suggest do develop one with text for people to work though it at home afterwards, and drop the text for the presentation? Which one do we distribute then for the tutorial? The no-text one? __eou__	User

TP	do we have a real-world dataset that uses dict-vectorizer? Or any real-world dataset that has categorical features encoded as strings? Btw, what is the recommended method to deal with that? Pandas? Or converting to dict and DictVectorizer? __eou__	User

TP	I would say it depends how much we reuse old tutorials. If it is all new (unlikely) generating that much content will be tough - in that case I would go with pics only. But if we mostly reuse I think we should do text + images in main notebooks we put in tutorial repo, then just have a separate folder of "presentation ready" which is basically titles, pictures and key math or whatever else is necessary to talk on. Or vice-versa (text versions in a separate folder called like "self_teaching") to avoid confusion __eou__	User
TP	sounds good__eou__	Agent

TP	I'd go with the "self teaching" approach __eou__	User

TP	yeah. that way it takes extra effort to open the wrong content :) though I am sure it will happen anyways... __eou__	User

TP	@kastnerkyle I don't see where cross-validation is introduced in the notebooks... hum... __eou__	User

TP	``` from sklearn.utils.extmath import fast_dot from sklearn.exceptions import EfficiencyWarning import warnings warnings.simplefilter('always', EfficiencyWarning) import numpy as np fast_dot(np.array([1, 2, 3]), np.array([4, 5, 6])) ``` This code which is supposed to raise a warning doesn't raise one!  However this one does -_- ``` from sklearn.utils.extmath import _fast_dot from sklearn.exceptions import EfficiencyWarning import warnings warnings.simplefilter('always', EfficiencyWarning) import numpy as np _fast_dot(np.array([1, 2, 3]), np.array([4, 5, 6])) ```  Ref: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L151 __eou__	User

TP	The GMM model in http://scikit-learn.org/stable/modules/dp-derivation.html is not standard. The distribution of \mu_k  does not have variance depending on the Gamma distribution. It is just a constant. I think that is why the current implementation is kind of weird.... __eou__	User

TP	The `NonBLASDotWarning` (to be converted to `EfficiencyWarning`) doesn't seem to work as intended! __eou__	User

TP	how do you mean? __eou__	User

TP	Please check the above code sample :) The `EfficiencyWarning` in the above code is supposed to be `NonBLASDotWarning`... __eou__	User

TP	It is being disabled at [utils/validation.py](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py#L42).. maybe it has something to do with it? Nope I tried removing that line and this issue still persists! __eou__	User

TP	I am not very good with working with the warnings registry. __eou__	User

TP	hmm okay! Thanks for looking into it :) Does this issue seem worthy enough  for a new issue on github? __eou__	User
TP	probably not.__eou__	Agent
TP	Maybe ping @ogrisel ?__eou__	Agent
TP	Okay! meanwhile I'll also investigate :)__eou__	User
TP	great!__eou__	Agent

TP	I noticed that gmm code is a little messy. Some functions should be decomposed, and the way of initializing parameters is not completely implemented. According to the derivation draft, different estimators of responsibility, weight, mean, covariance could be combined together to represent GMM, BGMM, DPGMM. I think, in terms of maintainability,  it is good to create bunches of estimators with inheritance and then combine them on demand in  just use one class to represent all three models with four types of covariance. Is that a good idea? __eou__	User

TP	usually we try to restrict inheritance to mixins, so that there is no complex overloading of functions. If you can implement it by providing mixins for the four covariance types, that would be great. I'm not sure how well these factorize, though. @kastnerkyle it would be great if you could give me your feedback on my redoing of the notebooks. __eou__	User

TP	OK looking now. Key question - does StarCluster still work? __eou__	User

TP	I was under the impression that it broke at some point a ways back but I am not super informed on these things The hashing stuff / out-of-core stuff seems like maybe we should do it *before* parallel. It is quite nice I think. Now onto rendered_notebooks I think we should focus on py 3.4 compat - seeing at least a few print blah in there. I can work on that if you want? Also... describing all the different types of sparse matrices may not be pleasant in the intro. Maybe leave them there but gloss over subtleties of different representations? If you know guts I am all for it - but I am definitely a high-level sparse matrix user. I use what works normally and check stack overflow __eou__	User

TP	Should we add an image telling what petal and sepal are? Or just say what it is. For sure *somebody* will ask  :) __eou__	User

TP	@xuewei4d do you think changing the handling of the \mu_k will have a significant impact on the runtime performance? Besides Bishop's PRML do you have another "standard" formulation / derivation of the Variational GMM in mind? +1 for not introducing a deep hierarchy of estimators for GMMs but ok for using mixin class and possibly a _BaseGMM abstract base class if that can help factoring too redundant code. @kastnerkyle I have not tried to launch a StarCluster instance in a long time but the project is still active. The configuration might have changed a bit (e.g. AMIs ids) so it should be possible to adapt it to make it run again. However I would not do that during a tutorial if you are not a regular user yourself. You can just mention that it exists in passing. it works here __eou__	User

TP	Is it just me or this page takes forever to load :@ - I've been trying since yesterday ;( https://travis-ci.org/scikit-learn/scikit-learn/jobs/66302262 __eou__	User

TP	Thanks a lot for checking :) __eou__	User
TP	try https://s3.amazonaws.com/archive.travis-ci.org/jobs/66302262/log.txt__eou__	Agent
TP	if it does not work, I can paste-bin it for you__eou__	Agent

TP	Thankss a ton for this txt log link! Will be really useful for me! :D and yea it does work :) __eou__	User

TP	Hey @ogrisel BTW could you check why  ``` from sklearn.utils.extmath import fast_dot from sklearn.exceptions import NonBLASDotWarning import warnings warnings.simplefilter('always', NonBLASDotWarning) import numpy as np fast_dot(np.array([1, 2, 3]), np.array([4, 5, 6])) ``` doesn't raise the intended warning? Am sure I must be missing something simple here :| __eou__	User

TP	@rvraghav93 which PR number is this again? __eou__	User
TP	#4826 :)__eou__	Agent
TP	Please do take a look if you are able to find time :)__eou__	Agent

TP	this warning is only raised for old versions of numpy < 1.7 if np_version < (1, 7, 2) and _have_blas_gemm() __eou__	User
TP	Ahhh... Thanks!!__eou__	Agent
TP	thanks__eou__	User
TP	Indeed we need to escape this.__eou__	User

TP	@ogrisel @amueller pyspark now deserves a mention along with https://pypi.python.org/pypi/sparkit-learn/0.1 I think. Would be of interest for many of the same people who care about StarCluster __eou__	User

TP	also useful, although not directly related: - http://yelp.github.io/MOE/ - http://pythonhosted.org/airflow/ I have not yet found to the time to test any of those __eou__	User

TP	@ogrisel The distinction is that whether \mu_k depends on \Lambda_k. PRML and MLAPP(P750) models that dependence. In the literature, some work has that ,some work does not. I think those are two kinds of modeling. Modeling the dependence would give more accurate approximation. The exercise 10.20 in PRML says if you have many data, the pdf of q(\mu, \Lambda) will become a delta function, which recover the classic EM algorithm. But the pdf of current variational distribution will not, since the variance of \mu is fixed.  For refactoring, I would intend to build a BaseGaussianMixtureModel and with different estimators for different variables. For example, there are 8 covariance estimators. full, diag, spherical, tied, times variational or not. Then GaussianMixtureModel could be implemented by inheriting from base class and combined with one of 8 estimators for covariance variables. I don't know estimators should be taken as a mixin class. I would prefer to let GMM includes the estimators. __eou__	User

TP	I would rather keep the covariance choice as an hyper-parameter of the class instead of dedicating it a sub-class. side remark, I think for the full covariance type, it might be interesting to experiment with a shrinkage estimator such as ledoit-wolf, at least in the Maximum likelihood / EM formulation. If that can improve the cross-validated log-likelihood we might consider it for inclusion. http://scikit-learn.org/stable/modules/generated/sklearn.covariance.ledoit_wolf.html __eou__	User

TP	so the GMM would be like ```class GMM(FullCovMixin, BaseGMM)```,```class GMM(DiagCovMixin, BaseGMM)``` ? OK. I will try to build some prototypes :) __eou__	User

TP	@xuewei4d please feel free to open a [WIP] PR with that refactoring with mixin / base classes so that we can have a discussion on concrete code. Also private classes should start with a `_`. __eou__	User

TP	So what about the approximation? Do you like to consider the dependence? OK. __eou__	User

TP	@kastnerkyle the "rendered notebooks" are still the old ones. I haven't redone them. I agree we should do more out-of-core stuff before doing clusters. @kastnerkyle I am currently somewhere around 3. and 4. __eou__	User

TP	http://scikit-learn.org/stable/modules/generated/sklearn.covariance.ledoit_wolf.html The section 'notes' seems has a little problem. '+' sign is interpreted as a list mark. __eou__	User

TP	> So what about the approximation? Do you like to consider the dependence?  It depends how the runtime, memory usage and the  complexity of the code base will evolve if we do so :) I am still clear about the details. Reading PRML at the moment. It would be great if we could have an example that demonstrates that the VBGMM asymptotically joins the solution of the MLE estimate on some toy dataset leveraging ledoit wolf shrinkage is not a priority as we already have the `min_covar` hack to regularize the covariance estimation. But I think we should keep it in mind and it would be great to explore the importance of covariance regularization, especially on high dim data __eou__	User

TP	Great. I would continue to work on the PR #4802 __eou__	User

TP	I think we should also improve the examples to discuss model selections for GMMs. I started to run some experiments here: https://github.com/ogrisel/notebooks/blob/master/gmm/Model%20Selection%20for%20GMM.ipynb I have to go offline now, see you later. __eou__	User

TP	I tried some toy experiments with VBGMM before, but it did not work correctly. May I ask why would you like to do model selection for GMM? See you __eou__	User

TP	@amueller I noticed the other ones were shorter and swapped. Only got to 2.x but I can try to do some PRs tonight. __eou__	User

TP	@kastnerkyle cool :) I'm working on the intro to unsupervised currently @xuewei4d could you have a look at #4845 ? __eou__	User
TP	Gotcha.__eou__	Agent

TP	Where is the discussion about #4511? I mean on the ML. Thanks~ __eou__	User

TP	the conclusion was that we want to raise a value error that is deprecate it for now and raise a value error in the future @xuewei4d https://sourceforge.net/p/scikit-learn/mailman/scikit-learn-general/thread/20150501175859.GE1362450%40phare.normalesup.org/#msg34075913 __eou__	User

TP	@ogrisel what do you think about adding a ``shuffle`` option to cross_val_score and GridSearchCV (@jnothman or anyone else feel also free to chime in;) ? I feel it would be useful for cv=integer __eou__	User

TP	Hey guys, does anyone else have issues with big memory usage because of Python dicts going crazy with high dimentional data for text vectorizers? I was gonna implement it as a Trie, but it would be a big change and maybe change external API unless we do a dict abstraction i went from 50gb used to 200mb so no need to use hashingvect yeah looks like it's not happening __eou__	User

TP	another tangentially related question... since the cost of creating a vectorizer with millions of features is so large in terms of memory, there is a need for preemptive feature selection i mean for me... things like forward selection, mcmc based, etc. I am not sure if it belongs in scikit-learn this is to avoid generating the large matrix with unnecessary features when they will likely be discarded anyway __eou__	User

TP	@lqdc I like the idea of the Trie - even if we just called it something else (in case where it can't match external API) it sounds insanely useful. As for pre-emptive feature selection it would be cool if this can be done in a general way, but all the tricks I know are domain specific. Any ideas in that direction seem nice, since it is a real-world issue __eou__	User

TP	@lqdc there is some very simple feature selection based in min_df and max_df. But that needs to built the whole dictionary first. how large is your dictionary? The idea of a Trie came up before. So you actually implemented it? __eou__	User

TP	some discussion on ties here: https://github.com/scikit-learn/scikit-learn/issues/2639 __eou__	User

TP	and yes, the feature selection would be for avoiding building the whole dict which perhaps wouldn't be a problem in the first place if we used a trie for medium-sized datasets __eou__	User

TP	I don't remember the size of the dictionary exactly but it was taking up 50ish gb __eou__	User

TP	what do you mean by "looks like it's not happening"? the question is a bit how complex the code is and the speed and memory compared to C++ dicts and python dicts __eou__	User

TP	in the PR, the memory footprint was 1/3 of CountVectorizer. You reported 1/200 above, right? __eou__	User
TP	yeah__eou__	Agent
TP	his earlier tests__eou__	Agent

TP	      CountVectorizer(): 94MB;     CountVectorizer(ngram_range=(1,2): 666MB;     MarisaCountVectorizer(): 1.2MB;     MarisaCountVectorizer(ngram_range=(1,2)): 13.3MB; also he was doing it in a way that doesn't help my casse. basically  he made the python dict fist and then populated a trie with the dict then replaced the dict with the trie the use case he has is build the vocab on a beefy machine then when you actually want to use it, unpickle and use __eou__	User

TP	I was also using MARISA trie __eou__	User

TP	but building it from the streaming input. anyway, as @larsmans put it in that thread, there is understandable hesitation of merging a large c++ dependency that no one fully understands besides people who wrote the trie I assume. __eou__	User

TP	well but allowing it as an optional replacement might work __eou__	User

TP	Hello, I have a question about pull requesting policy. I have a PR and I made some changes, what I should to do with new changes:  add new commit to PR, amend initial commit? I used to use amended commits, it keeps history look nice, but PR discussions seems ambiguous because of `outdated diffs`. __eou__	User
TP	Either is fine__eou__	Agent
TP	usually we ask to squash commits before merging, so if you always amend you don't have to do that__eou__	Agent
TP	the PR discussions are not handled in a great way by github, no matter which method you use.__eou__	Agent

TP	Hello, I'm not sure that gitter is proper place to ask this question, if so please point me the right place.  I have a problem with building the whole project, when I do `make` i get this tests result: `FAILED (SKIP=14, errors=2)`  ``` ERROR: sklearn.tests.test_cross_validation.test_cross_val_score_pandas ... IndexError: arrays used as indices must be of integer (or boolean) type ```  pandas.__version__ = 0.13.1 numpy.__version__ = 1.6.2  Thank you __eou__	User

TP	Hey @mr0re1 ... do you have a particular PR number for which this test fails... or incase you haven't already, could you just raise one so we can all see the modified code that caused this test failure? :) __eou__	User

TP	And btw this is indeed the place to ask such questions :) __eou__	User

TP	Tests fail in master branch. There is no my changes. It seems like problem is in my environment. __eou__	User

TP	@mr0re1 the problem might be in sklearn. Testing doesn't cover all possible combinations of platforms and third-party libraries. I suggest you create an issue and put all debug information about failed testcases __eou__	User

TP	@mr0re1, that test behaves differently whether pandas is installed or not. I assume the error occurs on the Pandas-only code path. Pandas is known to break API often. This is a problem with the test, in my opinion, but your installation seems to be mostly functional. The test might pass if you update pandas. Still, could you raise an issue for this on github? It's something I think we should fix.  What is the other error, by the way? You said you get 2 errors but only showed us one. @rvraghav93, since this is a cross-validation indexing issue, could you look into it? If you use anaconda you can easily set up a virtual environment with a specific version of pandas. also, @rvraghav93, when you said PR number above, did you mean opening a Github issue? Ah I see @mr0re1 what test do you get the error for, I mean? __eou__	User

TP	Sure I'll look into it :) __eou__	User

TP	@vene, will open an issue, the second error is the same. __eou__	User

TP	Since @mr0re1 was asking about raising a PR a few chats above, I assumed he had been working on something which had failed the tests ;) So I suggested that he raise a PR... not that the failure is in master... that is moot :P *now :D __eou__	User

TP	This [link](http://eprints.pascal-network.org/archive/00006964/01/vedaldi10.pdf) which is a reference to additive chi2 sampler seems to be down! There was an issue regarding broken links right? #4344 This [comment](https://github.com/scikit-learn/scikit-learn/issues/4344#issuecomment-77744354) in particular! __eou__	User

TP	chatting from the gitter irc bridge! __eou__	User

TP	:) __eou__	User

TP	Its cool!! I am planning to resume work on the irc-gitter bridging thing that we discussed long back (when gael wasn't happy with us using gitter over irc) when I find time! i.e apart from my regular work ;) __eou__	User

TP	I actually really like gitter's interface, what were Gael's concerns? __eou__	User

TP	That the discussions were getting diluted at multiple platforms... (gitter, irc, ml, github etc...) __eou__	User
TP	good point, but wasn't IRC kind of dead?__eou__	Agent
TP	https://github.com/scikit-learn/scikit-learn/pull/4248#issuecomment-74413361__eou__	User
TP	yes!! but its active during sprints I think..__eou__	User

TP	i like the auto-links to issues,  and the activity pane in gitter __eou__	User

TP	And history, code formatting, markdown too! ;) Speaking of activity pane :p @vene could you take a look at #4860 its a very minor PR! :) __eou__	User

TP	I left a comment there __eou__	User

TP	@rvraghav93 what do you plan to write about in this week's blog post? __eou__	User

TP	Nested CV!! Is that okay? Hey BTW do u happen to have a link of the mailing list thread that u had mentioned previously? I am unable to find it :/ And I'll resume work on cross validation as soon as I am done with fit reset / partial fit tests... :) Okay and yes sure!! Okay!! Sure I'll do that! :) __eou__	User

TP	@rvraghav93 the discussion was here. I think this thread has some very valuable insight about evaluation in machine learning. http://sourceforge.net/p/scikit-learn/mailman/message/34102242/ __eou__	User
TP	Thanks!!__eou__	Agent
TP	I would acknowledge @eickenberg and @satra and maybe even link to the thread too.__eou__	User
TP	The "extreme" example (where the dataset has two consistent but different biases) would make for a nice running example__eou__	User

TP	And I am 2 blog posts behind -_- nested CV was supposed to be last Sunday's ! For this sunday can I post about online learning / `partial_fit` support in sklearn and the usefulness of the proposed tests (in ensuring `partial_fit` behaves as expected)? __eou__	User

TP	That sounds good, I think you can do a smaller post about this mid-next week. This is more internal stuff, it's hard to write too much interesting stuff about it. Maybe talk about how useful it is to have common tests that enforce API conventions to future code as well __eou__	User
TP	Sure! :)__eou__	Agent
TP	@amueller what do you think?__eou__	User

TP	also @rvraghav93 could you put down a brief plan for what you think we can get done in the next 2-3 weeks? A bit of structure might be useful, and I think this is better than just going back to the original timeline, since things are always in flow :) __eou__	User

TP	Any comment on #4802? I mean in terms of class definition. Most of methods are not implemented yet, but I hope I am on the right track. __eou__	User

TP	btw irc is totally unused during sprints ^^ btw @rvraghav93 what is the status of the data-dependent cv? sorry I was out for the weekend it would be great if you could catch up on the blog posts. The IRC bridge is really not that important and I saw there was a lot of discussion on the assert helpers. are they needed for any of the gsoc PRs? __eou__	User

TP	I just wanted to finish it off as it was lying around for a long time :/ I'll resume work on it by tomorrow :) Planning to finish off my blog post, the helpers and tests by today :) __eou__	User

TP	hi there __eou__	User

TP	im using gridsearch to train a visual object detection pipeline with a few skimage transformers, pca and svm... since i acquired my larger dataset, gridsearch explodes in a very strange way <unconvertable> deep in python: http://nopaste.ghostdub.de/?1132 <unconvertable> and i cant quite make any sense of that :( ... would anyone happen to have an idea in which direction i could search? __eou__	User

TP	@Nebukadneza try with n_jobs=1 that should make it easier to debug __eou__	User

TP	with single threading it doesnt happen at all :( __eou__	User

TP	does all of it run through? it is maybe one specific parameter setting that does that __eou__	User
TP	yes, it looks like all of it runs__eou__	Agent

TP	trying to dig into the delayed and multiprocessing pool with ipdb now Oo but i just dont see where this format-string-fu even comes from or what its used for Oo __eou__	User

TP	@vene Thanks for that ML link! It was really useful!! __eou__	User

TP	TIL: don't try to make a learning curve example with a non-parametric model __eou__	User

TP	Why do you say so? :) __eou__	User

TP	because the training error doesn't go up with the number of samples __eou__	User
TP	Oh! TIL ^ :P__eou__	Agent

TP	Hey Andy,  Earlier you had given me the feedback for my 1st blogpost! Sorry for the delay in response...  > why do you use alpha when plotting points?  That makes it look a little faded and better... I think this was a suggestion by someone when I was working on silhoute plot example ;) Do you feel I should remove that?  > I feel this sentence is unclear: "Even when the model is optimized with the constrain of maximizing the score based upon the test set, there is still a chance of overfitting as the information about the test set can leak into the model and hence the model could be optimized for the test set alone." it would be more explicit to say the information leaks via the selection of hyperparameters.  Have updated so! Thanks!  > cross-validation does not entirely overcome this. Overfitting to cross-validation is harder than overfitting to a single test set, but it is still possible, which is why people do nested cross-validation.  Like Vlad said a few chats below yours, that was what I was getting at... I have reworded it to make it clear!  > grid.bestestimator.score(X_test, y_test) is also not great btw. You can just use grid.score  fixed :)  > talking about gamma=0 for the SVM is also a bit weird. This is an odd way that we used to select the default, which is 1. / n_features , I think.  I've fixed it to 0.5... which is (1 / n_features)  I am now working on the 2nd blog post (Nested CV) :) __eou__	User

TP	@amueller @vene I've also started a ML thread for my GSoC project to interact more! :) http://sourceforge.net/p/scikit-learn/mailman/message/34213648/ __eou__	User

TP	@ogrisel do you have a second? __eou__	User

TP	How scikit can help beginners? __eou__	User

TP	@BastinRobin http://scikit-learn.org/stable/tutorial/ __eou__	User

TP	@amueller yes, sorry I was on the phone __eou__	User

TP	> TIL: don't try to make a learning curve example with a non-parametric model > because the training error doesn't go up with the number of samples  this is actually an interesting example if you constrast it with the learning curve of a parametric model __eou__	User

TP	yeah, but maybe to advanced for an introductory course. never mind, I was confused by the timing of https://github.com/scikit-learn/scikit-learn/pull/4844, but I think the code is just overly complicated __eou__	User

TP	Anybody around? :) I have a few (probably lame) questions regarding nested CV! __eou__	User

TP	hi @rvraghav93 __eou__	User

TP	yeah I'm around I have a meeting in 10 minutes but I'll answer stuff here __eou__	User

TP	Why is nested CV not generically possible with our current API... I understand what needs to be done to make the iterators data indep. but I don't get why nested cv is difficult as Mathieu had commented in that issue description... `cross_val_score(GridSearchCV(est, ....))` would be enough right? __eou__	User

TP	this works currently if you use e.g. `cv=3` in the inner one __eou__	User
TP	if anyone has time, the timing on #4844 really confuses me btw.__eou__	Agent
TP	Slicing an euclidean distance into smaller bits makes the code run faster?!__eou__	Agent
TP	but if you need to pass a CV object that takes y, what do you set y to?__eou__	User
TP	(such a nesting bug with LOLO bit me really hard a few months ago)__eou__	User
TP	(:baby: :baby_bottle:)__eou__	User

TP	or even n_samples. If the size of your dataset is not divisible by the cv in the outer loop, the training sets in the inner loop will have different lengths __eou__	User
TP	exactly__eou__	Agent
TP	or LeaveOneLabelOut__eou__	Agent

TP	I can't even get my emoji right, I'm useless __eou__	User

TP	This is a general apology for all lame questions that may follow :P please bear with me! Ok so the inner loop does hyper param optimization using a separate cross validation and finds the best model which the outer loop uses to find the cross validated score... why should these two affect each other? ( Like apologized earlier, please bear with me :( ) __eou__	User

TP	don't even think that you have a GridSearch on the inside consider the simpler case with just a cross_val_score inside a cross_val_score hi @xuewei4d what do you mean by current code? if it's in the context of GMMs I think commenting in your PR would work well since most people who could answer are already following the PR also you could use `git blame` and ping the author(s) of the piece of code __eou__	User

TP	[offtopic - I think gitter supports only a subset of the emojis ;) ] __eou__	User

TP	[the bracket messed it up I think: :baby: :baby_bottle: ] __eou__	User
TP	ah :P__eou__	Agent
TP	BTW I still am in favor of `GaussianMixture` vs `GaussianMixtureModel`__eou__	User
TP	@rvraghav93 let's think about it__eou__	User
TP	say we have a dataset (X, y) with 100 samples and want to use `KFold`__eou__	User
TP	we need to fill in the blanks in the following__eou__	User
TP	__eou__	User
TP	__eou__	User

TP	If I need to discuss  the current code, should I open a new issue or add comments on my working PR which is addressing the issues? __eou__	User

TP	and if its not too much trouble could you give me a line or two of code that will help me clearly understand this? __eou__	User
TP	I mean the code of master branch__eou__	Agent
TP	ok.__eou__	Agent

TP	you can edit gitter chat messages :) __eou__	User

TP	``` clf = GridSearchCV(grid, cv=KFold(n_samples=?, n_folds=3)) cross_val_score(clf, X, y, cv=KFold(n_samples=?, n_folds=3)) ``` (ignore the fact that the parameter is currently called `n` and not `n_samples`) how would you fill the question marks? the one on the 2nd line is easy __eou__	User

TP	won't both be 100? :P ahhhh I get it ! yes.. sory.. this is too lame :|  thanks a lot :) __eou__	User

TP	no, because the inner `clf` (the GridSearchCV) only gets a fold each time if `len(X)` were `99`, you could set the first `?` to `66` I think __eou__	User

TP	but 1) this would be hacky 2) users shouldn't need to do this kind of error-prone math 3) this doesn't work at all for many cross-val strategies also the fact that you can delete and edit messages is a bit worrysome __eou__	User
TP	but u can do so only for 10 minutes...__eou__	Agent
TP	anyway, glad I could help, if you want me to take a look at the blogpost I'll be around today__eou__	User

TP	This is clear now!! Will finish up my blog and resume the work :) Thanks!! __eou__	User
TP	oh God why does gitter display inline gifs?__eou__	Agent
TP	I'm starting to understand Gael's hate for it :D__eou__	Agent
TP	Is it bad? :P I'll remove :P__eou__	User
TP	haha!__eou__	User

TP	sure!! I'll publish in an hour or two! and also post it to our ML... btw if you have time could you check if the current implementation of the assert helpers look okay? __eou__	User
TP	I will try to do this later today__eou__	Agent
TP	Thanks! whats your timezone btw?__eou__	User
TP	okay :)__eou__	User

TP	EST (I think) it's 2pm 2:17 __eou__	User

TP	@amueller for #4844 do you get the same timing on your computer? __eou__	User

TP	@rvraghav93 btw, midterm is rather soon... __eou__	User

TP	@vene yeah. The student just showed me that doing array ** 2 is super-linear in the array size __eou__	User

TP	how about array *= array? __eou__	User
TP	so if you want to square an array it is faster to do so by chunking it... that seems wrong__eou__	Agent

TP	even with chunks of size 2? how about size 1? That would just be a loop over the items __eou__	User
TP	for the code in #4844 chunks of size 1 are fastest__eou__	Agent
TP	%timeit array *= array doesn't work lol__eou__	Agent
TP	having the loop over samples be the fastest just seems so wrong__eou__	Agent

TP	I was thinking in #4844 there might be some indexing thing, but if you narrowed it down to np.pow, that's very wrong how about np.square? This works though (but adds the overhead of a copy) __eou__	User

TP	``` In [11]: %timeit Xco = X.copy(); Xco *= Xco The slowest run took 4.31 times longer than the fastest. This could mean that an intermediate result is being cached 1000 loops, best of 3: 969 <unconvertable> s per loop  In [12]: %timeit Xco = X.copy(); Xco ** 2 1000 loops, best of 3: 1.25 ms per loop  In [13]: %timeit Xco = X.copy() The slowest run took 4.42 times longer than the fastest. This could mean that an intermediate result is being cached 1000 loops, best of 3: 525 <unconvertable> s per loop ``` `[elem * elem for row in Xco for elem in row]` is much slower __eou__	User

TP	@amueller you mean Gsoc midterm? it usually comes in July only right? __eou__	User
TP	@rvraghav93 it's June 26__eou__	Agent
TP	it opens June 26 until July 3rd I mean__eou__	Agent

TP	Ah its from 26 - 3 july... How much would you like to see me completed with my goals before the midterm? I am thinking  1. model_selection refactoring 2. Data independent CV Iterators. 3. Multiple Metric support - I won't be able to complete this though :/  Does it look okay? __eou__	User

TP	I think 1&2 completed (in such a way in which there's a MRG branch where all tests pass and one can do nested CV nicely) would be good. But by MRG I mean MRG :) @amueller, what do you think? __eou__	User

TP	yeah 1 & 2 merged would be great I also thought there was something wrong with the slicing but I'm very confused now :-/ and the problem is both in the squaring and in the outer product computation I feel stupid for not seeing what is happening __eou__	User

TP	@vene @amueller My second blog post - http://rvraghav93.blogspot.com/2015/06/gsoc-2015-psf-scikit-learn-nested-cross.html Please take a look and let me know your views! I'll also mail it to the ML as soon as I get a +1 from either of you! :) __eou__	User

TP	Thanks. I'll have a look tomorrow first thing in  the morning __eou__	User

TP	@tw991 I'm still not sure about #4844, I hope I have time to look at it soon. In the meantime, once #4874 is done, you could either look to finish https://github.com/scikit-learn/scikit-learn/pull/4539 which is boring but straight-forward, or investigate how to make sure the two k-means algorithms always give the same results in #2008, which is more interesting but also more involved __eou__	User

TP	@rvraghav93 I think the book in your blog post is just by Petersohn, and John Vogt Verlag is the publisher (Verlag means something like publisher in German I think) __eou__	User
TP	Oh :laughing:  thanks!!__eou__	Agent
TP	But it's a pretty simple idea and a pretty obscure book__eou__	User
TP	I think you can explain it better semi-formally without including that__eou__	User

TP	yeah if anything I'd try to reference esl (which is what I always reference for everything) __eou__	User

TP	for cross-val stuff I started referencing Gilles' thesis :) __eou__	User
TP	I stil have not really read that :-/__eou__	Agent

TP	Me neither, not fully, just the first two parts, but what I've read is great, very readable. Better than most ML textbooks. __eou__	User
TP	sweet__eou__	Agent
TP	so, @rvraghav93, could you simplify a bit the 3-step description of nested CV that you currently have? I find it a bit verbose, and it kind of makes everything seem more complicated than it is__eou__	User
TP	Okay! I'll remove that reference and make that description simpler :)__eou__	Agent
TP	Does the example seem okay?__eou__	Agent
TP	there are things that you can save for later in the blog post__eou__	User
TP	such as "based on the selected scoring technique. (At present only one loss function..."__eou__	User
TP	you can leave this for a kicker later__eou__	User

TP	Hi, can anyone point me in the direction of a explanation of how to do model ensembling? __eou__	User

TP	Okay! thanks for the feedback :) @Callipygian0 http://scikit-learn.org/stable/modules/ensemble.html __eou__	User

TP	Sure! I'm still reading through the blog post so give me a few more minutes please __eou__	User
TP	Please take your time!!__eou__	Agent
TP	Thanks :)__eou__	Agent

TP	@Callipygian0 If you mean ensembling heterogeneous models, you might want the [VotingClassifier](http://scikit-learn.org/dev/modules/ensemble.html#votingclassifier) in the current development version, not available in the last stable release. so, @rvraghav93 I'd actually not manually calculate `n_samples` that way If you want to show an example that's currently possible, I would use `cv=3` instead this currently does stratified kfold if given a classification dataset (I think) and then you can say something like... if you needed more customization, or a different CV such as LeaveOneLabelOut, it wouldn't work anymore oh I just noticed you do nested CV with a for loop __eou__	User
TP	I'm using version 0.15.2__eou__	Agent

TP	@rvraghav93 I wouldn't say cleaner way in the first sentence. Many things are straight up not possible. I would say to enable cross-validation with cross-validation objects. Or make it more flexible.... @Callipygian0 do you want to ensemble heterogeneous models? Then there is nothing to help you. Though implementing a voting classifier is pretty trivial. @rvraghav93 : " In each iteration (split), the dataset (X, y) is partitioned into training, validation set.` " should be "into a training and a validation set" also there is a backtick the no should be number. I feel the split in bullet points is not very clear well that could work... @vene not sure what you mean @rvraghav93 I'm not sure it is necessary do mention RandomizedSearchCV, it is pretty unrelated to the issue, right? well yeah __eou__	User

TP	also this thing "This becomes necessary especially when the dataset is too small to split it into three" it kind of depends on your audience, but technically, it's never too small (unless you have two samples) I fully agree, I just want to phrase it more clearly technically if the data is truly IID, it wouldn't matter, would it? __eou__	User

TP	I had all my models vote but it didnt really seem to work very well. This is my first machine learning experience so i'm very new! There is a kaggle style competition at my work. I was doing very well with GBM but everyone is overtaking me with ensemble now! __eou__	User
TP	@vene would you not agree? Well you could argue it is always more robust.__eou__	Agent
TP	GBM is an ensemble ;)__eou__	Agent
TP	I was told the best thing to do is combine like 5 different random forests, 5 different gbms 5 extra trees etc__eou__	User

TP	So, say I have a bunch of data that I want to model. I'll leave out, say, 25% as a test set, do GridSearchCV on the train set, and report the score on the test set. This is probably the most straightforward way, right? But what if I got lucky with my choice of test set? usually yes well if your model is perfect and if your data is IID you can have small variance, right? "The final result of the nested CV is the collection of n best Models" unless the 3rd point is exactly in the middle of the two test points hmm you're right anyway I was only trying to say that it might not be clear what "too few samples to leave a test set out" means it is indeed a question of variance and because of the variance, you can get particularly lucky or unlucky if you do one single outer fold on small data __eou__	User

TP	the variance will just be very high with a small dataset, right? __eou__	User

TP	@rvraghav93 The example with the code is pretty clear, why I feel the initial explanation is not. __eou__	User

TP	@rvraghav93 However, I wouldn't say you need to estimate n_samples. It is not something you are guessing or using statistical methods on. You just need to know it. but you can't with the current setup @vene I don't think it is. If you have three data points sampled from a Gaussian, you use two for training and one for test, and your model is fitting a gaussian to it, the variance of the log-likelyhood will be high *likelihood even though the data is perfectly iid and you are using the true model well if you have three points in a line and do 3-fold cross valiation .... variance of the estimate scales with the number of samples and cross-validation gives an unbiased estimate of the variance iirc __eou__	User

TP	so iris is small, and the outer CV variance in @rvraghav93's example is not large __eou__	User

TP	By variance do you mean to say the variance in the hyper param points or the performance of the best models? __eou__	User
TP	the performance of the best models__eou__	Agent
TP	ok__eou__	User

TP	> "The final result of the nested CV is the collection of n best Models"  You were going to say something about this right? __eou__	User
TP	actually I think you should pick an example that can currently not be made to work__eou__	Agent

TP	I'm fond of LeaveOneLabelOut :) but that works if you do cv=4, no? __eou__	User

TP	if you use StratifiedKFold with k=4 on iris it will already be impossible yes let's say you want to shuffle, though ;) or you want KFold __eou__	User
TP	why you'd want KFold is hard to explain__eou__	Agent
TP	if it doesn't shuffle by default that's perfect!__eou__	Agent
TP	maybe a better example: use regression and say you want to shuffle__eou__	User
TP	because we currently have no stratification for regression__eou__	User
TP	yep__eou__	Agent
TP	draw a line, fit something to it, and oh no! because we don't shuffle we don't generalize__eou__	User
TP	@vene there are long arguments about that. shuffling might hide correlations. If you data order has meaning, like being temporal, then shuffling will give you too optimistic results__eou__	User

TP	what would stratification look like on regression? __eou__	User

TP	that might suggest that the problem is not shuffling by default __eou__	User
TP	discussion: https://github.com/scikit-learn/scikit-learn/issues/4757__eou__	Agent

TP	I'm just saying the phrasing should be such that it doesn't lead to "just make shuffling default" but instead to "oh, we need to be able to tweak parameters of the CV object" you can't? can't you use cv=4 both outside and inside? ah, yes so say you want more iterations for the inner loop? first show an example with cv=4 both outside and inside ? btw, what's the intuition when choosing between StratifiedKFold with [Stratified?]ShuffleSplit? the docs just say "finer control on the number of iterations and the proportion of samples" I just shuffle first and do StratifiedKFold usually __eou__	User

TP	I was thinking about adding a shuffle keyword to GridSearchCV and cross_val_score at multiple points alright but wait no the argument should be different if you use cv=4 on the outside with iris you can't use shuffle split on the inside I'm stupid yes. But you can't use shuffle split on the inside because shuffle split needs the number of samples I think that is the right example alright __eou__	User

TP	well, you can control the number of repetitions and the test set size independently because having too small a test-set will also give you a bad estimate __eou__	User

TP	oh man, perfect post for a coffee break: http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-neural.html __eou__	User

TP	loving the low level ones __eou__	User

TP	I think I'll get a print of this for the office: http://1.bp.blogspot.com/-XZ0i0zXOhQk/VYIXdyIL9kI/AAAAAAAAAmQ/UbA6j41w28o/s1600/building-dreams.png __eou__	User

TP	@amueller I updated the #4874. would you like to take a look? __eou__	User

TP	@amueller @vene I've revised the blog post based on all your suggestions... However the example is still an illustration of a working nested CV. I thought once the data indep iterator work is over... I will show an example using LOLO to highlight the benefit of having data independence! Please take a look now :) http://rvraghav93.blogspot.in/2015/06/gsoc-2015-psf-scikit-learn-nested-cross.html __eou__	User

TP	Okay I'll sleep and finish this data indep before sunday :punch:   My next blog on mon/tue should be on nested CV using the data indep LOLO!! :) good night!! BTW is the 3 part explanation still verbose?  Okay :) will add one!! __eou__	User

TP	@rvraghav93 I'll have a look in a bit. I still think you should definitely also show an example that is currently not working. I feel the current breakage is pretty bad and showing it emphasizes how important your work is __eou__	User

TP	Hi, I adjusted the displaying of verbose messages in ```GaussianMixture```, what do you think? The extra messages displayed when verbose=2 compared to those when verbose=1 are put the same line with 'Iteration'. I think that would be more clear. @ogrisel @amueller [Comment](https://github.com/scikit-learn/scikit-learn/pull/4802#issuecomment-113303471) __eou__	User

TP	I have finished the most methods of ```_MixtureBase``` and methods of ```GaussianMixture``` related to full covariances. I think it's time to write tests before further implementation. In the master branch, the tests is implemented with ```unittest.TestCase```, while simple testing functions for other modules. Which one should I use? I think displaying verbose messages makes ```fit``` a little messy. Any better idea? __eou__	User

TP	@xuewei4d We usually prefer simple functions for testing. If you feel that using TestCase would simplify the testing code or make it more clear, you can use that, too. __eou__	User

TP	Okay. __eou__	User

TP	@xuewei4d btw, please also ping @lesteve when asking gmm questions, he is @ogrisel's co-mentor ;) __eou__	User

TP	Okay. I forgot to ping him in the last comment. __eou__	User

TP	@vene @amueller status update... the main part of data independent refactor is done.... all the new split / validate tests pass... working on grid search tests :) Will do the documentations and examples after pushing the grid search tests and an initial round of reviews (so as to finalize implementation) :) one minor question will ask at the PR itself... __eou__	User

TP	Thanks @rvraghav93. I am super busy right now but I'll review your blog post and PR in the afternoon __eou__	User
TP	Sure please take ur time :) I'll be done with all the tests by then :smile:__eou__	Agent

TP	Hi @rvraghav93, thanks for the update! I'll do my best and try to review tonight as well. __eou__	User
TP	Thanks a lot! :)__eou__	Agent

TP	I sent you detailed comments on the blog post per pm so as not to flood the channel. I still think it would be good to add an example that is currently impossible and will be possible with your contribution Has anyone ever seen gcc errors with printf in cython code? sklearn/manifold/_barnes_hut_tsne.c:7211:5: error: format not a string literal and no format arguments [-Werror=format-security]      printf(__pyx_k_t_SNE_Checking_tree_consistency); I get that on my box but not on travis. __eou__	User

TP	@rvraghav93 so I should review #4294, right? no wait, that is not the right one __eou__	User

TP	@rvraghav93 you should reference the issues you are working on in the blog post ;) __eou__	User

TP	any reviewers for https://github.com/scikit-learn/scikit-learn/pull/4621 btw? or maybe https://github.com/scikit-learn/scikit-learn/pull/4840 ? __eou__	User

TP	 Sorry, tonight is a bit tough for me for a more thorough review. I'll be on top of things in the morning. __eou__	User
TP	no worries vladn :)__eou__	Agent

TP	Thanks for the reviews! Will address them ASAP :) __eou__	User

TP	@ogrisel and after barnes-hut and LDA you review the GP rewrite? ;) __eou__	User

TP	Hi @amueller  and @ogrisel, I tried to adress all comments in #4444 if you have time for a quick review :) __eou__	User

TP	@amueller @vene Could I trouble you for a small doubt? - https://github.com/scikit-learn/scikit-learn/pull/4294#issuecomment-114523116 __eou__	User
TP	@rvraghav93 commented there__eou__	Agent
TP	thanks!!__eou__	User

TP	yes I think @amueller is right, returning cv.split(...) should do it is check_cv in utils? __eou__	User
TP	no in cross_validation__eou__	Agent
TP	so there will be a new version now in model_selection__eou__	Agent
TP	ah, yes__eou__	User
TP	and the old version will behave the old way__eou__	Agent
TP	perfect__eou__	User

TP	Is there any reason why we can't have it to return the iterator? __eou__	User

TP	so there are instances when the len(new_cv.split(X, y)) doesn't work? __eou__	User
TP	yes!__eou__	Agent
TP	That's odd, that line contains enough information to know what the length should be__eou__	User
TP	no?__eou__	User
TP	add it where? I don't get what you propose__eou__	User
TP	return the cv and the n_folds from check_cv?__eou__	User
TP	this means you'll have to change this line__eou__	User
TP	https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/grid_search.py#L512__eou__	User
TP	right?__eou__	User

TP	not without expanding it right? I mean `new_cv.split(X, y)` returns an iterator... __eou__	User
TP	yeah just add ``new_cv.n_folds(X, y, labels)``__eou__	Agent
TP	Thanks!__eou__	User
TP	because we don't want to instantiate the whole thing there. we usually know this anyhow__eou__	Agent
TP	add `get_n_folds(X, y, labels)` / `n_folds(X, y, labels)` it to the new cv classes__eou__	User
TP	Yes!__eou__	User

TP	and you only need to support the new classes. I get it now, this is good. __eou__	User

TP	Sorry to pester again! but why do you think we can't have `cv, len_cv = check_cv(...)`... is it so we can find the length without generating the cv? That will make it support old cv classes too right? yes... but  check_cv(..) would... is that why you are suggesting so? :) oh! so you don't want it to `return cv.split(X, y, ...)`? Then fine! ahh! okay! thanks :) __eou__	User

TP	what do you mean without generating the cv? the ``cv.n_folds(X, y, labels)`` desn't generate the cv, right? and the wrapper of the old classes can just call ``__len__`` yeah well what do you mean by "generate"? __eou__	User
TP	I guess the same as you__eou__	Agent
TP	instantiate the folds__eou__	Agent
TP	no! it should just return a new-style cv object, given either an old-style or a new-style one__eou__	User
TP	or a number__eou__	User
TP	@JeanKossaifi stupid question but is the splitting of the lables into folds equivalent to bin-packing?__eou__	User
TP	I'm just trying to figure out if the optimal solution is np hard or not ;)__eou__	User

TP	no, check_cv doesn't generate it either not until you iterate over what it returns right? __eou__	User

TP	@amueller  at least it is very similar. If I get it right for bin-packing we want to spread different weights into the smallest possible amount of bags (all bags having the same size). Here the number of bags (folds) is fixed and we want them to have (approximately) the same weight at the end. @amueller thanks for the review :) Yes, I had the same problem when I first needed that functionality. __eou__	User

TP	I think you can show equivalence with bin-packing by binary searching the bin-size your heuristic was always adding to the smallest bin, right? yeah seems good. I suggested minor refactorings should we try to merge #4444 and #4583 before #4294? __eou__	User

TP	yes, starting with the biggest weights Yes, I'm correcting it at the moment, thanks a lot! __eou__	User
TP	np. sorry for the delay. we have quite a few prs__eou__	Agent
TP	@vene @ogrisel do you want to review #4583 ?__eou__	Agent

TP	Sure __eou__	User

TP	This gives me an idea for renaming the ugly `p` in `LeavePLabelOut` into `test_size`, WDYT? __eou__	User

TP	I suggested n_labels (or n_groups if we rename) why test-size? n_test_labels? but ok, makes sense __eou__	User

TP	that's the meaning of `test_size` in #4583 if it's an integer it means you'll have `test_size` labels in the test set well, what I mean is, if `test_size` isn't right, we should change it in #4583 `test_size` is a bit ambiguous (people could expect it to refer to samples, not labels) __eou__	User

TP	@amueller in the current implementation I sort tuples of (weight, corresponding labels). if I use np.bincout I will have to sort it anyway and keep the correspondence to the original labels, so the complexity would be the same, right? __eou__	User

TP	`n_labels` seems good!! __eou__	User

TP	How about for LeavePOut, would you suggest `n_samples`? I wouldn't wait, both have their problems @rvraghav93 you're very quick to agree with things :) let's think about it in #4583 `train_size` and `test_size` are natural because they're delegated up to `ShuffleSplit`. They're not the best names but they're the friendliest parametrization. (Also, these parameter names are kind of mixing up test and validation...) __eou__	User

TP	ah! okay `test_size` it is! :P `p_samples` ? __eou__	User

TP	@JeanKossaifi you have to sort it, the correspondence would be stored in ``unique_labels`` the quadratic comes from looping over unique_labels and then doing fancy indexing. I want to avoid the fancy indexing __eou__	User

TP	(It's hard to teach someone and ramble for 10min about validation vs test, and then you code test_size=0.25) But ignoring this, `test_size` makes sense for LeavePOut (one could imagine even supporting fractions and `train_size`, but that's not important) __eou__	User

TP	There are currently no common tests for cross-validation generators, right? __eou__	User

TP	there are! `tests/test_cross_validation`? which I've refactored  into `test_split.py` and `test_validate.py` __eou__	User
TP	Those aren't really common tests, it's just regular tests__eou__	Agent
TP	I mean something like https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/estimator_checks.py#L1213__eou__	Agent
TP	oh! okay!!__eou__	User
TP	okay ;)__eou__	User

TP	Do you find the name of [`_check_is_partition`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cross_validation.py#L1107) appropriate? I just got totally misled by it. __eou__	User

TP	How about `_is_permutation_of_arange_n`? __eou__	User

TP	from the docstring it checks whether it is a permutation so ``_is_permutation`` seems appropriate? __eou__	User
TP	also from the code. I was just curious whether there's some obscure meaning of `partition` that I wasn't aware of.__eou__	Agent
TP	@jnothman  raised a good point in the pr: we probably still want to support passing iterables__eou__	User
TP	so maybe we want check_cv indeed to return an iterable?__eou__	User
TP	yes!! That seems like a nice solution to me :)__eou__	Agent
TP	@vene there is not really a reason to do that, right? the check_cv is instantiated where we actually have X and y__eou__	User
TP	@rvraghav93 yeah__eou__	User
TP	proxying labels? what do you mean?__eou__	User
TP	It makes the function more heavy, but not that much. And no, then I think the ``split`` would only be called there. the ``n_folds`` will probably be called outside of it, which kind of makes it tricky again.__eou__	User
TP	currently we only have labels and they are present in all the function calls__eou__	User
TP	what would that do?__eou__	User
TP	I think we should change the name if it gets more responsibility but how would that impact how people can use it?__eou__	User
TP	please don't include sample_weights changes in this pr__eou__	User
TP	there is still quite a bit to do for this, I think__eou__	User

TP	The thing is, after all the refactoring, we'll want it to return the new-style CV object, right? __eou__	User
TP	and about that helper I think we can do without it... its used only in one place... I'll remove that__eou__	Agent
TP	IMHO check_cv is used only internally right? we will be able to do  ``` for tr, te in check_cv(...):    ... ``` if we return iterable...__eou__	Agent
TP	That will pass all the responsibility of proxying labels etc to check_cv__eou__	User
TP	I'm not saying it's a bad thing, just that it makes the function pretty heavy__eou__	User

TP	will the API of the new CV objects ever be used outside of check_cv then? passing weights, labels (sample_props in general) to the cv generator __eou__	User

TP	or we could have `get_cv_safe`? __eou__	User

TP	I mean rename `check_cv` to `get_cv_safe` so ppl could use that? I have a crazy suggestion... why don't we takle `sample_weights` along with this? will it make the PR too big? tackle __eou__	User

TP	The main todo yet to be done is `check_cv`... other are minor right? documentation must be the next big thing... fixing examples should be quite easy... __eou__	User

TP	well yes, the way we now discussed check_cv there will be no changes to the estimatorCV code __eou__	User
TP	yes :)__eou__	Agent
TP	and make the tests pass__eou__	User

TP	yep! on it! __eou__	User
TP	alright :)__eou__	Agent

TP	@amueller Currently the shuffle parameter is not used given the heuristic used (I could shuffle the labels having a same weight but this case might not appear and the result could be misleading). --sorry with the new notation it would be the labels having the same number of samples :) Great, thanks a lot! :) @vene since you already saw the code, would you have time to take a look? :) __eou__	User

TP	oh, right. sorry. brainfart. Then please remove the shuffle parameter. for shuffling people could use #4583 __eou__	User

TP	Thanks, done! __eou__	User

TP	I think it looks good now apart from minor cosmetic things that I commented on. Maybe find another reviewer ;) __eou__	User

TP	@vene I am asking since you had raised the question on whether we had common tests for CV... could I add "Raise an issue to add common tests for CV iterators" as a todo to be done after `MRG+2`? __eou__	User

TP	I don't think it's important at the moment __eou__	User
TP	OK! thanks!__eou__	Agent

TP	Hi, it looks like we don't have Old Faithful data set, right? __eou__	User

TP	@xuewei4d no we don't. As it's very small I think we could include a copy in the `sklearn.datasets` folder as we do for iris (just check that the copyright allow that but I am pretty sure it does). __eou__	User

TP	OK. I will check it out. I would like to repeat some experiments described on PRML. @ogrisel __eou__	User

TP	Would be a good sanity check indeed. __eou__	User

TP	If you include the old faithful dataset, please do so in a separate PR and rebase your GMM PR on top of it to be able to merge the dataset PR first (without waiting for the end of the GMM work) while still being able to use it in your GMM examples. __eou__	User

TP	Got it. __eou__	User

TP	@vene @amueller @ogrisel If you have a few mins to spare... could you take a look at #4294 and let me know if it looks okay so I can proceed with updating documentation and fixing examples in parallel with the main review? In particular please let me know if you feel `len_cv` `iter_cv` and the new impl of `check_cv` look okay? __eou__	User

TP	There is a copy of data in R, which has GPL license. It definitely doesn't work. The original data is publish on JSTOR. Terms and Conditions of Use of JSTOR prohibits the commercial use, but which is permitted under sickout-learn's BSD-license. So I don't think we could add old-faithful data. @ogrisel [Terms and Conditions of Use of JSTOR](http://www.jstor.org/page/info/about/policies/terms.jsp) __eou__	User

TP	too bad. __eou__	User

TP	Maybe you could try to send an email to the original author of the paper / dataset to ask for explicit permission for inclusion of the data in the scikit-learn toolkit (mention explicitly that its license is the BSD license) if they have the copyright on this data? We will off-course credit their paper in the description of the dataset. @amueller @vene quickfix in #4893 by @jmschrei for the windows test failures that causes appveyor to be all red since recently: https://ci.appveyor.com/project/sklearn-ci/scikit-learn __eou__	User

TP	BTW, good news: I was contacted by the appveyor maintainer and we were upgraded to a much faster infrastructure (still for free). The builds run in 3-5 mins instead of 15-20min. This means that we might be able to run appveyor on the pull requests as we do for travis. Right now we only run them post-merge to master as it would have been too slow to process the full PR queues on the slow infrastructure. __eou__	User

TP	OK. I will send the author an email, and cc to @ogrisel, @amueller. Who else should I cc to @ogrisel ? __eou__	User

TP	it would be great to put the mailing list in CC but it's not possible to reply if there are not subscribed. So just us is fine. In case of positive outcome we can forward there authorization to the mailing list for the record. __eou__	User

TP	I have a question! Should the examples in the old classes (at `cross_validation.py` et al.) use imports from `model_selection` or should I leave it as such? Any reviews for #4826 would be awesome! It will help in moving those Warnings out of the model_selection pr :) __eou__	User

TP	virtualenvwrapper is cool! ;) Just incase any one is interested... This is a simple tutorial - http://simononsoftware.com/virtualenv-tutorial-part-2/ @ogrisel thats great!! btw why doesn't appveyor do the doc tests? only travis does it? (not that it should be of any concern) __eou__	User

TP	Also is there a way I could debug appveyor failures without having a win build? virual box? any suggestions? __eou__	User

TP	http://dev.modern.ie/tools/vms/ __eou__	User

TP	> @ogrisel thats great!! btw why doesn't appveyor do the doc tests? only travis does it? (not that it should be of any concern)  Because on windows some type `repr` are slightly different (I think for int vs long on Python 2 IIRC) and some doctests would have a different representation. Porting the doctests to be cross-platform would be painful. doctests should be considered a way to check that the documentation is up to date with the code. So if it passes on one platform, we know that the doc is up to date. For unit tests on the other hand we want to test on all the supported platforms to make sure that the code is fully cross-platform. __eou__	User

TP	Thanks :) @vene thanks for the link :) __eou__	User

TP	Note that last time I checked, the modern.io vms were 32 bit only. That might be a limitation in some cases. __eou__	User

TP	good point. @rvraghav93 you might be able to get a free Windows license through MSDN Academic Alliance or dreamspark, if your university is partnered with them. I had a dreamspark win7 for a while I __eou__	User

TP	I too got one via the IEEE comsoc offer!! :D but never used it ;/  Anyway the current failure is in 32 bit/ Python 2.6.. so this should suffice for now :) wow! @vene The setup is super fast... All I had to do was extract and open the ova file :O __eou__	User

TP	BTW could I start resurrecting #2759 (multiple metric support) in parallel? But since this depends on the data indep PR, I am not sure how to proceed... Should I maybe start a ML thread to collect ideas? __eou__	User

TP	@rvraghav93 let's finish the data dependent cv first sorry I have been offline for the last 4 days __eou__	User

TP	No issues :) Would you be able to spare a few mins to take a look at that? :) __eou__	User
TP	tomorrow__eou__	Agent
TP	Okay! I'll do that now... :) gn :)__eou__	User

TP	I have 300+ unread emails and it is 23:06 __eou__	User
TP	Sure ;) I forgot its 11PM there :P__eou__	Agent
TP	Just one more question!__eou__	Agent
TP	sure__eou__	User
TP	Can I finish the documentation / examples too?__eou__	Agent
TP	without waiting for the review?__eou__	Agent
TP	yeah sure__eou__	User
TP	thanks__eou__	User

TP	@amueller @jnothman @ogrisel are you happy with the changes in #4444? :) __eou__	User

TP	@JeanKossaifi sorry I can not review any time soon __eou__	User

TP	A friendly reminder to all the mentors that mid-terms are coming up this Friday! __eou__	User

TP	I am writing the mid term evaluation, but I don't think my PR could merge right now. Do you think I need to pick ```GaussianMixture``` and ```_MixtureBase``` out of my current PR and make it ready to merge?  @amueller __eou__	User

TP	@xuewei4d no, I don't think you should do that Its fine if there is good progress __eou__	User

TP	Just submitted the evaluation form. Hope everything is fine. __eou__	User

TP	Mentors... does anyone have access to Artem's GSoC evaluation? Apparently I am still not added correctly as a mentor! Working on getting the right access but would like to put an initial evaluation even if it means someone else copy-pastes in my words. __eou__	User

TP	Is there a reason RMSLE isn't in the metrics module, or would a PR for that be welcome? __eou__	User

TP	@zacstewart what does RMSLE stand for? __eou__	User

TP	Root mean squared logarithmic error. Same as RMSE, but take the log(y_pred + 1) and log(y_true + 1) instead of just the values I don't know if it is. I am bringing it up because it is the evaluation metric for a competition I am starting on today __eou__	User
TP	and I can't find anything relevant on google scholar quickly__eou__	Agent
TP	Seems like a good idea for variables with a lot of variance__eou__	User
TP	But I'm admittedly more of a programmer than a statistician__eou__	User
TP	anyway, you can easily define your custom metrics__eou__	Agent
TP	Yes, I've done so. I just wondered if it would be generally useful and worth pushing upstream.__eou__	User

TP	is this a mainstream thing? I see the first result is from Kaggle :/ __eou__	User

TP	If you want you can prepare a PR Of course it would need tests and docs (with references) and maybe a motivating example. __eou__	User

TP	Okay. That's reasonable. Not sure I can provide a motivating reason to use it, other than it's the metric for the competition I'm working on __eou__	User

TP	I don't know whether it belongs in scikit-learn by default. But I'm pessimistic because I personally never heard of this metric before. I'm rather clueless though. It also may be the case that it's been around under a different name I mean PartitionIterator, not CVIterator __eou__	User

TP	Ugh. I just noticed that the CVIterator, when given a class that implements _iter_test_indices, converts the indices to masks and then back into indices. __eou__	User

TP	seq of seq support for `LabelBinarizer` is [earmarked for removal in 0.17](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/multiclass.py#L195)... should we remove it?? __eou__	User

TP	@vene Thanks heaps for the patient reviews!!! I'll address them and push before tomorrow along with the documentation and examples... with that #4294 is done!!! (except for further rounds of reviews, ofcourse!!) tomorrow (in NYC time) Yeaa :/ Thanks anyway :) After addressing your comments I'll split it into one commit per file... this should make it marginally easy to review? __eou__	User

TP	I still have, like, half of the PR to go through.  it's a big one! :) Don't worry about label binarizer right now. Probably yes, it should be removed. I use  git diff to compare to the old files, so I don't really care how the commits are grouped. I just look at the head __eou__	User
TP	Ah okay then :)__eou__	Agent

TP	Anyway, before I go to bed, I'm not sold on using labels for the predefined split. It shouldn't stray too much from the old api I'll think about it some more tomorrow Happy hacking! __eou__	User
TP	Okay!! I'll leave it as such :) Good night!!__eou__	Agent

TP	@zacsteward  I never heard of that metric before either. which community is it from? Google told me it is used in a couple of kaggle competitions, but it doesn't have a wikipedia entry i guess it is for exponential regression tasks. Basically you log the target and then to rmse? For most models it would probably be better to log the target before training as regression models are more likely to minimize rmse __eou__	User
TP	I agree. I will probably try that in my model, but unfortunately I can't change the evaluation metric that this competition uses__eou__	Agent
TP	alternatively you could create your own scorer for this, but I don't think it belongs in sklearn. The problem is that if you do your own scorer, you probably also want a meta-estimator that logs the target before fitting / predicting with your model and then transform back.__eou__	User
TP	you don't need to change the evaluation metric. just apply log to y. then do grid-search with rmse on your ridge regression__eou__	User
TP	the error will be the "right" one\__eou__	User
TP	and it will perform much better than using ridge on the original target with rmsle__eou__	User
TP	(log(y + 1) and exp(y - 1) that is to not get nans)__eou__	User

TP	Kaggle is the only place I've seen it as well __eou__	User

TP	Hm, I suppose then I would just have to exp my predictions for submission __eou__	User
TP	yes__eou__	Agent
TP	Not sure if they have internal optimizations to save you the extra op__eou__	User
TP	or use numpy's log1p and expm1__eou__	User
TP	Anyway, I didn't want to solicit free advice on how to win a Kaggle competition. Just wanted to find out of this evaluation metric was noteworthy enough to merit a PR__eou__	User
TP	Thanks for the discussion__eou__	User

TP	np. I wouldn't worry about optimizing this, though using log1p and expm1 are probably a good idea __eou__	User

TP	For stability yes. For speed, this is just pre and post processing, so I doubt it matters http://www.johndcook.com/blog/2010/06/07/math-library-functions-that-seem-unnecessary/ __eou__	User

TP	if curious, here's the numpy implementation of log1p if the math.h one is not available https://github.com/numpy/numpy/blob/master/numpy/core/src/npymath/npy_math.c.src#L86 __eou__	User

TP	@amueller did you see this notebook ~~re~~implementing the google blog post from earlier? https://github.com/google/deepdream/blob/master/dream.ipynb __eou__	User

TP	wow! :O __eou__	User

TP	it generates things that look like this https://slack-files.com/files-tmb/T04T7B1ST-F074J6EDD-5630fb79d2/frames_360.png __eou__	User
TP	too awsome!__eou__	Agent

TP	@vene Do you have any suggestions for a practical example for nested cv using LOLO... ? :) Any dataset / public domain problem (like iris classification)? Okay!! __eou__	User

TP	I can't think of any datasets with meaningful groups you could generate one yourself __eou__	User

TP	like a regression problem with multiple noisy observations per subject if you accidentally leave observations of the same subject in both train and test folds, you'll get overly optimistic results __eou__	User

TP	(I'm just thinking out loud, this might not be right) say you have 2 features and the target is `y = w_1 + w_2 + noise` but `w1` is very correlated within the same subject and w2 is essentially independent. like if `w1` is essentialy the label of the subject + noise, and `w2` is just noise say maybe `w_1`is the person's weight (fluctuates slightly but not a lot) and `w_2` is how many minutes the person walked outside today __eou__	User

TP	@vene yeah saw the blog post. pretty cool I have to work on the scipy tutorial today, sorry __eou__	User

TP	Is there a good way to use an LabelEncoder to encode several columns of categorical variables? This is exactly what I want. But I need to encode those string categorical values to integers so I can use it :) I am actually using pandas. I've done this various hacky ways in the past Would making the OneHotEncoder accept strings be relatively straight forward or are there design problems blocking it? I'd love to issue a PR for that __eou__	User

TP	you probably want OneHotEncoder (only it doesn't do strings at the moment which makes me sad) you could make it a dict and use dict vectorizer... or use pandas? I was really surprised when I recently realized that there is no good way to do this in sklearn. you can open an issue if you like __eou__	User

TP	it seemed slightly non-trivial but didn't look blocking. maybe open an issue and ping @jnothman what he thinks of it __eou__	User

TP	:+1: __eou__	User
TP	sweet thanks__eou__	Agent

TP	[![download (1).jpg](https://files.gitter.im/scikit-learn/scikit-learn/6cOi/thumb/download-_1_.jpg)](https://files.gitter.im/scikit-learn/scikit-learn/6cOi/download-_1_.jpg) Deep dreaming on sklearn... My assessment: blue blob is pure machine, and orange blob is learning. FWIW, the crazy stuff in the background was imagined from a blank white canvas __eou__	User

TP	Cool!! How *how'd u do that?? BTW andyy happy birthday!! :) __eou__	User

TP	@rvraghav93 I spent several hours trying to get caffe installed :smiley: and then played with a few of the params at https://github.com/google/deepdream And then, things got weird... http://i.imgur.com/5QA87gU.gif __eou__	User

TP	if there were no eyes this would probably look even better! eyes pop out almost everywhere... :anguished: __eou__	User

TP	@trevorstephens  you might want to take a took at this :laughing: http://i.imgur.com/OPbPA4M.gif __eou__	User

TP	that's terrifying __eou__	User

TP	@xuewei4d I think you should have a look at https://bitbucket.org/michaelchughes/bnpy/ It looks like it might be a good reference for the dp gmm __eou__	User

TP	Thanks. I am looking into it. __eou__	User

TP	Hi I have a question about git/github. How can I create a branch from a branch of other's forked repository? __eou__	User

TP	You can add the fork as a remote with git remote add <name> <uri> __eou__	User

TP	I just tried, but there are too many conflicts. What should I do? __eou__	User

TP	I don't see how "git remote add" could trigger conflicts, what exactly is the problem? the way I'd do it is:  ``` git remote add xuewei4d https://github.com/xuewei4d/scikit-learn.git git fetch xuewei4d git checkout xuewei4d/<whatever-branch> git checkout -b <my-new-local-branch> ``` if the branch you want is a PR you can also use [this trick](https://gist.github.com/piscisaureus/3342247) HTH __eou__	User

TP	@rvraghav93 there was a question on the mailing list very relevant to what you are working on __eou__	User

TP	Thanks for pinging me!! I'll look into it and reply :) __eou__	User

TP	Also if you could spare a few mins could you let me know if there is anything left to be done for the #4294 :) I wish to get started on the next goal (sample props) asap :) __eou__	User

TP	I need to review it, it will take a bit more than a few minutes I hope I can do it today __eou__	User

TP	Thanks :)) also do you feel it is an apt time to send a mail on the mailing list requesting for the comments on sample props? (ML instead of issue since it will reach a wider audience) __eou__	User

TP	I'd discuss in the issue first __eou__	User

TP	Okay! Thanks! __eou__	User

TP	There is already discussion in progress there It seems like the appropriate place __eou__	User

TP	That's just my 2c After polling dev opinion, you can summarize and post on the ML too __eou__	User

TP	As for Luca's question, you could advertise how that sort of thing is possible with your branch, introduce the new API and see whether it suits his needs. It's good to get the users' point of view. Also it might attract another round of review, which would be great __eou__	User

TP	Thanks @vene. I had an internet connection problem. I cannot login gitter yesterday. __eou__	User

TP	@vene okay :) thanks!! __eou__	User

TP	Hi, when I am using %timeit on one function, how to disable caching intermediate results? __eou__	User

TP	good morning everyone! sprints are starting now! __eou__	User

TP	Good morning!! Scipy sprints?? __eou__	User

TP	yes! __eou__	User

TP	Awesome! good luck with that! :) __eou__	User

TP	Hi @amueller, sprint still going? __eou__	User

TP	yeahg __eou__	User

TP	@rvraghav93 @vene so what do we want to tackle next? I feel the sample_props is not clear enough to go ahead Another possibility would be the multiple metrics grid-search or nesting GridSearchCV and EstimatorCV __eou__	User

TP	is #1626 where I should read about the second suggestion? multiple metrics seems more straightforward to me right now I agree sample_props is not ripe yet __eou__	User

TP	Okay!! multiple metric it is! BTW the reason I suggested sample props to be done next was because it seemed more framework-ish... __eou__	User
TP	@rvraghav93 it would have been a great thing to work on next, but the solution isn't clear enough, as @amueller noted__eou__	Agent

TP	yes indeed!! BTW I heard from Joel that he wouldn't be available for the next few weeks... too bad since he vouched for multiple metric support very much! My reply to Luca was brief... will expand it as a blog post and reply to that mail like you had suggested... __eou__	User
TP	it was a good reply__eou__	Agent

TP	Also I scoured through the notification email and found bits and pieces of the lost conversation on `classifier` param... have commented it there!! __eou__	User

TP	I'm finally resuming my review! sorry for the delay thanks for finding the e-mails, I thought there had been more discussion that I missed __eou__	User

TP	@rvraghav93: didn't we agree to not duplicate the code that doesn't need to be duplicated? eg the old gridsearch, can't it be just imported from the new path? basically we'll have duplicated functionality in the old and new CV classes for a while, but the rest of the things that were moved should just be imported from the new place, assuming their behavior with old-style CV classes doesn't change. __eou__	User

TP	+1 __eou__	User

TP	@amueller what do you think about the placement on the `labels` argument in `cross_val_score` and the like? If we leave it at the end, we don't need to duplicate that code either, right? __eou__	User

TP	(About reducing duplicates) already done at https://github.com/rvraghav93/scikit-learn/pull/2 :) I also reused a few docstrings... That is a bit hacky not sure if that's correct!! @amueller discussion abt the labels arg here - https://github.com/scikit-learn/scikit-learn/pull/4294#discussion_r34417412 __eou__	User

TP	I'm curious about the [MRG] convention I see being used on pull requests. Does that just indicate that the author believes it's ready to merge? I think I wrote a good test for PR #4961. I hesitated before because I thought it would be weird to test something that should be somewhere around 0.5. But then I taught myself about assert_almost_equal. It was fun sprinting with some of you, I'll be back on later today or this week to hopefully finish that k-means example. __eou__	User

TP	@mrphilroth exactly! It's a way for reviewers to tell at a glance what state a PR is in. __eou__	User

TP	https://cloud.githubusercontent.com/assets/1180956/8710149/a6437600-2b15-11e5-9524-d503200f01a5.gif __eou__	User

TP	Hi all. Here is a ```BayesianGaussianMixture``` demo on rescaled old faith data. __eou__	User

TP	Andy uploaded the docs here: http://scikit-learn.github.io/dev/ __eou__	User

TP	please everybody check scikit-learn.github.io/ and stable and dev and 0.16 subfolders https://github.com/scikit-learn/scikit-learn/issues/4993 report issues there ok looking good now. waiting for the dns move __eou__	User

TP	added 0.15 __eou__	User

TP	dns is up over here, thanks @amueller! hi @rvraghav93 __eou__	User

TP	@xuewei4d sorry for the lack of feedback, but would you have time to look at this: https://github.com/scikit-learn/scikit-learn/pull/1292#issuecomment-122038677 ? @rvraghav93 I'll have more time on wednesday to look at your code __eou__	User

TP	Yes. He is right. I have fixed this problem in https://github.com/scikit-learn/scikit-learn/pull/4802/files#diff-a498a8ef6ad37ebc525591d722e0a7ceR220 __eou__	User

TP	The kmeans initialization in GMM is not good. It only initializes the means_. My PR will initialize the responsibilities first, then all parameters including means, weights, covars. __eou__	User

TP	@ogrisel AppVeyor is blocked on a build __eou__	User

TP	Hi, I'm fairly new to machine learning/deep learning, but I'm currently working on a project where I want to classify some biological images. I want to initially run an unsupervised clustering of similar images that don't have  labels. Later on I want to take a set of labeled images and run some type of deep learning algorithm to aid in classifying the images. What are some ways i can get started? Any help would be really appreciated, thank you! __eou__	User

TP	Hi @vene @amueller sorry! Got a bit busy :/ will resume work in a few hours!! :) __eou__	User

TP	Hi @rvraghav93. Do you have a draft of your next blog post? I will be on a 16h flight tomorrow, maybe I can make myself useful. __eou__	User

TP	Hey! I'll add one tonight! :) __eou__	User

TP	Vlad is choosing k in KNN a good example for LOLO? (About labeled data I was thinking like some N samples from M patients and we leave one patient out for validation to make sure we are able to generalise to other patients too) Sounds good? I am unable to find a nice example for LOLO otherwise :/ any suggestions? __eou__	User

TP	Also if you find time could you take a look at #4826 __eou__	User

TP	just to let u know, cluster.DBSCAN (and I guess lots more) is not working with scipy 0.16.0 on OSX, at least not with a conda install (maybe the library is missing in the conda binary?) https://github.com/scipy/scipy/issues/5092 well, others worked it, im just happy it does work again. ;) Its really scary if scipy doesnt work... ;) but thankfully, its so easy to quickly downgrade with conda: `conda install scipy=0.15` __eou__	User

TP	@rvraghav93 sorry for my proonged abscense. I am back from my travels and will catch up with all your activity michaelaye: what is the error you get? @michaelaye what is the error? I'm pretty sure that is working for other people. kinda ;) well np.dot "doesn't work" so .... __eou__	User

TP	master is failing? was that me?! is there a way to see when master started failing? __eou__	User

TP	https://travis-ci.org/scikit-learn/scikit-learn/builds May not just be master __eou__	User

TP	@amueller i was not alone, e.g. https://github.com/ContinuumIO/anaconda-issues/issues/392 but its all fixed now. __eou__	User
TP	@michaelaye glad you worked it out :)__eou__	Agent

TP	@TomAugspurger which makes it very hard to find out which are actual master commits and which are not __eou__	User

TP	great, the error was first here: 24e962cfe1c348d0c1de95f546b2091fe75a2c06 failure: https://travis-ci.org/scikit-learn/scikit-learn/jobs/72957365 success: https://travis-ci.org/scikit-learn/scikit-learn/jobs/72439348 and all the versions seem to be  the same __eou__	User

TP	ah, new scipy __eou__	User

TP	@rvraghav93 you should have pinged me using my username, I didn't see your message. I'm behind the Great Firewall so my connectivity is poor until the 31st To answer, I don't see why knn would be better or worse than anything else. The gist of the problem is the estimation of the score. If you use kfold instead of lolo you will overestimate. The idea of validation is to estimate how your model would do in a realistic setting. If your observations are grouped and they arrive in groups, it's not realistic to assume that you can be able to train on some samples from the same groups that you will run it on. I guess knn is likely to overestimate. But it's not a question of cchoosing k. It's one of methodology Think of search queries. If real life users would look for exactly the same queries you have in your training set, 1nn can return perfect results. But that's not a realistic of interesting case. __eou__	User

TP	If you "contaminate" your evaluation with this kind of data you can think your system generalizes much better than it really does. Because the model can implicitly learn to recognize the latent group label, and then get some of the test points predicted really well I hope this makes sense. I gtg __eou__	User

TP	Excuse me, what kind of test cases should I write for some computation functions? __eou__	User

TP	which functions? __eou__	User

TP	like update functions... https://github.com/scikit-learn/scikit-learn/pull/4802/files#diff-47bf98f4dd63f89baa089da3ffe28652R650 https://github.com/scikit-learn/scikit-learn/pull/4802/files#diff-47bf98f4dd63f89baa089da3ffe28652R197 __eou__	User
TP	not easily ;)__eou__	Agent
TP	you could test it against simple cases that you worked out on paper and that are easy to check?__eou__	Agent
TP	though that is rarely the case with formulas involving digamma functions :-/__eou__	Agent
TP	Honestly I don't know. unit-testing variational inference ....__eou__	Agent

TP	OK.. I will try it against simple cases. __eou__	User

TP	reviews for #5049 and #5047 would be very welcome so we can fix travis __eou__	User

TP	do we have a way to check if a regressor or classifier supports multi-output / multi-label? cc @arjoly __eou__	User

TP	not a lot happening here at the moment ^^ __eou__	User

TP	I just got out from behind the Great Firewall. But I'm coming after a 35h trip. It'll be a little while before I can be of any help :( __eou__	User
TP	haha yeah have a nap ;)__eou__	Agent

TP	According to the emails from Prof.  Azzalini and Prof. Bowman, I think, we cannot use old-faithful data set without the permission of Royal Statistical Society. We cannot use data from R either, right? @amueller @ogrisel __eou__	User

TP	@xuewei4d It probably depends, what dataset do you mean in particular? __eou__	User

TP	I think based on their mails we shouldn't worry too much about including it. __eou__	User

TP	could someone merge #5077? :) __eou__	User

TP	why is that imporant? but sure __eou__	User

TP	When attempting to import from the old `cross_validation` module which is deprecated... I noticed a weird behaviour in ipython... simply tabcompleting brings up the deprecation warning... is that normal...?  ``` from sklearn.cross_validation import KF/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:40: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. Refer model_selection.split for more info.   "Refer model_selection.split for more info.", DeprecationWarning) ``` notice the warning start after `KF<tab>` @amueller That was failing #4294 ;) Thanks for the merge! __eou__	User

TP	That's expected if the deprecation warning is at module level AFAIK __eou__	User
TP	okay! Thanks :)__eou__	Agent

TP	Just realised we crossed 7k stars!! :beers: __eou__	User

TP	It is old-faithful data set. @vene __eou__	User

TP	wb @ogrisel :) __eou__	User

TP	@vene can you have a look at the discussion topics at the top of #4294 ? __eou__	User

TP	sure thing __eou__	User

TP	I'm slightly confused by the PR that to @rvraghav93 's branch that removes the code reuse. Is only the newest commit relevant? That commit seems to have some extra things squashed in it. __eou__	User

TP	I haven't really looked at that recently I think we should first get the one with all the duplication merged what do you think about making the submodules private? __eou__	User

TP	with underscores? I think it's a good idea __eou__	User

TP	yes, with underscores. ok please say in the PR, then he can do that. apart from the docs /examples that is the only major thing left. __eou__	User

TP	@rvraghav93 how are things looking? __eou__	User

TP	Hey thanks a lot for the reviews!! Ive fixed most... Ill fix the rest (mostly trivial) and We are merging  that pr this weekend ;) u guys will be available right?? So we could have last minute reviews?? Also do u feel that or should have +3 since its a major refactpr?? Pr* (Apologies for typo - using my mobile :( ) __eou__	User

TP	Hey @amueller @ogrisel , I got some medical issues, and probably cannot work for GSoC project in the recent few days. Sorry. :worried: __eou__	User

TP	@xuewei4d sorry to hear that, get well soon! __eou__	User

TP	@rvraghav93 I will be around this week. I agree a 4th person would be nice, maybe @jnothman can take a look. PS: Don't forget about the blog! __eou__	User

TP	@rvraghav93 I'll be around but I'll also have to work on some other things. When are you going to Paris. @xuewei4d get well soon! __eou__	User

TP	@ogrisel are you project owner on appveyor? we need to do something: http://help.appveyor.com/discussions/problems/2721-getting-message-error-creating-build-entry-please-contact-appveyor-support-in-every-build  otherwise all PRs have failing tests. That was easy: https://github.com/scikit-learn/scikit-learn/pull/5093 __eou__	User

TP	ah, I found the appveyor integration stuff ^^ @ogrisel sent me that at some point so the appveyor builds are running again and I'm excited to see what happens with #5093 ^^ __eou__	User

TP	hum.. @ogrisel I still need help :-/ https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.1490/job/26vknwf7qa70r868 __eou__	User

TP	@amueller is waiting the thing to do or is there something missing from #5037 that I can do to get it towards MRG+n's? __eou__	User

TP	@betatim you can change the name of the PR with [MRG], to show you ask for some review __eou__	User

TP	I feel we could add more to the doc string of the dataset loaders... (perhaps a description or at least a link?)  For example in the diabetes documentation no domain related info is given... domain related as in no info as to what the targets represent or what the various attributes are? @amueller @agramforte said there were delays in the visa process ;( I've tried pestering him too :P I really hope I'd be there by September at the least :| __eou__	User

TP	Most of them have readmes after loading, I guess. __eou__	User

TP	wasn't the diabetes fixed recently? @rvraghav93 last time I checked there were still unresolved comments on the model selection PR. __eou__	User

TP	No it is still the same AFAIK... Yes a few... I'm working on them :) I'm actually finishing up on my blog post! Will publish the same in a few mins... Would you be around? If so I'll trouble you for a review! __eou__	User
TP	I can try. I'm pretty busy at the moment but I'll have a look__eou__	Agent
TP	Does anyone know where @ogrisel is? Holidays?__eou__	Agent
TP	`/ogrisel` ;)__eou__	User
TP	Performing a vim search for @ogrisel ;)__eou__	User
TP	@amueller @vene This is my new blog post... could you check it and let me know if it looks okay?__eou__	User
TP	http://rvraghav93.blogspot.in/2015/08/gsoc-2015-new-cross-validation.html__eou__	User
TP	I'll check it tomorrow__eou__	Agent

TP	? ah ^^ __eou__	User

TP	Can this get another review: https://github.com/scikit-learn/scikit-learn/pull/4924 so we can merge it before  https://github.com/scikit-learn/scikit-learn/pull/4421 ? also we should really work on https://github.com/scikit-learn/scikit-learn/pull/4421 __eou__	User

TP	@o __eou__	User

TP	Sorry for the interruption. I am back, although not fully recovered. Hi @amueller @ogrisel , I just finished test cases for DP. Where should I update the mixture documentation and the mixture examples? __eou__	User

TP	@xuewei4d great :) yeah, that sounds like a plan. __eou__	User

TP	I think I finished what I must do.  @ogrisel mentioned the "mixture documentation" and "mixture examples" several days ago, but I don't know where should I put. __eou__	User
TP	doc/modules/mixture.rst ?__eou__	Agent
TP	OK. Would it be better to add the animation? :wink:__eou__	User

TP	I don't think this is a priority but it would be kinda nice. The actual implementation and tests are much more important obviously. sorry for the slow reviews :-/ __eou__	User

TP	I agree. I think I could do the examples after the soft pencil down deadline. What else should I begin right now before next Monday? Never mind. :sweat_smile: The equations for BGMM are really cumbersome. __eou__	User
TP	I know ;)__eou__	Agent

TP	@rvraghav93  "in the year 2010" I'd just say "in 2010" __eou__	User

TP	I think there's a methodological issue with the set up in your blog post. If you do plain Kfold in the outer loop, you can still have the same group label in the outer train and test. Why not use leave-one-label-out in both inner and outer? So at a "big picture" level I don't see the point of the final section that looks at the folds.  I think that's common practice anyway, doesn't seem specific to nested or group-aware CV. I would prefer if you showed an actual example of overfitting without LOLO. Your explanation of why it can happen is good, but it would be much better to illustrate it. You suggested earlier using KNN and generating some data based on the patient id. __eou__	User

TP	@xuewei4d btw were you aware that semantics of np.diag are changing? in new versions, np.diag returns a view. Not sure that is relevant, I haven't reviewed this parts in detail __eou__	User

TP	also I cann't load kernelsvm.tripod.com __eou__	User

TP	' __eou__	User

TP	@amueller I checked np.diag usages in my code, I think it is OK. I didn't write sth into the diagonal elements of an array. I just added the verbose flag back into mixture modules. What else should I do before soft pencil down? @amueller @ogrisel __eou__	User

TP	@vene Thanks for the review!! I couldn't find a nice dataset to illustrate LOLO :/ There is one dataset inside the proprietary `perClass` package (small_medical... I'll have to mail them and ask if it can be used in a blog post... moreover it is a matlab package so I'll have to convert it to a csv file)   Do you have any suggestions?? Earlier I recall yourself suggesting search query... could you expand a bit on that pl?? And I can load kernelsvm.tripod.com :O That seems to be a pretty famous reference for SVR... Appveyor doesn't seem to test the model selection module :/ Do I have to add something somewhere? @amueller @ogrisel __eou__	User

TP	does someone know how to "restart" a appveyor build? The build for a PR failed with some weird errors that I am pretty confident have nothing to do with the contents of the PR #5037. thanks, appveyor is doing its thing :) __eou__	User
TP	:)__eou__	Agent

TP	simplest way would be to push a commit / make some amends to your previous comment, squash and force push it... __eou__	User

TP	@rvraghav93 you can always create a syntethic dataset __eou__	User

TP	Okay! I'll look on how to create one... Any tips?? :) Also could you take a look at #4919 ? __eou__	User

TP	@rvraghav93 regarding the dataset: you can take the diabetes dataset and invent arbitrary group ids (as you already have), but generate a new `y` based on a formula you come up with (a function of one or two features and also of the group_id) or you could just use `make_blobs` with a large number of blobs and arbitrarily assign half of them to the positive and half to the negative class. Then the blob assignment is the group_id.  Imagine if half a blob is in training and half in testing, a classifier like KNN will predict really well. Makes sense? __eou__	User

TP	@amueller @vene Could you confirm if [this](https://github.com/scikit-learn/scikit-learn/pull/4294/files#r35808353) can be left untouched?? @vene Thanks!!! And yes it helps! I'll modify the example that way!! Using `make_blobs`, I will also be able to illustrate it graphically :) __eou__	User

TP	Also could anyone help me with [this](https://travis-ci.org/scikit-learn/scikit-learn/jobs/75260268#L1509) failure :/ I am unable to comprehend why this test should fail :| __eou__	User

TP	@rvraghav93 regarding the failure: maybe the doctest should have the random seed fixed? __eou__	User

TP	@xuewei4d btw were you aware that semantics of np.diag are changing? in new versions, np.diag returns a view. Not sure that is relevant, I haven't reviewed this parts in detail (hm sorry sent that before) @xuewei4d I'm sorry, I don't think I'll be able to review before the pencil down. I'm pretty busy and have to look at @rvraghav93's work. I'm not sure where @ogrisel is. Where is loic btw? __eou__	User

TP	@vene No it passes in master... it used to pass in my branch too... since I squashed a few commits I am unable to get the exact point at which it broke :sob: __eou__	User

TP	you can always git bisect but have you tried that piece of code locally in ipython with random_state to None vs other fixed values? __eou__	User

TP	Thanks!! git bisect is cool... This seems to happen after the OnlinLDA pr.. (#3659)  I'll send a PR to fix the random state... __eou__	User

TP	Wait thats not right... Fixing random_state fixes this but I am unable to figure out why master doesn't fail.. __eou__	User

TP	where is the random state not fixed? __eou__	User

TP	https://github.com/rvraghav93/scikit-learn/commit/97d4f3eaba284c07406b82a5d75d9da8196e95e7 __eou__	User

TP	That's not in the LDA branch.. did you sent a PR, I didn't see it. __eou__	User

TP	No... I just added it as a commit to #4294.. and yea it has nothing to do with LDA... sorry I misread git bisect output... Also appveyor is not testing model selection... why is that?? __eou__	User

TP	  __eou__	User

TP	ah, but the commit is not in the addition to the examples. I'm not entirely certain but maybe it's better to merge these two so they are self-contained  s/tests/examples/ __eou__	User

TP	`E486: Pattern not found: tests` :p Ok so you mean we can merge https://github.com/rvraghav93/scikit-learn/pull/3 into #4294 right? __eou__	User
TP	 s/examples/tests I mean__eou__	Agent
TP	yes__eou__	Agent
TP	okay I'll do it now... https://github.com/rvraghav93/scikit-learn/pull/3 is just one commit..__eou__	User
TP	ok__eou__	Agent
TP	why do you think appveyor is not testing model selection?__eou__	Agent

TP	For the last few failures... Only travis seemed to be unhappy... appveyor didn't raise any errors... I'll confirm with a dummy failing test in a moment... merged... Is there a way to make travis build the documentation automatically and host it at a temporary place somewhere? (maybe using pythonanywhere.com + additional travis build just for docs)? Sorry :/ http://rvraghav93.github.io/scikit-learn/doc/_build/html/stable/ __eou__	User

TP	we have a setup hosted on rackspace but it is non-trivial and you don't have access, sorry we should improve that. __eou__	User

TP	Could you review [this commit](https://github.com/rvraghav93/scikit-learn/commit/b5077d2f817b7c78782da3703c7cf4847809092a) which tests `_CVIterableWrapper` alone? Its a minor one... (And I did this since you said that there were no tests covering it.. (your comment got hidden..)...) __eou__	User
TP	It looks ok__eou__	Agent

TP	This is the documentation - http://rvraghav93.github.io/scikit-learn/ Once this gets an OK I think #4294 is finally done :D __eou__	User

TP	@rvraghav93 that 404s for me __eou__	User

TP	I'll give it a (hopefully) final review this afternoon :) __eou__	User

TP	Hi, how could I preview the html generated by rst file ? __eou__	User

TP	Open doc/_build/html/stable/index.html in your browser :) @amueller thanks!! __eou__	User

TP	Thanks @rvraghav93 after ```make```? __eou__	User

TP	Thanks. I got it. __eou__	User

TP	after make in the doc folder @rvraghav93 there are no changes to doc/ or examples/ in #4294. did you forget to push or something? oh, wait, I thought you merged https://github.com/rvraghav93/scikit-learn/pull/3 and https://github.com/rvraghav93/scikit-learn/pull/4 but you didn't? well there were no changes in the grid-search part? it still had the header grid-search and was referencing grid_search.* __eou__	User
TP	ohh okay! Now its fixed :)__eou__	Agent

TP	the built at http://rvraghav93.github.io/scikit-learn/doc/_build/html/stable/modules/grid_search.html doesn't seem to be using the documentation change branch? I'll review rvraghav93/scikit-learn#4 now, but it would be good to have a built of it __eou__	User

TP	2 mins! :) __eou__	User
TP	sure :)__eou__	Agent

TP	It should be updated now!! How did you detect it was not updated? (asking so I could check that part of doc and see if it reflects the change :) ) __eou__	User

TP	And yes I didn't merge https://github.com/rvraghav93/scikit-learn/pull/4... I only merged https://github.com/rvraghav93/scikit-learn/pull/3 since I got a +1 from yourself and Vlad... Do you suggest that I merge https://github.com/rvraghav93/scikit-learn/pull/4 too? __eou__	User

TP	well I wanted to give it a final pass as a whole. but I'll review the 4 now on its own __eou__	User

TP	I can help __eou__	User

TP	thanks @vene. I think the main question on rvraghav93/scikit-learn#4 is whether to keep the old stuff in the references and whether to "fix" the references in the whatsnew (loads of non-links otherwise?) I'd actually like at least one more feedback on keeping the old methods in the references or not. @rvraghav93 are you around? I know ;) did you fix the other examples, though? if so, you should push... cool and the doc pr was rebased on top? __eou__	User
TP	yea :)__eou__	Agent
TP	the model_selection branch doesn't include the fixes yet.__eou__	User
TP	they are not in the PR__eou__	User
TP	:)__eou__	User
TP	also, have a look at the reflog command__eou__	User

TP	@vene thanks!! yayy with all 3 of us online... its getting merged by today I suppose ;) :D __eou__	User

TP	@rvraghav93 did you do "make html" in the doc folder to run all the examples? That shouldn't give any deprecation warnings wrt the move same goes for running the tests. the old classes should be tested, but all deprecations should be caught. __eou__	User

TP	@rvraghav93 does rvraghav93/scikit-learn#4  include the example fixes? __eou__	User

TP	2 mins... the make html takes a looooong time :( Example fixes were merged into #4294 of `model_selection` branch which includes example fixes.. yes :) __eou__	User

TP	something got screwed up :sob: now I deleted the merged branch too :/ will have to redo the work... give me a few mins... its quite trivial only... yay I was able to recover it as a patch from [here](https://github.com/rvraghav93/scikit-learn/commit/beec231002e722ea19a494dfc411140ac6327842) :D __eou__	User

TP	with git, things are rarely truly lost __eou__	User

TP	@rvraghav93 sorry I gotta run. I'll continue reviewing tomorrow __eou__	User

TP	Anyone interested in giving some feedback on https://github.com/scikit-learn/scikit-learn/pull/5123 ? __eou__	User

TP	@basveeling sorry, feedback often takes some time. It sounds like a good idea to me, though I'm not the authority on this subject. When designing the classifiers, we didn't have "most entries are zero all the time" in mind, which is the case for hashing. __eou__	User

TP	@amueller no worries, thanks for the feedback! I'll spend some more time on benchmarking the sparse structure __eou__	User

TP	@rvraghav93: gentle reminder that Friday is the firm GSoC pencils-down date. __eou__	User

TP	Apologies for the delay!! I got my French work permit and am shopping some stuff and also applying for the visa :D I'll finish the documentation comments by today!! __eou__	User

TP	sweet @rvraghav93 ! congratulations! __eou__	User

TP	ping @vighneshbirodkar we often hang out here for discussions, too [though the more we stay on github the better] __eou__	User

TP	Cool __eou__	User

TP	@ogrisel btw I didn't have time to look into the doc build server. do you think you'll find time? __eou__	User

TP	@rvraghav93 how are things? I haven't heard from you in a while... __eou__	User

TP	Hey!! Looking for apartments in Paris ;( Looks like I'll get the Visa only after I confirm the accom... and I won't be getting accom from univ... Hoping to be there atleast by Sept end/Oct 1st I'll have to finish up the documentation no? Sorry It won't be delayed anymore... I'll do this in a day or two... Only a very few comments to be fixed up... __eou__	User

TP	@ogrisel are you around? __eou__	User

TP	@vighneshbirodkar I think this would be interesting to work on: https://github.com/scikit-learn/scikit-learn/issues/4920 __eou__	User

TP	does anyone have opinions about including issue numbers into whatsnew? __eou__	User

TP	Just that sklearn will outlive github :) __eou__	User

TP	well we can stop using them then ;) Also, outlive the existence of github or the use? As long as the links still work it might be useful, even if we transition to the next version control system ;) __eou__	User

TP	Possibly the existence. But we could always save a copy of the issue text. It seems useful to have the numbers. __eou__	User

TP	I just got a stop gap accommodation till Dec 31 thanks to Mainak Jas and Airbnb... I am back... @amueller is there anything I need to worry about w.r.t this comment? (https://github.com/scikit-learn/scikit-learn/pull/4270#issuecomment-136450796) __eou__	User

TP	@rvraghav93 Great to have you back! No, it's just that we won't include the model selection changes in the upcoming release. Still it would be great to get them done soon thanks cool :) Do you maybe want to have a look at #4924 ? cool. have fun tonight :) __eou__	User

TP	Okay!! :D and thanks :) __eou__	User

TP	@amueller I have rebased the LDA deprecation in #5245. Let's wait for CI to check that I did not break anything in the process. __eou__	User

TP	also #5236 __eou__	User

TP	For #4924, it will need to be updated to work on top of #5245. __eou__	User
TP	yeah but that should be a quick change that either of us can do if it has two reviews__eou__	Agent
TP	do you have any opinion on adding PR links to whatsnew?__eou__	Agent

TP	I gave my +1 > do you have any opinion on adding PR links to whatsnew? I am fine with it. It would be great to add the PR numbers on all of the items but that sounds tedious to do :) I will have to run soon to take my shuttle. I will be busy tonight so I don't think will be able to work on the release much more today. I should be able to focus on that tomorrow though. You might want to have a look at #5104 as well. __eou__	User

TP	`LabelEncoder` right now is doing a binary search using `np.searchsorted`. Can't we speed that up by using a dictionary ? __eou__	User

TP	@amueller @vene Do we need `LabelKFold` in #4294? Why can't we just pass the labels in `split(X, y, labels)` instead of having a new class? (Same for `LabelShuffleSplit`) @ogrisel appveyor doesn't test model selection [![appveyor.png](https://files.gitter.im/scikit-learn/scikit-learn/x9Q7/thumb/appveyor.png)](https://files.gitter.im/scikit-learn/scikit-learn/x9Q7/appveyor.png) __eou__	User

TP	[![appveyor2.png](https://files.gitter.im/scikit-learn/scikit-learn/gKbL/thumb/appveyor2.png)](https://files.gitter.im/scikit-learn/scikit-learn/gKbL/appveyor2.png) 1st image you can see one of appveyor builds passing successfully despite a failing test in model_selection, in the 2nd you can see travis working correctly... https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.2085/job/qd27oykpd2ubc1yr#L3734 - After `sklearn.mixture` `sklearn.model_selection` should have been tested... Any clues to debug this? __eou__	User

TP	[![appveyorfinal.png](https://files.gitter.im/scikit-learn/scikit-learn/KW30/thumb/appveyorfinal.png)](https://files.gitter.im/scikit-learn/scikit-learn/KW30/appveyorfinal.png) __eou__	User

TP	The newest merge seems to have modified _tree.c __eou__	User

TP	which merge? __eou__	User

TP	"ENH add sag solver in LinearRegression and Ridge" __eou__	User
TP	indeed__eou__	Agent
TP	git log sklearn/tree/_tree.c__eou__	Agent
TP	I will recythonize it from the current _tree.pyx__eou__	Agent
TP	coolios. Weird that it got changed__eou__	User
TP	running the tests locally to check that I did no break anything.__eou__	Agent
TP	yeah__eou__	Agent

TP	pushed the cythonized tree code to master directly __eou__	User

TP	appveyor is failing? or is that from the previous tree code ah, appveyor is really behind __eou__	User

TP	here is the state of the queue: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/history __eou__	User
TP	okay, cool__eou__	Agent
TP	@rvraghav93 which PR is that?__eou__	User
TP	appveyor does not run the doctests, maybe a doctest is failing?__eou__	User

TP	@ogrisel I'm in ;) sorry meetings stuff __eou__	User

TP	no pbm __eou__	User

TP	I'll do #5104 and then the tsne example the mlp is starting to look good btw #5214 __eou__	User
TP	#5245 should be good to go (it's just appveyor that is slow as a dog today)__eou__	Agent
TP	have you seen this: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.2086/job/drvmlx86c9swelx4 ?__eou__	User
TP	#5206 should also be ok, but could be made shorter__eou__	User

TP	> have you seen this: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.2086/job/drvmlx86c9swelx4 ?  Hum that's bad __eou__	User
TP	yeah__eou__	Agent
TP	this is also new to me: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.2090/job/toaww87ohll9pxv9__eou__	Agent
TP	wait that is the same__eou__	Agent
TP	sorry__eou__	Agent

TP	That's weird that it only fails on 32 bit Python, both 2 and 3. It's seems completely unrelated to the architecture. maybe it's just random? the fact that we get it with Python 2 is really weird: it means that it cannot be caused by the use of the multiprocessing context /  start method in  joblib because this does not exist under Python 2. __eou__	User

TP	here the failing pattern is different: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.2090 __eou__	User

TP	@ogrisel #4294 And no it was not a doctest... I specifically made a failing test inside model selection to confirm my observation... :) __eou__	User

TP	maybe this is caused by a change in the way nose run the tests. I will open an issue to track this problem @rvraghav93 your issue is probably not related to the appveyor problem we are discussing (which has to do with multiprocessing #5254) __eou__	User

TP	~~Okay but this is localized to model_selection right? maybe I am doing something incorrectly?~~ Also @amueller @vene do we need the new `LabelKFold` and `LabelShuffleSplit` as separate classes or can we specify the labels in `split(X, y, labels)` in the (new) `KFold` class itself? __eou__	User
TP	#5254__eou__	Agent
TP	Oh sorry I thought it was a reply to my comment... :D Any ideas on how to debug mine?__eou__	User

TP	@rvraghav93 do you have a windows machine at hand? you can try to replicate it locally by following the install stepts in appveyor.yml __eou__	User

TP	@rvraghav93 they do somewhat different things. What is the benefit of putting them in the same class? __eou__	User

TP	`LabelKFold` is `KFold` with `labels` (somewhat like group labels) specifying  that the points in the same label should not be used for both testing and training right? Would it benefit from grouping together? __eou__	User

TP	@amueller unfortunately I will have to leave soon and won't be able to work on the release this WE. I think we should fix the appveyor issue before cutting the branch. I have opened a PR there #5255 to try a quickfix even though I don't understand the problem. I can work on that on monday if that does not work I added some quick benchmark in the comments of #5253. appveyor has a network problem on the fast infra and so the queue is running on the old Azure based infra this is why the build are slower than usual I don't know if that explains the weird multiprocessing issue though (it seems unlikely) ok I have to go see you later __eou__	User

TP	ok ttyl. Can you work on the release next week? __eou__	User

TP	@amueller @vene Actually that won't make sense especially when we want to group `Stratified{KFold|ShuffleSplit}` and `{KFold|ShuffleSplit}` together making stratify an option as suggested by Joel! Sorry for the noise... I'll add it as such :) __eou__	User

TP	AppVeyor tweeted about slow performance recently; I guess that's what is holding up all the tests __eou__	User

TP	@ogrisel the MLP is good to go, too, I think. do you want to merge it after release or before? I don't really see a reason not to merge now. __eou__	User

TP	Hey guys, can we merge this? https://github.com/scikit-learn/scikit-learn/pull/4525 __eou__	User

TP	I think so. You have my +! +1 I think we should fix these. @ogrisel didn't want to fix them I think. I mean it does add clutter to the docstrings, but it is not rendered on the webpage __eou__	User

TP	I'm taking a quick look as well Looks great, thanks for the work! __eou__	User

TP	@rvraghav93 what is the status of your issue? I have a windows machine, I can take a quick look. __eou__	User

TP	which issue is that? __eou__	User

TP	I'm not sure, I just saw Olivier asked if he had a windows machine to reproduce an Appveyor issue,. __eou__	User

TP	ah that is the joblib one that keeps appveyor from failing just run the test suite __eou__	User
TP	no, the joblib issue is a separate issue__eou__	Agent
TP	ok then I don't know which one we are talking about__eou__	User
TP	> [![appveyor2.png](https://files.gitter.im/scikit-learn/scikit-learn/gKbL/thumb/appveyor2.png)](https://files.gitter.im/scikit-learn/scikit-learn/gKbL/appveyor2.png)__eou__	Agent

TP	@jmschrei thanks a lot!! In my PR #4294 the tests in the new module `model_selection` are not being run in appveyor... Any help would be really awesome!! :) I could use virtual box... but earlier I had little success doing so :( __eou__	User

TP	That is a lot of files changed. __eou__	User
TP	Yea :grin:__eou__	Agent
TP	You'll squash all these commits soon?__eou__	User

TP	You can replicate that by simply creating a foo folder and a tests directory with a simple failing test :) Yes that's a whole lot of commits ;) I'll probably squash it to less than 5 It will help tracking things easier.... __eou__	User

TP	I don't understand, is this a PR meant to refactor CV or reorganize the modules? __eou__	User
TP	Both :p__eou__	Agent
TP	Alright, I'm just going to look for bugs, this conversation is too long for me.__eou__	User
TP	Haha okay :)__eou__	Agent

TP	Refractor as in making then data dependent... and reorganise into `model_selection` folder *independent __eou__	User

TP	After months of developing in Ubuntu, I am remembering why Windows is such a pain. __eou__	User
TP	^_^__eou__	Agent

TP	@rvraghav93 could you clarify what about the labels param I should comment on? __eou__	User

TP	I am getting 12 failures Mostly related to string format Is this what you are getting, @rvraghav93 __eou__	User

TP	These errors seem to stem from getting longs instead of ints This manifests as getting (10L, 2L) instead of (10, 2) __eou__	User

TP	Is there any way to force numpy to use ints? I'm not finding anything, unfortunately. In the shape, I mean. __eou__	User

TP	@jmschrei No my concern is that the model_selection tests are not at all run by appveyor... only travis seems to detect the module and run the tests... Is that because there are no public python sources in `model_selection`? (all three are private) @vene You had earlier said that the doc for labels param was not apt... Does `" Class labels to be assigned to the samples and used while splitting the dataset into test/train set."` sound like a good doc for `labels` param? (of `split(X, y, labels)`?) __eou__	User

TP	The unit tests are failing on my machine. raise self.failureException("ImportError('No module named sag_fast'm) != None") Ubuntu 64 bit, they were all running fine on Friday. __eou__	User

TP	did you miss a make in ? __eou__	User
TP	Yep__eou__	Agent

TP	@rvraghav93 it shouldn't be "class labels" they are not class labels. maybe group labeles? also not really assigned to the samples? __eou__	User

TP	Guys, I opened this PR https://github.com/scikit-learn/scikit-learn/pull/5271 to fix a minor typo in doc/modules/neighbors.rst L470 I'll do a git grep to check for more typos, if there are any :sweat_smile: __eou__	User

TP	how about `"Group labels for the samples used while splitting the dataset into test/train set."`? @amueller @vene This would be for all the `labels` parameter in `.*Label.*` classes... __eou__	User

TP	yes that sounds good __eou__	User

TP	@amueller @vene I've fixed the documentation and the `labels` param... I've hosted the doc (with examples and all the new changes but without plots) [here](http://rvraghav93.github.io/doc_builds) The doc with plots is building... It will hopefully get over in a few hours and I'll host it once its done... Ah one more thing... the appveyor not testing the `model_selection` is not yet fixed... :/ I suspect its because that module has not public python files? That would be the only thing left to investigate apart from your final reviews on rvraghav93/scikit-learn#4 And moving `grid_search.rst` to `search.rst` once the review is over :) __eou__	User

TP	Thanks @rvraghav93. I'll try to review soon. I have a couple of hundred emails I need to read, and we want to release this week, though @ogrisel (or anyone else) do you remember the recent blog post that explained tree-based models as linear combinations ? I don't think that was it. there was a way to explain the prediction made for a single point as a simple function of the features somehow __eou__	User

TP	@amueller do you mean Tianqis? https://github.com/scikit-learn/scikit-learn/pull/5222 __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/5293 reviews for 0.17? @vighneshbirodkar This one would be good to fix: https://github.com/scikit-learn/scikit-learn/issues/5089 __eou__	User

TP	Can @amueller or someone else OK this ? https://github.com/scikit-learn/scikit-learn/pull/5234 It is a minor change and can be easily included in the upcoming release __eou__	User

TP	@amueller There are some warnings due to PIL not being there. I can either install PIL on travis or skip those tests like this https://github.com/scikit-image/scikit-image/blob/master/skimage/future/graph/tests/test_rag.py#L51 __eou__	User

TP	 scikit-learn/scikit-learn#5234 already has my +1 __eou__	User

TP	@vene would you be coming for the sprint?? :) - https://github.com/scikit-learn/scikit-learn/wiki/Upcoming-events ;( May I ask - funding or busy?? :) __eou__	User

TP	Unfortunately not :( __eou__	User

TP	Both... I'm in a slightly awkward position in my PhD I might visit Paris in the winter though No clue, it's just hope for now. __eou__	User
TP	Okay!! See you there ;)__eou__	Agent

TP	Thats great!! when?? __eou__	User

TP	@amueller reviews for  rvraghav93/scikit-learn#4 please? :grin: (2 comments of Joel are yet  to be addressed there) __eou__	User

TP	Sorry I'm still quite sick and stressed. I'll get to it asap __eou__	User

TP	Oh!! Okay get well soon :) __eou__	User

TP	I'm toying with the idea of doing a PyData NYC pilgrimage next month. Anyone planning on being there? Chance for a sklearn sprint perhaps? __eou__	User

TP	Count me in __eou__	User

TP	Do we need to have the notifications for test failures in gitter activity bar... wouldn't notifications on PR activity suffice? @ogrisel __eou__	User

TP	@rvraghav93 we can try adding them in gitter. If it's too noisy, we can remove it. __eou__	User

TP	Okay :) __eou__	User

TP	BTW from discussions at #4254 I don't think #4225 will be in soon... Should it still be tagged 1.7? __eou__	User

TP	I agree, re-tagged. __eou__	User

TP	Where is the best place to find help on deprecation issues for scikit?  I'm having trouble using Multilabelbinarizer and neither stackoverflow nor googling are helping. __eou__	User
TP	What kind of issue?__eou__	Agent

TP	I'm trying to use the accuracy_score function (or even the Confusion matrix function) and I've transformed my Ytest using an multilabel binarizer so it's type is "multilabel-indicator" but the predicted values are in the form of binary. so I get a "ValueError: Can't handle mix of multilabel-indicator and binary" error, but multilabel-indicator seems to be supported according to line 93 in classification.py __eou__	User

TP	@TracyMRohlin please write this a small reproduction code snippet on small random data (e.g. using `np.random.randn` and such) and post it as a question on stackoverflow. If you think this is a bug, post it as an issue on the scikit-learn issue tracker instead. __eou__	User

TP	#3123 can be closed! __eou__	User

TP	@ogrisel Manoj wants you to review his #4242 if possible :P __eou__	User

TP	I would like to focus on bug fixing for the release this week... __eou__	User

TP	Okay :) __eou__	User

TP	@TomDLT Can I take #4523 up? __eou__	User

TP	yes sure! __eou__	User

TP	@ogrisel I feel #4826 can be included in 0.17?? It was already [reviewed by Andy](https://github.com/scikit-learn/scikit-learn/pull/4826#issuecomment-125715318) a second review should make it merge-able? __eou__	User

TP	opening my sklearn inbox now everybody brace themselves for spam (by me) __eou__	User

TP	I'm eagerly waiting ;) __eou__	User

TP	did https://github.com/scikit-learn/scikit-learn/pull/4826 __eou__	User

TP	What is the right way to get a nice unique and consistent hash value for an estimator? can I hash something like `type(est), est.get_params(), ...`? __eou__	User

TP	with or without the part that is estimated from data? Without that should cover it. __eou__	User
TP	lets say *with*__eou__	Agent
TP	If est.get_params() contains a random state, we probably need to fix it to something?__eou__	User
TP	or rather, optionally with__eou__	Agent

TP	well with is harder. I think we opted for storing the data along, right? I think storing the class, get_params, and the data is enough. With a fixed random_state that is. actually, what is the definition of unique consistent hash? should they be the same if a) they are the same object [probably not] b) they behave the same way? c) they are the same in memory? I guess the answer is c)? __eou__	User

TP	I'm trying to solve this more generally and in isolation from the dasklearn project.  Is there a consistent set of attributes on a BaseEstimator that define it?  After I call `estimator.fit(X)`is there a set of attributes on the object that I can consistently check?  Or does this vary estimator-to-estimator? is there a way to check if a model is fitted? or to revert it to a non-fitted state? __eou__	User

TP	If I understand you correctly, What you require is partly similar to the model similarity checking problem at #4841 AFAIK you can only have a relative equality check and not a(n) (absolute) hash value that can uniquely identify a fit-model... __eou__	User

TP	@mrocklin beware of `est.get_params(deep=True)` that includes both the subestimator instances and their params e.g.: ```python >>> from sklearn.svm import SVC >>> from sklearn.decomposition import PCA >>> from sklearn.pipeline import make_pipeline >>> p = make_pipeline(PCA(3), SVC()) >>> p.get_params(deep=True) {'svc__probability': False, 'svc__decision_function_shape': None, 'svc__degree': 3, 'pca__copy': True, 'svc__tol': 0.001, 'svc': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',   max_iter=-1, probability=False, random_state=None, shrinking=True,   tol=0.001, verbose=False), 'steps': [('pca', PCA(copy=True, n_components=3, whiten=False)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',   max_iter=-1, probability=False, random_state=None, shrinking=True,   tol=0.001, verbose=False))], 'svc__cache_size': 200, 'svc__max_iter': -1, 'pca__n_components': 3, 'svc__coef0': 0.0, 'pca__whiten': False, 'svc__shrinking': True, 'pca': PCA(copy=True, n_components=3, whiten=False), 'svc__gamma': 'auto', 'svc__verbose': False, 'svc__C': 1.0, 'svc__kernel': 'rbf', 'svc__class_weight': None, 'svc__random_state': None} ``` __eou__	User

TP	> After I call estimator.fit(X)is there a set of attributes on the object that I can consistently check? Or does this vary estimator-to-estimator?  It varies on an per-estimator basis. Attributes learned from data (by the call to fit) ends in `_`. `get_params` only returns constructor parameters (aka hyperparameters) not the fitted parameters we don't have a good abstraction to introspect / serialize / deserialize fitted models. to revert to a non-fitted state you can use:  ```python >>> from sklearn.base import clone >>> unfitted_est = clone(fitted_est) ``` the `clone`  name is not necessarily a good name... __eou__	User

TP	Is there an equivalent for to ask if it is fitted? __eou__	User

TP	Have not read any history on this, but https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py#L636 might help? Need to know what attributes get set when fitted for a given est though I think. I think if you grep'd the repo you could find an exhaustive list of those being used to check. __eou__	User

TP	@mrocklin there is no way currently to know if something has been fitted. The "idiomatic" way is to try and predict. If it hasn't been fitted, it will raise an appropriate error. But you need to know the number of features is might have been fitted with. @ogrisel did you have time to set up the doc build server yet? __eou__	User

TP	wow this is the worst: https://github.com/scikit-learn/scikit-learn/issues/5267 __eou__	User

TP	Is having an issue for splitting the current utils into private/public worth it? __eou__	User

TP	not sure. we should get the model_selection stuff done first. I'm catching up right now ping @GaelVaroquaux are you here? also ping @ogrisel I might be able to help with releasing this week. should we? Or wait for the sprint? we cam we can't really merge model selection before releasing __eou__	User

TP	If we have the model selection merged by this week I can work on the multiple metric thing during sprint :) You are coming right? Gael is not in gitter ;) __eou__	User

TP	I just updated to numpy 1.10.1 and now I get a lot of test failures. Anyone else? TypeError: Cannot cast ufunc subtract output from dtype('float64') to dtype('int64') with casting rule 'same_kind' running conda __eou__	User

TP	I can confirm that! (numpy pip installed) http://stackoverflow.com/a/14270230/3109769 __eou__	User

TP	`np.cancast(np.float64, np.int64)` is `False` from `numpy 1.10.1` __eou__	User

TP	Setting the `dtype` at `check_array` stage fixes these failures... Do we need a travis build for 1.10 or should we update one to check for numpy 1.10.1? (@ogrisel?) - PR at #5398 @vene la multi ani!! :P __eou__	User

TP	I finally took the time to fix the docbuilder machine to update the dev/ website. It seems to work correctly but let me know if you spot problems. There is a couple of broken example in master. __eou__	User

TP	@amueller I would like to release the beta tomorrow. @lesteve is working on the joblib 0.9.0 release right now. __eou__	User

TP	Thanks @rvraghav93 :) __eou__	User

TP	@ogrisel cool. Anything I should look at in partticular? @ogrisel the numpy 1.10.1 looks bad. can you confirm? __eou__	User

TP	@ogrisel I'm still catching up with github notifications and my health sucks :-/ __eou__	User

TP	numpy 1.10.1 need fixes but seemingly not too complicated I have not checked the LogisticRegressionCV issue __eou__	User

TP	Ok. just got to sklearn notification inbox zero. I'll have a celebratory dirty chai latte and then look at LogisticRegressionCV and other issues that we enthusiastically tagged for 0.17 the doc build seems to be working. pushed 30 seconds ago! Awesomeness!! Thanks @ogrisel ! (and sorry for the constant nagging about it ) and scikit-learn.org/dev/auto_examples/preprocessing/plot_function_transformer.html that would be sweet. sure @rvraghav93 do you want to build the docs and see if you find the errors? or I'll do it. I haven't checked doc build errors or testing errors recently __eou__	User

TP	> the doc build seems to be working. pushed 30 seconds ago:  I just did a fix :) __eou__	User
TP	http://scikit-learn.org/dev/auto_examples/model_selection/plot_roc.html is not rendered__eou__	Agent
TP	meh__eou__	Agent

TP	there are errors in some examples I need to deploy an HTTP server to publish the doc build log. +1 a DNS don't have time to do that tonight though +1 for the dirty chai latte :) I'll go and get some dinner now, see you later. __eou__	User

TP	sure :) It would be good to get https://github.com/scikit-learn/scikit-learn/pull/4478 merged. I'll add a whatsnew now also https://github.com/scikit-learn/scikit-learn/pull/5395 __eou__	User

TP	@amueller Sorry I saw the chat just now! I'll build the docs and look for errors  :) __eou__	User

TP	@rvraghav93 I was planning to look at the broken examples too. Let me know if we can split the work between the two of us. __eou__	User

TP	Yes sure! I'm looking at http://scikit-learn.org/dev/auto_examples/model_selection/plot_roc.html Let me know if you want to take over... yea! __eou__	User

TP	Do you have a list of all the broken examples? I am generating the doc right now but if you already have the full list I could look at another broken example. __eou__	User
TP	Wait Andy posted one more broken link right? - [scikit-learn.org/dev/auto_examples/preprocessing/plot_function_transformer.html](scikit-learn.org/dev/auto_examples/preprocessing/plot_function_transformer.html) Unless you are more than 20% done with the doc generation I'll generate the list and let you know shortly :) I need to generate the docs with plots for the one of my PRs anyway...so I could save you the trouble... If you want please take over the http://scikit-learn.org/dev/auto_examples/model_selection/plot_roc.html too... that seems to be due to a problem in importing (`from scipy import stats` seems to fail(?))__eou__	Agent
TP	Is there a way to work with 2 branches at once? :P__eou__	Agent
TP	Okay!! thanks :)__eou__	Agent

TP	OK I'll take a look at http://scikit-learn.org/dev/auto_examples/model_selection/plot_roc.html __eou__	User

TP	> Is there a way to work with 2 branches at once?  clone the scikit-learn repo twice, create 2 conda env or 2 virtualenvs and `pip install -e .` each repo in each env. __eou__	User

TP	@amueller any review for the joblib sync #5399? It reverts a broken experimental change in the pickle format that was in introduced in joblib 0.9.0b2 (hence not part of scikit-learn 0.16.1). See:  https://github.com/joblib/joblib/blob/master/CHANGES.rst#release-092 RandomTreesEmbedding looks like a real regression :( __eou__	User

TP	I generated the doc locally so I am going to create one ticket by broken example. FWIW I found 5 broken examples: examples/applications/plot_tomography_l1_reconstruction.py examples/ensemble/plot_random_forest_embedding.py examples/manifold/plot_lle_digits.py examples/model_selection/plot_roc.py examples/svm/plot_rbf_parameters.py Note the tomography one is broken only for numpy 1.10 __eou__	User

TP	you did doc with plots right? How did it get over so fast? It takes forever on my machine :/ Anyway let me know if you want me to look into any of those while you work on other things... __eou__	User
TP	yeah make html. Too ~25 minutes on my machine.__eou__	Agent

TP	Let's split the work on fixing the examples. __eou__	User
TP	I'll take the last 2 if no one else is working on it?__eou__	Agent
TP	plot_roc and plot_rbf_parameters? sure I have a look at something else.__eou__	User
TP	I will have a look at plot_random_forest_embedding.py__eou__	User
TP	I can take plot_lle_digits should be trivial__eou__	Agent

TP	@rvraghav93 the plot_roc one is due to the roc_curves not all having the same shape, not sure why ... __eou__	User

TP	I'm back __eou__	User

TP	I did the plot_tomography_l1_reconstruction fix while I was at it. __eou__	User
TP	@ogrisel are you planning on uploading the website for the rc? No, right?__eou__	Agent
TP	I think it would be nice to fix https://github.com/scikit-learn/scikit-learn/issues/5324__eou__	Agent
TP	should I go for that?__eou__	Agent
TP	oh right there is https://github.com/scikit-learn/scikit-learn/pull/5402__eou__	Agent
TP	https://github.com/scikit-learn/sklearn-docbuilder/pull/6__eou__	Agent

TP	Is there a reason why we don't have the `requirements.txt` in our repo? __eou__	User

TP	yeah because it would need to include numpy and scipy and we don't want people to install this via pip on linux is anyone doing #5407 ? __eou__	User

TP	Okay.. and yes I am... __eou__	User

TP	http://scikit-learn.org/dev/modules/classes.html is entirely broken it's the sphinx version with the fun __eou__	User

TP	btw, do we want to fix the "FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison" ? @ogrisel which version of sphinx is on the doc build bot? __eou__	User

TP	@ogrisel I mean it is not really release related but for the build bot we need to fix the sphinx version to 1.2.3 not sure how to do that with a salt state oh wait, just  name = sphinx == 1.2.3 yeah that is no good. I just sent you a PR to fix it to 1.2.3 current stable doesn't build the api docs website build is fixed. Thanks @ogrisel __eou__	User

TP	> @ogrisel are you planning on uploading the website for the rc? No, right?  Updating the /stable/ part? No I don't think we should do it for the beta. __eou__	User
TP	I agree.__eou__	Agent
TP	so the plotting examples are "not that critical" for the release. Though it would be nice to fix them__eou__	Agent
TP	for the final they are__eou__	User
TP	yeah but not for the rc.__eou__	Agent
TP	I thought you wanted to do an RC/beta today? Or the full release?__eou__	Agent
TP	@amueller  >  which version of sphinx is on the doc build bot?  It using the latest stable version installed by pip: https://github.com/scikit-learn/sklearn-docbuilder/blob/master/srv/salt/sklearn-docbuilder.sls#L107__eou__	User
TP	It's not updated as long as the machine does not crash and we restart it though (which is very rare).__eou__	User
TP	ok__eou__	User
TP	I just put some instructions to let you try to run it. If it fails, tell me and I will do it.__eou__	User
TP	good__eou__	User

TP	ok. gotta grab some lunch now @rvraghav93 if you're bored you can try to bisect https://github.com/scikit-learn/scikit-learn/issues/5267 __eou__	User

TP	do you want me to do the sklearn-docbuilder stuff? you can always give it a try later guten Appetit! ok __eou__	User

TP	merci I can try it later I don't see any fires at the moment __eou__	User

TP	fixing more warnings in master and fixing the doc-build would be nice __eou__	User
TP	ok__eou__	Agent

TP	how about the "FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison" ? from numpy? anyhow, my stomach demands attention cool. I'll fix "FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison" in a couple of minutes. I'd like to include that in the RC. Then you can cut it tomorrow morning? __eou__	User

TP	what is the timeline? how long will you be around today? __eou__	User

TP	many of the the "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison"  are due to us comparing parameters that might be arrays to strings. like ``if init == "something"`` __eou__	User

TP	I will soon logout. I wanted to do the RandomTreeEmbedding example fix but we can do that after the cut of the 0.17.X branch. Do you want to cut it today? Otherwise I can do it tomorrow morning (Paris time). I just merged the joblib upgrade __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/5413 __eou__	User

TP	@ogrisel do you understand this https://travis-ci.org/MacPython/scikit-learn-wheels/jobs/85847030 ? install_scripts failed __eou__	User

TP	@amueller it's weird I just did it (bdist_wheel) on my mac and I don't get the error. python 3.5.0 as well. __eou__	User

TP	Maybe the version of the `wheel` project pre-installed on the travis host is too old for Python 3.5.0. Will change the config to make it upgrade it. > Requirement already satisfied (use --upgrade to upgrade): wheel in ./venv/lib/python3.5/site-packages __eou__	User

TP	@ogrisel I saw you commited something. did it fix it? the weird gcc flags were... weird but I didn't see it come up again after I added the flags ah, wheels work. we only need to sync master for the 3.3 fix __eou__	User

TP	For 3.5 on the OSX  wheel builder, yes it's fixed (just ugraded the `wheel` package on travis). For your 3.3 fix that I merged I plan to only include it for the next release. I would like to have the tag match the content of the archive. __eou__	User

TP	What do you mean by that? that the version tag in the sklearn repo is the same as the wheel with the same name? makes sense. I have to work on other stuff today. You are busy with Pycon FR right? it looks like #5008 would be good to have / review __eou__	User

TP	I'd like to start contributing and work on #5089, so I was wondering if it's better to submit one PR with a lot of warning fixes or PRs in chunks that deal with one specific type of warning? __eou__	User

TP	@oolongtea  Start with one that fix related stuff if it's your first PR. It's easier to give you feedback to get started. __eou__	User

TP	does anyone know a good "real world" dataset for regression on which regularization helps? In diabetes and boston linear regression does as well as any other linear model, which is sad. the only examples for regularization we have are using ``make_regression`` so diabetes is from the lars paper, but lars doesn't actually make better predictions than ols on diabetes..... __eou__	User

TP	How about high dim. data, say movie review stars from text __eou__	User

TP	yeah that would work. but I don't want to go too high dim. I settled for polynomial features on boston, which works well It's for the book, and I don't want to explain bag of words at this point. Not sure I want to explain polynomial features, but it's a little easier. might be an interesting example for non-synthetic data (for the examples folder I mean) __eou__	User

TP	      __eou__	User

TP	should there be need contrib tag in #4687? Thanks :beers: __eou__	User

TP	@zermelozf #3846 ? There are like 90 odd examples to be added ;) (@ogrisel and others - Do you feel this one would be useful or should he pick something else from the recently tagged pool of issues to work on?) here too (need contrib to be removed) - #5322 @glouppe If you are able to find time could you review #4294 ? ;) Lol okay :D I am eagerly waiting for Andy ;) __eou__	User

TP	@glouppe Could you please remove the "Need Contributor" tag from #5474, #4687, #5455, #5447, #5432, #5380 ? __eou__	User

TP	all done __eou__	User

TP	@glouppe also from this one - 5290 and maybe assign it to Arthur? (sorry for repeatedly pinging you ;) ) __eou__	User

TP	done __eou__	User

TP	unfortunately, I cannot assign to people outside of the scikit-learn team dunno why __eou__	User

TP	This one can be closed - #5060 (It was fixed by #5084) __eou__	User

TP	@ogrisel in sklearn.metrics.pairwise._parallel_pairwise, the joblib Parallel loop is slowed by thread locking, which is weird with the multiprocessing backend __eou__	User

TP	issue #5481 is actually a batch of small issues on estimators that fails on read only memory map data once check_array process memory map without copying their content unnecessarily. I guess it could be labelled as easy as it could be addressed by new contributors __eou__	User

TP	wow, this is poisoned gift you are giving me there __eou__	User
TP	:P :P__eou__	Agent
TP	I never touched these modules though__eou__	User

TP	hi guys, not sure if that's the place to ask but I'm looking for a way to use a function-call instead of already having the target value next to the features. So I'd like to put in the features as usual but don't know the results yet. This is because I don't know them and also don't want to run them since it would take to long to do this with a grid-brute. I'm hoping to save time by using a more advanced search mechanism instead that I can feed a function that then puts out the results. I've only found examples so far where the target values are already known. Any help on what to look for? another thing I'm looking for is a function that I can give a sample of numbers, let's say a np array of 1000 numbers and then have that function create a sample of N numbers with the same distribution characteristics as the input numbers... __eou__	User
TP	__eou__	Agent

TP	can anyone get me? I'm in the lobby __eou__	User

TP	Olivier is coming! __eou__	User

TP	wohoo wifi __eou__	User

TP	\o/ __eou__	User

TP	@ogrisel can we trigger circleci on https://github.com/scikit-learn/scikit-learn/pull/5451 again? __eou__	User

TP	Btw, if you add an entry to whatsnew.rst, please include a link to the github issue / pull request! __eou__	User

TP	Hello, I am trying to understand the current Gradient Boosting code Can someone point out where step 3 is being performed ? According to the this https://en.wikipedia.org/wiki/Gradient_boosting#Algorithm __eou__	User

TP	@rvraghav93 In addition to finding issues to fix at the sprint (which is great!), could you also try to promote reviewing :) You can tell people to look for PRs with the [MRG] tag, these are the ones ready for review __eou__	User

TP	@vighneshbirodkar you mean 2.3,  right? __eou__	User

TP	if anyone has a PR that needs review that is what I am focusing on - though some stuff may be outside my wheelhouse __eou__	User

TP	@kastnerkyle #4294 would need some love, but this is huge (though both arnaud and andy have been reviewing it) #5291 maybe? or any help with all the mrg+1 PRs is welcome, so that we can have some of those merged __eou__	User

TP	@kastnerkyle https://github.com/scikit-learn/scikit-learn/pull/5358 ;) __eou__	User

TP	Anyone wanna weigh in on #5319 (adding k modes to related projects) Anything anyone want's me to have a look at? __eou__	User

TP	I have an urgent bank work :( I'll be coming in the afternoon. Apologies :grin: @glouppe sure!! Will do :) __eou__	User

TP	@rvraghav93 no problem. @rvraghav93 hopefully by then I will have finished reviewing your CV PR ;) __eou__	User

TP	can we discuss #5023 __eou__	User

TP	@amueller #5358 looks like a +2 now with @ogrisel __eou__	User
TP	@arthurmensch needs to rebase it first__eou__	Agent
TP	I mean #5358__eou__	Agent

TP	@ogrisel opinions on #5023 ? I wanna ask gael and arnaud, too lol https://github.com/scikit-learn/scikit-learn/pull/5498/files#r42594166 I think this is a first, Gael telling me to be more pragmatic not sure I should screenshot __eou__	User

TP	@ogrisel you agree that GaussianProcesses should be removed in 0.20, right? then we need to fix and backport __eou__	User

TP	old GP is deprecated in 0.18, removed in 0.20 indeed __eou__	User

TP	ok my bad then i'll fix that done __eou__	User

TP	#5504 and #5505 need contributors: those are documentation related issues. @rvraghav93: @amueller told me to tell you to leave those issues for others ;) __eou__	User
TP	thanks @glouppe__eou__	Agent

TP	hm similarly here: #5452 we should change the version. or backport the deprecation to 0.17 __eou__	User

TP	#5500 as well documentation fixes __eou__	User

TP	ah needs contributor as well... __eou__	User

TP	@kastnerkyle if you are bored, maybe look at #5008 (needs reviews) __eou__	User

TP	I though #5141 was merged, whoops (randomized_svd default parameters and normalization) reviews appreciated __eou__	User

TP	I am +1 on 5141 so it is +2 now - if tests pass you can go ahead and hit the button @amueller I thought those were addressed, but we can let him hit the button instead __eou__	User

TP	you can go ahead and merge Kyle :) __eou__	User

TP	Yes @amueller , I mean 2.3 __eou__	User

TP	@glouppe traaaaaaaaaaaaavis. so slow. it needs to hurry up before andy stops eating... he is faster than me :D I'll never win the "merge button shootout" __eou__	User

TP	@vighneshbirodkar in ``update_terminal_region`` I think. not sure though @ogrisel had some thoughts about doing only two power iterations, I think __eou__	User

TP	does anyone at the sprint have ibuprofene? @vig @vighneshbirodkar about the one hot encoder: we are fine if dtype=object, right? __eou__	User

TP	can people still look at travis logs? https://travis-ci.org/scikit-learn/scikit-learn/builds/86595504 for example? I think travis just throttled our IP __eou__	User

TP	keeps "loading" here __eou__	User
TP	so it is not our IP__eou__	Agent
TP	https://www.traviscistatus.com/__eou__	User

TP	H99 errors https://devcenter.heroku.com/articles/error-codes#h99-platform-error could be anything __eou__	User

TP	@ogrisel Lol okay ;) and thanks a lot for the reviews :) __eou__	User
TP	I still need to finish that one.__eou__	Agent

TP	I remember there was a discussion about how to call a function that gives uncertainty estimates on regression values. Where was that? I can't find it any more We need reviews on #4490 and want to backport it. __eou__	User

TP	I'm going to lie on the couch in front of the room if anyone is looking for me. I'm beginning to question my decision to take a flight yesterday ^^ __eou__	User

TP	> I remember there was a discussion about how to call a function that gives uncertainty estimates on regression values. Where was that? I can't find it any more  `return_std` on the `predict` method of old GPs I thought we discussed the use of the same parameter for another non-GP regressor (I checked RidgeCV but it's not it apparently). __eou__	User

TP	That is doing interval predictions. that is slightly different. sometimes you want to evaluate the density at a certain point __eou__	User

TP	@amueller While you are there, can you have a discussion with the other about the `OneHotEncoder` issue and in general how we can handle strings with it ? others* __eou__	User

TP	the joblib fun on windows is back: https://ci.appveyor.com/project/amueller/scikit-learn/build/1.0.1408/job/7sldjuplbqf2t31u @vighneshbirodkar what was the open questions? if there are  columns with mixed strings and numbers? so the problem is only if you pass a list? but maybe the new code based on unique would work for both integers and objects? well ok that is not that important __eou__	User

TP	Yes, specially an array with `1` and `"1"` in the same column @amueller What is your definition of "fun" ? :D __eou__	User

TP	something that you cannot reproduce on your dev environment Maybe @TomDLT can give it a try. __eou__	User

TP	I have faced similar issues before somewhere else. __eou__	User

TP	Yes: it used to be very frequent in the past and at some point it stopped appearing. But now it's back. I am not sure whether it reveals a true problem in the way we use multiprocessing under windows or is caused by a problem on the appveyor CI infrastructure. __eou__	User
TP	I have windows on dual-boot, but I have to setup scikit-learn over there before I can run tests. Does anyone else over there have it ?__eou__	Agent
TP	I have a windows VM in the cloud that I use for such debugging. I can give it another try.__eou__	User
TP	@amueller You mean it could be computed by some function in `scipy.optimize` or similar ?__eou__	Agent

TP	PR #5492 is ready for review. Caching + removal of .c file seems to be working It's a bit hackish though, waiting for some proper build system... __eou__	User

TP	@amueller In `2.3` when they say "argmin" I assume there will be some sort of a loop ? I can't seem to find that anywhere __eou__	User
TP	@vighneshbirodkar there doesn't necessarily be a loop__eou__	Agent

TP	@ogrisel I can not reproduce the issue with conda locally __eou__	User

TP	FYI the https://ci.appveyor.com/project/amueller/scikit-learn/build/1.0.1408/job/7sldjuplbqf2t31u failure might be caused by the fact that this was deployed on @amueller's appveyor account which runs of a different infra than the one we should usually run on (that is using the @sklearn-ci account). I reconfigured the appveyor webhook on the scikit-learn repo and hopefully the future appveyor builds will run on the correct infra and not fail this way anymore. If this is not the case we will have to investigate. > @ogrisel I can not reproduce the issue with conda locally  So it might be a consequence of the past travis outage __eou__	User
TP	@vighneshbirodkar  or there could be a closed form solution__eou__	Agent
TP	@ogrisel yeah I think it was a fluke__eou__	Agent
TP	no have have not asked him, and yes we can talk about strings in ohe__eou__	User

TP	If you see this https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L254 for example,  only the learning late is multiplied. __eou__	User

TP	@vighneshbirodkar I'm not sure. Maybe ping @pprett ? @ogrisel can we talk about handling strings in OneHotEncoder ? __eou__	User

TP	@ogrisel have you asked arnaud about the greater is better issue? __eou__	User

TP	Yes __eou__	User

TP	Yes, and I am assuming based on the dtype of the input, we will have 2 different pieces of code to process them (object and dtype) __eou__	User
TP	why?__eou__	Agent
TP	Because the current code only works for integers__eou__	User
TP	right.__eou__	Agent

TP	maybe we do ``check_array`` and if we get back a string dtype, we do the conversion again with dtype object. Does that solve all problems? __eou__	User
TP	Even with #5270 the code cannot support non-integer types__eou__	Agent
TP	the problem is if converting something like `[1, '1', 2 , 3 , 4]` to an integer, casting to integer works and gives wrong results__eou__	Agent

TP	Now that I think about it, since the `LabelEncoder` has a lot of the functionality needed, instead have a subclass of `Pipeline` called `OneHotObjectEncoder` __eou__	User

TP	So the KNN docstrings mention from place to place the idea that `metric` can be `"precomputed"`, but I can't get that to work, the current code doesn't seem to implement it. I can't seem to find any documentation or any open issues about this. hmm I think it's fixed in master actually, sorry about the noise __eou__	User

TP	good morning :) __eou__	User

TP	@vighneshbirodkar For the one hot encoder: that that's why I said try to do check_array, and if we get a string type back, instead make it an object type. your example will give a string type if we detect that and instead convert with an explicit object dtype, it'll work __eou__	User

TP	good morning __eou__	User

TP	good morning __eou__	User

TP	hey, by any chance, what would you recommend for a good introduction to linear models? (to be recommended as reading materials for a workshop with an audience of physicists) (covering linear regression, lasso, svm, etc) okay, thanks Kyle =( Given my +1 and partial reviews from @amueller, @ngoix and @jmschrei, can we merge #5487? thanks alex :) __eou__	User

TP	good morning - @ogrisel might comment more but I would probably say elements of statistical learning or the intro version of that (can't remember the name) "Introduction to Statistical Learning" __eou__	User

TP	I don't have a better suggestion __eou__	User

TP	is it me or github is very slow at the moment? __eou__	User
TP	it seems fine now__eou__	Agent

TP	@glouppe you just missed the introduction of the scikit-learn advancement proposal __eou__	User

TP	@glouppe I think it is important to note that SLAP is our new acronym... __eou__	User

TP	@kastnerkyle @GaelVaroquaux called the repo "enhancement" :-/ https://github.com/scikit-learn/enhancement_proposals stupid git question: how to I update a local branch that is a pr/1234 branch? i.e. that comes from a pull request I think I'll have to head out soon @kastnerkyle that is not working yet, there is a PR __eou__	User

TP	does anyone know if CircleCI is pushing the doc to github? Or just creating an artifact that some other thing can get I am trying to make CircleCI push a doc (for another project) after succesful build __eou__	User

TP	https://skll.readthedocs.org/en/latest/run_experiment.html#param-grids-optional https://github.com/EducationalTestingService/skll/blob/5ea61b8dfc23570e661468457a262b6c2242daa9/skll/learner.py#L62 __eou__	User

TP	ok - I got the build part working on sklearn-theano pretty easily, and I think Fred will take a look at it for Theano as well. This is a lot better than a cron job... __eou__	User
TP	indeed__eou__	Agent
TP	lol I don't think I wrote a line of code this week__eou__	Agent

TP	@amueller ... http://opendatascicon.com/scikit-learn-code-sprint/ ... Looking forward to it! __eou__	User

TP	@trevorstephens cool :) glad to have you there! __eou__	User

TP	will we have MLP by today? :) and isolation forest? that would be very nice __eou__	User

TP	sadly mlp seems unlikely IMO - lots of hard choices but it is getting closer. if there are other things to be reviewed I can take a look - I am also hopeful the PCA fixes by Giorgio will go today (if they didn't last night!) __eou__	User

TP	it's been more than a year since the MLP PRs have started :/ if these decisions are only about internals, we should try to be pragmatic at some point \o/ __eou__	User

TP	yes it is mostly logical - some of the core logic is spread over several classes which makes it hard to reason about its behavior especially w.r.t to stopping criterion. But Andy has a student on it full-time (ish) who is quite good so I don't think it will be much longer and the documentation is basically done IMO - which was a huge chunk of the work @glouppe things are looking positive for the MLP - you might get your wish :D #5299 is very close if anyone wants to review __eou__	User

TP	anyone wants to review #5540 ? @kastnerkyle but if it's only refactoring, we don't really need to do that for merging if it's not public API, right? __eou__	User

TP	we need to do ##5502 if anyone is looking for an issue to pick up it's needed for 0.17 __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/5274#discussion-diff-42377195 https://github.com/scikit-learn/scikit-learn/pull/5565#issuecomment-150609391 __eou__	User

TP	@glouppe it's gonna happen. If Travis ever runs __eou__	User

TP	I have a stupid general question here: What are possible ways to do key words/sentenses extraction from a large text corpora? __eou__	User

TP	@bawongfai have you tried textrank? __eou__	User

TP	@vortex-ape not really. How does it compare to document vectorisation? Then is it good bad for a short sentence that contains potential keyword? bad i meant So what do you suggest as an alternative? __eou__	User

TP	Text rank tries to order sentences by their importance A sentence is of more importance  if it talks about a large number of things and thus gets a higher score. __eou__	User

TP	From what I've used it, bad __eou__	User

TP	It generally gives a long sentence talking about lots kf things Can you give me an example of an output you would expect? You can give this a try, but I have no idea what kind of output to expect. http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html __eou__	User

TP	@vighneshbirodkar I would like to have a summarisation of conversation conversation, by nature, could be short I guess the summary would be about text summarisation and keyword extraction __eou__	User
TP	You can try the nmf example I posted.__eou__	Agent

TP	For example we are having one, what would you expect the summary to be ? __eou__	User

TP	would you want an abstractive summary or an extractive one? I used textrank once to get extractive summaries of a conversation and it seemed to work well, though I didn't compare it with other methods __eou__	User
TP	I don't know of any such technique. Since I had worked with text rank I thought I'd give my 2 cents__eou__	Agent
TP	extractive is fine for me__eou__	Agent
TP	Just pick 2 most important sentences and display__eou__	Agent

TP	You could experiment with both these methods and explore about other ones too, and see which one gives you the desired results, I used textrank as a quick hack in a hackathon so I'm no expert in this regard :smile: __eou__	User

TP	@vortex-ape thanks, i will try textrank first I would like to go into deep learning approach later __eou__	User

TP	@vortex-ape tried textrank, not too bad __eou__	User

TP	@ogrisel you think we can release weekend or Monday? Should we talk to conda folks? hm was about to reach out to asmeurer but I guess he is not the right contact any more ^^ http://asmeurer.github.io/blog/ trying ilanschnell now __eou__	User

TP	@amueller are you back ? __eou__	User

TP	@vighneshbirodkar https://www.youtube.com/watch?v=Q2J9F2sJMT4 __eou__	User

TP	sweet I didn't realize gitter embedded youtube videos. really important for github collaborations. __eou__	User

TP	https://youtu.be/FJbmB9k2Y88 __eou__	User
TP	:D__eou__	Agent

TP	@amueller some appveyor builds seems to be fast again, especially on 64 bit for some reason: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.3621 maybe there is some variability in the platform __eou__	User

TP	@amueller #5164 re-opened by accident? __eou__	User

TP	@trevorstephens thanks for the mail. #5164 reports more issues __eou__	User
TP	np__eou__	Agent
TP	yeah__eou__	User
TP	I never thought about the fact that anyone can edit the wiki and there is no tracking... hum__eou__	User

TP	i thought that one was limited to graphviz? there are a couple of other issues with other 32-bit fails\ __eou__	User

TP	there's tracking click 'edits' sorry 'revisions' under the title __eou__	User

TP	i moved the recent paris sprint info to 'past sprints' do core contribs get notified of the changes to wiki? __eou__	User

TP	not that I know of.  maybe I could subscribe yeah I saw, looks good :) __eou__	User

TP	reviews for https://github.com/scikit-learn/scikit-learn/pull/5661 would also be welcome @trevorstephens the review suggestions are not explicitly for you, I just like to spam the channel ;) __eou__	User

TP	haha ok. i have no familiarity with tsne anyhow :-/ but int64? i didn't even know that existed! number of ants to stack to get to the moon? __eou__	User
TP	damn, I confused #5534 and #5164__eou__	Agent
TP	you were right, #5164 should have been closed__eou__	Agent
TP	thought so. just the one failure__eou__	User

TP	anyone who has a spare cycle and cares about package organization, comments about the location of `partial_dependence` in #5653 welcome :-) __eou__	User

TP	anyone who as spare cycles, I just opened 7 pull requests, 4 of which are release blockers ^^ I just commented also, don't worry about it tonight ;) __eou__	User

TP	@amueller Where did you see the `OneHotEncoder` warnings ? __eou__	User

TP	No warnings on python 2.7.6 and numpy 1.8.2 __eou__	User
TP	yeah you need numpy 1.10__eou__	Agent
TP	I think__eou__	Agent

TP	Reproduced it, but yeah, I am better of looking at this tomorrow :D __eou__	User

TP	I would really like to get some feedback on how to treat the 32bit test failures __eou__	User

TP	Could you tell me more about them ? __eou__	User

TP	the 32bit failures? They are precision issues and I don't know whether to reduce the precision or ignore the tests or what else to do. __eou__	User

TP	Do you have some logs somwhere ? somewhere* __eou__	User

TP	@amueller , on #5682, would a test to ensure an index error is not thrown for the second example i gave be sufficient? or test the value error string ? __eou__	User

TP	Testing the value error string would be good  @vighneshbirodkar logs are here: https://github.com/scikit-learn/scikit-learn/issues/5534 but it is more a question to the other core devs on how we handle this. __eou__	User

TP	Ok __eou__	User

TP	hm... anyone have an idea why github stopped notifying me for comments on pull requests I created? ok I was just hallucinating, never mind __eou__	User

TP	. __eou__	User

TP	? I'm trying to check if we missed anything in whatsnew for 0.17 by running a diff against 0.16 some idiot changed the docstrings of all the classes so all files are changed __eou__	User

TP	sorry that . was a typo.. (gitter android app sucks... :/) and lol `*`silently hopes I don't show up in the git blame`*` :p __eou__	User

TP	there is an android gitter app? hm... meh __eou__	User

TP	yea its just the mobile site wrapped as an app... too slow and buggy... :/ __eou__	User

TP	They updated it __eou__	User

TP	anyone want to make the pdf docs build on 0.17.X ? ^^ __eou__	User

TP	http://cs.nyu.edu/~vnb222/temp/user_guide.pdf I can see figures overflowing in some pages ? __eou__	User

TP	thanks for having a look when I called "make dist" there were latex errors and the build didn't finish __eou__	User

TP	does anyone have matplotlib 1.5 to test the examples? I'm trying to pip install it in my conda env. I'm sure that's going to go great @MechCoder if you're bored, you can have a look at https://github.com/scikit-learn/scikit-learn/pull/5721 if anyone wants to help me find out why "clustering" isn't properly linked on the website, that would also be sweet ^^ __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/issues/5724 is also fun __eou__	User

TP	Do I need to have great knowledge about machine learning and AI to start with. or knowing some basic about it will help. I'm confused as of now I know python and some basic of machine learning and AI __eou__	User

TP	@manipalsingh013 knowing some basics is good but even that is not necessary to get started __eou__	User
TP	all right @amueller . Thanks__eou__	Agent

TP	Thanks for all your work @amueller, congrats on the release! :beers: __eou__	User

TP	btw sorry I missed your responses, I'll have a bit more of a tinker with MLP at some point __eou__	User

TP	Hey Folks, congrats for the release! Awesome work! Quick question, why does KMeans accept y in its fit() ? __eou__	User

TP	For API compatibility with other supervised learning algorithms... (and to safely pass y through kmeans in a pipeline) __eou__	User

TP	Thanks a lot @rvraghav93 :) __eou__	User

TP	Firefox users might find this handy ;) - https://addons.mozilla.org/en-US/firefox/addon/git-done/?src=search __eou__	User

TP	@rvraghav93 out of interest what does it look like in the PR, does it just add a "Done" comment when you click on the "Done" button? Not sure how useful this is to be honest. Generally when you do something following a comment, it gets hidden into an "outdated diff" section anyway. __eou__	User

TP	for some reason it doesn't work for me ;( It might be useful for those comments that aren't hidden after the change I think... ;) Apart from that you are right... its not really that useful since the comment gets hidden anyways... __eou__	User

TP	I'd love if someone did a firefox extension which shows "outdated diff" comments. I know there are some bookmarklets on the internet but it never worked for me somehow, I always end up having to copy and paste the code in the Javascript console, not great. __eou__	User

TP	as as in does not hide them by default u mean huh? __eou__	User

TP	see for example https://coderwall.com/p/akdgoq/expand-all-outdated-diff-comments-in-a-github-pull-request __eou__	User

TP	Install [greasemonkey](https://addons.mozilla.org/en-US/firefox/addon/greasemonkey/) and add this script to it, (you can enable or disable greasemonkey if you aren't using it for anything else) to enable/disable expanding the diff comments... ``` // ==UserScript== // @name        github expand outdated diff comments // @namespace   rvraghav93@gmail.com // @include     https://github.com/scikit-learn/scikit-learn/pull/* // @version     1 // @grant       none // ==/UserScript== $(".outdated-diff-comment-container").addClass('open') ``` One thing I wish the github PR page to show is a list of all the referred links and PRs or Issues in a neat side bar like stackoverflow has at the right, next to the question area... I am planning to write a greasemonkey script for that... I think it will be super useful to have all the links at the right instead of searching through the PR... __eou__	User
TP	Looks interesting, I'll give it a shot, thanks !__eou__	Agent

TP	Hey @amueller , I'm trying to decide between heckling the presenter at the scikit-learn tutorial @ ODSC or learning some d3.js ... Do you have a strong opinion on the matter? :smile: __eou__	User

TP	I hope all of you in Paris are safe :worried: __eou__	User

TP	master does not compile on py35 for me: __eou__	User

TP	__eou__	User

TP	This also did not go away by checking out `0.17`. __eou__	User

TP	Could you try "make clean" then make... __eou__	User

TP	`make clean; make` finishes fine, but then? should i `make install` as usual for C-progs? __eou__	User

TP	yes.. does that succeed now? __eou__	User

TP	```bash $ make install make: *** No rule to make target `install'.  Stop. ``` __eou__	User

TP	do you want to globally install? or user install or build in the folder? python setup.py install or python setup.py develop or python setup.py build_ext -i do some of these __eou__	User

TP	yes, but i have been advised above not to use that. i want to just install for my user account, what is the advised sequence of commands (as written above, `python setup.py install` failed). __eou__	User

TP	@michaelaye it would be python setup.py install --user __eou__	User

TP	hah, after the full run of make clean; make the `python setup.py install` actually worked now. thanks for your help. in case u wonder, i still need to self-compile even so im a conda user,  b/c i have pinned my numpy to 1.9.* b/c numpy 1.10 has performance probs with character arrays. and theres no scikit-learn for conda py3.5 numpy  1.9 on offer in conda. __eou__	User

TP	welcome everybody to the sprint! __eou__	User

TP	 hey all! __eou__	User

TP	Hello folks! I'm having an issue building sklearn from source. I've included my full logs here (http://pastebin.com/13LKxi55). I'm using pyenv with anaconda. Has anyone seen any issues with sklearn/cluster/_k_means.c failing to compile?  This looks a lot like my error https://github.com/scikit-learn/scikit-learn/issues/3114, but I don't understand the resolution. __eou__	User

TP	Which version of python are you using? __eou__	User

TP	``` (root) <unconvertable>  scikit-learn git:(master) pyenv version anaconda3-2.4.0 (set by /Users/maxlikely/src/scikit-learn/.python-version) ``` which is 3.5 __eou__	User

TP	I think that might be your issue Possible to build against an 3.4? comment on it __eou__	User

TP	suppose I've found an Easy issue to address, having a Needs Contributor label. how do I claim this as my own? __eou__	User

TP	@joshuacook I tried building against 3.4.3, I had the same issue. I can try 3.4.0. __eou__	User

TP	http://scikit-learn.org/stable/developers/contributing.html __eou__	User

TP	Here is the documentation on contributing code, for the person who just asked: http://scikit-learn.org/dev/developers/contributing.html#contributing-code __eou__	User

TP	@maxlikely I had issues with required libs being built against different versions. But I'm not using `conda` __eou__	User

TP	When I run `make` on a forked branch of master, I get 1 error and 1 failure.  Here is the traceback for the error http://pastebin.com/C8EpQT75 It involves reading a jpeg with PIL, which hasn't been working for me the past few weeks.  I have Pillow 3.0.0 installed __eou__	User

TP	Looking at https://github.com/scikit-learn/scikit-learn/issues/5686 __eou__	User

TP	looking at 5581, and need to reconcile the built path mentioned in the bug with the path in the source. bug mentions file:///home/andy/checkout/scikit-learn/doc/_build/html/stable/modules/neural_networks_supervised.html#more-control-with-warm-start, but the source path is actually doc/modules/neural_networks_supervised.rst. After updating such a doc, what is to be done to verify all is well before submitting a PR? __eou__	User

TP	When I run '''make''' on my branch, the build fails. Here is the traceback: http://pastebin.com/VgHFvhPF# Any help appreciated. __eou__	User

TP	I got the same error as @hallr   Any ideas why it doesn't "make"? __eou__	User

TP	Me too I have the same problem @hallr  @lazarillo __eou__	User

TP	any restructured text gurus? __eou__	User

TP	answering my own question, `make doc` builds html docs under doc/_build/html/stable __eou__	User

TP	Any traction on that make test error with the infinite value? __eou__	User

TP	hi all, if anybody would like to review my pull request for 'adding cython to requirements' in documentation, it is here: https://github.com/scikit-learn/scikit-learn/pull/5834 __eou__	User

TP	Hi @joshuacook  I don't know if I'd say guru, but I understand it fairly well. __eou__	User

TP	I'm going to take a stab at https://github.com/scikit-learn/scikit-learn/issues/5606. __eou__	User

TP	I have a PR for an error that happens in the tests: https://github.com/scikit-learn/scikit-learn/pull/5836 __eou__	User
TP	FWIW: I bypassed the build issue with `pyenv` by installing anaconda directly.__eou__	Agent
TP	@lazarillo where can I find you? I am red head in red flannel__eou__	Agent

TP	@amueller #4533 can be closed... __eou__	User

TP	Has anyone found an issue that they are working on that is simple and would enjoy more collaboration? __eou__	User

TP	Is there a norm against implementing visualization methods directly within an sklearn module? __eou__	User

TP	@hallr @NTBlok Can i work on issue #5827 with you? __eou__	User

TP	What kind of visualisation methods do you mean? @jonoleson __eou__	User

TP	@rvraghav93 Like I was considering adding a plotting method to the RFECV module that would graph the cross-validation scores for each subset of features. Just a simple line graph with num_features on the x-axis and cv_score on the y-axis. __eou__	User
TP	I am going to work on issue #5656__eou__	Agent
TP	edited pull request and ready to merge #5834__eou__	Agent

TP	I'm looking into #5364 __eou__	User

TP	we're looking into #5804 __eou__	User

TP	for someone interested in figuring out a segfault, you can take a stab at https://github.com/scikit-learn/scikit-learn/issues/5724 btw, fixing any errors in the build of the documentation (running make or make html in the doc folder) or fixing any warnings in the tests is also very welcome @MrChristophRivera @hugobowne just asked me about it. it is probably not as easy as the other issues. please check with him if he is working on it is @maxlikely in SF at the sprint? __eou__	User
TP	yes__eou__	Agent

TP	@NTBlok @fluxtransport and I submitted PR for #5827 - add contributors for 0.16 and 0.17 to docs __eou__	User

TP	@lazarillo and I are taking a stab at #4920 __eou__	User

TP	Is the problem with the news group downloads b/c the underlying data has moved to a new web site? https://github.com/scikit-learn/scikit-learn/issues/4711 __eou__	User

TP	I had a look at the segfault -- I cannot reproduce when using %run inside an ipython notebook. Using either of the new matplotlib methods to fetch the yahoo data. __eou__	User

TP	Pull request for the single dead link: https://github.com/scikit-learn/scikit-learn/pull/5829#partial-pull-merging __eou__	User

TP	@amueller Is issue Meta-estimators for multi-output learning #5824 still available? __eou__	User

TP	@MrChristophRivera i'm just looking into it now -- not sure whether i'll attack it . are you at the code sprint. thanks @amueller __eou__	User

TP	@MrChristophRivera probably is ;) you can also use the physical audio channel __eou__	User

TP	@amueller made changes you requested: https://github.com/scikit-learn/scikit-learn/pull/5841 __eou__	User

TP	Hey, I just wanted to check if anyone has claimed https://github.com/scikit-learn/scikit-learn/issues/5851 yet? __eou__	User

TP	git@github.com:RubyW/scikit-learn.git oops __eou__	User

TP	can someone review my change here: https://github.com/scikit-learn/scikit-learn/pull/5858 __eou__	User

TP	coupla PRs here: https://github.com/scikit-learn/scikit-learn/pull/5833 https://github.com/scikit-learn/scikit-learn/pull/5856 __eou__	User

TP	probably __eou__	User

TP	looks like the dataset for the scipy2015 tutorial sessions is not coming down? http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip download does not start. im using the fetch_data script in your github repo. ah worked now, just took ages for the 81 MB with no bytes flowing for minutes... __eou__	User

TP	@jonoleson I am unable to gauge how useful that usecase is... Once you convince @amueller it is useful enough (:P) you can add a plotting function similar to the `plot_partial_dependence` (in `ensemble/partial_dependence.py`)...  (But IMHO that particular usecase seems simple enough to not warrant a plotting helper function!) __eou__	User

TP	I have data with a mix of categorical and numerical values. It classifies very well using one-hot encoding + random forests but terribly using any non-tree method I have tried. I am looking for a clustering method that might work on this sort of data.  What might be suitable? I tried some standard methods that rely on euclidean distance but they are a disaster it seems I noticed that http://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering.htm exists __eou__	User

TP	#5765 can be closed... __eou__	User

TP	hi @rvraghav93 __eou__	User
TP	Hi @lesshaste :)__eou__	Agent
TP	Are you interested in random forests by any chance?__eou__	User
TP	I think there are some interesting directions to take the scikit-learn work__eou__	User
TP	not for that sorry.... I was going to make longer term suggestions for when that is all done..which I hope is ok__eou__	User
TP	but first I should say a huge thank you for the work you are doing__eou__	User
TP	adding to your TODO may not be a great way of saying thank you :)__eou__	User
TP	but actually.. before I get to that. Does scikit learn have any equivalent of the R package randomGLM ?__eou__	User
TP	It is described at http://labs.genetics.ucla.edu/horvath/RGLM/. I couldn't quite tell from the scikit-learn docs. There are lots of things that look similar but maybe not the same__eou__	User
TP	Thanks :D and no I'd be happy TODO :) You can actually raise an issue and add your suggestions of features which you feel are worthy of adding and why you think they should be added... That way you will get a larger participation of people  in discussions... :)__eou__	Agent

TP	Yes I will be working on Trees and RFs :) __eou__	User
TP	great!__eou__	Agent

TP	For me the priority is https://github.com/scikit-learn/scikit-learn/issues/5212#issuecomment-155387289 :) Do you have any suggestions in mind? __eou__	User

TP	good point.. because there is a risk my ideas are already done or just plain stupid, do you mind being a first stage filter? __eou__	User

TP	And about GLM what does it do? Generalized Linear Models? translates roughly to our `linear_model` module? Oh sure a stupidity filter... I'm in ;) __eou__	User

TP	the full title of the randomGLM talk is "Random generalized linear model:  a highly accurate and interpretable  ensemble predictor" . http://labs.genetics.ucla.edu/horvath/RGLM/TalkRGLM.pdf   In more detail it is an ensemble predictor based on  bootstrap  aggregation (bagging) of  generalized linear models  whose  covariates are selected using  forward regression  according to  AIC criteria. maybe we already have something that is equivalent to that? @rvraghav93  https://followthedata.wordpress.com/2013/10/10/random-generalized-linear-models/ has an explanation of randomglm too __eou__	User

TP	We got `bagging` and `boosting` in the `ensemble` module! (Is that what randomGLM does?)... I will read the links in a moment... __eou__	User
TP	thanks__eou__	Agent
TP	Ah that was interesting... we don't have it...__eou__	User

TP	But the paper - http://www.ncbi.nlm.nih.gov/pubmed/23323760 - has only 12 citations... I am not sure the core devs might want to take this into scikit learn, since this is not popular/old/academically established enough... See [this FAQ](http://scikit-learn.org/stable/faq.html#can-i-add-this-new-algorithm-that-i-or-someone-else-just-published) :) __eou__	User
TP	I like that rule__eou__	Agent
TP	:)__eou__	User

TP	ok back to my real suggestions :) __eou__	User

TP	for random forests, it would be great if we had a method to make a small interpretable versions.  One of the main drawbacks of random forests is that they end up like a black box. Can you read http://link.springer.com/chapter/10.1007/978-3-319-18356-5_20 ? or even to infer a single decision tree http://scikit-learn.org/stable/faq.html doesn't mention neural networks! @rvraghav93  see above __eou__	User

TP	ah..200 references.. I will work on that :) it's not a bad rule __eou__	User

TP	@rvraghav93  under the 200+ citations rule I withdraw all my suggestions :) __eou__	User

TP	#5834 ready for merge. minor changes to documentation. straightforward. __eou__	User

TP	200+ citations is not a hard and fast rule... if the suggestion is for an improvement/technique that is not fundamentally different from a well established algorithm and gives a very significant performance improvement, it would be worthwhile to implement the same... Basically, the idea is that we don't want code that might rot over time without a substantial userbase or maintainers to support or both... Essentially u can compress that rule to this --  `((Will a lot of people who already use sklearn benefit from this?) || (Will it help bring *a lot* of new people to sklearn?, if its a completely new feature)) && (Does it fit well within our API?) && !(Will it make life tougher for the existing users)` ;) __eou__	User
TP	@rvraghav93  I think these are really sensible rules and I completely agree with them :)__eou__	Agent
TP	:D__eou__	User
TP	That one seems useful... Please raise an issue!! I'll try to implement if possible :)__eou__	User
TP	Nevermind I raised an issue (#5884)... Lets see how it is received... Thanks for the suggestion! :)__eou__	User
TP	Could someone review #5823 please? @amueller ?__eou__	User
TP	yes we don't support them...__eou__	User
TP	Probably ;)__eou__	User

TP	@rvraghav93  I realise it's a little cheeky to ask here but.. do you have a view on my earlier question? That is ... I have data with a mix of categorical and numerical values. It classifies very well using one-hot encoding + random forests but terribly using any non-tree method I have tried. I am looking for a clustering/unsupervised method that might work on this sort of data. What might be suitable? I tried some standard methods that rely on euclidean distance but they are a disaster it seems That is what took me to the unsupervised random forest method __eou__	User

TP	Another suggestion, this one I hope uncontroversial. We seem not to have the Gower distance implemented. As in http://stats.stackexchange.com/a/15313/53128 __eou__	User

TP	@rvraghav93  Thanks! I am impressed again :) __eou__	User
TP	@amueller Also #5834__eou__	Agent
TP	And #5703... (sorry :P)__eou__	Agent
TP	OK thanks. Something for the future maybe :)__eou__	User

TP	@rvraghav93  is daisy https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/daisy.html something that been discussed (this is relevant to the Gower coefficient). I have  a very vague memory of seeing it in some scikit learn discussion but I may well have that wrong ah no.. I think I was remembering http://stackoverflow.com/a/26387936/2179021 Although there is a very interesting and related PR about LambdaMART I see where the conclusion of the discussion seems to be that we are better off using GBRT __eou__	User

TP	I just realised that scikit learn has no support at all for ordinals currently.. is that right? __eou__	User

TP	I think so... By ordinals you mean something like `{"small", "medium", "large"}` correct? @glouppe Your PhD thesis is awesome! Thanks... I am loving it... __eou__	User

TP	@rvraghav93  more 1,2,3,4,5,6,7 where all you know is that 1<2<3<4<5<6<7 that is you just know the order but you can't do 1+3 = 4 we could call them ranks __eou__	User

TP	@amueller If you come online I have a list of minor PRs for you to review/merge ;) BTW was #5883 discussed during the sprint?? __eou__	User

TP	@lesshaste scikit-learn doesn't really have any support for categorical variables at the moment. #5883 wasn't discusses during the sprint afaik @rvraghav93 I have a loooot to review at the moment. Trying to catch up __eou__	User

TP	@lesshaste Oh!! Sorry I haven't followed that PR... :( @amueller Do you have anything in mind that I could be of help (in reviewing)? __eou__	User

TP	things by tw991, the neural network improvements, the huber regressor. Anything by vignesh, tian or manoj (I'm supervising them). Your stuff is after that ;) __eou__	User

TP	Do you mean to say I can review any of these or are u just listing ur todo list? :P __eou__	User

TP	@amueller right.. well I suppose the tree improvements will be  the first major support for categorical variables.. ? I am referring to https://github.com/scikit-learn/scikit-learn/pull/4899 @amueller and my suggestion for the Gower coefficient gives an easy way for mixed types including categorical variables if all you need is a distance. This could really help for clustering I suspect. __eou__	User

TP	@amueller  in relation to our previous conversation about using shufflesplit to subsample for CV, I realised the use case I really had in mind is doing this out of core so the 10^9 feature vectors stay out of core and shufflesplit samples 6 10^5 feature vectors for training and 4 10^5 feature vectors for testing from the large out of core data set.  This may all be better done by bespoke code however rather than something built into scikit learn @amueller  That sounds like a good idea to me @amueller  maybe http://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html or https://docs.python.org/2/library/mmap.html ? Is it worth opening an issue? I see that some support for out of core processing is entering into scikit-learn or are you thinking the mmap'ing can happen in the user code and shufflesplit will just use it efficiently as is? __eou__	User

TP	I was listing my todo list ;) @lesshaste you could possibly use memory mapped arrays to do this out of core. Not sure though yes that might be possible yes all fit methods do in particular to make pipelines easier to understand __eou__	User

TP	Why would the PCA fit methods accept target vectors `y` if they dont do anything with it? Just for API exchange-a-bility ? __eou__	User

TP	Thanks. So, if I may ask one more: Im a bit confused how to reduce dimensionality for a regression task? It is clear to me that some features are more related to variance in the target vector than others, so I believe to understand that applying a PCA should make sense in general, but I dont understand how to apply `sklearn` for this. Should I make the target vector part of the X-matrix before handing it to the PCA? I want to learn which of my features are related to variability in the target vector, and how much. All your examples and very nice tutorials only ever mention how to apply PCA to classification but never a case for regression, it seems. __eou__	User

TP	Youre probably better of with something like Lasso or RidgeRegression if youre interested in knowing which features are related to your target vector. PCA will mix together your inputs, meaning your regression coefficients wont have a direct interpretation. But if you do want to use PCA, then making a pipeline something like `pipeline.make_pipeline(PCA(n_components=N), LinearRegression()).fit(X, y)` ought to work (untested code) __eou__	User

TP	reviews for #5728 would be cool __eou__	User

TP	I thought PCA might be the right tool, as it provided this percentage-wise composition of the feature vectors for the resulting decomposition which I find very neat, also the way the explained variance results tells you how many new components are needed to explain variety. Your pipeline code will not consider the target vector in the PCA so I wonder how the PCA can be helpful that way. Ill have a look at how Lasso/Ridge Regressions work, thanks. ok, gotcha, thks. __eou__	User

TP	Is fuzzy c-means clustering implemented in scikit-learn? __eou__	User

TP	@h4k1m0u currently not. Why do you want that and not GMMs? It is just optimizing GMMs with spherical covariance and optimized using hard EM, right? @michaelaye PCA never uses y, it is an unsupervised method. Therefor it is likely not a good choice for your application. __eou__	User

TP	@amueller because I'm trying to cluster an image and for that I've found in the literature that it's possible to take into account the spatial context (neighbourhood) when using fuzzy c-means. But, I would be really interested to know if GMM offer this possibility as well in image clustering (considering the spatial dimension besides the intensity), although I had issues with the GMM on matlab when operating on a multi-dimensional data (multivariate gaussians). __eou__	User

TP	The number of open PRs are slowly increasing... From 300 to 368 :O We should have one sprint just for reviewing ;) That would be awesome! __eou__	User

TP	BTW github's new repo layout is good! ;) everything on top... __eou__	User

TP	@rvraghav93 That's a great idea! __eou__	User

TP	When clustering an image (distribution of pixel intensities), is it possible using GMM (implemented in scikit-learn) to include the spatial context besides the pixel brightness during the clustering (GMM using multivariate gaussian distribution)? __eou__	User

TP	@h4k1m0u  That's quite a tricky question (I am not an expert).  I suspect your best bet for getting a high quality answer would be actually implement a solution using GMM on some publicly available data and show the problem. Can you do that? even better if you can find another system that does exactly what you want and compare them There is nothing as compelling to a developer as a worked example :) @h4k1m0u  It's very useful you have code that does it. Your question is good but I wonder if this the right place for it. If you are not asking for a new feature or asking about an existing one maybe a stackexchange site would work better @h4k1m0u  sorry I wasn't very helpful! please feel free to post the url to the question here __eou__	User

TP	@lesshaste I know that what I'm trying to do has already been achieved with fuzzy c-means (including the neighborhood context) to the clustering (see [Zhang and Chen 2004] A novel kernelized fuzzy C-means algorithm with application in medical image segmentation). But, since the fuzzy c-means is not implemented in scikit-learn, I'm looking for something similar but with GMM. @lesshaste yes in python (https://github.com/scikit-fuzzy/scikit-fuzzy) @lesshaste ok I will try to test the GMM implemented in scikit-learn, to cluster images (taking into account the spatial context). Otherwise, I will ask my question in stackoverflow... thanks.. __eou__	User

TP	is there code in some other language for fuzzy c-means? __eou__	User

TP	@lesshaste it's okay, my issue is not really related to machine learning (clustering). just an image processing issue (maybe I should ask to the scikit-image community): http://stackoverflow.com/questions/33834883/include-the-spatial-context-of-pixels-during-image-clustering __eou__	User

TP	@h4k1m0u the standard is to also include the x and y coordinates. slic for example is just k-means on x,y,lab same for quickshift look at the image segmentation module in scikit-image. it does what you want. __eou__	User

TP	@h4k1m0u answert on SO, too @MechCoder starting to review your HuberEstimator. Teaches you to be careful what you wish for ;) __eou__	User

TP	@rvraghav93 please don't ping joel, he is offline at the moment. __eou__	User

TP	Yes I heard about him... Thats so sad... :/ Hope he stays strong!! And yes sure I won't... (BTW are you telling me in the context of any particular comment where I accidentally pinged him or are you just letting me know?) BTW I just realized you are down to your last but one task in your TODO... That means I am next :smiling_imp: (Don't worry I've a very small list - #5823 #5703 #5568 #4115) :P __eou__	User

TP	No, there is like three more tasks :P Still on Huber, then Vighnes and Tian you pinged Joel 5 days ago somewhere. How did you hear. He did not post publicly, so I think we should not make it overtly public. __eou__	User

TP	Check PM :) well then `amueller._todo_queue.put_nowait([5823, 5703, 5568, 4115])` ^_^ __eou__	User

TP	http://i.imgur.com/aOChOa2.png (note the last line) __eou__	User

TP	In my defence `put_no wait` fails when the thread is busy... so u dont have to worry about it :p I'll work on something else at the moment :grin: __eou__	User

TP	Hey guys! What's the best way to perform Kernel Logistic Regression or Import Vector Machine (or anything that will do binary classification + output probabilities) with SkLearn or some other python package? I can't seem to find anything. Does no one ever use KLR or IVM ? Thanks, but I don't have that much data, so I wanted to use it since I really need probabilistic outputs. I don't care about scaling for now. __eou__	User
TP	@AAnoosheh you can use SVM(probabilities=True) which will use Platt's method for calibration of the decision function. Or you can use CallibratedClassifierCV to use Isotonic REgression to calibrate an SVM__eou__	Agent
TP	you can use Nystrom together with LogisticRegression__eou__	Agent
TP	just set n_components in Nystroem to n_samples and it will give you exact kernel logistic regression__eou__	Agent
TP	@amueller  Isn't the Platt's method really expensive? Or is SVM+Platt's similar expense to KLR?__eou__	User
TP	make_pipe(Nystroem(n_components=n_samples), LogisticRegression())__eou__	Agent
TP	does kernel logistic regression__eou__	Agent
TP	platt's method in libsvm does 5-fold cross-validation, so it is 5 times as expensive at one SVM.__eou__	Agent
TP	I don't even know how one would implement KLR in any efficient way, as the dual is dense__eou__	Agent
TP	I guess you could do SMO or a similar coordinate descent solver?__eou__	Agent
TP	anyhow, I would expect SVM+Platt do be much faster than KLR, unless you set the parameters such that all samples are support vectors__eou__	Agent

TP	I have not heard of IVM. and yes, no-one ever uses KernelLogisticRegression because it scales even worse than SVMs ;) __eou__	User

TP	Interesting, thanks.  Also this is the IVM; just heard of it recently: http://www.ipb.uni-bonn.de/ivm/?L=1 __eou__	User

TP	interesting. that's my almer mater but I haven't heard of that professor paper is here: http://papers.nips.cc/paper/2059-kernel-logistic-regression-and-the-import-vector-machine.pdf you can set n_components to a smaller number in Nystroem for an approximation of kernel logistic regression ah the improved paper is foerstner makes sense __eou__	User

TP	if you have benchmarks that show IVM is superior to svm + platt let me know ;) ah IVM is kernel logistic regression with one-step look-ahead greedy selection of the non-zero dual coefficients. I think people are just not so excited about kernels any more, so people don't really care for practical implementations of kernels __eou__	User
TP	Perfect, thanks @amueller__eou__	Agent

TP	@amueller I didn't realise people don't really care about kernels any more. Is this because everyone has moved on to random forests and deep learning? __eou__	User

TP	This looks interesting :) https://github.com/google/skflow __eou__	User

TP	People certainly do care about kernels, @lesshaste "Random forests and deep learning" are certainly not the solution to everything. __eou__	User

TP	@tw991  Thanks! __eou__	User

TP	@jmschrei  It would lovely to see a blog post investigating that question more deeply. __eou__	User

TP	Investigating what question? If people use other methods than RF and deep learning? That's pretty interesting, I didn't know that was a thing. __eou__	User

TP	@jmschrei  I didn't quite mean that :) If you look at kaggle winners their main tools are quite consistent  (GBRT and/or deep learning).  I was thinking of a blog post titled "Where kernels methods still rule" explaining with examples where they are still the best approach @jmschrei  that's very interesting... although I am a little surprised by the trees being slow @jmschrei I think you would be a great person to write a blog post on this. It's very interesting and not universally understood __eou__	User

TP	My understanding is high frequency trading uses kernel methods pretty extensively, because they need speed and tree based approaches are rather slow So they'll usually use some form of logistic kernel regression "Rather slow" in the HFT sense, not in the normal person sense, at making predictions Kernel methods are also pretty good for variable length sequences. For example in bioinformatics, the 'spectrum kernel SVM' is frequently used to compare protein sequences to each other to do domain classification or such. I mean, kernels extend far past just matrices of data. There are kernels to compare tree based structures, or graphs, to each other. Why? Doing an inner product is super fast, compared to traversing n binary trees. I'm not trying to demean either RF or deep learning, which are super powerful, but kaggle competitions are a small subset of ML problems out there. You haven't even touched my favorite models, probabilistic graphical models. People use Bayes nets, HMMs, GMMs all the time. Very fast for a normal person, but it's still orders of magnitude slower than an inner product using BLAS, and in the HFT sense, microseconds count. I also imagine you can put a logistic kernel machine on a GPU, but can't put trees on a gpu easily. What do you think it should cover? __eou__	User

TP	@jmschrei  only because there is a line of research on producing minimum and forests or even a single decision tree with similar performance to a random forest and all you are doing is comparisons. 1000 comparisons is very fast __eou__	User

TP	I am trying and failing to find some papers on inferring a singe decision tree from a random forest currently I can't seem to find the papers now... :( @jmschrei  Well... that would be up to the author :)  But how about a set of topics intersecting with... timings for prediction using random forests versus kernel methods, spectrum kernel SVM and how it is applied to variable length sequences. This would be even cooler if there were a test dataset and we could see how well a straightforward application of GBRT does in comparison,  practical examples with real data for kernels to compare tree based structures, or graphs and a comparison with what one would have to do using GBRT that sort of thing :) Basically, concrete classification or regression tasks where there is  a clearly understandable objective function and we can see how kernel methods are easier or just do better __eou__	User

TP	I don't think it's possible to infer a single tree from an entire random forest, except in special circumstances That also doesn't look exactly the same as turning a RF into a single decision tree I thought you meant turn a RF into a single tree which mimiced it identically __eou__	User

TP	@jmschrei  there is work on this. I am just struggling to find the papers again! __eou__	User
TP	I'd love to se it__eou__	Agent
TP	@jmschrei  can you read http://link.springer.com/chapter/10.1007/978-3-319-18356-5_20 ?__eou__	User
TP	The abtract__eou__	Agent
TP	not the whole paper?__eou__	User
TP	No :(__eou__	Agent
TP	ok.. let me try harder.. the papers I am referring to are in the "related work" section of that paper__eou__	User
TP	2 seconds :)__eou__	User

TP	@jmschrei  here is one https://www.researchgate.net/profile/Ulf_Johansson5/publication/221008645_One_tree_to_explain_them_all/links/0deec52ff78d51398e000000.pdf @jmschrei  here is another http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.6595&rep=rep1&type=pdf @jmschrei  this is a copy and paste of the related work section https://bpaste.net/show/54f0d4433bca let me know if you want any of the papers __eou__	User

TP	I don't know how well known it is.. or how useful  :) It would be great to understand how practically important http://jmlr.org/proceedings/papers/v37/beygelzimer15.pdf is too! section 5 claims it does better than vowpal wabbit __eou__	User

TP	Hello  I have been trying a lot of stuff to get #5689 to pass the tests. I am not able to reproduce the failures locally using `conda`.  I have narrowed the failure down to one line Any ideas why this commit https://github.com/vighneshbirodkar/scikit-learn/commit/02bf4df3ccd9f2eec5f1c0519caff7fbe7257969 causes this test https://travis-ci.org/scikit-learn/scikit-learn/builds/92637403 to fail ? __eou__	User

TP	Hi anyone know word2vec ? __eou__	User

TP	I cant import Word2vec in python. Am facing error File "/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/models/word2vec.py", line 690, in train     raise RuntimeError("you must first build vocabulary before training the model") RuntimeError: you must first build vocabulary before training the model Kindly any one help me __eou__	User

TP	Could you post a full code snippet? __eou__	User
TP	ya sure__eou__	Agent
TP	If its too long (>10 lines) use pastebin... :)__eou__	User

TP	Traceback (most recent call last):   File "<pyshell#0>", line 1, in <module>     import word2vec   File "word2vec.py", line 14, in <module>     model = word2vec.Word2Vec(sentences, size=100, window=4, min_count=1, workers=4)   File "/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/models/word2vec.py", line 432, in __init__     self.train(sentences) File "/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/models/word2vec.py", line 690, in train     raise RuntimeError("you must first build vocabulary before training the model") RuntimeError: you must first build vocabulary before training the model __eou__	User

TP	Oh I am sorry I didn't realize that you were asking about the library `word2vec`... This chat room is about scikit-learn :) This is not the correct place to ask... You would probably be better off, asking them at their mailing list... sorry :) __eou__	User
TP	okay :+1:__eou__	Agent
TP	i searched there is no room for Word2vec and gensim :(__eou__	Agent

TP	You could try - here - https://groups.google.com/forum/#!forum/gensim or here - https://radimrehurek.com/gensim/support.html :) __eou__	User

TP	Thank you so much :+1:  :) __eou__	User
TP	:)__eou__	Agent

TP	@Rahulvks if you follow the directions in this blog post it should work: http://rare-technologies.com/word2vec-tutorial/ in particular, I suspect you are missing the middle step here: ``` >>> model = gensim.models.Word2Vec() # an empty model, no training >>> model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator >>> model.train(other_sentences)  # can be a non-repeatable, 1-pass generator ``` __eou__	User

TP	@vigneshbirodkar  did u try pdb?? Its good sometimes to debug frame by frame... Its a bit irritating to get started with but its worth it... (Just a humble suggestion ;) ) __eou__	User

TP	@rvraghav93 I would have done that, but the tests don't fail on my system. __eou__	User

TP	Ah that's a drat... Only otherway is to skip all other tests temporarily and also skip 2.6, 2.7 tests and ram Travis till u figure it out :p (if u run only this one... u can run it in a minute I think) __eou__	User
TP	I have narrowed down the failure to one line so far, but I still don't know why that line causes those tests to fail__eou__	Agent
TP	The fit line right? Wait I'm bored... I'll try pulling ur branch...__eou__	User
TP	Ok. Thanks. Make sure you go to that commit the last 2 or 3 commits I have been trying other stuff__eou__	Agent

TP	I am wondering if I have misunderstood https://github.com/scikit-learn/scikit-learn/issues/5870#issuecomment-159511717 .  Is the question if isnan() is fast in C ? __eou__	User

TP	The thing is NaN in IEEE std has two possible representations (qNaN, which is the quiet NaN where we explicitly specify values to be NaN and sNaN where NaN is a signal NaN and is a (possibly unexpected) result of  numeric computation, like in Gael's comment...) So Gael was wondering if that would make `is_nan` computation in python/cython less efficient... Your pandas point was correct in that pandas does use a consistent NaN representation for both q/sNaN (atleast that is what I understood from that link), whereas numpy doesn't have one... I think it won't really affect the speed... but I am not sure... I am currently working on benchmarking that... __eou__	User

TP	@rvraghav93 interesting! In C you really just have to do x!=x which is true iff x = NaN. This is exactly one comparison @rvraghav93  I also looked at the assembly that you get from isnan() from gcc which is quite interesting too :) @rvraghav93  I looked it up.. it seems any C99 compliant C compiler is guaranteed to do x!=x correctly That is x!=x iff x is NaN __eou__	User

TP	Could you share? __eou__	User

TP	@rvraghav93  sure.. using math.h,  isnan() compiles to  jmp __isnanf  .  However return x != x compiles to xor eax, eax ucomiss xmm0, xmm0 setp    al ret @rvraghav93  however it turns out gcc had a performance bug and bleeding edge isnan()  compiles to something closer to the latter assembly.. Does this make sense? __eou__	User

TP	I am not sure if the latter x!=x will work in all compilers... but that is the most effective way to check for nan AFAIK... In general, IIRC `ucomiss` will handle the nan(s) effectively (i.e not distinguish between multiple nan representations) __eou__	User

TP	@rvraghav93  I think you are right C99 mandates the use of a macro for this.  x!=x certainly works in gcc however and I assume all sensible compilers @rvraghav93  if you are interesting.. this was the gcc performance bug https://sourceware.org/bugzilla/show_bug.cgi?id=17441 . Fixed on 2015-09-18 __eou__	User

TP	@rvraghav93  it is specified in http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf#page=525&zoom=auto,-193,767 section F.9.3 Relational operators but in any case...isnan() is much easier to read and gcc will compile it properly soon :) (I don't really understand the math.h versus cmath.h point in any case) __eou__	User

TP	I always wonder what is the best way to apply transformers to a dataset e.g. I want to OneHotEncode only certain variables X = [categorical_column, continuous_column, continuous_column] then throw it in a pipeline where a onehotencoder would only apply to the categorical column (similar with a standardscaler, only to columns where it "makes sense") how do you guys solve this issue? __eou__	User

TP	@ogrisel @amueller Why does sorceforge not show 0.17 as the latest version?? http://sourceforge.net/projects/scikit-learn/files/ __eou__	User

TP	@rvraghav93  hi __eou__	User

TP	I was attempting to read http://arxiv.org/abs/1504.04595 . There is still a big gap between stats and machine learning! This part in particular where they explain which classifiers they will compare with: "For comparison, we also present results for several state-of-the-art methods for high-dimensional classification, namely Penalized LDA (Witten and Tibshirani, 2011), Nearest Shrunken Centroids (Tibshirani et al., 2003), Shrunken Centroids Regularized Discriminant Analysis (Guo, Hastie and Tibshirani, 2007), and Independence Rules (IR) (Bickel and Levina, 2004), as well as for the base classi- fier applied in the original space" does scikit-learn have any of those? oh..maybe shrunken centroids are here? http://scikit-learn.org/stable/modules/neighbors.html#nearest-shrunken-centroid is our LDA implementation the same as  Penalized LDA (Witten and Tibshirani, 2011) ? on another note.. the two images at http://scikit-learn.org/stable/auto_examples/neighbors/plot_nearest_centroid.html#example-neighbors-plot-nearest-centroid-py look identical to me are they meant to be different? __eou__	User

TP	we also have LDA ^^ __eou__	User

TP	@rvraghav93  thanks.. I don't even know what  Shrunken Centroids Regularized Discriminant Analysis and Independence Rules are but the author of the paper is absolutely at the top of his field so I assume the new method is important @rvraghav93  do we have *penalized* LDA as in https://faculty.washington.edu/dwitten/Papers/JRSSBPenLDA.pdf ? __eou__	User

TP	Ah no... I am not sure how useful that is? And there is something wrong with the example that you had posted... Mind raising it as an issue for someone else to look into it...? `shrinkage` is supposed to have an effect... ping @robertlayton and @MechCoder in your issue... __eou__	User

TP	And lol no I don't either... I am hoping it gets named to LDA on steroids... must be easier to rememeber... on a serious note it seems to be a combination of regular LDA with shrunken centroids method (thought you must have figured that out already ;)) __eou__	User

TP	@rvraghav93  thanks. I opened an issue. It would be very interesting to know what exactly scikit-learn is missing from that list and if the things that are missing are worthwhile. __eou__	User

TP	Guys what about adding an interface to OneHotEncoder and other transformers that can take a matrix and only operate on certain columns? E.g. if max_unique_values < 5 per column or whatever.... I'm not sure how to use this encoder directly on a matrix mixed of continuous variables and categorical variables. Some others must have battled this? Also, is pd.get_dummies a solution here? I don't see though how it would work with train + test together.... __eou__	User

TP	@kootenpv I don't think we have a workaround for this ATM ;( We are working on making the pipeline objects more flexible... __eou__	User

TP	@rvraghav93  How are you solving that situation then? I understand if there is no solution, but there must be some way to "work around" it, no? Just a list of transformers for each variable or something? __eou__	User

TP	Hey guys! I was trying to work on an easy bug regarding RobustScaler under sklearn.preprocessing when i checked the data.py file in the given file hierarchy, the robust scaler class does exist and i'm not able to figure out why is it giving me an import error __eou__	User

TP	I've even tried to run the plot_robust_scaling.py in the examples folder but then again I end up with the same import error. __eou__	User

TP	Hey Guys, I want to contribute to this project, How do i start? __eou__	User

TP	Hi @chinmoysam, just dive into the issue tracker and see issues that are tagged Easy and Need Contributor. take a look at the developer guide too: http://scikit-learn.org/stable/developers/ __eou__	User

TP	@vortex-ape Thanks is this issue tracker in the github page?? and i have just started learning scikit i have some knowledge of programming in python, Can i directly go to issue tracker and try to solve something?? __eou__	User
TP	https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+is%3Aissue+label%3AEasy__eou__	Agent
TP	go through the easy issues, you should be able to solve most with a basic knowledge of python__eou__	Agent

TP	thanks a lot  @vortex-ape __eou__	User

TP	Hi, Anyone having sample svm text classification code in sklearn ? __eou__	User

TP	@Rahulvks have you looked at the text classification tutorial? http://scikit-learn.org/dev/tutorial/text_analytics/working_with_text_data.html you can also find several examples in the examples gallery: http://scikit-learn.org/dev/auto_examples/index.html __eou__	User

TP	@amueller, can you please help me out with this issue?? I was trying to work on an easy bug regarding RobustScaler under sklearn.preprocessing  when i checked the data.py file in the given file hierarchy, the robust scaler class does exist and i'm not able to figure out why is it giving me an import error I've even tried to run the plot_robust_scaling.py in the examples folder but then again I end up with the same import error. I currently use a mac operating on OS X 10.11 and tried importing the code on python3 [![Screen Shot 2015-12-01 at 7.13.59 PM.png](https://files.gitter.im/scikit-learn/scikit-learn/KIc6/thumb/Screen-Shot-2015-12-01-at-7.13.59-PM.png)](https://files.gitter.im/scikit-learn/scikit-learn/KIc6/Screen-Shot-2015-12-01-at-7.13.59-PM.png) __eou__	User

TP	@SumedhArani  Old version, it is really new __eou__	User

TP	@SumedhArani  What I meant was... you're importing an old version of scikit, while you might be looking at some other source code. Try to see the path of sklearn.preprocessing module after importing sklearn.preprocessing     import sklearn.preprocessing     sklearn.preprocessing __eou__	User

TP	@kootenpv Thanks for the reply!! <module 'sklearn.preprocessing' from '/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/sklearn/preprocessing/__init__.py'> This is the output that I got on trying out what you told me! __eou__	User

TP	I've recently upgraded my scikit version and prior to which I had installed 0.16 version whose source code I have been referring to. I'll check out once again. I used pip install -U scikit-learn and it says the requirement is up to date. __eou__	User

TP	hi anyone up here __eou__	User

TP	Yes? __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/issues/5947 i would like to implement this  if it is not already implemented (I couldnt find this in scikit library ) The closest was  spectral embedding I havent explained the idea of the algorithm in the issue  but i can do that and the applications of the same too , as in recent years its gainig some popularity __eou__	User

TP	How popular is it? __eou__	User

TP	Also what kind of problems does it solve? You may want to include the answer to the prev questions in your issue :) I haven't looked into it exactly... will do so and let you know my view :) __eou__	User

TP	as already mentioned some very popular uses are web graph minning , page ranking algorithms let me include some application based papers in the area http://cseweb.ucsd.edu/~atsiatas/pr_diffusion.pdf http://ictactjournals.in/paper/IJSC_Vol3_Iss3_P5_544_548.pdf __eou__	User

TP	I'll let you know my views after I look into it. You may have to wait for core devs to respond to you in that issue before you can proceed. You can also make a detailed email to our mailing list linking your issue to attract more comments. Could you also add these questions and answers to the issue? (So that this discussion could reach a larger audience) __eou__	User

TP	fot the other part of the questions :: it is used   in  a semi-supervised setup  and can be used at problems requiring local and global scales of information using some sort of  label transfer but not limited to this __eou__	User

TP	Hey Scikiters, __eou__	User

TP	I would like to know if someone has a parallel implementation of DBSCAN .. or Knows how to use it on top of Apache Spark .. Thanks in Advance __eou__	User

TP	I may be wrong but isn't our implementation parallel? __eou__	User
TP	No, the base one in Scikit learn is serial one .. I am trying now to use pySpark to paralleize it, but its really difficult as I am not aware of scikit learn, and I dont have plenty of time :) .. would appreciate if someone can help__eou__	Agent
TP	Afaik parallelization in scikit-learn is mostly done using joblib... maybe you should take a look at that?__eou__	User

TP	Do u mean, i should try to edit the scikit implementation to utilize Joblib, or u already using Joblib ? __eou__	User

TP	i will do that __eou__	User

TP	@halwai  .. if u mean DBSCAN, would u please let me know then ? __eou__	User

TP	@Elbehery   i am sorry  i was not reffering to DBSCAN __eou__	User

TP	no problem __eou__	User

TP	Could someone remove the need contrib tags from issues which have a PR open? @amueller @ogrisel @glouppe ? #5943 #4639 #4808 #4883 #5029 #5298 #5318 #5952 __eou__	User

TP	Also since there was sufficient interest in mailing list (by sufficient interest I mean the +1 from Joel and +0(?) from Andy ;)), could some one add the "Need Review" tag pl? Once added I have a list of PRs to be tagged ;) __eou__	User

TP	I'm really wondering if there would be ways to "cache" things in the Pipeline I mean... when you're doing a grid search... if you use some kind of static transformation in it E.g. you use a CountVectorizer that has no variation in parameters and it is put in the pipeline, I suspect that if you have some alpha values changing in a Ridge() in the pipeline, it still does the static CountVectorizer endlessly?     Pipeline({       'countvectorizer': CountVectorizer,       'ridge': Ridge})          grid = {'ridge__alpha': [0.1, 1, 10]} __eou__	User

TP	@kootenpv not currently unfortunately https://github.com/scikit-learn/scikit-learn/pull/3951 dask-sklearn tries to get rid of that, but that's more of a prototype see http://blaze.pydata.org/blog/2015/10/19/dask-learn/ __eou__	User

TP	@amueller nice catch __eou__	User

TP	hi.. I was wondering if http://arxiv.org/pdf/1109.0887.pdf "Learning Nonlinear Functions Using Regularized Greedy Forest" is of interest? It was the method that came second in the kaggle higgs boson competition __eou__	User

TP	hmm.. I see that despite this it hasn't been cited many times __eou__	User

TP	@kootenpv well it is a known longstanding issue __eou__	User

TP	man @agramfort is killing it on the issue tracker __eou__	User

TP	xD __eou__	User

TP	Btw can u create the "Need Review" Tag?? __eou__	User

TP	Hey! I've found that http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold if called under `print` shows first argument as named `labels`, but you actually cannot pass it this way. Is it intentional? __eou__	User

TP	A lot of issues need the `"Need Contributor"` tag removed - https://github.com/scikit-learn/scikit-learn/labels/Need%20Contributor __eou__	User

TP	@vighneshbirodkar maybe ping @ogrisel or @lesteve about the joblib thing rvraghav93: can you list the issues that need the "need contributor" removed? __eou__	User

TP	Hello @ogrisel , @lesteve , I can see a fix for #5956 by setting `copy_cov=True` on this line https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/dict_learning.py#L296 Also, passing `mmap_mode=readwrite' to `joblib.Parallel` gives a file descriptor error, but passing `mmap_mode=c' works, I am yet to find out what 'c' means for it,  I wasn't able to find any documentation Ok, see stands for Copy on write, is passing that ok ? __eou__	User

TP	added the needs review tag __eou__	User

TP	Yay yay :p I'll just send u a list for both (removal of need cont and addtn of need rev) __eou__	User

TP	sent where? ah sorry future yes __eou__	User

TP	@rvraghav93 sorry I'm a bit out of touch with what is happening at the moment. Have you worked on the multiple metrics stuff yet? @vighneshbirodkar would you be fine with a non-ml project? I think doing the website build would be cool #5578 #4986 Otherwise the multiple metrics would be very high priority to me unless @rvraghav93 worked on it. It might be that we need to first merge ragav's improvements / code deletions though __eou__	User

TP	Hey no I am breaking my head over tree code ;( Sorry I will try my best to start it by this weekend :) There are a few minor PRS of mine which u might want to consider for review xD __eou__	User

TP	sorry, not much time for reviews for the rest of the year :-/ __eou__	User

TP	Are Transformers also expected not to do anything in place ? __eou__	User

TP	`"Need Review"` - #5907, #5823, #5703, #5568, #4115, #5883, #5983, #5971, #5971, #5945, #5929, #5815, #5920  and PRs ready for merge - #5981, #5946, #5925 and Issues (or PRs) which need a removal of `"Need Contributor"` - #5986, #5876, #5868, #5824, #5789, # 5738, #5583, #5367, #5298, #5269, #4804  (Going a bit overboard with the usage of tags - take with a grain of salt - * a "Stalled Work" tag (hoping Gael doesn't oppose ;) ) -  #5316 (and a lot more...) (this tag could be added after the author of the PR  says he is no longer interested / have time to work on it or doesn't respond with the status in a month's time, this will help people who wish to work on the related issue, understand that their new PR is welcome... we could just add the `"Need Contributor"` back to the related issue... but people get confused why there is an existing PR for that issue) * a "Action Needed" tag - #4804 Along with `"Need Review"` to denote that review has been done and it needs the author to respond to the review... (So we can filter PRs for review like "Need Review" and !"Action Needed") __eou__	User

TP	@rvraghav93 2 of my PRs also need reviews, you think they need to be tagged ? __eou__	User

TP	"Need Review" of course ;) Could you list them here?? Nevermind! @amueller #5414, #5270 __eou__	User

TP	and awesome you've fixed #5689... sorry I tried and gave up... __eou__	User

TP	That's ok, that issue still hasn't been fixed, though. I was making a mistake by not passing randon_state, but there is no reason why that test should fail __eou__	User

TP	should we tag all "MRG" ones with "needs review"? __eou__	User

TP	I was wondering that... but that would be too much... So I was thinking maybe we could have only 20 odd `"Need Review"` tags at a time? How does that sound? We have at most 10 serious reviewers right? The number of `[MRG*` issues are over 150 By most recently commented... __eou__	User

TP	I tagged all of them, but we should untag those that actually wait for the person doing the review to get back well how do you choose the 20 ? how does that make sense? if someone created the perfect pull request and no dev reviewed it for 2 years, then it will be never reviewed __eou__	User

TP	Well rules are meant to be broken... :P We could have 10 more for such PRs? Or not... probably that was a stupid suggestion... nevermind ;) But there will be a question on how do we get to decide that 10 probably.. so its better that all `[MRG*` is "need review" tagged... __eou__	User

TP	we should remove the ones where we are waiting on the contributor to address comments __eou__	User

TP	Okay that sounds better :) If I catch something like that I'll just ping u here... and how about `"Work Stalled"` for stalled PRs? __eou__	User
TP	that is the same as PR ;)__eou__	Agent
TP	haha... I mean like it-not-gonna-be-updated-anymore PRs ;)__eou__	User
TP	?__eou__	Agent

TP	For example in [this comment](https://github.com/scikit-learn/scikit-learn/issues/5229#issuecomment-149628243) the author of #5316 said that he is not working on it anymore... There are quite a few PRs like that and a few more where the author has stopped responding... I was wondering out aloud if we could tag those with `"Work stalled"`... __eou__	User

TP	or close them? __eou__	User

TP	well how do you know if something is stalled and what would be the benefit of the tag? __eou__	User

TP	that is better... but I feel it should be done only for stalled PRs which have a related issue open and we should include a comment in the issue that "There was a stalled PR ##### That was closed due to inactivity"... but I am afraid that closing might be a bit rude? I was thinking of doing a random search from time to time ;) for comments  by authors saying so or comments which ask for status that go unresponded for more than a month... __eou__	User

TP	@rvraghav93 is waterponey colocated with you? __eou__	User
TP	Ah no why?__eou__	Agent
TP	do you know who he is?__eou__	User
TP	he is in paris, right?__eou__	User
TP	I met so many people at the sprint.__eou__	User
TP	right you are with alex. hm...__eou__	User

TP	Ah he was sitting with us... but I am unable to recollect who he is... __eou__	User
TP	hehe ok__eou__	Agent

TP	any Dutch :P? __eou__	User

TP	@amueller @rvraghav93 In sympy we have "PR: Author's turn". It is put once the PR is reviewed and we are waiting on the author to address the comments. This way it is sometime easier to identify stalled one's.  :smile: __eou__	User

TP	@vighneshbirodkar multi-class AUC might also be interesting: #3298 @vighneshbirodkar the ada-grad stuff here #3729 would be cool but is likely to be a bigger project @leosartaj how often do you reassign them? if the PR is quite active, it changes often who's turn it is. Or do you only do that if it is stalled for a while? it might be helpful __eou__	User

TP	I have seen it to work best: 1. It is a big PR. Reviews take time. 2. Author replies in a while. Probably not a good idea when the PR is quite active. Works well for slower one's. __eou__	User

TP	reviews on #4936 would be good .... thanks @leosartaj :) __eou__	User

TP	:smile: __eou__	User

TP	Is #5995 an easy one? If so could you tag it with "Need contribs". I found a contributor looking for an issue ;) ([ref](https://github.com/scikit-learn/scikit-learn/issues/5879#issuecomment-162382187) ) __eou__	User

TP	we need to merge https://github.com/scikit-learn/scikit-learn/pull/5578 first __eou__	User

TP	I have a probably lame question regarding cython code - Why is that we don't delete all the free the memory of all the members in the destructor? (Ref [the destructor of `Splitter` class](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_splitter.pyx#L105)) __eou__	User

TP	What do you think should be freed and isn't ? __eou__	User

TP	Any other attribute say `sample_weight`? (Fair warning this could be a very dumb question but I am quite new to cython ;)) __eou__	User

TP	It should be freed, but where it is freed depends on the context of the code. For example, any object declared in Python should not get freed with `free` and Python's GC will pick it up. If an object is declared here and not freed, it might be getting freed elsewhere __eou__	User
TP	Ahh!! That explains a lot! Thanks :)__eou__	Agent

TP	so basically `sample_weight` of this splitter will hold the reference to the mem block managed elsewhere (or probably by the python GC) correct? __eou__	User
TP	Yes__eou__	Agent
TP	Thanks!!__eou__	User

TP	I won't be surprised if that is infact a numpy array __eou__	User
TP	Yes it is.. It will be user specified so... Now I understand this better!__eou__	Agent

TP	Hey Folks, with PCA, if you fit with all the components, you should be able to transform with a subset of them only. If Im not wrong, this is currently not possible. Should I open an issue? __eou__	User

TP	@Djabbz  Sounds very strange. I don't think that's possible with PCA. Do you have literature where it says? As far as I know if you go compress 40 features into 10 features, you'd need measurements on all 40 features to result into 10. __eou__	User

TP	@Djabbz we could add a parameter to the ``transform`` method but usually we don't like to do that. Can you give an example application? You could just replace ``components_`` by ``components_[:n_features_you_want]`` __eou__	User

TP	@rvraghav93  [here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/multiclass.py#L193) This line assumes  `Y` to be a sparse matrix. But if target variable `y` has only one class then  in  `Y = self.label_binarizer_.fit_transform(y)` `Y` becomes `numpy.ndarray` which makes `Y.tocsc()` fail. Is this intended? __eou__	User

TP	there was some code to plot a neural network somewhere. does anyone remember where? __eou__	User

TP	@amueller  I am sure this isn't helpful but... http://www.texample.net/tikz/examples/neural-network/ is at least pretty __eou__	User

TP	thanks @lesshaste. I'd prefer python code but I have something similar than that now __eou__	User

TP	@amueller  I know already this is a dim question but.. I notice scikit-learn has said no to adding deep learning. However we do already have MLP and some people working on improving that code (e.g. adding dropout). Is there a clean line between MLPs and deep learning? I assume that an MLP with 20 layers is not deep learning for example? it seems that the three main types of neural networks are feed-forward, convolutional and recurrent.  If we just look at feed-forward neural networks, I am wondering if there is a clear view of what is in and what is out of scope? no is a perfectly acceptable answer :) __eou__	User

TP	@lesshaste neural networks are by now basically synonymous with deep learning. convnets are actually feed-forward nets I think the agreed scope is feed forward without convolutions __eou__	User

TP	@amueller  ok.. I ask as a recent mailing list question was "I'm interested in deep learning and wanna contribute to scikit-learn and try out for GSoC next summer. I was wondering if scikit-learn is looking to expand its neural nets package." __eou__	User
TP	and no support for custom architectures, i.e. you can specify a list with the number of nodes in each hidden layer, but they will all have the same activation function and there are no skip connections etc__eou__	Agent
TP	and the answer was no__eou__	User
TP	v. interesting re: custom architectures__eou__	User
TP	yeah we're not gonna expand beyond the model that is there. we want to make the mlp as good as possible, so we will add dropout, and if there is a consensus for a better of-the-shelf optimizer than adam, I think we'll be happy to add that, too__eou__	Agent
TP	it is highly unclear to me how much the bells and whistles actually make a difference when you measure classification performance__eou__	User
TP	there was a GSOC last year, but it did't finish yet :-/__eou__	Agent

TP	have you read sanders posts on the galazy zoo and plankton competitions? __eou__	User
TP	ah no... could you point me to them please?__eou__	Agent
TP	http://benanne.github.io/2014/04/05/galaxy-zoo.html__eou__	User

TP	on the topic of gsoc.. it would be great if someone could fix/rewrite the variational Bayes module __eou__	User
TP	http://benanne.github.io/2015/03/17/plankton.html__eou__	Agent
TP	the GMM you mean?__eou__	Agent
TP	http://scikit-learn.org/stable/modules/generated/sklearn.mixture.VBGMM.html__eou__	User
TP	reading your link...__eou__	User

TP	@amueller  I read that article.. it's still not clear to me how much worse you would do with a simpler architecture. But I do think that image tasks seem uniquely well suited to convolutional networks however I notice that neural networks are now being used more for non-image based tasks on kaggle @amueller  I think you told me that VBGMM was basically broken.. but I may have remembered that wrong sounds wonderful __eou__	User

TP	there is a pull request with a rewrite of the vbgmm but it is not finished and the status is unclear to me I'm a bit preoccupied with writing a book that I want to finish early spring it's a machine learning book for programmers without a lot of math (because there are many good stats / ml books out there already) it aims to be very practical thanks :) (and I'll go back to that now ;) check out the GMM rewrite pull request if you are interested __eou__	User

TP	oh great! On machine learning? __eou__	User
TP	machine learning in scikit-learn__eou__	Agent
TP	err machine learning with python__eou__	Agent

TP	I look forward to it! __eou__	User

TP	although I am a little worried that ML is going to be overtaken by neural network mumbo jumbo :) I will do.. thanks good luck! (p.s. did you consider cloning yourself? :) ) (as everyone wants your attention) __eou__	User

TP	I sometimes have a feeling he already has.... ^^ __eou__	User

TP	@kaichogami Apologies for the late response! That indeed looks like a bug... Apparently `_fit_binary` seems to handle the case of constant `y`. Could you also fix this and add a test too? __eou__	User

TP	@rvraghav93 no problem. I'll do that. Just to be sure, I am supposed to open another pull request for this issue right? __eou__	User

TP	If you have'nt raised one for that already you could just do it in a single pr... Or maybe multiple PRS ur choice :) __eou__	User

TP	hi @rvraghav93 __eou__	User

TP	Hey :) __eou__	User

TP	your suggestion of `rpart` is great! `rpart` seems to do surrogate splits and handles the missing values pretty well.. but I guess its a bit computationally intensive, so I am going with Gilles' suggestion of finding the best split, with all the missing values sent to either side of the split (left or right).... Lets see how it works... This one modifies a loooot of code and I'm struggling with refactoring - procrastination - refactoring - giving up - getting back up and all other cycles in between ;P Hope I can gift a missing value supporting tree for christmas ;) (rant w.r.t #5870) __eou__	User

TP	@rvraghav93  great! I look forward to it :) __eou__	User

TP	Hi everyone!  I'm trying to run a grid search on a dataset that is stored in a Pandas DataFrame (something very similar to this example http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#example-hetero-feature-union-py).  When I run my code I get the next error message: "ValueError: cannot label index with a null key" I've tried different approaches but I didn't be able to fix it. I have a working example that I can share with you if needed. __eou__	User

TP	@mac2bua Could you paste your code into github gist/ pastebin and post the link here please? __eou__	User

TP	Yes, of course I can! https://gist.github.com/mac2bua/94f0f15bc327684d16ba let me know if you need anything else __eou__	User

TP	@aron-bordin Welcome to scikit-learn! Please check if any of [these](https://github.com/scikit-learn/scikit-learn/labels/Need%20Contributor) issues interest you and start working on it. Let me know if I can be of any help! __eou__	User

TP	@rvraghav93 Thx, I'll check them __eou__	User

TP	Hi , First of all, wish you all a happy new year. I am new to scikit learn. I wanted to ask for help in working on issues which I have the ability to contribute. The issue regarding [meta-estimators](https://github.com/scikit-learn/scikit-learn/issues/5824) seemed nice thing to work on. Please let you know about your thoughts. I would be happy to work any other issue if this is beyond my scope. Thanks :) __eou__	User

TP	Could you ping @hugobowne and ask him if he's still working on it? If not please feel free to take it up. __eou__	User
TP	Thank you so much. Will do that.__eou__	Agent

TP	It would also help if some idea can be given regarding how to proceed. Sorry for my doubts on trivial issues, but I am new to the API. AFAIK, I see that the multi class classifiers are right now implemented to turn a binary classifier into a multiclass classifier. This issue is intended to implement meta estimators to turn binary classifiers to multioutput classifiers. Are multi output classifiers same as the multioutput-multiclass classifiers like dt, rf ? And also let me know if it is better to ask the doubts in here or do it at some other place ? __eou__	User

TP	The outline of what needs to be done here is -   * Make `n_outputs` numbers of single output estimators * Train using `X (n_samples x n_features)`, `y (n_samples x n_outputs)` * Predict using `X(n_samples x n_features)` * For `output_i` in `range(n_outputs)` --> `y_predicted_i = estimator_i`, where `y_predicted_i` is of shape `(n_samples x 1)` * Vertically stack all the `y_predicted_i` -s to get the final `y_predicted` of shape `(n_samples x n_outputs)` and as far are `multioutput.py` is concerned, it should provide a meta estimator that changes single/multiclass single output to single/multiclass multioutput... So essentially if you want binary_single_output_estimator to be made a multiclass multioutput you should be able to do both -  * `OVOClassifier(estimator=MultiOutputEstimator(estimator=binary_single_output_estimator))` * `MultiOutputEstimator(estimator=OVOClassifier(estimator=binary_single_output_estimator))` You could add a test making sure both of them return the same predictions I copied this to the issue so @mblondel or anyone else can correct me if I am wrong. Also, I am unable to edit, we require it to be horizontally stacked, not vertically __eou__	User

TP	Thanks @rvraghav93 for the detailed explanation :-) __eou__	User

TP	does someone have a face recognition example where using eigenfaces actually works better than raw pixels? I played around with lfw, but it seems to make results worse :-/ __eou__	User

TP	I am new to scikit and also want to contribute to, where should i start! __eou__	User

TP	http://scikit-learn.org/stable/developers/ __eou__	User

TP	Start with easy issues that are tagged "Need Contributor" __eou__	User

TP	Apparently we can protect branches from force push... hmmm... (https://help.github.com/articles/about-protected-branches) __eou__	User

TP	@rvraghav93 Can you have a look at my pull request #6114 or someone else? I have made changes to all the cython files which had descripancies. *discrepancies __eou__	User

TP	@amueller  what exactly is wrong with VBGMM __eou__	User

TP	I get a 404 on http://scikit-learn.org/stable/faq/ __eou__	User

TP	@rvraghav93  I'll finish reviewing soon I have two talks coming up soon which I need to work on __eou__	User

TP	@jmschrei Please take your time! Thanks heaps for your reviews!! __eou__	User

TP	I'm going to take a stab at parallelizing trees after yours is merged. I've never liked the criterion objects so I'm considering getting rid of them. But I think Gilles might not approve. <_< __eou__	User
TP	And pass score-like objects??__eou__	Agent
TP	I don't know. At least make them stateless, so it can be a single criterion despite the number of threads.__eou__	User
TP	I think the tree module, while pretty efficient, is kind of a mess.__eou__	User

TP	Really? I found it organized ;) Anyway do let me know if you feel I can be of any help. Alex has asked me to work on the trees for the next few months!! __eou__	User
TP	Okay__eou__	Agent
TP	I've been working with mxnet for a bit now but I might have more time soon.__eou__	Agent

TP	@michaelaye Looks like the / at the end is the problem. http://scikit-learn.org/stable/faq.html works. I'll take a look at fixing the link on the main page. @michaelaye the FAQ link at the bottom of the page is now fixed on the dev doc: http://scikit-learn.org/dev/ __eou__	User

TP	Ok, but the link on /stable is still broken, FYI. but good to know that it actually works without the / __eou__	User

TP	> Ok, but the link on /stable is still broken, FYI. Yep this will be fixed on the next stable release __eou__	User

TP	@halwai do you mean the old or the new? __eou__	User

TP	 __eou__	User

TP	@MechCoder https://github.com/scikit-learn/scikit-learn/issues/4497 __eou__	User

TP	Is there an easy way to see what the new features etc might be for the upcoming 0.18 by searching https://github.com/scikit-learn/scikit-learn/pulls ? clearly this would be subject to change but I haven't found an appropriate tag to search for yet __eou__	User

TP	@amueller I have started working on https://github.com/vighneshbirodkar/sklearn-stub __eou__	User

TP	@amueller I have deployed Travis, and Coveralls and CircleCI is now building the doumentation http://vighneshbirodkar.github.io/sklearn-stub/docs/ __eou__	User

TP	@vighneshbirodkar sweet! @vighneshbirodkar that is actually totally aweseom can you add a "user guide" like page to the documentation that explains what exactly a user has to do to make this work for them? @vighneshbirodkar this here can maybe help, too: https://github.com/uwescience/shablona __eou__	User

TP	@amueller   old one __eou__	User

TP	@halwai I think the algorithm is wrong. The update doesn't conform to the literature, and it doesn't seem to work very well in many settings. __eou__	User
TP	@amueller  can i  help to set it right ?__eou__	Agent
TP	well you can help fix the new implementation. have you looked at the pull request?__eou__	User

TP	@amueller  which pull request are you talking about can u share the link __eou__	User

TP	#4802 __eou__	User

TP	@amueller I'm on it __eou__	User
TP	Thanks :)__eou__	Agent

TP	@amueller I noticed one thing. Currently the CircleCI script installs python packages via both apt-get and pip. We could simplify it to only use pip. The cache in CircleCI caches pip packages and their subsequent installation will happen in no time. __eou__	User

TP	@vighneshbirodkar but installing numpy and scipy by pip is discouraged and will take forever and apt-get is also cached. __eou__	User

TP	@amueller I ran into some PYTHONPATH issues on CircleCI. Is it ok if I install numpy and scipy through pip for the stub package? It does need compliing. But only for the first time. Doing it this way let's us keep the configuration to a minimum. Do you think I should switch to apt-get ? __eou__	User

TP	that requires compiling, right? that will take very long. And the people that copy the stub will have trouble. one option would be to just use conda and not test a non-conda environment __eou__	User

TP	how do you mean for the first time? the first time in master? or the first time for any pull request? __eou__	User

TP	CircleCI is only built over master to deploy the documentation. So the first time on master. PRs will be built by travis which uses apt-get __eou__	User

TP	I'm sorry. Travis uses conda right now. __eou__	User

TP	@amueller could you take a look at #5568? __eou__	User

TP	@rvraghav93 It's on my list of the more urgent things ;) __eou__	User
TP	Thanks :D__eou__	Agent

TP	hello <unconvertable> __eou__	User

TP	Does sklearn kmeans uses Linde Buzo Gray algorithm for codebook generation? __eou__	User

TP	have a look at this - http://docs.scipy.org/doc/scipy-0.14.0/reference/cluster.vq.html __eou__	User

TP	@rvraghav93 how does that answer the question? @rajathkumarmp no it's just using lloyd's algorithm __eou__	User

TP	Sorry I assumed he wanted vector quantisation __eou__	User

TP	well yeah he asked which algorithm is implemented in kmeans. scipy's vq implements the same algorithm as sklearn does __eou__	User

TP	Okay :grin: (I stupidly assumed since it was a vq module, maybe it does it using lbg as lbg seems to be the preferred alg for vq) __eou__	User

TP	 __eou__	User

TP	@amueller thank you for clarifying. __eou__	User

TP	"The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features". This is mentioned in the docs. What is "k" here ? Also, is it O(KnT), Multiplication of all 3 parameters? __eou__	User

TP	Hi guys, im going to bombard this forum with a lot of newbee questions. Feel free to kick me out anytime. Ive got several IDs just in case. __eou__	User

TP	@Fredilly lol. Maybe try stackoverflow with the sklearn tag that will get you replies more quickly and more willingly ;) @rajathkumarmp k is the number of clusters, knt is the multiplication __eou__	User

TP	@jmschrei (This is probably a stupid idea), can we compute the accumulated sample_weights for all the samples and store it. (a double array of `n_samples` size)  Later we can compute the weighted n_samples for right or left by subtracting the last index's accumulated value from the first? (Would that speed up the impurity computation and also take us a step closer to having a state-less criterion?) (There is also the class count that is being stored as a state, that also could be accumulated and stored but not sure if it would take up a lot of space.) (Also we need not compute this accumulated sample_weights for all the samples, we could expand this 'cache' when we encounter new samples to avoid computing for samples well away any decision boundary?) __eou__	User

TP	@ogrisel if you need reviews for 0.17.1 let me know. I'm a bit out of the loop right now __eou__	User

TP	Hey guys, anyone has an idea why logistic regression doesn't give realistic probability estimates? with the number of features is large? random forest on the other hand gives more realistic estimates __eou__	User

TP	it is basically overfitting on the things that seem to happen only a few times in the dataset, because they are "good predictors" mostly by chance. More aggressive L1 regularization always results in poorer performance __eou__	User

TP	perhaps if we had bayesian logistic regression __eou__	User

TP	@lqdc have you tried the calibration module? __eou__	User

TP	How does scikit-learn currently upload packages to PyPI ? __eou__	User

TP	I recall @ogrisel or @amueller using twine for that I think. (Ref: https://gitter.im/scikit-learn/scikit-learn?at=552d71150e3138bb6be81ef4) __eou__	User

TP	I've been using setuptools but twine is also an option __eou__	User

TP	I added instructions for setuptools in the stub project __eou__	User

TP	hm anyone wanna review #5270? I think we should really move forward with OneHotEncoder __eou__	User

TP	Is it possible to cythonize and test a single module alone without building scikit-learn fully? __eou__	User

TP	If you compile scikit-learn it only compiles changed files. __eou__	User

TP	@amueller thanks for the tip, but calibrating doesn't help (http://i.imgur.com/k8pF5p7.png) perhaps, it's an issue with sparse datasets. I wonder what the general solution to this kind of thing is.. adding noise? I was thinking of adding support for bayesian logistic regression since it's commonly used in the R world, but it  doesn't seem to scale to large number of features, because of MCMC + then we need pymc as a dependency. __eou__	User

TP	Hi, lately I've been trying to think for a solution to project structure and organization, I've asked at [SO](http://stackoverflow.com/questions/35067412/python-machine-learning-data-science-project-structure) and [Reddit](https://www.reddit.com/r/Python/comments/43ima5/project_template_for_data_scienceanalysis/), since I'm using sklearn and creating new interfaces to the classes, I think this chat would be better. So... how do you guys organize the entire project folder? Also, do you use Pipelines, if so, wqhere do you place all the different transformer? __eou__	User

TP	@davidgasquez this channel is more for scikit-learn project development. SO is usually good. @davidgasquez I don't think there is any best practice for analysis code. I usually have a module that lives somewhere with the code. but you don't usually want your data under version control... __eou__	User

TP	@amueller Sorry for the question in the wrong place. Thanks for the reply! __eou__	User

TP	@vighneshbirodkar Ah yes. Thanks for the response :) When cythonize.dat is rewritten (for some reason, say build clean etc) and I have a build error at say the 5th cython file the cythonize.dat is not updated with the hash records of first 4. This makes it to cythonize the first 4 again and again until 5th (and all subsequent) cython file compiles without error... Also could anyone review https://github.com/scikit-learn/scikit-learn/pull/6254 please? @ogrisel ? @amueller We should add this :P - https://github.com/domgetter/NCoC __eou__	User

TP	Can I use a plain `linear_model.RANSACRegressor()` in `cross_validation.cross_val_score`?  Currently I am getting a `ValueError: No inliers found, possible cause is setting residual_threshold (None) too low.` __eou__	User

TP	Nevermind, seems to be a problem with my data __eou__	User

TP	@rvraghav93 we already have no code of conduct :P __eou__	User

TP	Haha __eou__	User

TP	@amueller Can you take another look at #5270 ? __eou__	User

TP	@vighneshbirodkar probably not today, but should be possible tomorrow __eou__	User

TP	Ok, thanks __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/4899 is this just waiting review? and this lovely PR seems to have stalled https://github.com/scikit-learn/scikit-learn/issues/5736 __eou__	User

TP	#4899 is waiting for more tests + reviews too. __eou__	User

TP	I'm super behind, sorry __eou__	User

TP	I don't think anyone can say sorry! You are doing an amazing job. __eou__	User

TP	Hi fellows, please read the abstract of this page and have a look at it quickly : http://arxiv.org/pdf/1211.1513.pdf Can this be a good addition to the lib ? And I can see 401 Open PRs. They are not being merged quickly. Do they need some reviewing ? I can assist in best possible ways. __eou__	User

TP	How can you tell that the input and response for the the iris data set are .data and .target? For digits, its something else.  Where does that information come from? __eou__	User

TP	@Fredilly Take a look [here](http://scikit-learn.org/stable/datasets/index.html#datasets) __eou__	User

TP	@leosartaj <unconvertable> __eou__	User

TP	Veteran contributors, please have a look at the link above ^^^ If you think this can be a good addition to the lib, I will try to implement it. I have read it quickly once, if I get a yes, I will thoroughly go through it and start implementing it as a new class. it is named the "K-Plane Regression" Oh,ok. I haven't see the specifications of the paper, will check it whether it meets the requirements. If not, I will upload it separate l It can get it reviewed and added as related projects. __eou__	User

TP	@karandesai-96 Have a look at [FAQs](http://scikit-learn.org/stable/faq.html#can-i-add-this-new-algorithm-that-i-or-someone-else-just-published) __eou__	User

TP	K-plane regression appears to have 2 citations which makes it too early for scikit learn if I understand correctly (IIUC?) __eou__	User

TP	Not just 2 citations. 2 citations since 2013... __eou__	User

TP	Just curious, @karandesai-96, why are you interested in this method? __eou__	User

TP	It has better results than mine, on a public data set I used once. Yeah, it seems too early for sklearn ^^ This was the only reason why I was curious to know how it yielded better results... xD __eou__	User

TP	@karandesai-96  what works well is if you make a separate scikit learn compatible implementation and it can be added to http://scikit-learn.org/stable/related_projects.html potentially __eou__	User

TP	Yes, even I am thinking about it. Also, I will try to implement ths on certain public datasets I know. I'll see if it gives consistent results. It will consume sometime though. __eou__	User

TP	Hey all! __eou__	User

TP	Just started thinking about building up Machine Learning for a problem I'm facing and hopefully SciKit-Learn would help solve it Is this the best place to ask for it? __eou__	User

TP	does anyone know any good software for clustering big graphs? It seems scikit learn doesn't have stochastic block model support yet sadly __eou__	User

TP	what's happening with appveyor? @ogrisel? https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.5177/job/tjveqfpn8bcdyrks __eou__	User

TP	@amueller  You mentioned a time series CV object in the mailing list. I would love to see that too __eou__	User
TP	Depending on how you parametrize, it's pretty easy to write__eou__	Agent
TP	if you assume homogeneous samples, you can just do a slight modification of kfold__eou__	Agent
TP	if you want to use a time index, it becomes way trickier (as we need to get a time series object in)__eou__	Agent

TP	to be honest I am not even 100% sure of the best way to do CV on time series data. If you sample randomly from the series you are likely ruin your feature vectors. What is the right approach? oh actually I think I see __eou__	User

TP	what do you mean by using a time index? __eou__	User

TP	lets say each datapoint is a day. and you want to use 5 fold on 100 days then training set 1 is days 0-20, test set 1 is 20-40 training set 2 is 0-40, test set 2 is 40-60 etc but say for each day you have some arbitrary number of datapoints. Then you need to know which day a datapoint belongs to. actually, it's not that hard, it just needs a "label" attribute... __eou__	User

TP	see http://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection for example __eou__	User

TP	thanks so in short.. it would be great :) __eou__	User

TP	I am not sure what you call it but there is also event data which is slightly different from time series data. That is you have a sequence of times when events happen as opposed to labels at every second, say. is there any support for that sort of data? __eou__	User
TP	that is what I mean__eou__	Agent
TP	oh I see!__eou__	User
TP	although how exactly you build your feature vectors is also less obvious I think__eou__	User

TP	with "having an arbitrary number of datapoint for every day" __eou__	User
TP	ah ok... so I am very interested in that__eou__	Agent
TP	then you would need to specify a label__eou__	User

TP	people who do machine learning on neuronal firing data are particularly interested in this __eou__	User

TP	the feature vector is a different problem ;) I think most people that do time series analysis are not necessarily in the sciences ;) __eou__	User
TP	ah :)__eou__	Agent
TP	then I am the odd one out :)__eou__	Agent

TP	on a different note, when I pip install scikit-learn, are all the C and Fortran dependencies compiled from source or are any binaries used? __eou__	User

TP	I don't know much about Python but it seems that it works close to how .bat executables work. Can scikit-learn be used to create new files based on input? E.g. input customer profile and it generates a recommended real estate proposal __eou__	User

TP	@Qoyyuum  yes but this is the wrong place to ask basic python questions __eou__	User

TP	Hi everyone !!!  @amueller I was watching the pull request #4802. It seems blocked for several months now. Do you know what's the problem ?  I've seen that a lot of new codes was submitted at the same time. I image it's a real problem for the reviewers. Maybe it could be easier for them to divide the work of the GSoC in several pull request. Do you think it's a good solution ? I can work on it if you want. __eou__	User

TP	@tguillemot @ogrisel was mostly working on this. There was some bug there. I'm not sure about the exact status at the moment __eou__	User

TP	has anyone seen this error on travis before? https://travis-ci.org/scikit-learn/scikit-learn/jobs/108346382 @ogrisel ? __eou__	User

TP	Ok. @ogrisel I'm the new engineer of Telecom and I'm working full time on scikit-learn, so tell me what I can do to help. __eou__	User

TP	Hi all @tguillemot , I replied to your email. Won't have time to follow up on that before next week though. @amueller merge #6260 ? __eou__	User

TP	@ogrisel No problem. Thx __eou__	User

TP	@amueller  Do you think it is worth my opening an issue about CV and time series data? __eou__	User

TP	Hello @lesshaste , here is one https://github.com/scikit-learn/scikit-learn/issues/6322 __eou__	User

TP	@ogrisel merged #6260 @ogrisel I can work on #6332 and the bug fixes in ~2 hours, not earlier __eou__	User

TP	@yenchenlin1994  thanks! How do I upvote it :) @yenchenlin1994  there were two interesting cases I think. One where you have some value for each time tick and one where you can have different numbers of events per day I have joined it! Can you upvote there? @yenchenlin1994  I see both cases are already listed by amueller __eou__	User
TP	Yes__eou__	Agent
TP	umm... how? By adding a comment or is there some upvote button?__eou__	User
TP	in any case I am very pleased you are taking this on__eou__	User
TP	thank you__eou__	User
TP	oh I see .. sorry :)__eou__	User
TP	me too :)__eou__	User

TP	@lesshaste  Join Github! __eou__	User

TP	Oh about upvote ... I mean join Github as a software engineer and add this function :)  __eou__	User

TP	loool @yenchenlin1994 (and I approve ;) __eou__	User

TP	although it's hard to visualise, is it true that the clustering algorithms perform roughly as the examples in https://github.com/scikit-learn/scikit-learn/pull/6305 when you increase the dimension? Maybe there is some numerical score that could be given on higher dimension data sets? __eou__	User

TP	@lesshaste there is a PR for stability scores, but evaluating clustering is really hard @lesshaste yes __eou__	User

TP	Hello @amueller , __eou__	User

TP	Would you please answer the question I asked in this issue lately? https://github.com/scikit-learn/scikit-learn/issues/6322  Oh and homogeneous and heterogeneous should be separated into two classes, right? __eou__	User

TP	@amueller  thanks. I hope that PR works out. I seem to remember it had made quite a lot of progress __eou__	User

TP	@amueller  https://github.com/scikit-learn/scikit-learn/pull/4301 ? __eou__	User

TP	@ogrisel are you around? __eou__	User

TP	Does anyone know if it is a better idea to form the `neighbors/binary_tree.pxi` into a proper `pyx` + `pxd` file rather than importing the pxi file at `kd_tree.pyx` and `ball_tree.pyx`. From [this](https://github.com/cython/cython/wiki/FAQ#what-is-the-difference-between-a-pxd-and-pxi-file-when-should-either-be-used) (Thanks  to @tomdlt) article, I understand that pxi file gets included at both the places. Would it be better to import than to include or am I missing something? __eou__	User

TP	ok I set up the travis correctly and reproduced locally, so I'll spend the rest of the day fixing numpy dev compatibility __eou__	User
TP	@amueller I'm free for the evening (and weekend). I am starting the work on multiple metric grid search... (I have some notes on how you had wanted it implemented) I'll ping you and joel if I get into trouble ;)__eou__	Agent
TP	have you talked to @MechCoder ? he was also thinking about the problem__eou__	User
TP	it's valentines day on Sunday, I might be busy ;)__eou__	User
TP	also I want to make sure everything is ready for the 0.17.1 release__eou__	User
TP	Yes I spoke to him too out of github ;)__eou__	Agent
TP	cool__eou__	User
TP	Hehe :P Have a nice time ;)__eou__	Agent

TP	Anyone who knows the sequential dataset well? I'm stumped on #6334 ok, all "fixed" __eou__	User

TP	@yenchenlin1994  hi.. In https://github.com/scikit-learn/scikit-learn/pull/6351 which of the variants listed in the R example are you doing for the homogeneous case? or all of them? __eou__	User

TP	Anyone familiar with the amazon employee access challenge on kaggle? http://bit.ly/1oj7FtX   It provides a training set and testing set. The training set contains target response but the testing set doesnt. Do I still perform train_test_split on training set? __eou__	User

TP	Hello @lesshaste   Yes, Ive only committed the homogeneous part by now. Actually, Im a little confused about the heterogeneous case. __eou__	User

TP	If I got 1 sample for 1st day, 2 samples for 2nd day and 3 samples for 3rd day, then I decide to do 2 folds heterogeneous cv, what will happen? ping @amueller @lesshaste Do all samples we collect in the same day should be in the same fold? __eou__	User

TP	sorry, I'm pretty busy right now __eou__	User
TP	Its pretty okay ... take your time :)__eou__	Agent

TP	@yenchenlin1994  even in the homogeneous part there are various options for how to do the cross-validation __eou__	User
TP	oh sorry for my ambiguous description. I implemented the basic case, like the example amueller gave in the discussion between you and him before.__eou__	Agent
TP	@yenchenlin1994  ah ok thanks. I am not even sure how interesting the other cases are to be honest__eou__	User
TP	we need some data and tests I suppose__eou__	User

TP	@lesshaste Hello, Can you annser my question above? If I got 1 sample for 1st day, 2 samples for 2nd day and 3 samples for 3rd day, then I decide to do 2 folds heterogeneous cv, what will happen? __eou__	User

TP	@yenchenlin1994  I am not sure exactly how you are doing the heterogeneous case I have to admit. Why are you splitting by days? maybe @amueller has a better idea what is going on in this case does R have anything for the heterogeneous case? __eou__	User

TP	@lesshaste Do you have any good datasets in mind with missing values for benching? __eou__	User

TP	@rvraghav93  is this for time series data or something else? __eou__	User
TP	I tried to test my implementation of MCAR and NMAR (missingness correlated with class 1) on the covtype dataset and looks like the implementation in #5974 seems to perform slightly better than or as good as imputation for NMAR cases, which was expected and really bad performance for the MCAR case, which is also expected as it tries to extract information out of utter randomness.__eou__	Agent
TP	CAR == completely at random. What is MAR again?__eou__	User
TP	ok I am back in context :)__eou__	User
TP	@rvraghav93  I think you mean MNAR__eou__	User
TP	missing not at random__eou__	User
TP	not missing at random sounds too much like the data is not missing to me :)__eou__	User
TP	Ah hmm__eou__	Agent
TP	but to answer you question.. I don't have any great test sets sorry.. but when I have a moment I will search online to see if I can find smoe__eou__	User
TP	does R use any particular test set for this?__eou__	User
TP	http://www.stat.columbia.edu/~gelman/arm/missing.pdf is quite definitive too__eou__	User
TP	of course what would be great would be to test your method against rpart on the same data :)__eou__	User
TP	Okay thanks for the link__eou__	Agent

TP	MAR is one where the missingness is dependent on either the missing values or the observed values (`X`). MCAR is where the missingness is totally random... and NMAR is where the missingness is correlated with the target... for our case we can assume MAR and MCAR are similar for they both will perform better with imputation... or Not Missing At Random ;) hehe both mean the same... some papers use MNAR and some NMAR I think Okay thanks!! __eou__	User

TP	I don't know this implementation seems intuitive and is supported by Ding and Simonoff's paper but apparently none of the R packages use this... A lot use multiple techniques of imputation and rpart alone uses a surrogate split method... __eou__	User
TP	ah...__eou__	Agent
TP	Yea :/ thats what I am planning to do now and see how good it performs...__eou__	User
TP	That seems to be pretty detailed... I am gonna give it a thorough read now and see how I can reproduce their results...__eou__	User
TP	That and rpart__eou__	User

TP	is this the paper you are using http://people.stern.nyu.edu/jsimonof/jmlr10.pdf ? __eou__	User
TP	yeapp__eou__	Agent

TP	@jmschrei Your thoughts on #5974 ? __eou__	User

TP	@rvraghav93  too much to read! http://link.springer.com/article/10.1007/s10115-011-0424-2 :) @rvraghav93  I think what you are doing is awesome :) and I really love the way things are done by the devs at scikit learn __eou__	User
TP	:)__eou__	Agent
TP	thanks for the links!__eou__	Agent
TP	np :)__eou__	User

TP	what I particularly love is the way that methods are rejected if they can't be shown to actually work on publicly available data! if only everything was so evidence based and also the aim to automate everything :)  I am excited by the PR to automate the choice of the number of clusters for example when clustering @rvraghav93  can you access that paper I linked to? @rvraghav93  do you think of any those "missing values" data sets could be useful? __eou__	User

TP	@rvraghav93  the paper I linked to also has links public data sets it uses to test its missing value imputation .  They are all fro http://archive.ics.uci.edu/ml/ I think @rvraghav93  there is even a "missing values?" field I see :) E.g. https://archive.ics.uci.edu/ml/datasets/Horse+Colic @rvraghav93 sounds good. __eou__	User

TP	Oops @lesshaste Thanks for correcting me... Its MNAR not NMAR :( I've been using it wrongly all along __eou__	User

TP	I just asked this on the hyper-parameter optimization PR but maybe it was better for here. I don't know how relevant this is but have generic optimization methods such as basin hopping been considered and rejected for hyper-parameter optimization? __eou__	User

TP	Yea thanks the adult dataset is reasonably big and has missing values... I'm gonna try that out by encoding all the categorical values as we don't have categorical value support yet... __eou__	User

TP	Any answer to [this](https://github.com/scikit-learn/scikit-learn/pull/2805)? __eou__	User

TP	@rvraghav93  actually it might be worth saying what I do in practice. I run dictvectorizer with categorical variables and then missing values just become another category. It would be great to compare to that approach as it works quite well, at least when I use a decision tree based classifier/regressor maybe it wouldn't work so well with other classifiers but that would also be good to know __eou__	User

TP	Is there any point to train_test_split when I could be simply using GridSearchCV? __eou__	User

TP	@Fredilly  speed? __eou__	User

TP	Hello, please review my pull request: https://github.com/scikit-learn/scikit-learn/pull/5900 __eou__	User

TP	I am trying to update my local scikit-learn folder by using `git pull upstream master` but then I am getting this: ``` (devscikit)kaichogami@kaichogami:~/codes/development_scikit-learn/scikit-learn$ git pull upstream master From https://github.com/scikit-learn/scikit-learn  * branch            master     -> FETCH_HEAD Updating 1aa0ec2..1b27536 error: The following untracked working tree files would be overwritten by merge:  continuous_integration/circle/build_doc.sh  continuous_integration/circle/check_build_doc.py  continuous_integration/circle/push_doc.sh  doc/tutorial/statistical_inference/unsupervised_learning_fixture.py  examples/cluster/plot_face_compress.py  examples/cluster/plot_face_segmentation.py  examples/cluster/plot_face_ward_segmentation.py  examples/mixture/plot_gmm_covariances.py Please move or remove them before you can merge. Aborting ``` I messed up some merge conflicts although that I was not in `master` branch.  What would be the best approach to resolve this? __eou__	User

TP	hmm __eou__	User

TP	im presuming you want to preserve the untracked working tree files? __eou__	User

TP	Assuming you don't care about any local changes to your master branch or uncommit changes:  ```bash git checkout master git reset --hard upstream/master ``` BTW, you should never commit anything to your local master. Always use branches. I thought your French lessons would only start next month ;) __eou__	User

TP	@amueller I am building the wheels for osx and windows for 0.17.1, how did you sync with conda people? __eou__	User

TP	@ogrisel I emailed them. give me a minute __eou__	User

TP	The 0.17.1 tag is already public I am ready to upload :) __eou__	User
TP	just email support@continuum.io__eou__	Agent
TP	actually no, I would like to run a couple more tests.__eou__	User
TP	Alright I will sync up with them__eou__	User
TP	No pbm :) Hope you'll feel better soon. BTW thanks for the PyData Berlin reco :)__eou__	User
TP	done :)__eou__	User

TP	I cc'ed peter wang, which might have helped the process lol thanks for working on the release again, and sorry I'm not more help I'm doing a company visit today and also I'm dead sick. hurray ^^ __eou__	User

TP	@ogrisel Ah caught you on gitter - Now could you please review and merge this - #6254 ? :P Merci ;) __eou__	User

TP	Next month I'll progress to full sentences in French ^_^ __eou__	User

TP	I sent an email to continuum. __eou__	User

TP	@nelson-liu __eou__	User

TP	@nelson-liu I think I accidentally committed in `master` which resulted in that.  @ogrisel Thank you, that helped me. I do use different branches. Will be extra careful next time! :) __eou__	User

TP	scikit-learn 0.17.1 is online! :beers: __eou__	User
TP	:clap:__eou__	Agent

TP	@kaichogami got it, glad you got it fixed! __eou__	User

TP	Never knew that the gitter channel was this active! @nelson-liu please add this to the doc! __eou__	User

TP	yeah I find it a bit odd that it isnt in there already...searching gitter in the repos issue history even shows many people referencing collaborating / talking on it. at least its much more active than irc :P __eou__	User
TP	haha yeah true__eou__	Agent

TP	 __eou__	User

TP	Why does predict_proba give better accuracy than the predict function? Whats the difference? What are the best parameters for param_grid when performing GridSearchCV on  svm? @ogrisel wohoo __eou__	User

TP	I'll try to answer your question in a few minutes when I return to a computer. __eou__	User

TP	@Fredilly as for your second question, it really depends on your dataset. Could you describe it? for predict_proba vs predict, theyre two completely different methods. predict_proba predicts the probability of your input being a certain class. Predict returns what class the model predicts your input to be (e.g. by taking the class with the highest probability from predict_proba) does that make sense? (if anyone else wants to clarify / correct / validate my explanation, please do) __eou__	User

TP	@nelson-liu That makes sense. From my understanding, predict_proba does the same thing as predict but it requires a threshhold value to activate in case of binary outputs. It still doesnt explain why it gets better results __eou__	User
TP	Im not sure what you mean by <unconvertable> get better results. are you doing binary classification?__eou__	Agent

TP	Yes...I get .55 score with predict and .88 score with predict_proba __eou__	User

TP	how are you getting a score with predict_proba? shouldnt it output an array with 2 elements? __eou__	User
TP	It does, but I slice the output and use the one that gives a true value.__eou__	Agent
TP	so to verify, you slice the output, find out which has the highest probability, and use that as the predicted, correct? what class are you using for classification?__eou__	User
TP	hmm how are you using two?__eou__	User
TP	well generally the way predict works it that it takes the likelihoods generated by predict_proba, and then chooses the most likely one__eou__	User
TP	so they should be the same.__eou__	User
TP	i feel like that shouldnt be happening haha but I have no empirical evidence to back up my claims. would you mind letting me see your code to ensure there are no random bugs?__eou__	User

TP	Logistic Regression and Gradient Boosting Classifier I tried both separately just to see which produced a better result and noticed that predict_proba gave a much higher accuracy in both cases. Should that always be the case? Im always discovering new techniques that completely invalidate everything else Ive learned. __eou__	User

TP	``` encoder = preprocessing.OneHotEncoder() encoder.fit(np.vstack((X, X_test))) X = encoder.transform(X) X_test = encoder.transform(X_test)  # LogisticRegression logreg = LogisticRegression(C=3) logreg.fit(X, y) y_pred = logreg.predict_proba(X_test)[:, 1] df = pd.DataFrame({'id': test.id, 'Action': y_pred}) df.tail() df.to_csv('kagglesubmission.csv') #scores about .88  ``` __eou__	User

TP	why do you slice [:, 1]? Maybe theres some pandas magic going on that Im not familiar with, but if you slice [:,1] arent you always getting the probabilities of the second element of model.classes_? Is there some way that you check whether this is greater than or less than the probability of the other class, and then put the correctly labeled prediction into the df? __eou__	User
TP	That way I select the column with a probability of 1.__eou__	Agent
TP	This is more magic than science to me.__eou__	Agent
TP	its possible that you might not be controlling for randomness between grid search and if you do a train test split or something. You should set a consistent random seed to ensure replicability.__eou__	User
TP	(in response to your previous comment about selecting the column with probability 1) hmm thats not how it works. lets say you have a model that takes in inputs x, y and you want to predict whether these inputs belong in A or B from it (binary classification). if you call predict_proba([x,y]), then it would output an array with the probabilities that the input [x,y] is each class, e.g. np.array([[ 0.4,0.6]]).__eou__	User
TP	lets say the output of `model.classes_` is `[A,B]`. This would indicate that the input vector you fed in (`[x,y]`) has a 40% chance of being something of class A, and a 60% chance of being something in class B.__eou__	User

TP	One quick question: Is it possible that gridsearchcv give worse score accuracy? I get .946 off the bat on an svm implementation but I get about .92 with the best parameters from gridsearchcv. __eou__	User

TP	I see what you mean but choosing the column with the highest true positives means you can immediately compare with null accuracy and decide whether to improve sensitivity of specificity. __eou__	User

TP	what do you mean highest true positives? __eou__	User

TP	Predicting a correct value of true. Eg predicting >0.5 when the real value is 1 In my classification problem, if I just predicted 1 everytime, I would be correct 94% of the time. One of the two columns, [X, y] would have more 1s. When I slice, I use that column for predictions. They represent whether a user is granted access permission or not __eou__	User

TP	Oh. is that just an innate feature of the dataset? So basically you want to predict based on some sort of confidence ratio? e.g. you know its most likely 1, so predict 1 every time unless theres a very high confidence for 0? what do the two columns [X,y] represent? no sorry i mean is [x,y] the input, the result of predict_proba(), etc? __eou__	User
TP	its the result of the predict_proba__eou__	Agent
TP	when you have the return result of predict_proba(), the confidence of 1 is in one column and the confidence of 0 is the other. you cant really pick out <unconvertable> which column has more ones?__eou__	User
TP	(on that note, the reason why predict() and predict_proba() are different is because of the way youre using predict_proba() is not the same as how predict() would generate labels for your input vectors)__eou__	User
TP	You certainly can. Simply print out the first X elements of both columns and youll see a pattern immediately.__eou__	Agent
TP	I see what you mean.__eou__	Agent
TP	when you print out the two columns, do you get 0s and 1s..?__eou__	User
TP	the 0s and 1s are categorical and are assigned to one column each__eou__	User
TP	ok, that seems fairly reasonable. I feel like the reason you see a pattern is simply because 95% of your dataset is labeled one haha.__eou__	User
TP	Exactly.....going through the trouble of squeezing that extra 1% is insane.__eou__	Agent

TP	I get 0.10~0.99 __eou__	User

TP	then why even bother using ml if you could just always guess `1` and be right 95% of the time? __eou__	User
TP	Tell that to Kaggle__eou__	Agent
TP	oh its a kaggle competition? would you mind sending me the link. that may help me explain haha__eou__	User
TP	https://www.kaggle.com/c/amazon-employee-access-challenge__eou__	Agent
TP	Getting my feet wet.....my head is spinning....most of this is just trial and error.__eou__	Agent

TP	hmm if you want to get your feet wet, id suggest titanic on kaggle. This is a pretty good tutorial https://github.com/savarin/pyconuk-introtutorial but regardless, predict_proba() doesnt work the way you think it does. Taking one column and encoding everything as that only happens to work because a large portion of your dataset is one label. did you use predict_proba() in titanic? its probably best to just stick to using predict() and further tuning your model / performing feature selection Its just a coincidence in this case that predict_proba() seems to work so much better than predict() Beyond that, feature engineering in terms of normalization and other transformations on the data can also be quite useful to do things like remove outliers, etc. __eou__	User

TP	I did that one....was pretty helpful. The employee challenge and vowpal rabbit challenge prepare you more thoroughly for real world ML problems __eou__	User

TP	From what Ive gathered so far, tuning features is tedious and not usually worth the hassle unless you have like a 100 features. __eou__	User
TP	thats not always true haha. feature engineering is quite important in the real world to distinguish signal from noise in data.__eou__	Agent
TP	e.g. if you wanted to predict whether an employee has access and you were given information about their salary, working hours, and favorite place to eat lunch on amazons campus. this is only 3 features, but its quite evident that where they eat lunch on amazons campus likely has no correlation / is not related to whether theyd have access.__eou__	Agent
TP	actually thats not necessarily true, maybe there are more high scale places reserved for executives or something but do you see my point?__eou__	Agent
TP	Yup.__eou__	Agent

TP	Youre probably right. Thats where <unconvertable> domain expertise <unconvertable> or just plain old common sense comes into play. __eou__	User

TP	Youre absolutely right. Have you tried gbm? Its awesome....I wanna learn more about XGboost. They produce high accuracies right off the bat. __eou__	User

TP	In https://github.com/scikit-learn/scikit-learn/pull/5491 I am confused by which classifier is having its hyper parameters optimized in the examples with graphs. Does anyone know? @MechCoder If you happen to be about I think this question is aimed at you :) __eou__	User

TP	if your possible target classes are consecutive integers like `[0, 1, 2]` the `predict(X_test)` should return the same as `predict_proba(X_test).argmax(axis=1)`. `predict_proba` is just a way to ask for the confidence levels of the model when it's making a prediction. The final classification decision should be exactly the same. __eou__	User

TP	@rvraghav93 and @jnothman I just uploaded a first draft of the issue / pr template we were discussing, please let me know what you think :) its at #6411. __eou__	User

TP	@Fredilly Here's my two cents... I think that the difference that you saw in the results is related to the metric used by Kaggle to measure the performance: Area Under the (ROC) Curve. According to this issue (https://github.com/scikit-learn/scikit-learn/issues/1393) the auc score will give better scores if you feed it with probabilities instead of binary decisions. __eou__	User

TP	hey @ogrisel not sure if youre around, but Im having some issues rebasing commits in https://github.com/scikit-learn/scikit-learn/pull/6417. I took the original contributors commits and rebased master on top of it. However, I am trying to squash it now and am unable to. Do you have any advice as to how to fix this? __eou__	User

TP	@nelson-liu done __eou__	User

TP	I meant I gave it a review. It seems squashed enough to me. What problems do you have? __eou__	User

TP	Ah, thanks! well, commit https://github.com/nelson-liu/scikit-learn/commit/6209098c38cb7aa4e7aad381407da6f42fe7b464 has quite a long diff, and Id rather not pollute the commit history with that additionally, itd be nice to squash the commits into 1 instead of having 4 separate commits __eou__	User

TP	when I currently try to squash the commits, i get a bunch of merge conflicts and such. running `git rebase -i HEAD~3` (to squash my last 3 commits) ends up for some reason pulling up interactive rebase for ~211 commits onto https://github.com/nelson-liu/scikit-learn/commit/09672f516d8592fb82f42e5da3ee0f29210d7366, even though the head is at https://github.com/nelson-liu/scikit-learn/commit/8bea87efc7f6db484f583c15d36a775d82381ef3 __eou__	User

TP	Right I did no see that from the github diff view of the PR. Indeed this comment should be removed. I would start over again from the original contributor's PR. Squash the top commits first, then rebase on master and fix the conflicts. Then add your changes on top and squash again. nelson-liu/scikit-learn@6209098 has many changes with conflict markers that should not be part of this PR. The commits in https://github.com/scikit-learn/scikit-learn/pull/5968 are clean though. __eou__	User
TP	got it__eou__	Agent

TP	how do you generally resolve merge conflicts? im using `git mergetool`, which is opening opendiff on my osx machine wondering if this might be why conflict markers are there? __eou__	User

TP	anyway, I just pushed a new version at https://github.com/scikit-learn/scikit-learn/pull/6419. can you let me know if there are any issues? ah ok, i wasnt aware that you could just do it through the text editor. I think ive properly done it in the last PR, could you let me know? __eou__	User

TP	edit the files with the conflicts,  look for section with conflict markers, edit the code to replace the segments in conflict with the expected code segment and remove the markers check that there are no other files with conflict markers and then `git add` the files where you resolved the conflicts and `git rebase --continue` __eou__	User

TP	does anyone have a good example of when l2 normalizer is useful? I only ever really use l1 hm... good point, but slightly to advanced ^^ __eou__	User

TP	if you want to compute cosine similarities: preprocess the data once and then use np.dot. __eou__	User

TP	Hello @amueller , sorry for disrupting you. Could you please answer my question in #6322 ? __eou__	User

TP	For TF-IDF vectorization, normalizing by l2 norm makes BoW representation more invariant to doc length (similar idea but rephrased :). __eou__	User

TP	Maybe a bit off topic, but should the topic in the IRC channel be changed to reflect the release of 0.17.1? (Does anyone ever use the IRC anymore? Ive been on it for the past week and havent seen a single message in the channel) __eou__	User

TP	I don't remember who has op rights. I think I do but I don't remember the password of my NickServ account. __eou__	User

TP	seems like only @amueller does, or thats what `/msg chanserv access #scikit-learn LIST` is reporting. __eou__	User

TP	@ogrisel could you close this - https://github.com/scikit-learn/scikit-learn/issues/5622 __eou__	User

TP	any reason why `GradientBoostingClassifier`'s `decision_function` does not allow sparse data? ([relevant line](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L1469)) basically you can fit on sparse, but can't predict Hmm, got it, it's because [`predict_stages`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_gradient_boosting.pyx#L101) doesn't support it. __eou__	User

TP	hi __eou__	User

TP	Hello @unautre __eou__	User

TP	BTW Can anyone give a final review on https://github.com/scikit-learn/scikit-learn/pull/5568 (@amueller @ogrisel @vighneshbirodkar?) __eou__	User

TP	Sorry to disturb. Can anyone please help review on #6371  and #6395 ? A merge can urge me to face my midterm tomorrow :pray: __eou__	User

TP	@rvraghav93 I'll try to review #5568 today __eou__	User

TP	Just had a doubt regarding issue #6443. How is it that this failure is not happening on travis but is just happening locally? Shouldn't both be synced atleast for the same python version? __eou__	User

TP	Is anybody else getting this error on their system or is it just me :/ __eou__	User

TP	@amueller Thanks a tonnnnnne for the review! @yenchenlin1994 I can have a look... __eou__	User

TP	@rvraghav93 sorry for being kinda offline at the moment. I really wanna get going with the book ;) btw, do you know if anyone started working on the multiple metrics? I think @MechCoder asked me about it. __eou__	User

TP	@rvraghav93 thanks a lot! __eou__	User

TP	could someone also help review #6176 and #6173 if time permits :P __eou__	User

TP	Hello! Can anyone tell me how can I make this python script ignore DS store files... very silly issue. I have this function, I've googled but I don't know where to put the code in my current function. Thank you!  def list_files(dir):  r = []  subdirs = [x[0] for x in os.walk(dir)]   for subdir in subdirs:   files = os.walk(subdir).__next__()[2]   if (len(files) > 0):    for file in files:     r.append(subdir + "/" + file)  return r      the_list = list_files(file_rep) __eou__	User

TP	I dont think this is necessarily the proper venue for your question. That being said, your code doesnt work properly on my mac. Theoretically though, you would wrap your `r.append()` statement in an if-block that checks if the filename is .DS_Store. __eou__	User

TP	@thejivester you can go to stackoverflow __eou__	User

TP	Did scikit-learn apply to Google summer of code? __eou__	User

TP	@nelson-liu it is under the umbrella of the PSF 7th of march __eou__	User

TP	Ah ok. I talked to Terri and she said that scikit-learn hasn't submitted a proposal yet, are there plans to do so? __eou__	User
TP	was the deadline already? Yes, there is a plan to submit a proposal__eou__	Agent
TP	Oh OK, just wanted to know. The deadline is March 8th iirc__eou__	User
TP	(don't quote me on that)__eou__	User

TP	@rvraghav93 do you have anything for the multiple metrics? I think it might be easier to work on that before trying to clean up If you're busy with your other PRs, I can look into it. __eou__	User

TP	Okay please go ahead. I am just fighting with the tree code. I am sorry :/ I will take up something else instead of it later.... __eou__	User

TP	Guys, take a look at http://contrib.scikit-learn.org/project-template/ __eou__	User

TP	Right now the because of the code and website being hosted in the same root folder, the source files are available on that url For example,  http://contrib.scikit-learn.org/project-template/setup.py. Is that a problem ? __eou__	User

TP	i registered scikit-learn.ml and redirected it to the main site haha [http://scikit-learn.ml](http://scikit-learn.ml) __eou__	User

TP	Thats sweet! You should ping @ogrisel __eou__	User

TP	on another note, does anyone know whether its possible to do multi-line links in markdown? If not, maybe we can get a scikit-learn link shortener to make the issue/pr template more digestible.  short things like [http://sklearn.ml/contributing](http://sklearn.ml/contributing) __eou__	User
TP	Did you register sklearn.ml too? :P__eou__	Agent
TP	haha yeah .ml domains are free and quite applicable to a variety of projects i feel haha__eou__	User
TP	was just trying to see if there was a way to cut down clutter in docs, so the idea of link shortening came up (e.g. how google does goo.gl)__eou__	User
TP	yeah, thatd be great. I didnt know we had that sort of functionality__eou__	User
TP	if only there was a way to default to preview mode or something...but I feel like that would get cumbersome after awhile once youve learned the guidelines / template. Raw output is hard to format nicely.__eou__	User

TP	smart move haha __eou__	User

TP	Really? I am getting one then ;) __eou__	User

TP	@nelson-liu Thanks for the nice idea :D __eou__	User
TP	np =)__eou__	Agent

TP	aw, just got an email that for some reason [sklearn.ml](sklearn.ml) was already registered and that i dont have the rights to it. oh well, [scikit-learn.ml](scikit-learn.ml) will do for link shortening. in that vein, should we move the discussion about the issue / pr template to https://github.com/scikit-learn/scikit-learn/issues/6394 or just create a new issue? __eou__	User

TP	Arrrgh some advertising company purchased sklearn.ml. It would have been cool to have it redirect to our page too! In the end I'm guessing github will realize they just need to improve their interface rather than providing these hacky workarounds __eou__	User

TP	why is it important to have different domains? __eou__	User

TP	Have you taken a look at the pr / issue template? A lot of the links in there are very long, and since you can't create multi-line links in markdown it greatly diminishes readability So I was thinking maybe use the  .ml domain for link shortening internally Basically: it's not important, just an idea haha haha that would be ideal. made a pr at https://github.com/scikit-learn/scikit-learn/pull/6470 __eou__	User

TP	@vene I thought it would be cool to have a .ml domain :p BTW @nelson-liu there was a thread claiming those free .ml providers as scam. Be careful! As for the link shortener we should implement it at our main site. Sounds great! __eou__	User

TP	Im drafting up a new, more template-y version of the templates right now __eou__	User

TP	we reached 10,000 stars today :beers: __eou__	User

TP	:+1: __eou__	User

TP	Hi __eou__	User

TP	I have an issue with affinity propagation __eou__	User

TP	10k stars!! :beers: __eou__	User

TP	:beers: __eou__	User

TP	@ogrisel someone just told me that doing conda upgrade --all breaks scikit-learn. I think that's related to the CI failure __eou__	User

TP	Yeah, they started bundling mkl with everything. To fix this, you should try to install `nomkl` first and then install everything else that is BLAS dependent. __eou__	User

TP	>  conda upgrade --all breaks scikit-learn  scikit-learn installed from conda or built from the source prior to the upgrade? What does break mean? A specific test fails? or a segfault at `import sklearn`? ok great __eou__	User

TP	@ogrisel running conda upgrade twice actually makes it work again it was a linking issue there is a specific version of sklearn on conda that is not properly linked, but I think they fixed it now (and I meant installed from conda) __eou__	User

TP	@vighneshbirodkar maybe also compare against https://github.com/jkarnows/rpcaADMM and possibly https://github.com/dganguli/robust-pca __eou__	User

TP	oh @vighneshbirodkar you should definitely try https://gist.github.com/bmcfee/a378bfe31a75769c583e first __eou__	User

TP	@vighneshbirodkar brian says to read this book ^^ http://www.web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf (the variable names are taken from there, in particular 3.11 and 3.12) __eou__	User

TP	hello guys, who do I badger about strange travis failures: https://github.com/scikit-learn/scikit-learn/pull/6166#issuecomment-194246460 in one it fails to find libgfortran and in the other some tests related to scorers and memmapping fail :-/ __eou__	User

TP	I opened an issue here https://github.com/scikit-learn/scikit-learn/issues/6513 __eou__	User

TP	@amueller I fixed a bug in Brian's code that was causing the low rank output to be scaled differently. It optimizes the objective better than the last PCP implementation. __eou__	User

TP	roles of data structure and algorithm in open source? __eou__	User

TP	I was reading https://github.com/scikit-learn/scikit-learn/pull/5974#issuecomment-194840168 can anyone explain what MV means? __eou__	User

TP	Missing Value ;) And yes thanks please share your thoughts on it. __eou__	User

TP	@rvraghav93  ah thanks... I do have some small thoughts in fact __eou__	User

TP	@rvraghav93  first.. what are the dotted lines? @rvraghav93  and what is the bootstrap you refer to? (sorry these are naive questions) also, one standard way to handle missing values when using a random forest is just to treat them as categorical values. That is make a new feature "X is missing" and set it to 1 if it is missing. would it be worth comparing to that approach? :) @rvraghav93  isn't that a lot simpler in that case? what I do in practice is make the feature categorical if it is missing and then just run dictvectorizer which handles it all for me it creates one new feature per feature at most so at most doubles I don't have any code here sorry @rvraghav93 thanks. __eou__	User

TP	I found the bootstrap option so please cancel that part of my question :) __eou__	User

TP	Yes! You are right. This approach does exactly the same thing. It tries to send the missing values to the best partition as if it were a categorical variable. BTW hurraayyy we have github reactions to comments and PR comments... Sorry. I don't get you. Simpler in which case? Oh wait you mean make a new feature for "X is missing"... Hmm no this approach does not do that... But how will you do that? What will you do with the missing values in the features which are not "This feature is missing"? __eou__	User

TP	That just explodes your feature space no? Also could you give me a minimal code example so I can be sure to follow what you mean. __eou__	User

TP	No my question is lets say I have a data `X = [[1.2,], [2.2,], [np.nan,]]` How does your new data (after your preprocessing for missing values) look like? __eou__	User
TP	__eou__	Agent

TP	I think it looks like [[1.2,0], [2.2,0], [0,1]] assuming I am parsing this correctly you just add one more feature for each feature that can have a missing value __eou__	User

TP	@rvraghav93  does this make sense? @rvraghav93 I am not sure what you mean by "What will you do with the missing values in the features which are not "This feature is missing"" __eou__	User

TP	Say we have 10 features and the 10th feature has missing values. We now have 11 features right? Will that mean we amplify the importance of the 10th feature and not the other features? Anyway this is an interesting case for comparison. I will compare that and let you know how it performs in comparison with the implemented method. My intuition is that, at a higher level, both these methods are similar... both as in the one that you propose and my implementation at #5974 __eou__	User

TP	@rvraghav93  "Will that mean we amplify the importance of the 10th feature and not the other features?" That hadn't occurred to me as a possibility as the 11th feature is only ever 1 or 0 I would love to see the results of your testing on this @rvraghav93  "We now have 11 features right? " yes __eou__	User

TP	"as the 11th feature is only ever 1 or 0" - Correct! But I'm not sure if the feature importance will now be shared between the 10th and 11th feature or will be independently assigned... Have to look into it. Nevertheless this is a good comparison for my method. Another thing that I tried was replacing the missing values by the 10*maximum across all the features... This seems to not perform as well as the implementation. Thanks for your inputs! Please feel free to share more thoughts! __eou__	User

TP	@rvraghav93  thanks. I should say that I am particularly interested in categorical variables so things like replacing missing values by huge values never occurs to me :) @rvraghav93  were the dotted lines the timings? I mean in the graphs @rvraghav93  you make a very interesting point about feature importance. I think we need a smarter imputation for categorical values __eou__	User
TP	Yes that is a valid point.__eou__	Agent
TP	The best way to handle is to consider missing to be a separate category.__eou__	Agent
TP	right!__eou__	User

TP	Yes! the dotted lines are time taken for `cross_val_score` . I'm now trying to plot the time taken for a single fit. __eou__	User

TP	@rvraghav93  actually categorical values in general make the imputation strategies for missing values tricky, or at least different I think this is an important use case @rvraghav93  mode could work but I am not sure what median would mean as there is no natural ordering @rvraghav93  I really hope https://github.com/scikit-learn/scikit-learn/issues/4899 makes progress although mode is a little worrying too.. imagine lots of categories which occur 7,8,9 or 10 times. It's not clear the missing ones should be given to the category that occurs 10 times @rvraghav93 Great and a huge thank you. __eou__	User
TP	:D__eou__	Agent
TP	actually, and this is somewhat off topic sorry, there is a nice problem where you have numerical values but some of them should really be treated as categories__eou__	User
TP	so 10 is nowhere near 11, say, but 1000 is near 1001__eou__	User
TP	you can imagine this comes from some measurements of the output of a computer__eou__	User
TP	what I do in that case is put the feature in twice, once as numerical and once as categorical and let the RF work it :)__eou__	User
TP	but that only works if the number of different numerical values is not too large__eou__	User
TP	end of off topic :)__eou__	User

TP	I think the imputation strategy for categorical value should be 'median' or 'mode' instead of 'mean' no? And if the categorical support is introduced (in https://github.com/scikit-learn/scikit-learn/pull/4899), handling missing values in categorical features is not difficult. The missing simply becomes an additional category. yes correct. Median is not appropriate. #4899 is next on my list, (as #5974 is brought to a reviewable state). __eou__	User

TP	what's the default scoring scheme when using  cross_val_score with RandomForestRegressor? If the docs say, I haven't seen it __eou__	User

TP	I found it ... I think it should be renamed :) __eou__	User

TP	Which one should be renamed? __eou__	User

TP	"score(X, y[, sample_weight])  Returns the coefficient of determination R^2 of the prediction." This is confusing I think. It's really a correlation coefficient. Why is it called R^2? it is a value between -1 and 1 as far as I can tell @rvraghav93  ^^ __eou__	User

TP	Hey guys, would it make sense to contribute confidence intervals for linear models? because without standard errors for each coefficient estimated from test data, it's hard to interpret coefficients __eou__	User

TP	i work with data that changes over time, so in my case I need confidence intervals on output probabilities for new samples. When using the standard output probabilities,  they are sometimes wrong and confidence intervals would help figure out a) how wrong you are and b) which coefficients have lower standard errors over time for more robust feature selection __eou__	User

TP	@lesshaste not really. Training R^2 is between 0 and 1, but on unseen data it can become negative.  And "coefficient of determination" is an established term in statistics. __eou__	User

TP	@vene Hi. If it's an established term then that is what we should use. Maybe a very brief explanation of the range it can take on test data and why could be added? @vene  I managed to get these scores [-0.38971809 -1.32178009 -0.20038367]  . What is the range? It clearly isn't -1 to 1  @vene  that's interesting. It would be great if something definitive and clear could be added to the docs about this. Please :) @vene that's a very interesting observation! I have found them to be great when I have a mix of categorical and numerical values and lots of data @vene  maybe I should send you my data to see what you can make of it :) because I am completely failing currently __eou__	User

TP	It's -INF to 1, I think. __eou__	User

TP	@lesshaste have you read this? http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score the intuition is: a model can never do worse than predicting the mean *on training data*. (at least a linear model that can set all its coefs to 0) but it can do much worse on test data if it overfits. @lesshaste did you try feature selection? @lqdc confidence intervals are cool, but it seems more in the domain of statsmodels. And I think it's actually already implemented in statsmodels. can you assign 127.0 to a char normally? __eou__	User
TP	yes__eou__	Agent
TP	what is it supposed to do? it doesn't typecheck to me to assign a float to a char__eou__	User
TP	really?__eou__	Agent
TP	But it works for me ...__eou__	Agent
TP	Thx for your help!__eou__	Agent
TP	It is not  assign a float to a char__eou__	Agent
TP	It is to declare a type that can accept either cython.char or  cython.float__eou__	Agent
TP	yes, the type is declared correctly. But you can't instantiate and assign to it like that, apparently__eou__	User
TP	It should work if you make it a function argument, if I understand correctly__eou__	User
TP	I mean, what are you trying to do in this example? Why do you want your type to be generic if you're assigning a float to it?__eou__	User

TP	on "hard" regression problems (few samples, many irrelevant features) MSE/MAE can lead you to believe you're doing well,  if you don't compare against a dummy baseline that predicts the mean, or something simple like that. I've fallen in this trap. __eou__	User

TP	@vene Oops! I hadn't read that. Sorry that's my bad @vene  I seem to have a hard regression problem currently :( __eou__	User

TP	hey, if anyone has any remote remote , designer,  DevOps  or Sysadmin jobs they can post them at http://webwork.io __eou__	User

TP	[for OLS at least](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLSResults.conf_int.html#statsmodels.regression.linear_model.OLSResults.conf_int) __eou__	User

TP	seems like it's there for [GLM](http://statsmodels.sourceforge.net/devel/generated/statsmodels.genmod.generalized_linear_model.GLMResults.conf_int.html#statsmodels.genmod.generalized_linear_model.GLMResults.conf_int) too __eou__	User

TP	@vene  I didn't.. I just assumed that randomforestregressor doesn't really benefit from that. Is that wrong? __eou__	User

TP	I haven't used random forests much, dunno. __eou__	User

TP	@vene  I think problem is that I have 140 samples where I am used to 100s of thousands so my intuition for what works is wrong __eou__	User

TP	I never actually managed to get random forests to outperform linear models on the datasets I worked with which are usually <<10k samples __eou__	User

TP	Hello guys, Is there anyone familiar with cythons fused type?  ```cython cimport cython  ctypedef fused char_or_float:     cython.char     cython.float  def show_me():     cdef char_or_float  cython.char a = 127  show_me() ``` Oh theres a typo Following script dosnt work ...  ```cython cimport cython  ctypedef fused char_or_float:     cython.char     cython.float  def show_me():     cdef char_or_float a = 127.0  show_me() ``` Can somebody tell me whats the problem? __eou__	User

TP	oh yeah I simplify the code alot __eou__	User

TP	it's not that; I don't think it makes sense to assign a literal to a fused type this works, for example: ``` %%cython  cimport cython  ctypedef fused char_or_float:     cython.char     cython.float  cdef char_or_float add_1(char_or_float x):     return x + 1  def show_me():     cdef cython.char a = 1     print(add_1(a))  show_me()``` note i'm declaring and assigning to a char, not to a char_or_float. But I can pass it to a function that takes char_or_float __eou__	User
TP	I see__eou__	Agent
TP	thanks for the clarafacation__eou__	Agent
TP	in theory I'd imagine, for your example,  that the compiler *could* just specialize your char_or_float to a float when you assign to it. But I can see why they didn't implement that, it doesn't really have a point.__eou__	User
TP	Are you familiar with C++ templates?__eou__	User

TP	@vighneshbirodkar thanks. Sorry, I'm still travelling. How does it compare in terms of runtime? __eou__	User

TP	@vene  That makes sense but stats models doesn't deal with a million features basically the whole lib doesn't work for omre than like 20 features @lesshaste sounds good __eou__	User

TP	Also statsmodels in general is not developed very quickly or actively. I used to follow it but gave up. __eou__	User

TP	Maybe there is a way to initialize the statsmodels result object with our coefs. And use their post processing __eou__	User

TP	@vene  that's a nice idea. __eou__	User

TP	@lqdc  Do you want to open an issue with an example large enough that  statsmodels can't cope? and paste in @vene's thought maybe __eou__	User

TP	just had a small doubt; how to compile a .cpp file into a python extension? I generate the cpp file by using --cplus extension but how can I compile after this? Is there an easier way to do the whole process? __eou__	User

TP	no probs. running `python setup.py build_ext --inplace` from source works plus you can see the command for compiling a c++ file into a python extension __eou__	User

TP	Hey everybody, I'm struggling a little bit with understanding how I'm going to deploy a scikit learn algorithm which has been implemented using scaled feature values You see, I'm creating a calculator which'll do a logistic classification based upon a few values given by the user. And I don't get how  I can scale the values I get from the user using the same scaler as I used in the algorithm itself. Would love to get help if anybody's interested ! __eou__	User

TP	@perborgen  I'm not sure if this is what you're looking for but, if you're using preprocessing.StandardScaler, you could set scale_, mean_ and variance_ explicitly once you've gotten those values from fitting the scalar with your training set. You'd only need to persist those somehow. __eou__	User

TP	Hey guys, can anyone tell me why the CI failed at [this line](https://travis-ci.org/scikit-learn/scikit-learn/jobs/116304478#L2329)? According to the [doc](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.rand.html), scipy.sparse.rand() should accept `random_state`as an argument. __eou__	User

TP	And there is no erro if I run `nosetests` on my own computer. Oh I got it ... CIs scipy version = 0.9 __eou__	User

TP	hi @rvraghav93 __eou__	User

TP	@rvraghav93  In relation to the email thread "Class Weight Random Forest Classifier " I should say that I remember it making no difference for me either when I tried it some time last year __eou__	User

TP	Hey everyone, could some please review #6221? Just need a second affirmative on that one. Thanks! __eou__	User

TP	sure ill take a look __eou__	User

TP	done @dsquareindia :) __eou__	User

TP	 Thanks a lot @nelson-liu ! __eou__	User

TP	@dsquareindia @maniteja123 @nelson-liu @yenchenlin1994 Please make sure you submit your proposals soon into the withgoogle website. I believe the deadline is in less than 3 days... Thanks and good luck :) __eou__	User

TP	Will do, thanks for the reminder! __eou__	User

TP	I'm not sure who else is interested. If there is any other interested person, please make sure you submit it within the next 3 days as you won't be able to do so after the deadline... (don't worry about it being perfect. **Just make sure that you clearly outline, at a high level what you wish to achieve within the timeslot**...) __eou__	User

TP	Hi everyone, (I'm not sure this is the best place to ask; else let me know.).  I'm trying to cross-val a KMeans clustering and retrieve the most likely cluster:  ``` kmeans = list() for x in X:     dist = pairwise_distances(x)     kmeans.append(KMeans().fit_predict(dist)) ```  Although the clusters are very similar across iterations, the cluster labels are (obviously) random.  Do you know how I can aggregate these labels to find the most robust clusters across iterations: e.g.  `cluster_idx = scipy.stats.mode([sample for sample in kmeans])` __eou__	User

TP	Hmm, you should x-post to stackoverflow. Theyd probably be more responsive __eou__	User

TP	@nelson-liu ok thanks! http://stats.stackexchange.com/questions/202883/how-to-combine-the-results-of-several-clustering-with-scikit-learn __eou__	User

TP	Great thanks for your reminder! __eou__	User

TP	I had some doubts regarding the project which I have listed in my [proposal](https://github.com/scikit-learn/scikit-learn/wiki/%5BDevashish%5D-GSoC-2016-project-proposal:-Adding-fused-types-to-Cython-files) itself.  It would be wonderful if anyone could give their opinions on them. Thanks! __eou__	User
TP	Hello @dsquareindia , you mean the type issue in ensemble?__eou__	Agent

TP	yeah there wouldn't be a huge difference by adding fused types there right? I could work on that later after crucial modules have already been worked on. wdyt? __eou__	User
TP	Yeah I think so__eou__	Agent

TP	I agree with yen __eou__	User

TP	hi @rvraghav93 __eou__	User

TP	Hi! __eou__	User

TP	@rvraghav93  Hi. Have you a moment to discuss the categorical features/random forest/benchmark issue? __eou__	User

TP	@lesshaste Yup! __eou__	User

TP	@rvraghav93  great!  So... a) what is going on? :) What I mean is, do xgboost and H20 actually support categorical variables at all? __eou__	User

TP	Yes apparently they do... :/ We are working on that and we'll become awesome in a few more months B) __eou__	User

TP	@rvraghav93  ok but the comment on the PR was that it would actually not help which is what confused me __eou__	User

TP	No I definitely do think introducing native support for categorical variables would indeed speed up our rf Hmmm I didn't see that.. Give me a moment! __eou__	User

TP	"It looks like they are using decision tree-based classifiers (i.e., RandomForestClassifier and GradientBoostingClassifier) rather than extra-random tree-based classifiers. And it looks like their dataset's categorical features (airlines, origin & destination airports) probably have cardinality > 64. These two factors together mean NOCATS can't be used." did you see that? thanks __eou__	User

TP	Okay so I think before benchmarking against H2O and xgb. We need to make the splitting of categories locally optimal (we should decide what way the categories go at each split) and not just globally optimal. Then if the cardinality is > 64, we need to investigate why they support such high cardinality and whether or not we could do the same... I think even R has restrictions on the cardinality of the categorical features... How about rpart? I hear good things about it... __eou__	User

TP	R does but the R RF code is bad well.. the default version is.. there are better versions and people also use xgboost with R rpart maybe be better .. hmm which version do they use in their benchmark? http://www.wise.io/tech/benchmarking-random-forest-part-1 is another example that shows how bad R randomForest is though ok thanks.  There is also H20 but I don't know how well their implementation us in any case.. it would be great to have somewhere where concrete improvements relevant to that benchmark could be discussed. It all seems slightly confusing at the moment __eou__	User

TP	Also one another thing to note is that xgboost works somewhat differently compared to sklearn's rf as they seem to use approximate splitting and a second order objective as described in the paper that got recently published by Tianqi Chen...  You should look into that paper... I think there is a section which briefly explains why they are faster than us... I haven't had time to take a good look into that paper. But if you do, please share your insights... I think our top priority, as far as the tree based modules are concerned, is to merge the missing value support and the categorical variable support soon into scikit-learn... Once that is done we can think of making it better comparing it with xgboost... Maybe if these two are done, I'll see if I can make a blog post with readable code that compares the rf implementations... ;) yupp! No that one is a condensed version... wait http://arxiv.org/pdf/1603.02754v1.pdf __eou__	User

TP	you mean [this paper ](http://learningsys.org/papers/LearningSys_2015_paper_32.pdf) ? __eou__	User

TP	Oh I am really interest in gradient boosting since Microsoft use it to [learn how to play Minecraft](http://research.microsoft.com/en-us/um/people/alekha/arxiv_geql.pdf) Thanks for the link and sorry to interrupt :worried: :smile: __eou__	User

TP	Or maybe both are same... I'm not sure... The last link is the one that I have on my table accumulating dust... Have to read it soon :@ __eou__	User
TP	@rvraghav93 That's a great idea!__eou__	Agent
TP	np__eou__	User

TP	@yenchenlin1994  interruptions welcome :) __eou__	User

TP	@rvraghav93  one more dim question.. :) I see in the NOCATS PR (RandomForestClassifier, full, One-hot) AUC: 0.712132537822 and  (RandomForestClassifier, truncated(8), NOCATS) AUC: 0.668807372591 why is the second so much worse than the first? is it the truncated part? I assume so.. so is there no example that shows NOCATS doing better? I am not sure I understood "the full dataset with NOCATS categorical splitting (actually no random forest in this case)," __eou__	User

TP	Hi guys, how are you able to make imports like "from sklearn.some_module import some_func" from any python file. Have you changed sys.path from anywhere? Thanks Actually I was just browsing code. For example in this file https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/covariance/tests/test_covariance.py We are able to do `from sklearn.utils.testing import something` I was wondering how were you able to do that? Is there any configuration for this? __eou__	User

TP	Can you `import sklearn`? __eou__	User

TP	@yenchenlin1994 there? __eou__	User

TP	@SaurabhJha No, there isn't any. This is how the import system of python works. __eou__	User

TP	a [reference](http://stackoverflow.com/questions/448271/what-is-init-py-for) __eou__	User

TP	it shows how `__init.py__` works! __eou__	User
TP	yeah. cool__eou__	Agent

TP	Hope this answers your question :smile: __eou__	User
TP	yes. Thank you :)__eou__	Agent

TP	Hi. Trying to use LogisticRegression with multi_class='multinomial'. Ending up with this error:  __init__() got an unexpected keyword argument 'multi_class' sklearn version is '0.15.2' Can anybody please help? __eou__	User

TP	Hi @VarunKShetty , it seems that such parameter doesnt exist. Check the documentation to see which parameters can be used. __eou__	User

TP	@ksafford Thanks a lot, that helped! __eou__	User

TP	@VarunKShetty  LogisticRegression in 0.15.2 doesn't support multi_class, if you update to the 0.17.1 version it should work. __eou__	User

TP	Thanks @dvdnglnd I'll now go figure out how to update it __eou__	User

TP	Hi everyone. I have a featureunion of several pipelines, is there any way I can turn the featureunion into a numpy array for use in other applications? e.g. in this case, Im using scikit-learn to do the preprocessing, and keras for the learning __eou__	User

TP	You should be able to call transform to obtain an array As long as your pipelines have only transformers and no predictors __eou__	User

TP	It doesn't really make sense to me to turn the "feature union" into an array, but to apply it on data to obtain an array. Or am I misunderstanding? __eou__	User
TP	ah, no you arent misunderstanding, I was.__eou__	Agent
TP	the featureunion is just a scaffold of various steps, which you then pass things into and get a numpy array. Is this what you were thinking?__eou__	Agent

TP	 @ogrisel do you know what happened to the pdf docs? They are gone :-/ __eou__	User

TP	@amueller Isn't ADMM guaranteed to produce an optimal solution irrespective of the value of `mu` ? I am referring to equation 5.1 [here](http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf) `mu` is multiplied by a factor which should be 0 for all feasible solutions. __eou__	User

TP	@vighneshbirodkar well the solutions are not feasible and with very small mu they are very far from being feasible __eou__	User

TP	hi @rvraghav93 __eou__	User

TP	Anyone can help on https://github.com/scikit-learn/scikit-learn/issues/6574? __eou__	User

TP	@nelson-liu   I love your GSOC proposal by the way __eou__	User

TP	thanks @lesshaste :) __eou__	User

TP	The GMM documentation says that all components are initialized to mean 0, identity covariance. How is it possible to update the components if each component is identical? __eou__	User

TP	I vaguely recall something about sklearn using kmeans++ to initialize the components somewhere. Maybe I'm imagining things. __eou__	User

TP	The code seems to imply it uses a round of kmeans to initialize the components, but the documentation doesn't say that. __eou__	User

TP	@lesshaste Hello! Sorry I've been a bit busy lately. ;( __eou__	User

TP	I feel like I should know that, but does scikit-learn have a doi? __eou__	User

TP	@amueller I dont think so? I just searched the docs and couldnt find it. It also isnt in the original publication. I also looked at a few papers that cited scikit-learn, but they omitted the DOI __eou__	User

TP	I don't recall us having one. __eou__	User

TP	hi, pls i will need some explanation. if i have a honeypot logfile, and i want to apply neural network to the logfile, so i can get analysis type of the logfile, which i want to use to generate intrusion signature. __eou__	User

TP	@ikennarene what is your question? __eou__	User

TP	 i am thinking of applying classification and clustering algorithm on a honeypot logfile, after which i want to generate intrusion signature based on the algorithm results, pls can i get more explanation on this, and approach i can use if possible. thanks __eou__	User

TP	so you want to detect intrusions? Do you know which ones were intrusions or not? __eou__	User

TP	@amueller unknown __eou__	User

TP	try outlier detections methods like isolationforest __eou__	User

TP	hi everyone. Im working with countvectorizer, and I already have a corpus that has been pretokenized and everything. How would i extract ngrams from it? setting `analyzer=str.split()` breaks the ngram_range argument. Would i have to write my own analyzer? setting the default (`analyzer=word`) does not work for me because that strips punctuation and I want to keep punctuation. __eou__	User

TP	@rvraghav93  no problem at all! __eou__	User

TP	http://scikit-learn.org/dev/auto_examples/ensemble/plot_isolation_forest.html has a nice looking example but it would be great if it had a little more explanation about the decision boundaries? Why are there 4 areas that are non-anomalous and not just two for example? __eou__	User

TP	hello everyone __eou__	User

TP	Nrlson-liu it should be str.split rather than str.split() __eou__	User

TP	ah, yeah sorry that was a typo on my part. i ended up just making a custom analyzer. __eou__	User
TP	str.split has worked for me__eou__	Agent

TP	using str.split seems to break ngram_range, though? __eou__	User

TP	e.g.  ``` >>> from sklearn.feature_extraction.text import CountVectorizer >>> ngram_vectorizer = CountVectorizer(analyzer=str.split, ngram_range=(1, 2)) >>> ngram_vectorizer.fit_transform(['The quick brown fox jumped over the lazy dog .']) <1x10 sparse matrix of type '<type 'numpy.int64'>'  with 10 stored elements in Compressed Sparse Row format> >>> print ngram_vectorizer.get_feature_names() ['.', 'The', 'brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the] ``` looking at the code for `CountVectorizer`, it seems like the analyzer argument is also responsible for making ngrams. So `str.split` would only make unigrams? __eou__	User

TP	@jnothman I am here now :) __eou__	User

TP	Okay. I think you're misinterpreting the numpy error's relevance to this situation. But np.where(mask_matrix.max(axis=0))[0] might not quite work, because `mask_matrix.max(axis=0)` returns a 2d matrix. `np.flatnonzero(mask_matrix.max(axis=0))` should fix that issue, though I don't get at all how you could be getting an error on a *print* statement! I hope you got that @maniteja123 __eou__	User
TP	Yeah thanks, that was the reason for the ValueError__eou__	Agent
TP	But still didn't get the comment regarding the *print* statement :worried:__eou__	Agent

TP	Also ``np.flatnonzero`` gives an error ``AttributeError: ravel not found`` __eou__	User

TP	Oh. Okay. I'm not in the right frame of mind. This should be easy for me! np.where(masked_matrix.max(axis=0).toarray().ravel())[0] should work! or equivalently: `np.flatnonzero(masked_matrix.max(axis=0))` or just masked_matrix.nonzero()[1] sorry, `masked_matrix.max(axis=0).nonzero()[1]` __eou__	User

TP	Thanks I got the idea now.  Will implement and get back to you. __eou__	User

TP	``` (0, 0) 4.0   (1, 0) 6.0   (2, 0) 7.0   (0, 1) 2.0   (1, 1) 3.66666666667   (2, 1) 6.0   (0, 2) 1.0   (1, 3) 1.0 ``` This is the transformed X for X = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]]). I think it is working now as you expected.  Thanks for all the help. __eou__	User
TP	Yay!__eou__	Agent
TP	I hope you're able to go through the steps and understand how this is doing what we want too...__eou__	Agent
TP	Gotcha.. it is clearer now..__eou__	User
TP	just a question, when you call ``eliminate_zeros`` all the zero entries are removed. Doesn't that happen by default when the sparse matrix is built ?__eou__	User

TP	Oh okay got it.. Building the matrix using the ``_with_data`` logic still populates the zeroes and needs to be manually removed to make it sparse. __eou__	User

TP	I will add some tests for sparse matrices too. Is there anything else you want me to look into here ? I need to go to my college now. Won't have access to the internet. Sorry. Will reply by evening. Thanks again for all the help. __eou__	User

TP	Hi all.  Is scikit-learn 0.17.1 the recommended version to use for new projects?  Thanks. __eou__	User

TP	@staffhorn generally, yes __eou__	User

TP	Anyone can help me review [this PR]? (https://github.com/scikit-learn/scikit-learn/pull/6593) :pray: __eou__	User

TP	Hey guys __eou__	User

TP	Whats the recommended way to recompile a single .pyx file after I modify it? Yeah @nelson-liu you are right, only compile is not what I want haha __eou__	User

TP	You just do the `python setup.py build_ext` or `python setup.py build_ext -i` (for inplace building) again. It generates c and recompiles for the changed cython sources only. ;) __eou__	User

TP	if you just want to compile, you could do `cython primes.pyx`. but you probably want to build in place again for it all to work together. yup, what @rvraghav93 said __eou__	User

TP	oh okay thanks for your quick help :smile: __eou__	User

TP	> Anyone can help me review [this PR]?  ![](https://i.imgur.com/FNqNVVw.png) ;P __eou__	User
TP	lol we should put that in contributing.rst :P haha  just kidding__eou__	Agent
TP	hahahahaha__eou__	Agent
TP	good idea__eou__	Agent
TP	(just kidding) we should add THIS in the PR template ;P__eou__	User

TP	:satisfied: __eou__	User

TP	Do we have a make target to cythonize only what's needed? __eou__	User

TP	By only what's needed, you mean to say - Only specific modules? __eou__	User

TP	is python 3.x fully supported by scikit __eou__	User
TP	yes, it should be.__eou__	Agent
TP	i know 3.5 is fully supported__eou__	Agent
TP	its been tested with 3.4 / 3.5, and 3.3 should also work. not sure about 3.1 or 3.2__eou__	Agent

TP	in that case if i want to get into contributing to scikit, using python 3 syntax for `print` should not be a problem in pull requests __eou__	User
TP	indeed__eou__	Agent
TP	if you look at https://github.com/scikit-learn/scikit-learn/search?utf8=%E2%9C%93&q=print, youll see that the <unconvertable> print' statements are all functions__eou__	Agent
TP	make sure to `from __future__ import print_function`, though.__eou__	Agent

TP	@rvraghav93 / others working with the tree module - if youve read breimans <unconvertable> Classification and Regression Trees, would you recommend it as a resource to get familiar with the theory? might as well tag @glouppe as well __eou__	User

TP	i'd like to do classification/regression with multiple outputs, but each example only has the observed labels for one of the outputs. Although the DataSet object has entires for masks, which seems like it should fit the bill, I'm getting the impression that the masks only works for rnns and not for feedfoward classification/regression nets. Is that correct? __eou__	User

TP	can you link the example you were looking at? __eou__	User

TP	Hi guys __eou__	User

TP	I have images with different objects in them such as trees, grass,river etc and want to classify which object is present in each image. Can anyone help me do the following a) load all the images one time from a folder b)Extract features b) Concatenate all features features into a matrix or a vector. This is my link to the working files https://github.com/Ben-Kobby/My-Project.git. Thank you. __eou__	User

TP	Hey everyone, just had a small doubt. I'm trying to implement a feature selection algorithm which uses a term-category matrix. I'm trying to implement it on the 20NG dataset. Currently I'm using `CountVectorizer` to produce the term-doc matrix (by  transposing the output of `fit_transform`). However I want to create a term-category matrix from this by mapping the count in these docs to another matrix with 20 columns. What would be the most efficient way to do this mapping? Thanks in advance! __eou__	User

TP	@nelson-liu Yes its a good resource, but is pretty huge. You could start with some simple youtube videos/blog posts explaining the concept of CART and learn more as you go (atleast that is what I did)... Please feel free to ping myself or Jacob (here or in e-mail) if you need any help. __eou__	User

TP	hey all, is there anyone experienced using LSTM-RNN for labeling each frame of a video data __eou__	User

TP	No,  but I can sound confident when I tell you stuff. __eou__	User

TP	haha :smile: __eou__	User

TP	@dsquareindia I don't think your question is answerable as currently stated. How do you intend to map counts to categories? __eou__	User

TP	@dsquareindia it sounds like you want to use [LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to get a categorical one-hot Y, then just do Y.T * X where X is the output of CountVectorizer. This should give you word counts per category. (I'm using * instead of dot because X is sparse) __eou__	User
TP	@vene +1 I suppose @dsquareindia is making the claim that if a document is labeled X, then a term in that document should also belong to the category X.__eou__	Agent
TP	correct?__eou__	Agent

TP	You can find reason to count word-category occurrences without making that strong assumption. Maybe he's using the count matrix to do some other calculations afterwards. __eou__	User

TP	@vene makes sense, thanks. __eou__	User

TP	I think our chi2 feature selector is asking these lines *along these lines __eou__	User

TP	Thanks a lot @Zintinio and @vene for the help! Yes I want to use that count matrix for some further calculations. Also, yes I'm making the assumption that if a doc is labeled X then the term belonging to that doc also belongs to the category X. Basically if a particular term occurs in a document categorized as X, then in the term-cat matrix the entry correspoding to that term and cat (X in this case) is updated by one. I then assign certain weights for each term for each category and see how much that term contributes to each category. This is in turn used in performing feature selection. __eou__	User

TP	hey all, is there anyone experienced using LSTM-RNN for labeling each frame of a video data __eou__	User

TP	@oakkas re-posting the same question will not get you a response ;) If you have a particular question that is not suitable for a quick discussion, I would suggest that you post it as a thread to our mailing list or stackoverflow. If people know about it and your question interests them, they will respond in detail. That being said, LSTM-RNN is deep learning stuff that is not a part of scikit-learn. You would be better off contacting the Mailing List of some deep learning library like theano, tensorflow or caffe... Or even better like I said before stackoverflow. __eou__	User

TP	reddit seems to be decent for deep learning discussions these days __eou__	User

TP	Any idea whats happening here ? Due to some reason numpy does not print strings with quotes https://travis-ci.org/scikit-learn/scikit-learn/jobs/123430159 __eou__	User

TP	hi everyone, I'd like to compare different scorers for univariate feature selection. Currently scikit-learn provides chi2 and f_classif but I'd like to use others like document frequency, infogain and bi-normal separation too (here is a comparative study http://www.jmlr.org/papers/volume3/forman03a/forman03a.pdf).  I've already written a vectorized implementation of infogain but according to the chi2 documentation (http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2) it seems that it should return the infogain scores along with an array containing the p_values of each feature. How should I calculate/interpret that p_values? __eou__	User

TP	Howdy folks, I've been playing with sklearn for an example using [dask.distributed](http://github.com/dask/distributed) `Futures`.  I suspect that I'm making poor choices regarding machine learning and would appreciate feedback.  https://gist.github.com/mrocklin/80b0d6f57dedc1628954ced5ef5500b0 __eou__	User

TP	http://matthewrocklin.com/blog/work/2016/04/20/dask-distributed-part-5 __eou__	User

TP	I read this blog post recently describing how the Atom editor developers manage github issues, and i think lots of the advice given could apply to  scikit-learn as well! http://blog.atom.io/2016/04/19/managing-the-deluge-of-atom-issues.html __eou__	User

TP	Sorry too late to reply: I commented on your post. I think besides the randomized parameter search or exhaustive grid search use case, the other common machine learning use case that really benefit from distributed training is gradient boosted trees. The implementation of boosted trees in scikit-learn is not really amenable to cluster-wise distribution in its current form. However xgboost is really mature in that regard and already provides hadoop yarn integration. I think it would run great on top of dask.distributed. They also plan better integration with pandas dataframe but this is still on the roadmap. __eou__	User

TP	I am a little concerned about example http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html#example-feature-selection-plot-feature-selection-py there is not blue bar for the second feature and smaller bar for the forth one thus I am not sure how the example proves that feature selection improves SVM.. __eou__	User

TP	Hi all, __eou__	User

TP	I am new to sklearn, and I really like this stuff. I want to contribute, but I think, I must use sklearn to a good level. And then start diving into methods. Can you suggest me a way to start? At end I want to be a one of the core contributor of sklearn, no matter how much time or years it takes. And I can start using sklearn from basics here? __eou__	User

TP	A good place to start is to try kaggle competitions __eou__	User

TP	are you trying to learn sklearn or are yout rying to learn data science? __eou__	User
TP	I think the former, but the best way to do so much s to learn basic data science imo__eou__	Agent
TP	*is to learn__eou__	Agent
TP	well it seems to me they are very different learning tasks__eou__	User
TP	If the goal is to eventually contribute, you'd want knowledge of how the various algorithms work, what their use cases are, etc. The easiest way to do this imo is to just use them in various applications.__eou__	Agent

TP	Yes sklearn. I am a pythonist, and I want to dwelve into sklearn functions and contribute. Right way is to strengthen my basics. I think I can go with what @nelson-liu  suggested. __eou__	User
TP	doesn't make any sense to me but whatevs__eou__	Agent
TP	@elbamos according to you, what is the right approach, if someone wants to be a good contributor to sklearn?__eou__	User
TP	Thanks @yenchenlin1994 @elbamos  @nelson-liu__eou__	User

TP	If the goal is to contribute, then [here](http://scikit-learn.org/stable/developers/contributing.html#easy-issues) has already answered your question __eou__	User

TP	@harshul1610 To answer your question:  Implementing a machine-learning algorithm is hard.  Its not enough to be able to program.  You have to really understand the *math*, and debugging the math is challenging.  And you have to also understand how data scientists actually use these algorithms. __eou__	User

TP	Hi.. I have a binary classification task with 300 positive examples 300 million negative Is there a sensible way to handle this? well I would like to use the knowledge about the  300 million negative examples to learn what "normal" looks like @hmha  I could just throw it at a random forest and ignore the massive skew. Is that a sensible thing to do? __eou__	User
TP	@lesshaste sorry, I don't have enough knowledge yet to help you__eou__	Agent
TP	@vene  An interesting suggestion.  Unfortunately it my case I don't think linear models will work__eou__	User
TP	Also, there are a lot of pairs of positive and negative examples aren't there?__eou__	User

TP	what do you mean by "to handle" ? __eou__	User

TP	@hmha  no problem at all it would be nice if some of these were in scikit-learn https://github.com/fmfn/UnbalancedDataset __eou__	User

TP	If you think a linear model could work, you could optimize for AUC by training on pairwise differences of positive and negative examples. __eou__	User

TP	there are a lot indeed, but you could stochastically subsample the pairs and do partial_fit sgd iterations it's the idea used in sofia-ml, which is pretty great __eou__	User

TP	now I need to look up sofia-ml! :) __eou__	User

TP	@vene  but for my problem I can't see that linear models would work the "numerical data" has special values that seem to indicate particular things. So 1,56,123 have some meaning from 200-10000 don't for example. Except I don't get told what those are random forests are good at picking these out __eou__	User

TP	true. You could discretize the data, I guess. In random forests you can just use sample weights to deal with class imbalance. actually it seems now you can actually use `class_weight="balanced"` @lesshaste I'm not saying there is no bug there, I am not familiar with the code but I'm saying that, even with linear models, `class_weight="balanced` does not necessarily lead to better generalization __eou__	User

TP	the problem is its not clear a priori how to bin the numerical data, if that is what you mean I am not 100% convinced that actually does anything :) there is an issue about that I think but I will certainly try that __eou__	User

TP	it should reweigh the samples accordingly. The problem is, that isn't guaranteed to be better. __eou__	User
TP	ah ok__eou__	Agent
TP	that's interesting__eou__	Agent

TP	If you're into deep learning stuff, you can use a similar sampling strategy + pairwise training there. I think they call it "contrastive loss" in that world :) __eou__	User

TP	@lesshaste other options for imbalanced data: 1) when it is that skewed, try anomaly detection. 2) I found this downsampling+bagging from Wallace et. al. to be principled approach (https://scholar.google.com/scholar?cluster=225520837537786880&hl=en&as_sdt=0,5&as_vis=1) __eou__	User

TP	@ogrisel @amueller  You think it's necessary to upload the wheels of the template project on Rackspace ? Granted the wheels won't be of much use, but it could serve as an example for projects which clone it. __eou__	User

TP	does anyone know when the argument `loss_func` was removed from GridSearchCV? Cannot find it in the changelog __eou__	User

TP	@HolgerPeters https://github.com/scikit-learn/scikit-learn/pull/3411 Seems like it was planned to be removed in 0.15, but might have been removed later. I couldnt find any mention of it in the changelog as well. __eou__	User

TP	date on that is july 17, 2014. 0.15.0 was released on July 15 2014, and 0.15.1 was released on Aug 1 2014. Im presuming it was thus gone by 0.15.1? __eou__	User

TP	@cfperez  thanks very much @vene I will look up "contrastive loss" thanks __eou__	User

TP	@cfperez  in my case I am wondering whether the extreme nature of the class imbalance (300 versus 300 millions) means that black box methods might not be appropriate that is i may have to do something other than try to infer the model from the data __eou__	User

TP	@vighneshbirodkar Sounds like a good idea __eou__	User

TP	@cfperez  downsampling and probably boosting? __eou__	User

TP	@amueller @ogrisel when is the 0.18 release? __eou__	User

TP	@vene How do you do a random forest embedding? Oh...reading about it now. __eou__	User

TP	hello @rvraghav93   Really sorry for the late reply, just survived from my midterm :smile:  Yeah Ill send it to the mailing list today! haha you too __eou__	User

TP	 yay, congrats! __eou__	User

TP	when was random forest embedding added to scikit-learn? @vene  thanks I will try that idea. Do you happen to know when was random forest embedding added to scikit-learn? oh.. 0.13! Not sure how I missed it do you have a view about https://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering.htm ? I can't quite tell if it would provide something significantly new to what scikit learn already has if you follow the citations it seems isolation forests come from a similar idea I could open an issue I suppose but I feel a little ignorant on this topic @amueller  As the author of the random forest embedding, does this provide anything extra? @amueller  where "this" is https://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering.htm __eou__	User

TP	I think at least 2ish releases ago Oh __eou__	User

TP	That tech report is really hard to skim @lesshaste, the random forest embedding uses totally randomized trees, ie there is no learning __eou__	User

TP	yes... I did try myself.   http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolationThis performs a comparison with this method apparently " In the first experiment we compare iForest with ORCA [3], LOF [6] and Random Forests (RF) [12]" where [12] is https://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering.htm __eou__	User

TP	I don't know anything about how isolation forests work. But it seems that the RFclustering approach trains a discriminative RF between the real data and randomly sampled data. __eou__	User
TP	ok that's interesting already__eou__	Agent
TP	and uses that repr for clustering__eou__	User
TP	Sampling-based approaches to anomaly detection never really struck me as a great idea. But in your case you might be able to tweak it more, because you know some things about the "ideal" generative process of your data__eou__	User
TP	ie if network traffic were random__eou__	User
TP	you might be able to hand-craft some sort of PGM mixture model__eou__	User
TP	probabilistic graphical model__eou__	User
TP	so a generative model__eou__	User
TP	is your task a regression task?__eou__	User

TP	what methods currently in scikit learn are suitable when the distance between features is highly non-linear? That is not at all Euclidean __eou__	User
TP	what do you mean by distance between features?__eou__	Agent
TP	btw if you plot a histogram of your data do you have some sort of "spikes" at the values that you say you want to treat as categorical?__eou__	Agent
TP	well yes sort of. Properly clustered there would be spikes in particular clusters but not in others__eou__	User
TP	what is PGM?__eou__	User
TP	ah yes.. well that would be great__eou__	User
TP	but it's hard to model network traffic__eou__	User
TP	you have some latent variables that correspond to your payloads, and when you sample, you have a chance to sample from a random poisson(?) or to exactly(?) pick out the payload__eou__	Agent
TP	I meant distance between feature vectors, not feature. In other words (1,23) might be very far from (1,25) but (2,24) might be very close__eou__	User
TP	and (1,25) and (1,26) might be very close__eou__	User
TP	so it's just not a simple euclidean distance__eou__	User

TP	Well, discriminative methods, even linear ones, could capture such a threshold I think __eou__	User

TP	interesting.. I did try linear regression on some labelled data and it was a disaster where a random forest worked really well I mean the linear regression essentially failed completely __eou__	User

TP	hey guys i'm new here needed some help  regarding machine learning i've just started studying about machine learning and everywhere i read that machine learning course on Coursera by Andrew Ng is good to begin with but it is not focused on python and i want to use machine learning in python so should i consider learning from somewhere else? help me out and what would you recommend on some good resources __eou__	User
TP	Good resources for what specifically?__eou__	Agent
TP	or anything that can help me learn faster__eou__	User
TP	i'm not sure i have just started__eou__	User
TP	maybe how to implement the algorithms in python__eou__	User
TP	where should i learn that from__eou__	User

TP	Whats up? :) __eou__	User

TP	You can do the assignments in Python if you want to __eou__	User
TP	If you can do machine learning in one language, youll have little to no trouble switching languages :)__eou__	Agent

TP	Has autoencoder not been implemented in scikit ? __eou__	User

TP	I suppose #2099 is a WIP for sparse auto encoder. __eou__	User

TP	Hello all, every time I get the error: "ERROR:py4j.java_gateway:Error while sending or receiving. Traceback (most recent call last):   File "/data/analytics/Spark1.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py", line 746, in send_command     raise Py4JError("Answer from Java side is empty") Py4JError: Answer from Java side is empty ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server Traceback (most recent call last):   File "/data/analytics/Spark1.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py", line 690, in start     self.socket.connect((self.address, self.port))   File "/usr/local/anaconda/lib/python2.7/socket.py", line 228, in meth     return getattr(self._sock,name)(*args) error: [Errno 111] Connection refused ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server Traceback (most recent call last):   File "/data/analytics/Spark1.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py", line 690, in start     self.socket.connect((self.address, self.port))   File "/usr/local/anaconda/lib/python2.7/socket.py", line 228, in meth     return getattr(self._sock,name)(*args) error: [Errno 111] Connection refused". My conf-file: "spark.serializer org.apache.spark.serializer.KryoSerializer  spark.kryoserializer.buffer.max 1500mb spark.driver.memory 65g #spark.driver.extraJavaOptions -Djava.io.tmpdir=/data/spark-tmp  spark.driver.extraJavaOptions -XX:-PrintGCDetails -XX:-PrintGCTimeStamps -XX:-PrintTenuringDistribution #XX:PermSize=20480m  spark.python.worker.memory 65g spark.local.dir /data/spark-tmp" The amount of data is about 5Gb. Does anybody know the answer? __eou__	User

TP	@AlexanderModestov please do not paste large (unquoted) error messages in the conversation but instead use a link to some gist or pastebin. Furthermore this seems to be specific to spark and not related to scikit-learn (the `sklearn` package does not even occur in the traceback) so I don't think this is the right place to ask such a question. You might want to ask this question on stackoverflow with the spark tag. __eou__	User

TP	@ogrisel I'm sorry. It's not about sklearn ... __eou__	User

TP	@vene  Hi. Sorry I meant logistic regression __eou__	User

TP	@amueller By making GridSearchCV work well with EstimatorCV did you mean implementing generalized cross-validation? (https://github.com/scikit-learn/scikit-learn/issues/1626) __eou__	User

TP	is anyone here very competitive about the relative capabilities of scikit-learn and R?  @amueller ? ... because with the R package I just pushed to my git, I would really enhjoy some trash talking __eou__	User

TP	@elbamos well I'd like to be scikit-learn as good as possible, and if you have an implementation that is much better than ours in some respect, I'd love to have your input ;) @elbamos what did you push? @rvraghav93 well part of that. I want to be able to use an EstimatorCV in GridSearchCV. @rvraghav93 like using a CV object in a pipeline. if you find a way to enable ``make_pipeline(StandardScaler(), LogisticRegressionCV())`` grid-searchable, that would be a start or ``make_pipeline(SelectPercentile(...), LogisticRegressionCV())`` __eou__	User

TP	@amueller largevis. It's like tsne but efficient on ultra large datasets because it scales in O(n). The algo is only two weeks old, they haven't presented their reference code yet. You have a few hours to catch up though - I found a bug in my C++ code that's gonna take me a while to fix __eou__	User

TP	^^ I don't think this is a race. you could also add python wrappers to your R package barnes-hut t-sne is also O(n) right? @elbamos  I hope it's BSD licensed ;) hm makes sense __eou__	User

TP	barnes hut t-sne is O(N log N) http://arxiv.org/abs/1301.3342 __eou__	User

TP	@elbamos let me know if you have an mnist picture ;) __eou__	User

TP	they say (n log n) i actually think its a little worse, at least as implemented.  whether largeVis really scales remains to be seen -- in theory its O(n) holding everything else constant, but when you grow n, you add nearest neighbors and sgd iterations too. @amueller Will do ;) Should be tonight actually -- the algo is working now on small data sets (does iris beautifully) but something is making it segfault when i scale up to mnist.  working on it now oh, and if I were in your shoes, I would totally agree its not a race __eou__	User

TP	hey guys, about decision tree how do we visualize it? __eou__	User

TP	you can export it to graphviz see http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html __eou__	User
TP	oh, randomforests do not have this?__eou__	Agent
TP	that's my actual question__eou__	Agent
TP	no `rf.tree_`__eou__	Agent
TP	no `rf.tree_`, but theres something else that I think you want__eou__	User
TP	`feature_importances_` not__eou__	Agent
TP	you can use `randomforest.estimators_` to get a  list of DecisionTreeRegressors__eou__	User
TP	I want to actually convert a randomforest into categorical variables__eou__	Agent
TP	ahhh, the separate instances do have the tree__eou__	Agent
TP	awesome :)__eou__	Agent
TP	yup, then just `export_graphviz` on all of them__eou__	User
TP	nice, good to hear__eou__	User

TP	I have the idea that the categorical variables as binary decisions in a sparse matrix would be awesome have RF as a preprocessing and then do sparse Ridge on top of it or NN rf.tree_.feature and .tree_.threshold are the ones huh :) `tree_.feature`, `tree_.threshold` yea it works __eou__	User

TP	hmm wheres `tree.feature_`? it seems like it would be what you want though __eou__	User

TP	it could basically transform a non-linear problem to a linear one, well.. if it can be captured by a decision tree __eou__	User

TP	@nelson-liu If your plan is to treat each tree's prediction as a a distinct categorical variable and then make your actual prediction by applying ridge regression or some other mechanism to those variables, I will bet you 10:1 odds that (a) you never implement this algorithm because of the dimensionality of what you'd have to feed into the ridge regression, or (b) if you do implement it, it overfits __eou__	User
TP	I was thinking to PCA it perhaps lol__eou__	Agent
TP	__eou__	Agent

TP	or at least to remove too strongly correlating kind of duplicates @elbamos  I thought it more of a preprocessing step indeed it'd be too intense __eou__	User

TP	oops sorry nelson, i mixed your two chats up __eou__	User

TP	what about from a single decision tree? is there a situation in which you could imagine that using the leaf node's id as a binary var for a next model useful? as kind of feature engineering step that sounds very interesting elbamos googled suppressor: one variable that increases the effect of another var lol strange naming I'm interested for this particular challenge: http://blackboxchallenge.com/ super nasty one __eou__	User

TP	wanna try something funky for preprocessing that would probably work?  take your data and cut it into square cunks, like if your data is 1000 dimensional, chop it into one 31 * 31 square and one 7 * 7 square.  The run a convolution over both squares using a kernel initialized to have mean 0 and unit norm.  you can control the amount of dimensional reduction by controlling the size of the kernel. @lqdc that's just the effect of a suppressor variable on overfitting. __eou__	User

TP	I think I've seen people use node ids as IVs before for the next estimator and they got "better" results Some people on kaggle at least improved their model by a fraction of a percent. It's very empirical, but was useful in their case. i.e. less overfitting because the number of splits is regularized? __eou__	User

TP	Hey guys, I have a question: How do you deal with unrealistic estimates for probabilities for some estimators?  I tried using CIs (#6773), which were fine in my case, but apparently this is not the preferred approach. Is that competition for spam filtering? i can imagine mail.ru dealing with lots of that __eou__	User
TP	it's a reinforcement learning comp__eou__	Agent
TP	on each round (there are ~1.2m)__eou__	Agent

TP	@lqdc no, any time you add a variable to your training data, performance on the training data will improve slightly, regardless of whether there's a genuine relationship.  That's just the math.  Its called a "suppressor" variable because it suppresses the true error.  But the result is just increased overfitting. __eou__	User
TP	you get 35 values__eou__	Agent
TP	you can influence the 36th variable by choosing 1 out of 4 actions__eou__	Agent
TP	@lqdc if you want an example of this, create a 20 * 100 normally distributed random matrix, and 20 randomly selected labels.  split the data into a training and a test set by row.  Run randomforest over the training data, trying to predict the labels.  observe your error (it will be zero).  then run the generated model on your test set.__eou__	User
TP	this is why having features that don't truly correlate leads to inferior performance__eou__	User
TP	no, it wouldn't.  it would be a reduction in information.__eou__	User
TP	a prediction based on a training data set can't ever contain any more information than the amount of information in the training set plus the amount of information in the example.__eou__	User

TP	the 36th var goes from -1.1 to 1.1 every round you also get the reward reward can be super delayed rules unclear 35 vars are roughly between -18 and 12 or something, mostly around -1 to 1 it is very non-linear __eou__	User

TP	But let's say the original random forest made an incorrect estimate for a split on some variable for which there was little data. Then just getting to that variable in some of the trees would be additional information i.e. trees too deep and too few of them sure, but an incorrect decision based on original information by an upstream estimator can be worse than just the original information in a downstream estimator. IE you are recovering information that was lost I'll try to construct a synthetic test for this __eou__	User

TP	@amueller okay thanks! __eou__	User

TP	When using EM, perplexity in LDA should be monotonic, right? __eou__	User

TP	is there ever a case when the requirements.txt is parsed automatically? __eou__	User

TP	 what do you mean? i occasionally run `pip install -r requirements.txt <unconvertable> upgrade`? assuming versions are not pinned, of course __eou__	User

TP	can anyone suggest a good "standard" dataset for evaluating the performance of a clustering/visualization package?  I'm looking for something with a few hundred thousand rows.  (I've tested on mnist, and my machine doesn't have the RAM to handle 1M+ row datasets.) __eou__	User

TP	I don' think there are many standard datasets __eou__	User

TP	This is regarding Scikit-Learn Day, Paris. Will the talks be in french or english? __eou__	User

TP	Hi everyone,  I am new to this room, found it through gitter listing. I am a usual scikit-learn user. Would this be a place to know more about projects using the tool? Thanks! __eou__	User

TP	Hmm not quite, thus room is mainly used for developer chat. __eou__	User

TP	@nelson-liu Hi, I guess you are referring to me? > Hmm not quite, thus room is mainly used for developer chat.  I see... I guess that it is the desired use. A quick scroll-up seems to show a plethora of different interesting topics... :) I will respect the main interest of the founders though. Thanks for the info! I hope there is no issue is I stay as observer meanwhile? __eou__	User
TP	Of course, we definitely talk about other things :)__eou__	Agent
TP	Thanks!! :)__eou__	User

TP	Np! BTW, does anyone have a 32 bit machine (ideally linux) and would be willing to spare a few minutes to pull down some code from one of my branches and run a nosetests? Yeah I mean running a 32 bit OS  I mean haha Ah got it. I have some tests failing on appveyor, only when running on windows 32 bit though.  I'm considering setting up a vm to debug, but I'm also curious if it breaks on Linux 32 bit bc it's a lot easier to get Linux set up and running on a vm vs windows haha __eou__	User

TP	a 32-bit linux machine? __eou__	User

TP	yeah i caught what you meant, its just my time machine is in the shop __eou__	User

TP	hah yeah i don't even try to support windows __eou__	User

TP	@amueller if you can spare a few minutes, review for https://github.com/scikit-learn/scikit-learn/pull/6697 please? ;) __eou__	User

TP	@rvraghav93 a few minutes, right :P thanks for the ping, though. I'll try to take a look. __eou__	User

TP	My hope is you'll get fully nerdsniped in those few minutes ;) And with that PR merged multiple metric is just a few lines of code away B) __eou__	User

TP	Given an estimator `est`, is there a standard way to determine if the estimator has previously been fit? Check if any `*_` attributes are present (e.g. `coef_`)? Hmmm, if there is it doesn't seem to be present on all estimators. __eou__	User

TP	i do believe there is a fitted attribute __eou__	User

TP	maybe this might help? https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/estimator_checks.py#L1035 oops wrong link although that might be helpful i was meaning to send this: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py#L650 is there any estimator you have in mind? or is this one youve built yourself or are you just looking for a more general solution that makes sense, hmm yeah im not too sure how to do that sorry :( __eou__	User

TP	Yeah, that's useful if you know what attributes to check for. Oh well. Thanks for the links. General solution preferably. I'm working with scikit-learn and dask (which does things lazily). It'd be nice to be able to catch not-fit errors at graph build time instead of at execution time. Not necessary though, just nice :) __eou__	User

TP	you can check if there is any attribute that ends in "_". If not it's not fitted. Unless it's stateless, like the HashingVectorizer that does not require any fit to start transforming :P __eou__	User

TP	Hi nice to know this group exists __eou__	User

TP	##hello #hello __eou__	User

TP	lol __eou__	User

TP	Seeing this a little late, @jcrist, you might try performing predict/transform, a `NotFittedError` should be indicative, but other errors may occur uninformatively. __eou__	User

TP	Hi, all. Sorry if this is the wrong place to ask my question - but I'd like to use Naive Bayes, training by a list of sentences with their own labels (i.e. multiple different sentences per label). However, each sentence carries a certain importance weight that I'd like for them to play a role in while classifying - some sentences I want to have more of an impact than others. I was considering just resampling these sentences and creating duplicates or whatever depending on importance, but that seems horribly inefficient (especially because importance values/weights are essentially continuous)  Is there any way to weight certain samples to be more important?  ***TL;DR:** In other words, my training data is of the form ```[label, sentence, weight]```, where I might have multiple different weights and sentences for the same label, of course. I'm not actually sure what the best way might be to go about classifying this using scikit-learn. Any ideas? __eou__	User

TP	Can't you specify the weight as a parameter at fit time? See: http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.fit Not sure if that answered your question __eou__	User

TP	That should do the trick! Thanks __eou__	User

TP	 is there any chinese develper ? I have an translate project ,hope volunteers join in it. here is the repo    https://github.com/lzjqsdd/scikit-learn-doc-cn __eou__	User

TP	Andy, could you close this one - https://github.com/scikit-learn/scikit-learn/issues/5669? @amueller __eou__	User

TP	you summoned me ah done __eou__	User

TP	lol... How's your writing going? :) __eou__	User

TP	What is the best way to save the trained models other than Pickle? __eou__	User

TP	@BastinRobin Quite unfortunately, there isn't an easy path to that in sklearn. @BastinRobin You're looking for a format to store some arrays and some random parameters? What are your constraints? __eou__	User

TP	hello ,everyone __eou__	User

TP	@mikegraham I just want to know if pickle is the best or not for storing __eou__	User

TP	@BastinRobin  It's OK ... imho sklearn is crying out for someone to volunteer to implement a better solution __eou__	User

TP	Where would I have to go if I want to learn about A.Is __eou__	User

TP	Hello everyone I've made a pipeline for a classification problem learning goes fast thanks to parallel with n_jobs=-1 but when i use the predict function, it goes very slow, and only one process is used is there a way to make prediction in parallel? __eou__	User

TP	@BastinRobin pickle is a very problematic solution: it is fragile, not safely transferable, not explicitly specified, etc. It is all that sklearn is geared to and it will be a lot of work to use something else. @BastinRobin For a narrow use case, you can hack something up. There isn't a more general solution @kmehl Probably not with plain sklearn, but you can probably make a specialized evaluator for the slow parts. Have you profiled? Where is the time being spent? __eou__	User

TP	@mikegraham  would making a better solution make a good discrete project? There are quite a lot of people who are keen to work on scikit learn it seems so if it was clearly advertised it might get someone to bite I think a good solution would be very popular __eou__	User

TP	@lesshaste I don't know how welcome it would be -- it would be a lot of work and a huge maintenance burden. __eou__	User

TP	@mikegraham  Ah. I am not 100% clear why it would be a huge maintenance problem. Doesn't that depend on whatever elegant solution someone comes up with? Or to put it another way, if the problem is stated in parts with "part 1) Devise a solution that minimises the maintenance needed" would this not be plausible? has someone done a survey to see what other solutions exist out there? For example in R or weka From a very quick look, the standard solution in R just seems to be saveRDS https://stat.ethz.ch/R-manual/R-devel/library/base/html/readRDS.html  __eou__	User

TP	spark has this https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html weka has https://weka.wikispaces.com/Saving+and+loading+models __eou__	User

TP	@lesshaste  @BastinRobin  @mikegraham you can use joblib.dump (based on pickle with some optimization on numpy arrays) too, see http://scikit-learn.org/stable/modules/model_persistence.html#model-persistence for more details. There were some discussion on the mailing list IIRC about this for example http://thread.gmane.org/gmane.comp.python.scikit-learn/14905/focus=14909. __eou__	User

TP	@lesteve  Thanks. That mailing list thread is somehow slightly negative. I would love to see an objective and technical analysis of the situation. For example, what is wrong with developing the PMML idea? iirc  joblib.dump makes a large number of small files. One simple improvement would be to reduce the number of files to 1 or 2 ah.. I see the other problems are mentioned __eou__	User

TP	> For example, what is wrong with developing the PMML idea? I don't think there is anything wrong per se. It's just that it is quite some work and it probably won't happen inside scikit-learn. Not an expert though. There may have been other discussions on the mailing list on this serialization issues. It does come up from time to time. __eou__	User

TP	OK thanks. I am just a big fan of having clearly stated tasks for keen volunteers to pick from. I feel lots of people want to contribute to scikit learn as it is so great :) @ogrisel  Oh that sounds very interesting. no I did :) I just sent a lot of links to be clear, the interesting part is that an expert (that's you) thinks it might be relevant my knowledge is very shallow in this area __eou__	User

TP	> iirc joblib.dump makes a large number of small files. One simple improvement would be to reduce the number of files to 1 or 2  For the record, joblib master creates only a single pickle file (and not one per numpy array as previously) __eou__	User

TP	PMML is a very verbose XML-based format. The new spark mllib lightweight format  would probably be better much more efficient. __eou__	User

TP	@lesshaste it's from the link you just sent __eou__	User
TP	@ogrisel  the spark link?__eou__	Agent
TP	yes, didn't you read it?__eou__	User

TP	@lesteve  I didn't know  joblib master creates only a single pickle file. Thank you __eou__	User
TP	It's only in the master branch of joblib so far. It will be part of the next release.__eou__	Agent
TP	If someone starts implementing tools to save sklearn models to / load from the spark 2 serialization format, please reference the repo in https://github.com/scikit-learn/scikit-learn/blob/master/doc/related_projects.rst#interoperability-and-framework-enhancements__eou__	Agent
TP	This will require a good python parquet implementation. The most promising implementation should be the official https://github.com/apache/parquet-cpp but the python bindings are not ready yet but progress  is happening, see eg: http://wesmckinney.com/blog/pandas-and-apache-arrow/__eou__	Agent
TP	very interesting__eou__	User

TP	any of you guys work with c++ __eou__	User

TP	Im trying to make an AI using python __eou__	User

TP	@lesshaste To avoid the problems of pickle in full, you need to be explicit. To be explicit, you need to have your data model change every time anything applicable changes. __eou__	User

TP	hello, I am seeing many feature scaling methods, but I cannot find things like RobustScaler and etc on the internet. what are they called in academic society? __eou__	User

TP	and what is difference between scale and standardscaler? is standardscaler just a class implementation of scale? __eou__	User

TP	@keonkim Yes, scale is a function that returns the result, StandardScaler can go in a pipeline or whatever. @keonkim I don't know that RobustScaler has a consistent academic name. To indicate what they did, someone might describe it. I've seen it contrasted with naive scaling by calling it "IQR", but that's not a formal name for the scaling technique, which merely uses the actual IQR to do its job. A lot of people might know what you mean if you just said "IQR scaling" or "Scaled to the IQR" though. __eou__	User

TP	@ogrisel @amueller Could you cancel all my appveyor builds. I forgot to use the CI skip and it seems to block other PRs... __eou__	User

TP	@mikegraham Thanks! __eou__	User

TP	@rvraghav93 what is this ci skip you speak of? __eou__	User

TP	Hey all! I have a question regarding CountVectorizer. Is is possible to 'reverse engineer' the original text from the vectors? I'm wondering as I just wrote an article on our methods, and I want to share our vectorized data, but cannot share if it's possible for people to figure out the original input text based on this (as the original text contains sensitive info). For reference, here is the article: https://medium.com/xeneta/boosting-sales-with-machine-learning-fbcf2e618be3 __eou__	User

TP	haha I saw this on hacker news earlier I dont think its possible to reverse engineer countvectorizer __eou__	User
TP	@nelson-liu Cool__eou__	Agent
TP	Even if you have access to the vectorizer through a file (using joblib)__eou__	Agent
TP	?__eou__	Agent
TP	yup, seems like it__eou__	User

TP	the countvectorizer code is here, https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py it doesnt store the raw documents although its possible ofc to get the vocabulary, so if individual words are sensitive then that might pose a problem. __eou__	User

TP	I see. And if you have the vocabulary, and the vectors, you can kind of recreate keyword-based descriptions. I think I'll rather just share the vectorized descriptions, and not the joblib vectorizer. That'll make it impossible to guess the words, right? Really appreciate your help @nelson-liu :) __eou__	User

TP	Yup that is correct @perborgen __eou__	User
TP	Thanks!__eou__	Agent

TP	What do you mean vectorized descriptions actually? __eou__	User

TP	Sorry for my late reply.  I mean after using the CountVectorizer to turn text into vectors. That's what I refer to aws 'vectorized descriptions' (as my input text is company descriptions). __eou__	User

TP	@mikegraham What's your view on the spark 2 serialization format ? __eou__	User

TP	@nelson-liu If you add `[ci skip]` to a commit message, Appveyor/Travis will skip the tests for that commit... This could be useful if you just push your unfinished work (for a review or to switch computers)... __eou__	User

TP	@lesshaste As in kyro or something else? @lesshaste In any event, I'm probably not fit to weigh in :) __eou__	User

TP	hello, I had a quick question. I wanted to take up the issue https://github.com/scikit-learn/scikit-learn/issues/6867 I could some one please share the link to new gaussian process. So the user guide is http://scikit-learn.org/stable/modules/gaussian_process.html __eou__	User

TP	2nd question (https://github.com/scikit-learn/scikit-learn/issues/6857) I cannot find the file references.rst in  the scikit-learn/doc folder. Where is this file present?. __eou__	User

TP	@krishnakalyan3 someone else who commented on that issue couldn't find it either. And neither could I from a quick look. __eou__	User

TP	@pdurbin  thank anyway :) __eou__	User

TP	@krishnakalyan3 I think @amueller meant that the `GPRegressor` class does not have a reference to that user guide. __eou__	User

TP	@rvraghav93 just created a pull request could you let me know if things are okay? __eou__	User

TP	Thanks for the PR! I'll look into that :) __eou__	User

TP	@amueller Could you share your suggestions about https://github.com/scikit-learn/scikit-learn/pull/6380#issuecomment-185699518 please? __eou__	User

TP	it would be great if one could visualise boosted trees (e.g. xgboost) too __eou__	User

TP	@lesshaste in scikit-learn you can xgboost no idea __eou__	User

TP	if you look at http://www.r-bloggers.com/an-introduction-to-xgboost-r-package/ and scroll down to https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/xgb.plot.multi.png can you do that sort of thing in scikit-learn? @amueller  Sorry I meant that for you oh I must have missed this completely.  Would you mind pointing me to the docs for ensembled tree visualization? I take your xgboost point of course :) I only ever use it through scikit-learn so sometimes forget it isn't part of it __eou__	User

TP	well for scikit-learn trees yes, for xgboost trees obviously not, because that has nothing to do with xgboost? that would go into the xgboost package... err has nothing to do with scikit-learn __eou__	User

TP	well you can plot each tree in the ``estimators_`` __eou__	User

TP	true.. they seem to have a nice trick for plotting one merged tree I am still intrigued if one can derive a single decision tree from an ensemble of trees which is easier to interpret and almost as good as a classifier/regressor this would seem potentially useful __eou__	User

TP	@amueller I think the issue #6857 is solved by #6886. Can you confirm it was what you wanted ? Thx ;) __eou__	User

TP	Hi all. I just wanted to ask, is there anything else I should do in https://github.com/scikit-learn/scikit-learn/pull/6874 or just wait? :) __eou__	User

TP	@yenchenlin @nelson-liu The GSoC midterms are approaching. Hope you guys are ready with your blogs posts? :) __eou__	User

TP	soon :shipit: :) __eou__	User

TP	Good to know!! __eou__	User

TP	@amueller @ogrisel Is #6897 the correct fix for the Circle CI build failure on master? __eou__	User

TP	@raghavrv sorry haven't looked __eou__	User

TP	But my book is now in beta if anyone wants to check it out ;) http://shop.oreilly.com/product/0636920030515.do __eou__	User

TP	just in case you need machine learning engine in java/scala, checkout [smile](https://github.com/haifengl/smile), which includes many algorithms. __eou__	User

TP	hi..sorry if I missed it but does scikit learn support regularized greedy forests? https://arxiv.org/pdf/1109.0887.pdf or is there a PR for it? __eou__	User

TP	@amueller I was looking into this issue https://github.com/scikit-learn/scikit-learn/issues/6120. Could you let me know if the images need to be updated should I just fix the print message?. __eou__	User

TP	@amueller I am looking forward to buying your book! @amueller  that's interesting.  I was going by the usage in kaggle competitions. But I really like your quality control system and support it entirely :) __eou__	User
TP	@lesshaste is it? what is it used for?__eou__	Agent
TP	https://github.com/TimSalimans/HiggsML__eou__	User
TP	oh wait, is that the paper that describes the regularization used in XGBoost?__eou__	Agent
TP	I didn't think so but I could be wrong. there is a distinct piece of software called http://stat.rutgers.edu/home/tzhang/software/rgf/__eou__	User

TP	 __eou__	User

TP	@lesshaste regularized greedy forests has only 18 cites. so no to both ;) __eou__	User

TP	"This is the code for my second place finish in Kaggle's HiggsML challenge. It is a blend of a large number of boosted decision tree ensembles constructed using Regularized Greedy Forest." as an example __eou__	User

TP	@amueller  were you thinking of https://arxiv.org/abs/1603.02754 ? hmm.. https://www.kaggle.com/c/higgs-boson/forums/t/10053/did-anyone-try-rgf-regularized-greedy-forest ... maybe can be ignored now that xgboost rules everything :) __eou__	User

TP	Hi, I am new to scikit-learn , I want to contribute, can i get some guidance. Frankly I am having problems in understanding the easy issues. Sorry for being naive __eou__	User

TP	Hi :smile: , which issues are you solving? or trying to understand __eou__	User

TP	I tried to understand more than one... but I am unable to narrow down the scope of the problem __eou__	User
TP	May you paste the link?__eou__	Agent
TP	https://github.com/scikit-learn/scikit-learn/issues/6796__eou__	User
TP	this is one of them....__eou__	User
TP	Actually I am not fimiliar with the code-base. So I dont know what to ask him__eou__	User
TP	Can you assign me a task, as a mentor__eou__	User

TP	I see, maybe you should leave a comments there and tag the issue opener for more specific elaboration. Doing this can help you make sure you are on the right path! No haha, Im not a pro here. __eou__	User
TP	atleast you can guide me!__eou__	Agent
TP	I am lost in here.... and i want to get some task for myself__eou__	Agent

TP	You can leave a comment about how you gonna solve this, and ask the issue opener whether you are correct.  It looks like to implement a function based on [this script](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) which can conveniently plot the confusion matrix, you can find doc of confusion matrix [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) __eou__	User
TP	Thanks Yen__eou__	Agent

TP	@amueller hey man, glad to see you here. Is your book out already? Got a potential buyer here ;) __eou__	User

TP	@c4ndym4n not andy, but its in beta rn http://shop.oreilly.com/product/0636920030515.do __eou__	User

TP	I have a question after doing a git pull of sklearn repository say I am woking on a bug how do I test my changes on the code? __eou__	User
TP	what do you mean test?__eou__	Agent
TP	like run the unit tests?__eou__	Agent
TP	nope__eou__	User

TP	hi __eou__	User

TP	or run random python code that might use scikit-learn on the version that you have modified __eou__	User
TP	let say i added a print statement in one of the classes__eou__	Agent
TP	that had a bug__eou__	Agent
TP	ah. so you want the print statement to show up when it is run.__eou__	User
TP	and I want to test that class in an ipython notebook__eou__	Agent
TP	yup__eou__	Agent

TP	so im not sure how you have it installed currently, but I uninstalled the pip version i had. then, I installed it again from source with `python setup.py develop` __eou__	User
TP	ah okay__eou__	Agent
TP	then, whenver you change code in a compiled extension (.pyx file, say youre making changes to it or repulling from upstream or switching branches), you have to recompile with `python setup.py build_ext --inplace`__eou__	User
TP	the way you can tell if it is working is if you run__eou__	User
TP	np, let me know if that worked for you__eou__	User

TP	``` In [1]: import sklearn  In [2]: print sklearn.__file__ /Users/nelsonliu/Documents/Github/scikit-learn/sklearn/__init__.pyc ``` that should point to wherever youve cloned the sklearn repo __eou__	User
TP	many thanks__eou__	Agent
TP	:)__eou__	Agent

TP	so every time a git pull is done you recompile __eou__	User

TP	with python setup.py build_ext --inplace ? ok __eou__	User

TP	uh you technically dont have to recompile if none of the .pyx files are changed, but i generally do so anyway because I dont want to bother looking at what was pulled its quick anyway __eou__	User

TP	thanks @nelson-liu  it worked __eou__	User
TP	good to hear__eou__	Agent

TP	I have limited data science experience, and reasonable programming experience. What are the best ways to get started?? __eou__	User

TP	@crimsonsoccer55 what do you want to do? __eou__	User

TP	Hi all :smile: __eou__	User

TP	I just want mention that Im working on a module which ports trained (sklearn) decision tree models to Java and C. Have a look if you are interested: https://github.com/nok/sklearn-decision-tree-porting  __eou__	User

TP	Does anyone know if there are plans to expand the MLP/RBM modules to include more hierarchical learning techniques __eou__	User

TP	@nok  404 error on your link __eou__	User

TP	@alayassir https://github.com/nok/scikit-learn-model-porting (under active development) __eou__	User

TP	@amueller could you please let me know how to proceed with https://github.com/scikit-learn/scikit-learn/issues/6120 __eou__	User

TP	It seems impossible to pass check_estimator for a sparse classifier that does not do multi-class out of the box. Because of [this test](https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/utils/estimator_checks.py#L301) @neale not sure what you mean by hierarchical learning, but in general any framework expansion of the neural network components is out of scope for scikit-learn __eou__	User

TP	@vene why is that out of scope, there is a beta MLP module that works pretty well Giving users the ability to build networks layer by layer should be feasible __eou__	User

TP	@neale I didn't say it's infeasable, it just doesn't fit within the simple API that scikit-learn strives for. There are great libraries that allow modular composition of deep nets, like Lasagne and Keras. Check out the scikit-learn [faq](http://scikit-learn.org/stable/faq.html) for more about this "vision". __eou__	User
TP	@vene I hadn't seen that before, I got it now__eou__	Agent

TP	Currently scikit-learn pipelines and feature unions are extremely simple objects, and steps in a pipeline cannot and do not communicate with each other, but instead are trained independently. __eou__	User

TP	Yeah I always assumed the contributions weren't there, not that scikit actively stayed away from those kinds of models __eou__	User

TP	GSOC admins need to resolve the evaluation situation immediately I'm on irc .. Python Gsoc Admin __eou__	User

TP	@meflin what is the issue? __eou__	User

TP	@jmschrei It seemed that the student evals were overdue @jmschrei (At this point, I think there is a risk of sklearn being blacklisted from GSOC) __eou__	User

TP	@mikegraham I submitted my student eval days ago __eou__	User

TP	okay seems like it has been resolved __eou__	User

TP	everything is resolved and good thanks for all your hard work __eou__	User

TP	Thanks for the update :) __eou__	User

TP	@amueller Would you be interested in doing the honor of giving a -1 and closing this? :P (https://github.com/scikit-learn/scikit-learn/pull/5883) __eou__	User

TP	Any node wrapper for Scikitlearn? __eou__	User

TP	Hi everyone , I am Khanh and newbie in Scikit-learn :D __eou__	User

TP	Hi everyone, I have this: ```rf = RandomForestClassifier(n_estimators = 1000, n_jobs = -1)     clf = Pipeline([('preproc', StandardScaler()),('classifier', rf)])``` fit the classifier with all of the training set     ```data = clf.fit(features, labels)``` where ```features``` & ```labels``` are of size 4 And I save it as a Pickle object. Now, How can I extract those array of features and labels again?? Any idea any one? :( __eou__	User

TP	what is it? What specifically are you saving as a Pickle object __eou__	User

TP	the ```data``` variable. Which is a pipeline class object. <class 'sklearn.pipeline.Pipeline'> __eou__	User

TP	hmm as far as i know there is no way to get the input features and labels from a fitted classifier... __eou__	User

TP	@girisagar46 what would be the point of storing the dataset in the model? __eou__	User

TP	Hello. I am using sklearn to try to predict lux values from a iOS camera data. I have created some training data for multiple models of iPad/iPods, and have it working to some degree, but still get something like 20% error. I am relatively new to sklearn, and am wondering where I can get feedback on my iPython notebook to improve my results. Is this a good place? If so, what is the best way to share a notebook and relevant external files? __eou__	User

TP	@nspaeth without looking at your approach I know that cameras automatically adjust themselves to keep the brightness of an image in a "useful range". I don't think this is a good project for someone starting in machine learning. The only way I see this working is that the algorithm needs to understand about certain camera "artefacts" and then use these to interpret the brightness. I can't see this working short of a deep convolutional network. __eou__	User

TP	hi guys i m having a doubt If i train a classifier and pickle it. How to make it relearn everytime when i introduce a new test example. Is it like everytime i want to predict something i need to retrain with updated training set or will the updating can happen on the go.? @akloster  thank you :) __eou__	User

TP	@BastinRobin In my opinion retraining with every new test example is a bad idea, mainly because it will change the accuracy characteristics of your model over time. Not necessarily always towards the better.  Some Bayesian methods support very natural "updating". Otherwise you can look into "Reinforcement learning" which may be better suited to your task if you need to learn "online". It may also be a good Idea to keep the new examples in a special "out of sample" test set, and evaluate ongoing accuracy on that test set. __eou__	User

TP	Hi  I have a txt file containing words and corresponding word vectors . I want to plot this using tSNE Can anyone point me to good example ? __eou__	User

TP	Hi, I have read a few articles on word embeddings and tsne but am new to these topics. Some of the websites were https://lvdmaaten.github.io/tsne/ http://blog.christianperone.com/2016/01/voynich-manuscript-word-vectors-and-t-sne-visualization-of-some-patterns/ And I came across this just now.. https://www.quora.com/How-do-I-visualise-word2vec-word-vectors/answer/Vered-Shwartz hope it is helpful. __eou__	User

TP	Hey Folks, Id like to understand the implementation of the Random Forests. Is there a specific reason why _parallel_build_trees() is a function and not a method? __eou__	User

TP	Because it is a helper which is called inside the `fit` method. The `n_jobs` param that you set at the initialization of `RandomForestClassifier` controls the number of processes used by `fit`... __eou__	User

TP	The `fit`  uses `joblib`'s `delayed` to run the `_parallel_build_trees` helper in batches to build `n_jobs` trees at a time. __eou__	User

TP	Is there a way to install sklearn to python package namespace under a different name (say `skmaster`) it would be nice to have two versions of scikit-learn installed under different names to compare against master... __eou__	User

TP	@raghavrv You can open 2 different shells and activate 2 different environments in them ? __eou__	User

TP	it'd be useful to be able to run both in the same script for easy comparison / benchmarking __eou__	User

TP	@vighneshbirodkar yes indeed but like @nelson-liu says it would be nice to use it both in a single script... I tried messing around with the setup.py file but looks like there are quite a lot of imports inside sources which do not use relative imports (hence explicitly importing from `sklearn.module`...)... I'd have to change all of that ;( __eou__	User

TP	@ogrisel could you reset travis cache at #5974 please? __eou__	User

TP	Now you can port a learned AdaBoost classifier based on pruned DecisionTree estimators to Java: https://github.com/nok/scikit-learn-model-porting/blob/master/examples/classification/adaboost_predict.py But note that the project is still under active development. :-) __eou__	User

TP	is https://github.com/mblondel/svmlight-loader still the recommended way to read in very large libsvm format files? It is quote old now and scikit-learn has been through many versions in the last 3 years @amueller True __eou__	User

TP	@lesshaste might still be faster __eou__	User

TP	@vighneshbirodkar Are you planning to continue work on the GBCV PR? Or would it be okay if I gave a hand?  (Either by push access to your repo or by cherry-picking the commits?) __eou__	User

TP	I have a general ML classification question.. I hope this isn't the wrong place. I have data which is labelled into A, B, C, D, E and Other. In the end I just want to tell if new data is in A-E or other. That is perform a binary classification. A,B,C, D and E are very different from each other however. Should I build 5 binary classifiers and combine them somehow or just go straight for a binary A-E versus Other classifier? I am using a random forest currently. I could try to build a multiclass classifiers but the class sizes are very imbalanced and I am not sure how well RF copes with that __eou__	User

TP	is there any tool to lint cython? through working on the tree .pyx files, ive noticed there are lots of style discrepancies... hmm is there any method that doesnt rely on pycharm? e.g. pep8 flake8 or a similar standard __eou__	User

TP	@nelson-liu https://www.jetbrains.com/help/pycharm/2016.1/cython-support.html not that I am aware of sadly thanks __eou__	User

TP	@nelson-liu https://udiboy1209.github.io/2016-06-18-cython-needs-a-flake-and-lint-tool/ that's a pretty up to date complaint my pleasure.. any idea about my general classification query above by any chance? not really sure where the best place is for question like that so sorry it is a tiny bit OT for this gitter __eou__	User

TP	haha ok, so seems like one doesnt exist sadly. thanks for the help @lesshaste :) __eou__	User

TP	ive seen general ML questions here so it should (?) be fine? I dont think RF copes very well with unbalanced multiclass data...do you have ample data for each of the classes? __eou__	User

TP	well I have millions of Other records but only thousands of the A-E cases is there anything better suited to the task? do you mean just sampling the same record repeatedly? I never saw the point in that to be honest __eou__	User

TP	hmm have you looked into resampling methods? __eou__	User

TP	hmm i mean you can try taking a random sample of your data, then fit your model on one set and use a held out set to find good probability cutoffs with an ROC curve or something... and you dont have to sample the same record repeatedly, but take a random sample of your other classes to generate a synthetic balanced dataset this answer on crossvalidated looks good, could be a starting point for more reading? http://stats.stackexchange.com/questions/131255/class-imbalance-in-supervised-machine-learning __eou__	User

TP	"Some methods, like random forests, don't need any modifications." I wonder how true that is __eou__	User

TP	yeah, same. please let me know what ends up working best for you? __eou__	User

TP	There's also https://github.com/fmfn/imbalanced-learn, which will soon become part of scikit-learn-contrib __eou__	User

TP	this looks great, thanks for the link @vene ! __eou__	User

TP	 weve reached 7000 PRs / issues :octocat:  not sure if its a cause for celebration, though :) __eou__	User

TP	@vene oh cool! __eou__	User

TP	Another basic question.. I have some data which is mixed categorical and numerical. There are various different ways I could preprocess it before trying to train my classifier. At the moment I just have a python script which I have to edit and rerun. Is there some neat way, using pipelines maybe(?) to make this all more systematic?  I would like to do the equivalent of gridsearch really but with the different ways of preprocessing the data __eou__	User

TP	Well depends what you want to do? You could write your own transformers and that would work __eou__	User

TP	hey sprinters! __eou__	User
TP	:wave:__eou__	Agent

TP	I tagged some issues with "sprint" that might be good entry points: https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+is%3Aissue+label%3ASprint other good tags are "easy" and "needs contributor" You can find the contributor guide here: http://scikit-learn.org/dev/developers/index.html Please start with something very simple, happy to talk about more complicated issues. You can also start reviewing other pull requests, or see if there are pull requests that have been stalled for a long time. __eou__	User

TP	@lesshaste thinking more about it, transformers are probably what you want. there are a bunch of preprocessing tools within the library natively but if youre writing your own scripts, its pretty trivial to wrap them in a transformer and put them into a pipeline __eou__	User
TP	Ok pasted the above message here: https://github.com/scikit-learn/scikit-learn/wiki/Scipy-2016-Sprint-instructions__eou__	Agent

TP	Hey sprinters. Thanks for helping us. Tell us if you need help for something. Have a good sprints. __eou__	User

TP	if anyone needs help on things or setting up dev environment or finding things to start, id be happy to help __eou__	User

TP	Btw, if any issue says "change X", if this is an API change or if it changes behavior of the code, instead of actually changing it, you need to deprecate the old behavior http://scikit-learn.org/dev/developers/contributing.html#deprecation btw, if you find bugs or stuff is not working properly, also feel free to open issues or talk to me or the other developers so if you want to create a new conda environment for the sprint, you should create one with all the dependencies, fork and clone the repo, and then do pip install -e . (while in the scikit-learn folder that you cloned) sure, thanks :) FYI, there are issues that need contributors that don't have any of the tags. the tags are a very rough approximation to the state of the issues __eou__	User

TP	Hi to sprinters from Paris! Have fun! :) __eou__	User
TP	@raghavr are you working today ?__eou__	Agent

TP	I got bored at home and came to the lab. The Internet speed is very good here ;) And I will be glad to help if there is any PR to review. :) __eou__	User

TP	@nelson-liu Hey, I'm a beginner in ML. I was looking for some good first time issues which don't involve completing documention. I went through the list of issues on GitHub but would really appreciate if you or anyone could point to any specific issue for beginners. Thanks. __eou__	User

TP	this PR stalled and might be a good place to pick back up? https://github.com/scikit-learn/scikit-learn/issues/6670 __eou__	User
TP	Sure, I'll ping if I face any  problems.__eou__	Agent

TP	I tried to make sure all the issues that are still available are tagged as "need contributors" __eou__	User

TP	Hi @amueller , I have some time and since there is a scikit learn sprint going on now, I was thinking of contributing remotely from Tuebingen. Is this possible?  I will be glad to work on some documentation stuff. __eou__	User
TP	@btabibian hey. Any contributions are always welcome :)__eou__	Agent
TP	the ones that are tagged sprint are very easy ones. If there is something that strikes your fancy, let me know!__eou__	Agent
TP	(I mean other than the sprint ones)__eou__	Agent
TP	I go for the most easy ones now :D, maybe this: https://github.com/scikit-learn/scikit-learn/issues/6865 ? :)__eou__	User

TP	For anyone joining recently important links and advice is here: https://github.com/scikit-learn/scikit-learn/wiki/Scipy-2016-Sprint-instructions does anyone want to pick this up? https://github.com/scikit-learn/scikit-learn/pull/5551 I'll be in 103 for a bit __eou__	User

TP	If you need a review of your PR just add @tguillemot in the PR and I will have a look __eou__	User

TP	@amueller , I just sent a requirement about the book. thanks! __eou__	User

TP	I am trying to follow the example at http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html but using a random forest. It seems decision_function doesn't exist for the random forest classifier what should I use instead (sorry for the simple question)? it is this line that is causing the problem y_score = classifier.fit(X_train, y_train).decision_function(X_test) ok cancel that..silly me how can you mix calibratedclassifier and onevsrestclassifier? I just want to do classifier = OneVsRestClassifier(RandomForestClassifier()) but with the classifier correctly calibrated apologies if this is slightly OT but.. my really simple xgboost code for the MNIST data set is way slower than ExtraTreesClassifer http://paste.ubuntu.com/19943014/ . What am I doing wrong? __eou__	User

TP	Using KNN in sklearn, is there anyway to have the weights be based on the labels of datapoints? More specifically, I have some unbalanced data and would like to combat this by weighting samples of less common labels higher. I could of course just add copies of the under represented classes but this adds some non determinism if I choose the extra copies randomly so I thought just weighting them would be better.  Is this possible? __eou__	User

TP	Hmm I don't think so. You could just set a random seed for sampling up if you're worried about reproducibility. __eou__	User

TP	yep sure, but should I add a feature request for this? __eou__	User

TP	Hey, Can someone explain what's the difference between [MRG] and [MRG+1] in case of PR's? __eou__	User

TP	@manu-chroma  What does MRG stand for to start with? __eou__	User

TP	MRG is for the PR when it is ready for review, MRG + X is when X number of core developers are okay with PR. In general it is around 2 or 3. __eou__	User
TP	Cool. what does MRG actually stand for?__eou__	Agent
TP	Merge p0ppp__eou__	User
TP	oh :)__eou__	Agent
TP	Sorry.. typos..__eou__	User
TP	Merge is MRG and work in progress as WIP__eou__	User
TP	oh cool__eou__	Agent

TP	@lesshaste https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist __eou__	User

TP	I have just tried for the first time to use xgboost with sklearn and am getting poor results and it is very slow.  I realise xgboost is not core scikit-learn but would any very kind person be able to give a newbie a hand please? my very simple sample script is just running on the MNIST digits data and it is at http://paste.ubuntu.com/20022811/  . What am I doing wrong? __eou__	User

TP	Hello, I am trying to do MNIST task with data from Kaggle and not data in scikitlearn. I am getting accuracies ~90% using LinearSVC, SGDClassifier and KNeighborsClassifier.  But every algorithm just gives a label for a test image as output. That is fine.  I just wanted to have log probabilities of each class. For example if a test image is given- I want an array of length of classes with probabilities that test image is from this class. Is there any way to do that using sklearn? No. I will go through that. __eou__	User

TP	@SnShine  Are you using predict_proba ? __eou__	User

TP	@SnShine As @lesshaste said, this might probably be what you are looking for http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba @lesshaste Even I am a novice and haven't used XGBoost before but maybe the loss function needs to be changed for multiclass classification ? __eou__	User

TP	@maniteja123 Let me try that. There must be at least one xgboost here soon :) __eou__	User

TP	@maniteja123  According to https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py it should use multi:softprob if the number of classes > 2 I think __eou__	User

TP	Yup I suppose you are right, it should set the objective automatically as in https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py#L405.  Sorry I have no idea about the performance then. Just to know, what is the cv score you are getting using XGBoost ? __eou__	User

TP	I get [ 0.9323022 0.93157206 0.92796762 0.93271406 0.93854216] from 5-fold cv of xgboost and it takes a really really long time I get [ 0.96216538 0.96465548 0.96428146 0.96558295 0.9670081 ] from ExtraTreesClassifier and it is really quick I am clearly doing something wrong  __eou__	User

TP	can you reproduce my problem? __eou__	User

TP	Oh thanks for the results. I too will try running the script on my machine and tune various parameters for the XgBoost model. My machine doesn't have a good RAM. But will try to reproduce in VM and get back to you. __eou__	User
TP	thanks! It shouldn't need much RAM__eou__	Agent
TP	sadly the xgboost google group seems pretty dead as does their gitter room__eou__	Agent
TP	xgboost seems to use 1.6GB on my machine for that dataset__eou__	Agent

TP	http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn Have you looked at the ``evals_result`` dict mentioned in the docs __eou__	User
TP	no! Do you think it might help?__eou__	Agent
TP	(got to go out for 30 minutes.. thanks so much for looking at this!)__eou__	Agent

TP	Also this page http://xgboost.readthedocs.io/en/latest//parameter.html has ``eval_metric`` which has ``merror`` for multi class error I should thank you for getting me started to look into this :) I am not sure if it might help but it seemed relevant to multi class but was mentioned it was specific to the objective.``eval_metric [ default according to objective ]``. So do have a look and let us know if you get any insight. Thanks! Oh I see. I don't know any other forum. Maybe stack overflow ? (Sorry for my naive suggestions) __eou__	User

TP	Interesting.. I am currently also confused why it is so slow when everyone says how fast xgboost is! @maniteja123  just changing to objective="multi:softmax" increased the CV scores! for reasons I am 100% unclear about __eou__	User

TP	Hi, sorry for the slow reply. That's great to know. Is the algorithm running faster now ? __eou__	User
TP	@maniteja123  no.. it is still super slow__eou__	Agent
TP	Oh okay. Hopefully someone experienced can provide an explanation for these results.__eou__	User
TP	it would be great__eou__	Agent
TP	Have you tried asking on kaggle ?__eou__	User
TP	yes... I asked on the forum there and have no reply yet__eou__	Agent
TP	I mean I asked on the forum for the digits recognizer challenge__eou__	Agent
TP	my friend also asked on SO__eou__	Agent
TP	no answer :)__eou__	Agent

TP	Okay so I too am waiting for the rra *reply :-) __eou__	User

TP	one problem is that I don't understsand the parameters despite the huge number of "guides" online which make xgboost run slower or faster? Can you find the answer to that? __eou__	User

TP	Maybe having a look at this paper https://arxiv.org/pdf/1603.02754v3 can help  us in understanding the algorithm better ? I too shall try reading it once __eou__	User
TP	it's definitely worth reading__eou__	Agent
TP	@lesshaste if I understand you correctly you are referring to__eou__	User
TP	The ROC for multi class right ?__eou__	User
TP	We need to binarize the output for that to work and plot for each of the classes ?__eou__	User

TP	@lesshaste have you tried with different number of threads ? I saw some issue about multi threading here https://github.com/dmlc/xgboost/issues/523 __eou__	User
TP	hmmm... no not yet but that page does say it is fine in python__eou__	Agent
TP	Yeah sorry just read the whole issue. I just thought it might be a reason.__eou__	User
TP	@lesshaste great. glad that it was helpful!__eou__	User

TP	@maniteja123  I have a much simpler question now :)  I simply want to modify http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html for the digits dataset we are using have you managed to get that to work? @maniteja123  thanks.. that was indeed the  answer __eou__	User

TP	hi.. I am trying to find the most important features in a large random forest and it takes about 1 second per feature and there are 10s of thousands of features. The random forest only took ten minutes to build in total. am I doing something wrong or is this expected? ah.. I think I might realise my mistake! for i in xrange(len(clf.feature_importances_)):     print clf.feature_importances_[i] __eou__	User

TP	does this recompute the whole feature_importances_ array in each step of the for loop? __eou__	User

TP	Hello can anyone help me on this [test-failing](https://github.com/scikit-learn/scikit-learn/pull/6913#issuecomment-234704911) issue? My test pass on 64-bit computer but fail on 32-bit computer, and the occurs at [this line](https://github.com/yenchenlin/scikit-learn/blob/cd-fused-types/sklearn/src/cblas/ATL_srefaxpy.c#L131). I wonder what is the difference to call [ATL_srefaxpy.c](https://github.com/yenchenlin/scikit-learn/blob/cd-fused-types/sklearn/src/cblas/ATL_srefaxpy.c) on 32-bit and 64-bit computer when fitting `np.float32` data? And the link you pointed out seems obviously wrong :smile: __eou__	User

TP	Hi everyone, just a small question. Is the link to ``nose`` package [here](http://scikit-learn.org/stable/developers/advanced_installation.html#testing) the expected one ? @yenchenlin1994 the tests are failing on AppVeyor , so it is Windows environment right ? Is it failing on 32 bit machines working on ubuntu or mac too ? __eou__	User
TP	Good point. I am not sure about ubuntu but not on mac__eou__	Agent
TP	@maniteja123 thanks.__eou__	Agent

TP	Yeah the other day someone was asking about the testing instructions and then I accidentally stumbled upon this. I just thought it should be clarified once ! __eou__	User

TP	And in case you don't have access to one, I can checkout your branch and check on my machine. I am running Ubuntu 14.04 on 32 bit VM machine. __eou__	User

TP	I only try to reproduce it using 32-bit Python but run on an 64-bit mac thanks a lot! It looks good on 32-bit Python with 64-bit mac __eou__	User

TP	Oh okay. I have no other idea about the possibility for the error. Sorry. __eou__	User

TP	Hello @maniteja123 , can you tell me the results on Ubuntu 14.04, 32 bit VM machine? thanks! __eou__	User

TP	hi... has there been any work in incorporating minhash to help cluster large sets of documents? __eou__	User

TP	also.. in this very highly cited paper "Clustering Large Graphs via the Singular Value Decomposition" http://www.cc.gatech.edu/fac/vempala/papers/dfkvv.pdf the main algorithm is to take a random submatrix (suitably chosen) to make the SVD computation feasible. Has this been proposed at all for sklearn? __eou__	User

TP	<unconvertable> , .,... .....   <unconvertable> .,, <unconvertable> __eou__	User

TP	@amueller I spotted you online ;) Could you take a look at https://github.com/scikit-learn/scikit-learn/pull/7071 please? ;) __eou__	User

TP	@raghavrv is it still wip? ;) __eou__	User
TP	I can change the title if it will help :P__eou__	Agent
TP	But no... Just one major thing to do... Switch to threading.__eou__	Agent

TP	Hi guys i'm new to the room i'm working on a subject and i'm looking for someone that have experience with this subject to advice me and hopefully point me to the right direction, my project is as following a document classification i have a big data base with texts for all kind off categories holidays politics sport etc i already managed for now if if i enter a text to my code that it detects which category it belongs using SVM classifier what i want to do next is to predict a category for a costumer but until now i don't know yet how to start this __eou__	User

TP	do you have any training data? you want a recommendation system it seems have you looked up that general term? but it's completely offtopic here __eou__	User

TP	yes i do have a training data yes i wnat to have a recommendation system to explain more my situation  i'm doing in an internship and my company has a data base with tones of messages each text is tagged to a catogery __eou__	User

TP	ok sure.. so the first thing to do is to look up recommendation systems really are you stuck somewhere? __eou__	User

TP	until now what i managed to do is classify this data and when i enter a text as an input my system manages to tell me to which catogery it belongs now my next step is prediction i have to predict what a user wants i looked up some terms like recommendation system but didn't find something really helpfull what i can start with __eou__	User

TP	try http://muricoca.github.io/crab/ __eou__	User

TP	what do you think about pyspark __eou__	User
TP	it's perfectly fine :)__eou__	Agent

TP	im on a linux debian trying to install it there some preinstalls to do java 8 scala etc... __eou__	User

TP	i used scilearn for my code so i thought ill get some from info here but thank you ill stop bothering you guys __eou__	User

TP	guys, what do you think about this post :D http://stats.stackexchange.com/questions/225773/sparsity-as-missing-data-mle __eou__	User

TP	does anyone know anything about the sample_without_replacement  function... it's kinda weird __eou__	User

TP	Has anyone used Amazon Machine Learning... I am sorry if I have asked question in wrong chat room __eou__	User

TP	Is there any problems with travis, appveyor, ... ? The new PR are not tested __eou__	User

TP	@87sanchavan_twitter yeah this is not a good place to ask. Go to stackoverflow @tguillemot I noticed that yesterday. Can you point me towards a PR? __eou__	User

TP	@amueller #6651 #7101 That's strange because I saw that the travis works on it on the activity (on the right) __eou__	User

TP	fuck that was me __eou__	User

TP	@amueller No pb ;) __eou__	User
TP	Need to check with @ogrisel__eou__	Agent
TP	@tguillemot can you try pushing again to one of the PRs?__eou__	Agent
TP	can you try again?__eou__	Agent

TP	@amueller Travis was launched on #6651 but not circle or appveyor Just to know what was the problem ? ok for appveyor and travis __eou__	User

TP	now I only need to find out how access to appveyor and coveralls and circleci was granted __eou__	User
TP	;)__eou__	Agent

TP	and again __eou__	User

TP	hm looks like I took away my ability to restart travis or clear the cache great ah, fixed __eou__	User

TP	@raghavrv you around? @raghavrv Where is the warm start used in the gradient boosting CV? __eou__	User

TP	I don't want to derail this chat so if there's a more appropriate place please redirect me.  I'm working on an ML library which draws a lot of inspiration from sklearn. I'd love to talk with some of the devs on sklearn who may be able to share some insights into things they'd like to be able to change/do differently. Or things that they think have worked really well that I might not appreciate. __eou__	User

TP	@raghavrv you around now? @AtheMathmo which language? __eou__	User

TP	@AtheMathmo have you read the API paper? And there are talks by me and by Gael about the api design __eou__	User

TP	@amueller It's written in rust. Happy to share here if you want. I also gave a talk on it yesterday that I would be happy to share.  I haven't read the API paper or seen the talks. Would you be able to provide me links to those? Thanks! __eou__	User

TP	@AtheMathmo heres the paper: https://arxiv.org/abs/1309.0238 __eou__	User

TP	@nelson-liu thanks! __eou__	User

TP	I really like how the transformer interface is described. It's a problem I haven't found a nice way to tackle yet in my own work. __eou__	User

TP	@amueller Sorry I totally missed the chat... :( BTW the warm start is being set at [this line](https://github.com/scikit-learn/scikit-learn/pull/7071/files#diff-439a21b751083bf0e4a535e8f9075520R794) __eou__	User

TP	yeah found it finally ;) __eou__	User

TP	Does "LGTM" mean +1, or it's kind of uncertain +1? Never seen this in other projects. __eou__	User

TP	Generally LGTM from a core contributor means +1 __eou__	User
TP	Thanks!__eou__	Agent
TP	You can then modify your PR title__eou__	User

TP	is there anyway to run gridsearchcv and save _all_ all of the models you train to disk? or is it only possible to get the `best_estimator_` and then just save that __eou__	User

TP	That would be extremely space costly for some models. For instance knn basically stores your training data. If you decide to store all the models that would explode the memory. This is why it is not implemented. Do you have any specific use case where you need this? Also the `results_`give you more statistics than before. Finally you could always loop through all the parameters and retrain the models if you need. __eou__	User

TP	can anyone pleaes tell me where should I start for data science __eou__	User

TP	Quora is your friend :)  https://www.quora.com/How-can-I-become-a-data-scientist-1 Its better to ask these kind of questions on quora (and youll get more responses), this place is more specific for scikit-learn development __eou__	User

TP	@yenchenlin Thanks for this But I have concern with language whether I should go with python or R __eou__	User
TP	I personally dont have much experience with R, so I cant answer this question, sorry.__eou__	Agent

TP	@yenchenlin Currently I am working with python  and I am new to machine learning and for that I was asking __eou__	User

TP	correct choice. __eou__	User

TP	@leem2714 it depends a lot on what you want to do. Are you more interested in prediction or inference? What kind of methods would you like to use? __eou__	User

TP	hello guysss i m new here and what t learn data science can anyone suggest good stuff for learning data science __eou__	User

TP	 @phalodi You will probably get more info on Quora or even just doing a search on Google.  This area is more suited to specific scikit-learn development. __eou__	User

TP	@amueller @ogrisel coveralls is still down, can you reset it ? __eou__	User

TP	hey guys, i'm new here. i just want to confirm if anyone has successfully installed cuda in ubuntu 16.04 for theano gpu usage? __eou__	User

TP	Hey, I'm not able to locate the ``load_breast_cancer`` in http://scikit-learn.org/dev/modules/classes.html#module-sklearn.datasets. The only reference I can find in docs is here http://scikit-learn.org/dev/datasets/index.html#breast-cancer-wisconsin-diagnostic-database There is no reference to it's method details. Why is so ? __eou__	User

TP	@amueller Andy are you around and have a few minutes to talk about the gbcv? __eou__	User

TP	@raghavrv sorry not today have you talked with @pprett ? __eou__	User

TP	Np. I'll try sending him an e-mail and let you know... __eou__	User

TP	Hello, is there an easy way to get leaf node count from decision tree? I can get total node count, but is there a way to get also leaf node count? Ok, thanks. I'll do that __eou__	User

TP	@mkoske first get `tree._tree.left_children`and right_children I think it's called. Then count the number of elements where the value is - 1, those are leaves __eou__	User

TP	``` In [1]: from sklearn.datasets import load_boston In [2]: from sklearn.tree import DecisionTreeRegressor In [3]: boston = load_boston() In [4]: tree = DecisionTreeRegressor() In [5]: tree.fit(boston.data, boston.target) In [6]: internal_tree = tree.tree_ In [7]: left_children = internal_tree.children_left In [8]: right_children = internal_tree.children_right In [9]: leaf_count = 0 In [10]: for i in left_children:     ...:     if i == -1:     ...:         leaf_count += 1     ...: In [11]: for i in right_children:     ...:     if i == -1:     ...:         leaf_count += 1     ...: In [12]: leaf_count Out[12]: 940 ``` __eou__	User

TP	oops, sorry you only have to go through it once..... going through it twice is double counting __eou__	User

TP	since left_children and right_children are -1 if they dont have a left or right child, (thus leaf) and a node cant have only a left or a right child. __eou__	User

TP	Does anyone know a good reference for how the verbose arguement acts? __eou__	User

TP	how does decision tree find the cut point? __eou__	User

TP	haha youre getting yourself into a pretty deep rabbit hole here ;) do you mean decision trees in general or specifically scikit-learns implementation __eou__	User
TP	scikit-learn's implementation__eou__	Agent
TP	ill pm you, its a bit convoluted and i dont want to spam the channel. is that ok?__eou__	User
TP	yes, it's ok__eou__	Agent

TP	Hi, is  `skl.svm.LinearSVC` handling unknown/NaN values? I couldn't find that information in the class documentation page __eou__	User

TP	@nirizr You need to impute those missing values (using `sklearn.preprocessing.Imputer`) first before passing on to the `LinearSVC`. __eou__	User

TP	@raghavrv I got you! Can you give #7187 another quick review? __eou__	User

TP	@raghavrv Thanks a lot! Are there easy ways to get more advanced strategies? regression, nearest neighbors? __eou__	User

TP	@yenchenlin Thanks for the ping. Done... Some minor comments and +1 :) __eou__	User

TP	@ogrisel you around? ;) __eou__	User

TP	hey everyone, i have a _curiosity_ for you! ``` def pipeline_to_weights_and_bias(clf):    <unconvertable> """ Turns a SKLearn StandardScaler() --> LogisticRegression()  <unconvertable> <unconvertable> pipeline into single weights and bias. """  <unconvertable> <unconvertable> assert set(clf.named_steps.keys()) == {'logisticregression', 'standardscaler'}  <unconvertable> <unconvertable> lr = clf.named_steps['logisticregression']  <unconvertable> <unconvertable> sc = clf.named_steps['standardscaler']  <unconvertable> <unconvertable> W = (lr.coef_ / sc.scale_)  <unconvertable> <unconvertable> B = lr.intercept_ - (lr.coef_ / sc.scale_).dot(sc.mean_.T)  <unconvertable> <unconvertable> return (W, B) ``` for those doing logistic regression, this turns a StandardScaler + LR pipeline into a single weight+bias matrix you use it like this: ``` X = np.random.randn(10000, 512) W,B = pipeline_to_weights_and_bias(clf) assert np.allclose( clf.decision_function(X), W.dot(X.T) + B )  %timeit clf.decision_function(X) #=> 10 loops, best of 3: 122 ms per loop  %timeit W.dot(X.T) + B #=> The slowest run took 7.54 times longer than the fastest. This could mean that an intermediate result is being cached. #=> 1000 loops, best of 3: 265 us per loop ``` the speedup is significant __eou__	User

TP	the real funny part is that the resulting calculation runs only on a single core, even though it's hundreds of times faster than sklearn vanilla pipeline now, i have 1800 of these classifiers to run. it's much faster to do a (1800,512) * (512, N) matrix operation than to call 1800 pipelines in sequence this makes me wonder if sklearn couldn't benefit from some pipeline optimization tricks? i know i'd love like a `FastAssortmentOfScaledLogisticRegressionClassifiers` class but of course there could be other possible ways to hand-tune other combinations of linear transformations (e.g. PCA and random projections are the same thing) __eou__	User

TP	hmm, thats pretty interesting __eou__	User

TP	for that question, I suggest you raise an issue; not everyone is on / checks gitter :) __eou__	User

TP	I want to build a recommendation system for movies. What are all things I should learn. I am presently doing undergraduate course with basics in python, web dev and java. __eou__	User

TP	I had completed ML course by Andrew ng and ML through case study by Carlos on coursera  __eou__	User

TP	just out of curiousity, am I supposed to pour the whole dataset to `gridsearchcv.fit`? my dataset has roughly 15M (* 500 features) in total, and I am testing with just 2M of them, I wanted to throw them all into `.fit` but kept getting out-of-memory warning (obviously) __eou__	User

TP	Can you pass .5mil at a time 4 times? I mean if you are being limited by the memory.. probably make data set smaller? :) @shivakrishna9 depends on how you are trying to relate those 2. __eou__	User

TP	@Jeffrey04 a common thing people do to get around this is to just get more ram ;) might be worth looking into using an AWS machine or something for a little bit. Passing .5mil 4 times isnt really theoretically sound, because then its difficult to discern which model is actually the best because the results might be affected by the fact that the model only sees part of the dataset. __eou__	User

TP	@nelson-liu ahh i see, but is a model with .5mil better then a model with .5mil * 4 model? There might be a chance for certain training set, the second is better? I mean, i think i see what you mean (error from each .5mil being fed adds up (maybe compounds)). __eou__	User

TP	@Jeffrey04 It might be worth seeing how diverse is your data, perhaps you can neatly drop a certain percentage of it without impacting the model too much. __eou__	User

TP	sure, but the point of using GridSearchCV is to pick the best model for an unseen test set, is it not? <unconvertable> more data is better <unconvertable> is a common adage that is generally true. but lets say you have 1.5 million mislabeled samples (for some reason); if you were to  luckily select just the .5 million samples that were clean and train on them, youd do well. By selecting a subset of the data, youre inherently biasing the model a bit. training on 4 partitions of a set and picking the one that does best on the most out of 4 != training on the whole set and picking the best one __eou__	User
TP	But that assumes there is an easy way to sort through the 2M to narrow it down to .5m. in which case, what the 2 of you recommended sounds good.__eou__	Agent
TP	I didn't mean have 4 separate trained instance.. hmm, but it any case, it's not possibly to feed .5 at a time , does it's stuff, frees some memory and you add to it? is what i was trying to ask.__eou__	Agent
TP	ah sorry i misunderstood then. yeah, thats called <unconvertable> warm start. some models in scikit-learn implement it, but Im not sure if its gridsearchcv compatible...__eou__	User
TP	Ahh, i am very new to all of this so questions make more sense in my head than when i type. ^^__eou__	Agent
TP	yeah, doesnt seem like gridsearchcv can use warm start__eou__	User
TP	well warm start is only implemented for models where it makes theoretical sense to do it. like in SGD Estimators, warm start lets you start at a previous solution instead of randomly initializing.__eou__	User
TP	Ahh..__eou__	Agent
TP	hmm I dont think so @nirizr but im not 100% sure__eou__	User

TP	Is there any other keyword i can search for? "warm start scikit" isn't giving me anything that explains concepts. :s __eou__	User

TP	Can gridsearchcv implement partial_fit for estimators that support it? __eou__	User

TP	It's definitely not possible now. I'm not familiar with sklearn's code at all, but looks like a call to `fit` may be replaced with a call to `partial_fit`, when it is supported. __eou__	User

TP	Thats good to know! partial_fit is something like a warm start then? This is a bit offtopic but : http://webhostingegg.com/vps/top-alternative-amazon-aws-ec2/ Wondering if there were non aws recommendation. __eou__	User

TP	`partial_fit` feeds the estimator with partial data iteratively, having it only process portions of all available data at a time. There are some considerations there (how to split the partitions, sizes, the order in which data is fed) and sklearn doesn't support that for gridsearchcv as far as I can tell. __eou__	User
TP	Ahh! thank you kindly for the detailed explanations!__eou__	Agent

TP	@nelson-liu Hello! You gave me earlier (few days ago) short code snippet to count leaf nodes. But It seems not to work and I don't know why. The value for `leaf_count` variable seems to be even one more than `tree.node_count`. So, my tree has ~2300 leaf nodes :) That's large tree __eou__	User

TP	i mentioned after the snippet to remove one of the loops so if you take out one of the for loops, that should do the trick __eou__	User

TP	``` In [1]: from sklearn.datasets import load_boston In [2]: from sklearn.tree import DecisionTreeRegressor In [3]: boston = load_boston() In [4]: tree = DecisionTreeRegressor() In [5]: tree.fit(boston.data, boston.target) In [6]: internal_tree = tree.tree_ In [7]: left_children = internal_tree.children_left In [8]: right_children = internal_tree.children_right In [9]: leaf_count = 0 In [10]: for i in left_children:     ...:     if i == -1:     ...:         leaf_count += 1     ...: ``` because left_children is an array that maps each node to another node that is its left child but if it has no left child, its marked as -1 a node cannot have a left child and no right child or no right child and a left chlid, so iterating through one of the children arrays is enough __eou__	User
TP	Ok, thanks :)__eou__	Agent

TP	http://scikit-learn.org/stable/auto_examples/decomposition/plot_image_denoising.html  Didn't realize you can do crazy stuff like this.. :o __eou__	User

TP	@ItchyJunk OOOoo? gridsearchcv allows multiple calls to `.fit`? __eou__	User

TP	OOOOOOOOH, yea, I am using sgdclassifier, lemme try (: > It's definitely not possible now. I'm not familiar with sklearn's code at all, but looks like a call to `fit` may be replaced with a call to `partial_fit`, when it is supported. __eou__	User

TP	well after a quick test i am able to do multiple `.fit` calls, but not sure whether the classifier uses all the training set tho i misread for some reason *facepalm* argh... markdown __eou__	User

TP	I really need to get some sleep, i misread and thought this is already implemented #facepalm > It's definitely not possible now. I'm not familiar with sklearn's code at all, but looks like a call to `fit` may be replaced with a call to `partial_fit`, when it is supported. __eou__	User

TP	@Jeffrey04 multiple .fit is not the same as calling 1 .fir for the 2M data apparent.y __eou__	User
TP	yea, I realized that__eou__	Agent
TP	^.^__eou__	User
TP	Oh, the general agreement here was you should have a way to narrow you 2 mil down to .5 mil better training data__eou__	User
TP	But i was curious about the memory management itself .. so partial_fit and warm start was mentioned for some cases..__eou__	User
TP	So is there a way for you to narrow it down or was that not the case? :P__eou__	User

TP	@ItchyJunk that's what I did previously, i was wondering if that's the proper way to do it if i fetch fair enough random sample for gridsearchcv, I probably should get parameters that should be quite close to the "ideal" ones then i can proceed and retrain the proper model with the not-so-ideal parameters ^ my previous workaround __eou__	User

TP	also I am trying to break into multiple smaller classifiers, so each classifier should be built with much smaller dataset so whenever i try to classify some data, i would run through all of them, and pick the best result with highest probability or something __eou__	User

TP	Yeah I suppose if the difference is small enough, approximation should be fine. __eou__	User

TP	Quick question:  After having downloaded scikit-learn on my machine, How can I compile the modified code package to iPython (ie import sklearn2 as sk ?) . Where should I put the repository scikit-learn so Python can recognize it ? __eou__	User

TP	@Jeffrey04 I do think it is quite easy to add, as far as I can understand sklearn's code. __eou__	User

TP	@arita37 Not sure what you are asking? are you asking how to import the scikit in your python code? In which case, you just import it like any module. I might be misunderstanding though. __eou__	User

TP	Hey. Is it me or the current python version does not have neural networks ? I followed that : http://scikit-learn.org/dev/modules/neural_networks_supervised.html and got an import error with "from sklearn.neural_network import MLPClassifier" Ok thanks a lo t __eou__	User

TP	neural nets are in 0.18, not 0.17.1 (which is the stable version) theres a release coming soon, but you can get it in the meanwhle by downloading and installing the master branch __eou__	User

TP	Does anyone know how the images are generated in the "example gallery" documentation?Im trying to contribute an example but the plot legend and title are cut off on the html page in my local build. __eou__	User

TP	@mlliou112  I don't think dropout has been implemented yet which afaik is crucial for getting good out of sample prediction performance I once saw a PR for it I think but I am not sure what happened __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/issues/6175 and https://github.com/glennq/scikit-learn/tree/mlp_dropout_new . Maybe these are stalled? on a similar note... has https://github.com/scikit-learn/scikit-learn/pull/4899 stalled? I am hugely looking forward it. __eou__	User

TP	http://www.telegraph.co.uk/technology/2016/01/28/first-driverless-buses-travel-public-roads-in-the-netherlands/ __eou__	User

TP	@ItchyJunk  yes but what's the point :) Bus drivers take a lot of passengers so I can't believe it saves much money __eou__	User

TP	@_@ what the point of automating vehicles? __eou__	User

TP	if i am getting mostly score=0.2 in my gridsearchcv for sgdclassifier, what can I do to increase the score ```    classifier = GridSearchCV(SGDClassifier(),                               {                                   'loss': ['hinge', 'modified_huber', 'log'],                                   'penalty': ['elasticnet', 'l2', 'l1'],                                   'alpha': [0.000004,                                             0.0000045,                                             0.000005,                                             0.0000055,                                             0.000006],                                   'warm_start': [True],                                   'l1_ratio': [0.12, 0.13, 0.14, 0.15]                               },                               iid=False,                               n_jobs=8,                               verbose=10,                               scoring='f1_weighted',                               refit=False) ``` __eou__	User

TP	http://scikit-learn.org/stable/modules/grid_search.html#gridsearch-scoring __eou__	User

TP	https://plus.google.com/+VincentVanhoucke/posts/8T7DSJhGY3u __eou__	User

TP	did in room speak about opencv??? i want to make real-time  object tracking with open-cv in python and i want to sent cordinate of object to tracking module??? who can help me. Thank a lot .;-) __eou__	User

TP	 __eou__	User

TP	got it. thanks! i understand markov chain mechanics, but was wondering if there are other better alternatives plus you mentioned tensorflow using syntaxnet, i think that is awesome __eou__	User

TP	@kennetham ahh there are different algos each with their ups and downs i think. __eou__	User

TP	@ItchyJunk is it possible to use for example NLTK to do NLP parsing, then I use tf-idf and Bag of words to classify the tokens, and possibly attempt to make a prediction? does that even work? __eou__	User

TP	Hello everyone! Is anyone enrolling in Hinton's ML Coursera course https://www.coursera.org/learn/neural-networks ? __eou__	User

TP	Hi, how do you guys do dl experiments (in terms of infrastructure). Will Amazon ec2 instances suffice for research purposes? Thanks for your help. __eou__	User

TP	@kennetham I know very little about language processing, but in theory, it sounds like it would work? pretty sure  you'll find out why it doesn't when you code it out :P __eou__	User

TP	@vyraun it realls depends on the scale of the analysis that you're doing. I'm a neuroscientist who works mostly with fMRI data, which can be pretty big, so our labs tend to have dedicated computers with many cores and a good deal of RAM, but I could certainly replicate that environment on ec2 if I wanted to pay as I went __eou__	User

TP	@dankessler thanks. __eou__	User

TP	Hi guys. I'd like to learn how to use machine learn as a black box. Do you recommend a book or course? Thanks __eou__	User

TP	Not sure what you mean by "black box" here. https://gist.github.com/off99555/b6190df237562aa6e8c922c485dc7ad0 Here is some machine learning resources in general, if that helps. Sorry for that link expanding now sure how to make it not do that __eou__	User

TP	Hello @amueller @nelson-liu @GaelVaroquaux I had a look at #7319. Well I agree, nose has some deprecated code now Shifting to pytest can be a really good choice. I recently completed GSoC 16 with _TARDIS_, I am staying there as a regular contributor, although I always wanted to get involved in an ML centric community. Throughout my project I have worked a lot with pytest, and read a lot of its docs - I can take up this issue to solve on a regular basis. While I have used a little of scikit, I am planning to get comfortable with the codebase for sometime, and then move ahead with a series of PRs. Would you accept my PRs a few weeks down the line ? ( Because I see you are choosing it for next GSoC ) __eou__	User

TP	hi @karandesai-96, thanks for inquiring! Id suggest you copy and paste your message onto the issue, since not everyone checks gitter. that would help it get more visibility + there might be nuances in the switch to consider that would require input from other developers, so best to see if you can get a solid green light there first __eou__	User
TP	Great, done. I'll be happy to take it up if :traffic_light: :checkered_flag:__eou__	Agent

TP	+1 on what @nelson-liu said __eou__	User
TP	@nelson-liu very well :D__eou__	Agent

TP	great, id also be happy to pitch in :) i dont know too much about py.tests and this seems like a good opportunity to learn __eou__	User

TP	Thanks @ItchyJunk __eou__	User

TP	just out of curiousity, for `RadiusNeighborsClassifier` I am trying multiple `.fit()` calls,  and the code didnt throw any exceptions, can I assume it is possible to fit my whole collection into it with multiple `.fit()` calls? __eou__	User

TP	Probably no? __eou__	User

TP	oh? __eou__	User

TP	so subsequent `.fit()` calls overwrite the previous call? ok, apparently that's the case S: __eou__	User

TP	@Jeffrey04 Yes. You're correct. each `fit` call will overwrite previous calls. Some estimators have `partial_fit` for what you're looking for, which isn't easily achievable for each classifier. __eou__	User

TP	Nice, I guessed correctly = ) __eou__	User

TP	@vyraun i suggest you use AWS EMR directly, which supports hadoop and spark 1.x. you can quickly try it w/o spending hours on setting up the cluster. who is taking the Udacity "Intro to ML" class here? __eou__	User

TP	I'm takin Stanford (coursera) and MIT (OCW) ML classes __eou__	User
TP	@tjgerot you are awesome. I took a few Stanford lessons and feel difficult.__eou__	Agent

TP	Haha I usually have to watch them 2-3 times over. They start off head first in the math and I haven't taken calc yet so it's been pretty slow learning. But MIT posts a lot of their courses on YouTube, so I'm going through the Artificial Intelligence course, much easier. How's the Udemy one? __eou__	User
TP	Udacity's "intro ML" is good. the tools (sklearn, ntlk) they use is more recent, not R, octave, numpy. see results quick :)__eou__	Agent

TP	 __eou__	User

TP	@txie thanks for the suggestion. Will try <unconvertable> __eou__	User

TP	 __eou__	User

TP	Are there plans for a codesprint at #ODSC this year? __eou__	User

TP	@txie I am just about completed with the Udacity ML Nanodegree. __eou__	User

TP	Guys from scikit-learn team, could someone check and add +1 to this https://github.com/scikit-learn/scikit-learn/pull/6116 ? __eou__	User

TP	hello, i am begin learn deep learning i am reading http://www.deeplearningbook.org/ __eou__	User

TP	Does scikit-learn already have or intend to have metric-learning algorithms included? __eou__	User

TP	Hi  guys. Id to `train_test_split` but on `stratify` I need of a multi-label data. Any idea how can I do, it? __eou__	User

TP	On my Pandas a had 4 columns. `[X, y, area, stratify]`. On `stratify` I populated with a string concatenation of `area` and `y` values. But this hack does not working. `train_test_split(df[X], df[y], test_size=0.3, stratify=df[stratify])`I got the error: `ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of labels for any class cannot be less than 2.` __eou__	User

TP	Why is there exit() in the middle of the code at http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html ? the code has nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf) exit() print("done in %0.3fs." % (time() - t0)) __eou__	User

TP	idk maybe NMF does something that requires you to exit() to go back into your code? __eou__	User
TP	@ItchyJunk  that would be surprising__eou__	Agent
TP	exit is a synonym for quit I believe__eou__	Agent
TP	surely it's a typo?__eou__	Agent
TP	ok, it quits out of what ever NMF does to get back to your code__eou__	User
TP	you don't think it's a bug?__eou__	Agent
TP	it might be..__eou__	User

TP	@lesshaste the exit() is not in the dev doc so it has been fixed in master. See http://scikit-learn.org/dev/auto_examples/applications/topics_extraction_with_nmf_lda.html. __eou__	User

TP	@amueller , is there any interest in merging the Metric Learning algorithm implemented [#4789](https://github.com/scikit-learn/scikit-learn/pull/4789) ? And whether any Metric Learning algorithms are in the pipeline? __eou__	User

TP	@bhargavvader Yes there is interest. but currently we are mostly working on a release only the things in PRs are in the pipeline AFAIK @raghavrv can you maybe work at making the new grid-search docs render correctly? __eou__	User

TP	@amueller , cool, I'll start by giving it a shot. It can be reviewed after the release is done and you guys get some time :) __eou__	User

TP	Hey is anyone coming for PyCon Rennes from here? __eou__	User

TP	only india ;) __eou__	User

TP	Hi guys, I recently re-installed the dev version of sklearn, and I get an mkl error   ```  from .murmurhash import murmurhash3_32 .Intel MKL FATAL ERROR: Cannot load libmkl_avx.so or libmkl_def.so. ``` If I don't first call a sklearn function (e.g. call `linear_model.Ridge()` first prevents the error, but calling `linear_model.LogisticRegression()` doesn't) Do you have an idea where I should look to track this error down? __eou__	User

TP	update conda? __eou__	User

TP	@amueller yep looking forward to your talk there! __eou__	User

TP	Hi guys,  I meet a MemoryError problem when using minibatch k-means for clustering a data set with 1,700,000 rows and 5 cols. My desktop has 8G RAM and the scikit-learn version is 0.17.1. I searched google and found a same issue fixed by updating sklearn (https://github.com/scikit-learn/scikit-learn/issues/3048). Due to the algorithm of minibatch kmeans, I think 8G RAM should be enough. Anyone has ideas about this situation? Thanks! __eou__	User

TP	> @raghavrv can you maybe work at making the new grid-search docs render correctly?  @amueller Sorry I missed the chat. Where does it not render correctly? __eou__	User

TP	@raghavrv there are a bunch of errors when generating the sphinx doc @CasiaFan have you tried ``init="random"`` __eou__	User

TP	@amueller  dude tell me what are some cool things i can do with sci-kit learn and i promise i'll do them. __eou__	User

TP	I tried it. Still running out all memory @amueller __eou__	User

TP	Sorry, I check the script again and find this error is caused by plotting cluster patterns after clustering using matplotlib. Thanks anyway! @amueller __eou__	User

TP	hi i have a question __eou__	User

TP	So... you can ask :) __eou__	User

TP	@amueller I will push the 0.18rc tag soon __eou__	User

TP	@ogrisel thanks :) __eou__	User

TP	If i am python programmer and no knowledge about Ml but i want to go in ML so how can i start with python ? __eou__	User

TP	you can follow the tutorials on http://scikit-learn.org then read a book such as @amueller's and work the examples, then try a kaggle.com challenge then follow a class online. basically alternate between theory and practice to get the theory you need basic linear algebra and stats / proba also AI is wider than ML but ML is a very very rich subfield of AI what I said:  linear algebra and stats / proba + basic differential multivariate calculus (what is a continuous function, a differentiable function, a gradient...) @amueller I pushed the 0.18rc tag. I also triggered the wheel builder. I tried with Python 3.6.0b1 and we have broken tests because of int / str comparisons: need to look in details __eou__	User
TP	i'll try to trigger the conda builder__eou__	Agent
TP	after the 0.18rc__eou__	User
TP	all numpy tests pass on Python 3.6.0b1, 8 errors for scipy.__eou__	User

TP	I know AI theory and reading Modern approach but i am lacking to understand equations and expression in ML , for understanding those euations and expression what should i learn first?? __eou__	User
TP	then you don't know theory ;)__eou__	Agent
TP	Ok for understand expressiona and equations what should i learn?__eou__	User
TP	And will i able to understand regression funtion after reading linear algenra ?__eou__	User
TP	I don't have any particular recommendation in mind but if you ask that question to google you will probably get answers__eou__	Agent

TP	ok i see you mentioned Linear algebra ! Any good tutorial or blog or book for linear algebra ? __eou__	User

TP	regression is a stats / machine learning concept, not a linear algebra concept. But generally it is presented in terms of vector space with a euclidean metric so you need to know about vector spaces and norms and distances first __eou__	User
TP	which math course would you recommend for understanding regression (vector..etc)__eou__	Agent

TP	to apply ML you don't necessarily need to understand the underlying math in details though. It's good to start with a bit of ml practice (e.g. sklearn tutorials) then learn a bit about the underlying maths and then iterate __eou__	User
TP	I am working with tensor flow api__eou__	Agent
TP	but i want to work with SNN__eou__	Agent
TP	learning linear algebra + stats + proba is typically 1 or 2 semesters of BSc in a math related cursus. So learning on your own without application to ML will probably be too dry.__eou__	User
TP	alternate with practice to keep with the motivation.__eou__	User
TP	then go an read a tensorflow tutorial instead__eou__	User
TP	you might be interested in this : https://www.youtube.com/watch?v=cKxRvEZd3Mw__eou__	User
TP	you did not take any linear algebra class?__eou__	User

TP	I am also undergraduate student and this is last year of my graduation ! Ok , thanks :) Yet now but i will self learn no prob. __eou__	User

TP	sweet! __eou__	User

TP	@ogrisel err did you merge #7414 ? __eou__	User

TP	shouldn't the version be 0.18rc1? __eou__	User

TP	no I did not merge #7414, let me review it quickly, we can merge it and then tag 0.18rc1 will be after 0.18rc and nobody will ever know ;) __eou__	User
TP	;)__eou__	Agent

TP	actually I am not sure about updating the website. I think we should wait for 0.18 final to update the nav BTW I think 0.18rc is equivalent to 0.18rc0 __eou__	User
TP	ok__eou__	Agent

TP	it's not entirely impossible that we do manual builts of the stable website, which would be silly also https://circleci.com/gh/scikit-learn/scikit-learn-feedstock __eou__	User

TP	ugh test failures in the pickle test __eou__	User

TP	this is looking good: https://travis-ci.org/MacPython/scikit-learn-wheels / https://ci.appveyor.com/project/sklearn-wheels/scikit-learn-wheels (the Python 3.3 failure is known but we don't care for the RC) __eou__	User
TP	on python three__eou__	Agent
TP	travis is failing__eou__	Agent
TP	like our normal travis https://travis-ci.org/scikit-learn/scikit-learn/builds/159677032__eou__	Agent
TP	what...__eou__	User
TP	unicode stuff?__eou__	Agent
TP	that's the built in the 0.18.X branch?__eou__	Agent

TP	turns out that running a "replace" on a pickled string is not a good idea who'd thought? gimme a sec @ogrisel https://github.com/scikit-learn/scikit-learn/pull/7415 let's wait for CI, then backport __eou__	User

TP	yeah __eou__	User

TP	travis is slow... __eou__	User

TP	hm I'm having trouble with the feedstock https://circleci.com/gh/scikit-learn/scikit-learn-feedstock/4 __eou__	User
TP	Maybe @ jakirkham has an idea.__eou__	Agent
TP	hum that did not work__eou__	Agent
TP	how are the wheels?__eou__	User
TP	well, it's running__eou__	User
TP	I mean "they haven't failed yet"__eou__	User
TP	hehe alright__eou__	User
TP	hm I messed up the channel config somewhere__eou__	User
TP	thanks @ogrisel and good night :)__eou__	User
TP	hi @canadadeer_twitter__eou__	User

TP	I opened an issue here: https://github.com/conda-forge/conda-smithy/issues/304 I tried setting conda-forge as source channel not sure if that does anything it'll take like half an hour to complete anyhow :-/ __eou__	User

TP	travis must be completely overloaded by the californians... the wheels tests break because of the pickle issue under Python 3 I cancelled them to wait for #7415 The Python 2.7 wheels did work :) ... I going AFK, see you tomorrow. Good luck with conda-forge. __eou__	User

TP	@amueller I merged, backported and pushed 0.18rc1 the wheel builder is triggered __eou__	User

TP	@amueller the wheels are on their way. How are you doing with the conda-forge feedstock? coool __eou__	User

TP	wheel builder was going fine but no more travis workers at the moment unfortunately: https://travis-ci.org/MacPython/scikit-learn-wheels the windows wheels are almost all ready: https://ci.appveyor.com/project/sklearn-wheels/scikit-learn-wheels OSX travis workers are overloaded I have to wake up early tomorrow, I won't have time to wait for them, feel free to push everything to PyPI while I'm sleeping :) __eou__	User

TP	``` git checkout 0.18rc1 pip install wheelhouse-uploader python setup.py sdist fetch_artifacts ``` then `twine upload dist/` __eou__	User

TP	hi __eou__	User

TP	@amueller how do you test the feedstock without issue PRs? __eou__	User

TP	there is no real way to test the feedstock as far as I can tell __eou__	User

TP	@ogrisel have you see this https://circleci.com/gh/scikit-learn/scikit-learn/5456?utm_campaign=build-failed&utm_medium=email&utm_source=notification ? __eou__	User

TP	Indeed, I'm on it. done __eou__	User

TP	If travis is green after this fix, I think we can tag 0.18rc2 and trigger the wheel builders. How is the feedstock going? __eou__	User

TP	@amueller shall I tag 0.18rc2? __eou__	User

TP	@ogrisel sure @ogrisel feedstock is been running for like an hour to build the 0.18rc1 __eou__	User

TP	0.18rc2 is on the launch pad I believe the wheels will be ready by tomorrow ;) __eou__	User
TP	cool__eou__	Agent
TP	as soon as the conda package is ready any of us can fetch the wheels + build the sdist and publish to PyPI + tweet__eou__	User
TP	sounds good__eou__	Agent

TP	Hopefully the conda packages too I expect they arrive here: https://anaconda.org/conda-forge/repo?label=rc I'll update the feedstock for 0.18.2 __eou__	User
TP	great__eou__	Agent

TP	I think the conda package takes about 3h to build a single entry of the matrix takes 30 minutes, there are 6 and they all run in the same job __eou__	User

TP	@ogrisel do you want to fix the lbgfs thing or should I? rc3? ;) __eou__	User

TP	YES! https://anaconda.org/conda-forge/scikit-learn/labels __eou__	User

TP	:clap: __eou__	User

TP	ugh so conda users can't update unless they use conda-forge eh __eou__	User

TP	@amueller yes I do, at least I'm a contributor of the package; I'm a postdoc at nyu. @dankessler thanks for taking a look. So to clarify: `X` is 3D, e.g. `(n_samples, n_features, n_dimensions)` and we want to fit an estimator across all features for each dimension separately, such that we end up with `n_dimensions` estimators, and therefore `n_dimensions` scores. @amueller briefly at a meetup yes #7388 seems indeed relevant. If you allow `cross_val_score` to have scoring metrics that are not float but custom (e.g. numpy arrays), then our problem is solved at the sklearn level: we would directly do `cross_val_score(SearchLight())` where by default `SearchLight().score(X, y)`with `X` shape being `(n_samples, n_features, n_dimensions)` returns an array of `n_dimensions` __eou__	User

TP	Our `SearchLight` is indeed similar to nilearn's, but nilearn's does everything at once (parametrize how to move across dimensions of an MRI scan, fit/predict estimators, do the whole thing within a cv etc); by contrast we  implemented a single-purpose object: i.e. fit different classifiers over different dimensions of the data, but don't summarize or combine these classifiers. We can thus pipeline the search light : e.g. `make_pipeline(PrepareData(), SearchLight(Regressiont()))` and cross_val_predict this estimator However, we can't apply `cross_val_score` because this functions requires that the score is a float, not an array. __eou__	User

TP	But perhaps there s a way in sklearn to get cross_val_score compatible with arrays? i.e. if one wants to retrieve the cross-validated confusion matrix , instead of the average score? __eou__	User

TP	@kingjr have we met? Sorry If I forgot :( @kingjr also check out https://github.com/scikit-learn/scikit-learn/pull/7388#issuecomment-247565362 __eou__	User

TP	@kingjr I think that would be the best. We already want that for f1_scores without averaging for example, where you get one for each class. __eou__	User

TP	Hello, is this the right place to ask beginners ML questions? __eou__	User

TP	I think crossvalidated or stackoverflow would be better thank you for asking __eou__	User

TP	As an intermediate ML practitioner still stinging from being a noob, I found both of those communities very off putting in getting assistance. On cross validated, I was literally told that the answers to a question I asked were "in any basic econometrics text". They are invaluable resources to be sure, but not the place to learn as a beginner. I would advise trying to find a community around one of the MOOCs - udacity, coursera, ... Perhaps even Reddit I am biased in which I would choose as I got a lot out of the Udacity ML nanodegree and have found their slack community inviting and informative. __eou__	User

TP	Hello, how would one extract decision paths from a decision tree classifier? I mean I would like to get something like, "if x < 1 and ... then class 1" __eou__	User

TP	Thank you @joshuacook and @nelson-liu __eou__	User

TP	Hi. Reading the SVC doc(http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) has it `... may not work properly in a multithreaded context `  Is possible to execute SVC using all threads? how ? __eou__	User

TP	Hi, I'm trying to build an outlier detection system on textual data using the `EllipticEnvelope`. I was wondering if there's a better way to choose the optimal number of features out of a TFIDF vectorizer so as to not cause a memory error or a singular covariance matrix apart from repeatedly checking on the training or the cv set. Thanks a lot for the help and I apologise if this is a very noob-ish doubt! __eou__	User

TP	How can I see the most frequent word using a TfidfVectorizer instance? Is possible? __eou__	User

TP	Could anyone peep in to see how can I correct this issue? https://travis-ci.org/scikit-learn/scikit-learn/builds/162438563 __eou__	User

TP	hello! I am Satya Prakash wanting to work with scikit package and as it involves ML stuff I am intrested in it so any one please give me location to start and also looking forward to contribute in GSOC2017 any guidance will be really helpful __eou__	User

TP	Hello! I am new with this stuff, I intend extracting features from gray-scale images and then further do analysis using Decision Tree algorithm  but it doesn't seem working.I will appreciate any guidance. Below is a screenshot of my work. Thank you %matplotlib inline from matplotlib import pyplot as plt import numpy as np from skimage import io  count = 0  images = io.imread_collection('/home/data/Desktop/IMAGES/GRAY-SCALE/*.jpg')        x_images = np.vstack(images)   while count < len(images):      new_x = (x_images[count])     new_y =(images.files[count])             print(new_y)     print(new_x)      count = count + 1  %matplotlib inline from matplotlib import pyplot as plt import numpy as np from skimage import io  count = 0  images = io.imread_collection('/home/data/Desktop/IMAGES/GRAY-SCALE/*.jpg')        x_images = np.vstack(images)   while count < len(images):      new_x = (x_images[count])     new_y =(images.files[count])             print(new_y)     print(new_x)      count = count + 1  from sklearn.tree import DecisionTreeClassifier from sklearn.cross_validation import train_test_split from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score   features = x_images  targetVariable = new_y  featureTrain,featureTest,TargetTrain,TargerTest = train_test_split(features, targetVariable, test_size = 0.2)  model = DecisionTreeClassifier() fittedModel = fit.model(featureTrain,targetTrain) predictions = fittedModel.predict()  print(predictions) #print(TargetTest,predictions) #print(accuracy_score(TargetTest,predictions)) Thanks a lot __eou__	User

TP	Doesn't seem to be working as in you get errors? if so what? or do you mean the code words but it's not doing wwhat you want it to do? __eou__	User

TP	I get an error when i run the algorithm... __eou__	User
TP	Did you paste the error as well?__eou__	Agent

TP	__eou__	User

TP	__eou__	User

TP	Is there any line of code that can help extract features from these images(gray-scale) so I can further do analysis using decision Tree algorithm...@ItchyJunk...?? __eou__	User

TP	Hmm, not sure. I'll let someone who know better answer you. but looks like it's just x_image variable being used without being declared? is x_image supposed to be an array or string or something? x_image = [] or x_image = ''  would be the needed step. __eou__	User

TP	stable docs down, is this intended? @amueller @ogrisel __eou__	User

TP	@nelson-liu typo, sorry about that! __eou__	User
TP	nvm, just saw https://github.com/scikit-learn/scikit-learn.github.io/commit/d6c6bd03f4cabbce20a3ac579c243abe0c4f2235__eou__	Agent
TP	ah ok__eou__	Agent
TP	thanks :smile:__eou__	Agent

TP	should now be up as 0.18 :) __eou__	User

TP	:fireworks: yay! __eou__	User

TP	Hello! is there any simple way of extracting features from fifty  gray scale images for further analysis? few lines of code would be much appreciated.Thanks  I tried using numerical matrix of each  Grayscale image as a feature for further analysis but it doesn't working. Any guidance would help.Thanks __eou__	User

TP	have you looked at something like [the mnist example](http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html) __eou__	User

TP	__eou__	User

TP	__eou__	User

TP	Hello everyone. __eou__	User

TP	Hi :smile: __eou__	User

TP	hello __eou__	User

TP	I want some help on project based learning in Recommendation systems... I started with movielens dataset but now I'm stuck. can anyone help me please. __eou__	User

TP	@Ben-Kobby are you sure you want to be reshaping `D_images` and not `x_images`? __eou__	User

TP	Guys... I'm super confused about `DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.` How can we make our modules compliant with older and newer versions? __eou__	User

TP	@kootenpv by modules do you mean modules that interface with scikit-learn? __eou__	User

TP	Hello everyone, I want to contribute to scikit-learn. Can someone please direct me towards the documentation to get started? Thanks. __eou__	User

TP	have you read the contributing guide? thats a good start. __eou__	User

TP	Yes sir, I did give it a read. However, not having worked with such a huge library before, the code base seems a bit intimidating. Can you please tell me as to how long would it take to understand the code before I can make any contribution? __eou__	User
TP	well you dont have to understand everything to begin contributing__eou__	Agent
TP	the different methods implemented are largely independent of each other__eou__	Agent

TP	Okay sir, thanks for letting me know. I'll try to solve any issue marked 'easy' to start-off as mentioned in the guide then. __eou__	User

TP	@dankessler  thanks a lot for the correction,  Am suppose to reshape x_images instead... __eou__	User

TP	Hi, this is one error that I always seem to encounter when I pull from upstream (using 0.19.dev0) even after executing `sudo make`: ``` ImportError: /home/devashish/EXPERIMENTATION/scikit-learn/sklearn/utils/sparsefuncs_fast.so: undefined symbol: PyFPE_jbuf ``` Any solutions to this? I'm trying to import LogisticRegression here __eou__	User

TP	__eou__	User

TP	__eou__	User

TP	I'm getting pretty much the same error as reported [here](http://stackoverflow.com/questions/37577210/build-sklearn-error-cythonize-failed). I have cython installed however I still get this error :disappointed: __eou__	User

TP	reported it here: https://github.com/scikit-learn/scikit-learn/issues/7542 __eou__	User

TP	Hello everyone __eou__	User

TP	I just started using scikit-learn few weeks back and I am loving every bit of it. I want to contribute to scikit-learn but not able to understand where to start Can anyone guide me through __eou__	User

TP	@vibrantabhi19 their website has some contribution guidelines : http://scikit-learn.org/stable/developers/index.html __eou__	User

TP	Thanks @RohanVB. I went through these guidlines. Hope I can get over some issues and make my first PR. __eou__	User

TP	theres a low hanging fruit @ https://github.com/scikit-learn/scikit-learn/issues/7521 __eou__	User

TP	Thank you @nelson-liu  I am up for it :) __eou__	User

TP	Scikit learn is amazing! I just had a question regarding what is the plan of scikitlearn as tensorflow has written something called sci flow, which is intended to be replacement for scikit-learn. Please be advised that I still do not totally understand tensorflow! __eou__	User

TP	@nelson-liu The file testimonial.html is available only after we build the html docs.  And then in testimonials.html, even after fixing the issue we get 'working directory clean' So kindly help me with the file/code structure __eou__	User
TP	you have to modify about.rst__eou__	Agent
TP	thats the file name__eou__	Agent
TP	the html files are built from rst source files, so modify the rst source__eou__	Agent

TP	i bet you could grep rangespan and find wher eyou want it @thewhitetulip if youre referring to skflow, I dont think its a replacement for scikit-learn and i dont think its their intention to be a replacement skflow is useful for providing a scikit-learn interface to the deep learning capabilities of tensorflow, because scikit-learn does not do deep learning they occupy different roles __eou__	User

TP	@nelson-liu Done, Kindly review my PR __eou__	User

TP	I got through my first contribution, can anyone suggest few more issues that are easy to solve. I wanted to get some confidence before going to the core progamming structure. I also reviewed some fo  the 'Easy' labeled issues.  @nelson-liu any more low hanging fruit? __eou__	User

TP	@nelson-liu good to know! __eou__	User

TP	@vibrantabhi19 @thewhitetulip one thing that's also very appreciated is going through old issues and seeing if they are still relevant. __eou__	User

TP	@amueller Will surely look into it __eou__	User

TP	@raghavrv you know that the ping on numpy was Travis, the creator of numpy and scipy and CEO of continuum? ;) __eou__	User

TP	Wow. I didn't know that o.O __eou__	User

TP	hello, may I ask a question about sk-learn here? It's related to the deprecation warning. DeprecationWarning: Function residues_ is deprecated; ``residues_`` is deprecated and will be removed in 0.19 I'm wondering what should I use instead of 'residues_' to get residue values from LinearRegression (fitted) model. __eou__	User

TP	wow this makes me shiver: https://github.com/scikit-learn/scikit-learn/pull/6348/files @naokishibuya yes you should __eou__	User

TP	err sorry use ``_residues`` or  just compute them from the predictions? np.sum((y_train - lr.predict(X_train)) ** 2) __eou__	User

TP	hm is there a way to see how many issues I closed today? __eou__	User

TP	@amueller understood, thx! __eou__	User

TP	Is there anything to do discretization on scikilearn? __eou__	User

TP	@Piperod__twitter not really right now. You can just use numpy though numpy.digitize is the thing you want to look at iirc __eou__	User

TP	@amueller not sure, bu pulse can give really good insights https://github.com/scikit-learn/scikit-learn/pulse Hi, I had a feature request can we have a `mean_squared_log_error` or a `root_mean_squared_log_error` in `sklearn.metrics` ? former is preferred looking at the current set of metrics __eou__	User

TP	Frequent competitions on kaggle have this evaluation metric https://www.kaggle.com/competitions?sortBy=prize&group=all&page=1&site=main&eval=rmsle for now, i make a new column in my dataframe by taking `np.log(1 + x)` transformation, and then calculate `mean_squared_error` on it. Then while making a submission I reverse it as `np.exp(x) - 1`. Thoughts ? [![blob](https://files.gitter.im/scikit-learn/scikit-learn/VhqO/thumb/blob.png)](https://files.gitter.im/scikit-learn/scikit-learn/VhqO/blob) __eou__	User

TP	hello, I was working on https://github.com/scikit-learn/scikit-learn/issues/6120. I am not sure I understand what needs to be exactly needs to be changed. what do I need to do in order to use only 20 components? __eou__	User

TP	How would one submit a feature request on the documentation. With the addition of float elements to some hyper parameters it might be assumed that values can range from (0-1] , but in fact ranges often have a minimum i.e.( (0-1]*n_elements  must be greater than 2 , (0-.5] ) __eou__	User

TP	@dylanbstorey on the issue tracker __eou__	User

TP	Does mean_score_time in GridSearchCV.cv\_results_ mean time that it took to predict classes (in classification case) and calcuclate the relevant evaluation scores? __eou__	User

TP	if you fit randomforestclassifier with verbosity  =1, say and then pickle it it seems you are stuck with this verbosity level forever. Is that correct/wanted? unless you can secretly set the verbosity level when doing .predict_proba? __eou__	User

TP	> Hi, I had a feature request  #7655 :) I'll be waiting eagerly for the reviews __eou__	User

TP	has anyone successfully added sklearn to a requirements.txt with a specific version? __eou__	User

TP	yes i have by version do you mean release #? @alexbednarczyk whats the error you are getting right now? __eou__	User

TP	> Does mean_score_time in GridSearchCV.cv\_results_ mean time that it took to predict classes (in classification case) and calcuclate the relevant evaluation scores?  @mkoske Yes. `mean_fit_time` corresponds to the training time... __eou__	User

TP	@nelson-liu what does your requirements.txt line for sklearn look like? I've tried scikit-learn==0.17 and sklearn==0.17 both give me errors __eou__	User

TP	it doesn't seem like sklearn interacts with pip3 freeze. when i use pip3 freeze it displays the version as 0.0 when im using 0.18 Collecting sklearn==0.17 (from -r requirements.txt (line 3))  Could not find a version that satisfies the requirement sklearn==0.17 (from -r requirements.txt (line 3)) (from versions: 0.0) No matching distribution found for sklearn==0.17 (from -r requirements.txt (line 3)) __eou__	User

TP	hmm thats odd. my pip outputs `scikit-learn==0.18` perfectly fine although i am on python 2, so ill try with 3 __eou__	User

TP	``` conda create --name test python=3 source activate test pip install numpy scipy cython scikit-learn  $ pip --version pip 8.1.2 from /Users/nfliu/miniconda2/envs/test/lib/python3.5/site-packages (python 3.5)  $ pip freeze Cython==0.24.1 numpy==1.11.2 scikit-learn==0.18 scipy==0.18.1 ```  seemed to work for me @alexbednarczyk __eou__	User

TP	alright. im using pip 8.1.1, maybe i need to update did it work with 0.17? __eou__	User

TP	yeah it did __eou__	User

TP	ok ill try updating pip, thanks __eou__	User

TP	Hi, https://github.com/scikit-learn/scikit-learn/pull/7655 is ready. Can any core team member help me improve it ? :D __eou__	User

TP	Hi Sklearn community. Is there a way to do clustering with a constrain such that there is an equal number of training samples in each cluster? __eou__	User

TP	Hey Guys, I was going through the issues on github, so have a few  questions regarding  Neural Network modules(MLPclassifier/MLPregressor): 1. I think Dropout has already been implemented, is the PR already merged with the 'master' branch 2. What are the other things in the pipeline related to neural networks Also, are there any plans for implementing Word Vectors in scikit-learn? @amueller ? __eou__	User

TP	iirc there was a proposal to add word2vec awhile ago but it got dismantled pretty quickly since gensim is already a thing  also i dont think dropout is implemented / merged, see https://github.com/scikit-learn/scikit-learn/pull/7407 heres the word2vec one https://github.com/scikit-learn/scikit-learn/issues/6247 __eou__	User

TP	@nelson-liu true that. and although gensim is pretty straight forward to use, do you think that, if we had the word2vec module in scikit-learn itself, then one doesn't have to rely on two different libraries, besides, we already have couple of existing vectorization methods for text, in that way you can simply try word vectors instead of tfidf/count vectors etc. __eou__	User
TP	i dont think thats worth implementing word2vec/glove in scikit-learn though; as gael said in the issue > Text modelling probably requires more than just machine learning, including specific rules. Thus I think that word2vec belongs more in gensim (where it already is) than in scikit-learn: it is a model very tuned to text.__eou__	Agent
TP	and gensim does indeed have a scikit-learn interface (or should have one soon i believe)__eou__	Agent
TP	yeah right, just went through it. I think as @mblondel mentioned in this issue, I'd rather add it in https://github.com/scikit-learn/scikit-learn/wiki/Third-party-projects-and-code-snippets.__eou__	User

TP	yup or perhaps https://github.com/scikit-learn-contrib __eou__	User

TP	Decision trees support categorical data or do I have to convert categorical data into numbers? __eou__	User

TP	you have to convert categorical data into numbers __eou__	User

TP	Hello -- does anyone know what's the best way to serialize scikit learn models to string/bytes etc, which can be stored in a database? I have tried pickle -- but many people seem to advice against it. joblib is good for writing to a file, but does not provide support to converting to string. __eou__	User

TP	Hello, are the neighbors returned by NearestNeighbors.kneighbors() always sorted by distance? __eou__	User

TP	Hi, I suppose it is sorted by distance - nearest( least distance) being the first element. __eou__	User

TP	If I use same estimator in a loop but with different subset of data, should I always call .fit(X, y) with the new subset? __eou__	User

TP	Is partial_fit suitable for your purpose ? Though not all estimators support it. [This](http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning) might be relevant. Hope I am understanding your use case right. __eou__	User

TP	@rajhans  I'm curious, if you were to store it in a db, what would you want to do with the record __eou__	User

TP	@rajhans you could write the joblib file to disk, get a hash, and then store the hash? That might be a bit too much space for your application, though. __eou__	User

TP	@nelson-liu is that what you do? __eou__	User

TP	no, I dont work with scikit-learn and databases. Im just postulating a possible solution to his answer __eou__	User

TP	Having issues with akka netty I have two services in akka communicating over netty tcp. How can I dockerize them. I am as of now getting an error Can anyone help me on this? __eou__	User

TP	i dont think this is the proper place to ask a question about akka, this gitter room is for scikit-learn __eou__	User

TP	Hello, Just wondering, how can we setup path dependant input states for Random Forest ? This is similar to Recurrent Network,   where input Xt=(x0,...,xi,.. yt-1, yt-2) depends on past output states Yt.  If we could put the exact output states Yt, it creates a bias in the training. So, wondering how we can put  recurrent states in Random Forest training. Thanks __eou__	User

TP	hey everyone i am new here and want to learn machine learning __eou__	User

TP	@neerajwadhwa take andrew Ng's coursera course and by my book ;) https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413/ref=sr_1_1?ie=UTF8&qid=1477405931&sr=8-1&keywords=introduction+to+machine+learning+with+python __eou__	User

TP	@amueller Do you have any issue that needs fixing or review for `0.18.1`? __eou__	User

TP	@raghavrv  https://github.com/scikit-learn/scikit-learn/pulls?q=is%3Aopen+is%3Apr+milestone%3A0.18.1 ;) and https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+is%3Aissue+milestone%3A0.18.1+label%3ABlocker so two things that would be great are fixing this: https://github.com/scikit-learn/scikit-learn/issues/7563 (I'm investigating but getting no-where) and a non-regression test for this: https://github.com/scikit-learn/scikit-learn/pull/7724 __eou__	User

TP	@amueller Okay! BTW for #7672, you can try `curl -Ls https://goo.gl/jhGwkV | git apply -v --index; git commit -m "NOMERGE recythonize all";` to clear up the cache and make tests pass... (Resetting the cache at travis seems to not work...) Have addressed your comments at #5874 @amueller For #7724 do you want me to cherry-pick his commits and add a test or do we wait for him to add it? __eou__	User
TP	either way. first find a test?__eou__	Agent

TP	For `sklearn.svm.SVC` with a [callable kernel](http://scikit-learn.org/dev/modules/svm.html#using-python-functions-as-kernels), is there any way the callee knows if the outer estimator is in the midst of a `fit` or `predict` (or similar)? __eou__	User
TP	whether the kernel knows? Well if X==Y it's probably fit__eou__	Agent
TP	Yeah, that was first heuristic that came to mind__eou__	User
TP	but no, there is no flag or anything. The kernel shoudn't know__eou__	Agent
TP	why do you want that?__eou__	Agent
TP	Ok. I know others have done work on this, but trying to implement MKL as a callable__eou__	User
TP	use shogun if you want to do mkl ;)__eou__	Agent
TP	Idea is that during outer `fit` call, there's an MKL class that computes the various kernels, learns the gamma weights and stores them, then returns the combined gram__eou__	User
TP	but during `predict` it uses the stored weights__eou__	User
TP	Yeah, I suppose I could go to shogun. I just like sklearn so dang much__eou__	User
TP	I suppose so long as the weight learning is deterministic, it's effectively idempotent, so that heuristic would work__eou__	User
TP	i.e., if you're predicting the training data, it works the same as fitting the training data__eou__	User
TP	Yeah, I previously built that, too, but trying to see if I can fit this within the `SVC` spec__eou__	User
TP	nystroem it?__eou__	User
TP	nvm, presume you're talking about the `Nystroem` class from kernel estimator family__eou__	User
TP	ah, that makes sense__eou__	User
TP	My motivating case is multiple kernels of same functional form but defined on non-overlapping subsets of the feature space__eou__	User
TP	which is very group lasso-y__eou__	User
TP	Thanks @amueller! If you come across your MKL code, would appreciate a link, but no worries if it's not close at hand__eou__	User

TP	I think I did implement mkl with sklearn at some point... hm Well you should implement your own class that calls the kernels, I would say you can always just nystroem it and use a liner model on the combined embedding ;) compute the nystroem feature map (see Nystroem class), which makes the SVM problem a linear problem in that feature space. MKL is just doing a linear model on concatenated features I guess it's a group lasso, though __eou__	User

TP	ah, but that callable isn't going to have access to `Y`, even if estimator is doing a fit, so that kind of sinks learning weights based on labels Seems like feature xform is the way to go (if I'm going to fit this into sklearn) __eou__	User

TP	Hello, I am using NearestNeighbors.kneighbors. If there's some cases that are at equal distance from the query point, the number of neighbors might be larger than n_neighbors. Is there a way to find how many neighbors there actually were, i.e. counting the neighbors with equal distance. __eou__	User

TP	 Hi guys can anyone help me with some lines of code on how to extract features from 400,000 images dateset using color histogram...in a vector form?? Thank you. __eou__	User

TP	although joblib exists, I'm curious if people have experience using dill to pickle both arrays and models. as well as experience with cloudpickle __eou__	User

TP	Is there any algorithm to make understand the sentence in english.. Actually the meaning of sentence __eou__	User

TP	My code takes about 4seconds to run when using just NearestNeighbors(), but if I use n_jobs=-1 argument, it takes 8.5 minutes. Why it does this? __eou__	User

TP	@mkoske depending on how much data you have, parallelization might make it slower. Starting the jobs has some overhead. Try n_jobs=2 __eou__	User

TP	@amueller I think it is 2, since I have 2 cores, right? I'm using the digits dataset for testing __eou__	User

TP	@nilkanthshirodkar apologies, my native tongue is gibberish.  ^_^  Here's the meaning:  Lately, I've been using [dill](https://github.com/uqfoundation/dill) instead of `joblib.dump` to serialize models.  While dill can pickle the object itself,  it can also recursively pickle the object definition, which means you don't need to import the object's class(es) ahead of time.   Unfortunately, I get a variety of file handle errors when dumping some* scikit models using dill, and I'm not sure why.  I know there have been [thoughts to integrate dill into joblib itself](https://github.com/scikit-learn/scikit-learn/issues/5623).  And I'm curious to know if others have had similar experiences with dill or [cloudpickle](https://github.com/cloudpipe/cloudpickle). __eou__	User

TP	@saket1192 can anyone help me out with NLTK python i got hindi POS tagger and Hindi parser ,but I am not able to excess it. m new to this so i need some help for my in-house project __eou__	User

TP	Andy, for #7724, do you want me to send a PR to his branch to speed up the process? @amueller __eou__	User

TP	@raghavrv hm not sure. I mean I'd like to finish up this week, but there is no super rush __eou__	User

TP	Okie so I'll clean up that as a test, comment there and wait for him to make the changes. If it still remains at the end of the week unaddressed, ping me so I can carry forward the work... __eou__	User
TP	ok cool__eou__	Agent

TP	Anyone here wants to pick up an easy doc issue? - #7772 __eou__	User

TP	@raghavrv Please have a look at #7776 :) __eou__	User

TP	Oops, #7775 was submitted just seven minutes before mine and it solves the same issue :sweat_smile: Feel free to close mine if it works out well ! __eou__	User

TP	@karandesai-96 Thanks for the PR :) __eou__	User

TP	@raghavrv my pleasure :) on a similar note, i have one more PR which needs review ( #7655 ) __eou__	User

TP	Have done some review. Let me know once you address them! __eou__	User

TP	@raghavrv thanks, also I just checked out - #7775 was closed for some reason. Let me know if #7776 needs to be reopened again :smile: __eou__	User

TP	hi everyone __eou__	User

TP	I want to apply the elastic net to do sparse feature selection on my data, but I only want a subset of the features to be regularized (and the rest of the features to be unregularized) theres probably nothing that supports this right now in scikit-learn, right? not too keen on re-implementing the algorithms myself haha, so Id figure Id ask. @waterponey you should comment that on the original PRs, gitter is fairly low-visibility __eou__	User

TP	I also find it interesting that the elastic net is not mentioned anywhere on the [feature selection page](http://scikit-learn.org/stable/modules/feature_selection.html), since it tends to avoid the problem that the lasso has of selecting an individual variable out of a group of highly correlated features... __eou__	User

TP	#7659 and #7671 sould be closed __eou__	User

TP	@amueller I wonder if you could give a look to #6509 ok np __eou__	User

TP	@glemaitre I'm crazy busy today and tomorrow (ending a job today, starting a new tomorrow). I'll try to catch up with the issue tracker soon but feel free to ping me again. __eou__	User

TP	@amueller what's the convention when two PR are opened for the same bug? It's a minor one... very very very minor documentation issue. __eou__	User

TP	@NelleV you tell them both and maybe give a recommendation on which offers a better solution? @NelleV if they are both ready to merge, merge one and explain / apologize to the other? __eou__	User
TP	:D__eou__	Agent
TP	If you have a better idea, I'm all ears__eou__	User

TP	well, there is a second reviewer needed, if you'd like to review two 2-liners documentation change __eou__	User
TP	sure__eou__	Agent

TP	#7783 and #7784 The first one is better. __eou__	User

TP	the first one is also first.  this can be merged with one review as minor docs. I think we try a mixture of nice and pragmatic __eou__	User
TP	That's what I figured__eou__	Agent

TP	are the scipy docs down or is it just me? https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html __eou__	User
TP	@amueller It is down__eou__	Agent

TP	what is the best  external  PCIe Box where i can use titan X. I have mac book pro 2009 model. __eou__	User

TP	@amueller Today the docs are back online. __eou__	User

TP	<unconvertable> Transpile trained scikit-learn models to a low-level programming language like C, Java, JavaScript or Go with https://github.com/nok/sklearn-porter :smile: __eou__	User

TP	Hello, Need some help with this:  I'm working on a project where I'm creating a spam classifier (instagram photos) I have features from the user and features from the content, would you combine all data knowing that there are multiple photos from the same users. Thank you! __eou__	User

TP	@amueller Could we just have the `return self` fix refactored out of #6141 for 0.18.1? __eou__	User

TP	@nok  something I've been looking for! __eou__	User

TP	@tonnamb Ty! __eou__	User

TP	Hey everyone, with sklearn.metrics.roc_curve, is there a way to manually specify the thresholds/cutoffs I want to test at? __eou__	User

TP	Hey guys. Can someone help with this error I get  when I do : `>>> dataset = fetch_mldata('MNIST original'` any idea why I get this error: OSError: could not read bytes the .mat file downloaded, it has 1498112 lines The error itself is : `OSError: could not read bytes` I tried : ```dataset = fetch_mldata('MNIST original', data_home='/Users/myname/Virtualenv/virt1/lib/python3.5/site-packages/sklearn/datasets/')``` aswell __eou__	User

TP	the cached data might be corrupted try removing downloaded files and trying again __eou__	User

TP	Are Pandas dataframes supported for use in scikit-neuralnetwork?  It appears so, via the Lasagne implementation according to https://recordnotfound.com/scikit-neuralnetwork-aigamedev-8422   but I keep getting errors when using a Pandas dataframe in sknn. __eou__	User

TP	@cpoptic this room is for scikit-learn, and the developers of scikit-learn arent involved in scikit-neuralnetwork. youd probably be better off asking your question in the scikit-neuralnetwork gitter (if there is one) or raising an issue on their github repo / posting to stackoverflow __eou__	User

TP	@nelson-liu  Thanks, I checked and there is no scikit-neuralnetwork gitter channel, and haven't gotten an answer to this issue when I asked on the github, but I'll post to StackOverflow.  Thanks __eou__	User

TP	@amueller Do you want #7812 into 0.18.1? __eou__	User

TP	Also should we move #7627 out of 0.18.1 into 0.19? Also #7544 and #7173? __eou__	User

TP	Hello, I've proposed a project for chromosomes segmentation : https://github.com/jeanpat/DeepFISH . Up to now there are datasets (82146 images+ground-truth labels) and python notebooks (way to generate datasets, scikit-image based). The notebooks could be better:  the size of the datasets are limited by the amount of RAM. The datasets are waiting to train a classifier. __eou__	User

TP	Completely new to contributing to sklearn. Is there a recommended contributions guide? Considering fixing the memory issues with bh-tsne. __eou__	User

TP	@hvaara http://scikit-learn.org/dev/developers/contributing.html#contributing __eou__	User

TP	Hi, I'm new with machine learning, do you can help me for information? __eou__	User

TP	Hi forks, I am trying import ```from sklearn.model_selection import cross_val_score``` but it had an error ```ImportError: No module named 'sklearn.model_selection'``` and I installed scikit-learn package but I don't know what matter for me , I cannot import this module to my project. I am using python 3.5. Can you help me ? Many thanks :D __eou__	User

TP	@khanhnguyenneka you have to update your `scikit-learn` installation to 0.18.x version __eou__	User

TP	@VictorArias24 Hi, Victor! For some basics you can refer to: http://scikit-learn.org/stable/tutorial/basic/tutorial.html hope it helps! __eou__	User

TP	Just wondering if anybody in Scikit Community has already worked on Recurrent Decision Tree implementation ? __eou__	User

TP	Does https://github.com/scikit-learn/scikit-learn/milestone/22 need updating? __eou__	User

TP	Hi! Does anyone know how to show percentage of similarity of predicted data to one of train data? And how to group train data by labels? thanks __eou__	User

TP	Hi __eou__	User

TP	Hello, I'm not sure what metric my cross_val_score uses. The estimator is a RF regressor, and I leave the scorer for cross_val_score at None. Does that mean I get R^2? Because I somehow have an R^2 of minus 300 then ... __eou__	User

TP	I'm also running into an issue downloading datasets with `datasets.fetch_mldata()` (cc @RohanVB) urlopen() fails with a "Connection reset by peer", though downloading the same URL with curl works (but curl --verbose shows a warning) Did mldata.org change their server, and it now does something weird with HTTP that Python's urllib2 can't handle? __eou__	User

TP	``` $ curl -LI -X GET http://mldata.org/repository/data/download/matlab/mnist-original HTTP/1.0 301 MOVED PERMANENTLY Date: Mon, 21 Nov 2016 20:03:05 GMT Content-Type: text/html; charset=utf-8 Location: http://mldata.org/repository/data/download/matlab/mnist-original/ Server: fapws3/0.9  HTTP/1.0 200 OK Content-Length: 55440440 Content-Disposition: attachment; filename=mnist-original.mat Vary: Cookie Server: fapws3/0.9 Date: Mon, 21 Nov 2016 20:03:05 GMT Content-Type: application/x-matlab  $ python -c 'import sklearn.datasets; sklearn.datasets.fetch_mldata("MNIST original")' Traceback (most recent call last):   File "<string>", line 1, in <module>   File "/home/vagrant/.virtualenvs/digits-sklearn-opencv/local/lib/python2.7/site-packages/sklearn/datasets/mldata.py", line 143, in fetch_mldata     mldata_url = urlopen(urlname)   File "/usr/lib/python2.7/urllib2.py", line 154, in urlopen     return opener.open(url, data, timeout)   File "/usr/lib/python2.7/urllib2.py", line 435, in open     response = meth(req, response)   File "/usr/lib/python2.7/urllib2.py", line 548, in http_response     'http', request, response, code, msg, hdrs)   File "/usr/lib/python2.7/urllib2.py", line 467, in error     result = self._call_chain(*args)   File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain     result = func(*args)   File "/usr/lib/python2.7/urllib2.py", line 651, in http_error_302     fp.read()   File "/usr/lib/python2.7/socket.py", line 355, in read     data = self._sock.recv(rbufsize)   File "/usr/lib/python2.7/httplib.py", line 612, in read     s = self.fp.read(amt)   File "/usr/lib/python2.7/socket.py", line 384, in read     data = self._sock.recv(left) socket.error: [Errno 104] Connection reset by peer ``` __eou__	User

TP	Wow now I'm seeing "Connection reset by peer" from curl as well. I'll just try and self-host that somewhere, if I can ever grab it __eou__	User

TP	There is no way to load a MATLAB mldata file from a different URL though, is there? __eou__	User

TP	Today I found https://github.com/scikit-learn-contrib/scikit-learn-contrib , is it possible to add https://github.com/nok/sklearn-porter ? But it isn't an estimator like one of the listed examples. Who can help me? __eou__	User

TP	Hi guys. I'm a beginning undergrad researcher in data science and trying to run scikits svm model on 500k models. rows* It's taking 5 terms of life to complete. Can someone help point me in the right direction for how I should apporoach this? __eou__	User

TP	@Schachte Try using linearSVM instead of SVC, you can see in the documentation the following  The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. __eou__	User

TP	hi @firetix , thanks a lot. I ran this and it completes very quickly now. Can you explain how the testing works with the clf-score? @firetix does it automatically just split the input data into test/train for me? __eou__	User

TP	@Schachte it works the same way as an SVC you will have to do the split yourself and then call .score or run f1 evaluation __eou__	User

TP	I'm a newbie, so please pardon what might be a very simple / silly question.  I'm wondering whether the example code for various sample problems posted on the scikit-learn website are done in Python 2 or Python 3? __eou__	User

TP	@FLalani should be compatible with both __eou__	User

TP	@nok contact the scikit-learn contrib folks about that. I think the proper procedure is to raise an issue on their repo, but I'm not sure. __eou__	User

TP	Thanks @nelson-liu __eou__	User

TP	@nelson-liu:  Thanks!  It did indeed run with no exceptions on Python 2.7 after my comment so I can at least verify Python 2.  Thanks again. __eou__	User

TP	Hi, i am running into some really wierd behavior on gridsearchCV on an ESXI host. Although number of jobs is set to a fixed number, the spawned pythonw.exe will claim all available cores after the 2nd-3th task iteration and run into some sort of infinite loop __eou__	User

TP	Hi everyone,please is it possible to join two image feature vectors as a vector and use it to train a model for classification (example is raw pixel feature vector and color histogram feature vector of an image).I would appreciate your help, thank you. __eou__	User

TP	@Ben-Kobby can't you just concatenate the two vectors together? __eou__	User

TP	@remram44 I attained a fix if its still required. I'll have to look into my code to see exactly what I did, ping me if you need it. __eou__	User

TP	Is there any post-pruning implementation in scikit for decision trees? __eou__	User

TP	Any suggestions for how to implement handwriitten signature verification in Python ? __eou__	User

TP	General question, why haven't tensor factorizations (like Parafac, Tucker) become more widely adopted?  (or, are they widely adopted and I'm not aware?) __eou__	User

TP	(I apologize for being somewhat off topic here) __eou__	User

TP	@mrocklin they will be. For what application were you thinking? wow I haven't been here in a while.. whoops __eou__	User

TP	They've been around for a long while, why the delay?  And more importantly, why the optimism? __eou__	User

TP	in the context of machine learning they haven't been around that long, and I think for topic modeling for example they are only now becoming popular they've been around for CRFs for a while which applications / algorithms did you have in mind? __eou__	User

TP	I've had a request to build implementations on top of dask.array.  I'm polling to see how much value there is to the general community. If you happen to have strong opinions about the value of various algorithms I'd enjoy getting your thoughts. __eou__	User

TP	What does it mean, when I do grid search and in ther cv_results_ there's param_n_estimators, for example, and it's type is masked_array and then next to it, there's mask which is full of boolean false values? __eou__	User

TP	@mrocklin can you share your application domain? @mkoske it means none of the values is masked out i.e. it's a normal full array huh alright well in machine learning, tensor decomposition applications are somewhat "new" __eou__	User

TP	@amueller I genuinely don't know the application domain.  Government types are asking for a general library for tensor decompositions in Python. Which, while that ignorance is somewhat suboptimal, also means that we get to build software for the common case. (this is somewhat typical in my world) __eou__	User

TP	Given the wiggle room, how would you direct my time to best benefit the community if other people were paying for it? __eou__	User

TP	In RandomForestClassifier, if the max_features is sqrt or log2, does it round it to nearest int or what? __eou__	User

TP	@mkoske This is casted to an integer. Refer to [here](https://github.com/scikit-learn/scikit-learn/blob/b7e370244795c291fc8ac10686ce5867adde988e/sklearn/tree/tree.py#L211) for details. __eou__	User

TP	 am asked to only stimulate a smart device in c++ that could belong to the IOT. Although, I am not entirely sure what to choose. I have been given an example such as a weather device, also it is to produced data by 3 (simulated) sensors. For example the weather device, may register data from temperature, air pressure and wind speed sensors. Any ideas on a device i could choose? __eou__	User

TP	@glemaitre, thanks __eou__	User

TP	@mrocklin I talked to the person working on tensor factorization for LDA. That woudl be nice but I think they said it's not really ready yet. So I'm not sure there is a killer application. Maybe check out http://www.cs.columbia.edu/~djhsu/papers/power-jmlr.pdf __eou__	User

TP	Hello, what's the difference between mean_test_score and mean_train_score in GridSearchCV.cv\_results_? __eou__	User

TP	they are the mean of the score on the training folds vs the hold-out folds @mkoske only the hold-out test score is used to select the model the training score is good monitor overfitting / underfitting __eou__	User

TP	@amueller So hold-out folds means, that if I use e.g. 10-fold cross validation, there's 10 hold-out folds, i.e. dataset is split into 10 and then the model is trained on 9 of those and one is used for testing . And this is repeated until every one of those 10 is used as testing. Right? __eou__	User
TP	yes__eou__	Agent
TP	ok, so that part I understood__eou__	User
TP	what is the training score then?__eou__	User
TP	So, is the model is evaluated on those 9 folds too, at same iteration?__eou__	User

TP	scores are computed for each of the 10 iterations and each iteration has a 9 folds that the model is trained on and one that it is tested on the training score is the score on the training set, i.e. the 9 folds __eou__	User

TP	yes. training score meaning the data it was trained on __eou__	User

TP	Ok, I'll recap so I make sure I understood correctly :) In my example, one iteration consists of building the model on 9 folds and testing it with both those 9 folds and hold-out data. Is this correct? __eou__	User
TP	yes__eou__	Agent
TP	Ok, thanks for help__eou__	User
TP	Hm, you mean that if training score is high and test score is much smaller, then it overfits?__eou__	User

TP	well if there is a large gap then you might want to regularize more. if there is a very small gap you might want to try a more complex model __eou__	User

TP	One more question. When I run my script, Parallel reported that it took almost 3hours to complete. But when I sum all the fit and score times from GridSearchCV.cv\_results\_, the sum is significantly lower. Why? Is it that Parallel reports the total time my script was running and GridSearchCV exact times that scoring and fitting took? __eou__	User
TP	can't say without looking at your script__eou__	Agent

TP	@amueller nothing special, something like this: https://gist.github.com/mkoske/1f9b92b3f214260278ea115507cea72d I use Jupyter Notebook and it shows some timing information on red background @amueller ok, thanks anyway __eou__	User

TP	I updated that gist to contain example similar to my use case. I also updated my comment on that gist to explain __eou__	User

TP	sorry not sure I don't have time to look into it right now __eou__	User

TP	Hi everyone! I'm doing some research on hyperparameter optimization and I felt very impressed by the technique shown in this tutorial by @ogrisel  https://www.youtube.com/watch?v=iFkRt3BCctg Do you recommend any similar package or any library that implement that technique? I was playing with GridSearchCV and RandomizedSearchCV optimizers but those seem to be less interactive versions of the one in the tutorial (more "blackbox") __eou__	User

TP	Hi all, how would I do knowledge representation in SKL? __eou__	User

TP	Can someone explain what hyperparameter optimization? __eou__	User

TP	Hello everyone I wanted to contribute. Can anybody help me how to do so???? __eou__	User

TP	I have knowledge of ML and R but am completely new to scikit-learn __eou__	User

TP	Abinash check out the contributor docs on the website __eou__	User

TP	I apologize for a rather naive python question, but I've forked sklearn, made some local changes introducing new functinoality, committed them, and now want to use my modified sklearn in a test project. Should I manipulate sys.path to import my modified sklearn? Do I need to run `make` in it first? Do I need to re-run `make` if I edit sklearn again? Or should I be running `make` as in [here](http://scikit-learn.org/stable/developers/advanced_installation.html#testing-scikit-learn-from-within-the-source-folder)? __eou__	User

TP	Hi, I suppose the last link would be better in case you are okay to use dev version for all purposes. I Sorry sent message early. I use ``python setup.py develop``. In case you want both stable and dev versions, it would be better to use virtualenv to create a virtual environment for working on dev. hope it helps. __eou__	User

TP	Ah, great idea RE virtualenv; thanks! __eou__	User
TP	Sure Anytime :+1:__eou__	Agent

TP	Thanks  @amueller  :smile: __eou__	User

TP	I am having some trouble getting an MLPRegressor sample to work. Any hints or pointers appreciated. Thank you. P http://stackoverflow.com/questions/41069905/trouble-fitting-simple-data-with-mlpregressor __eou__	User

TP	Is it possible to write wrapper on python code to use it in Java , for NER Purpose ? __eou__	User

TP	I am using a decision tree to classify my data. Using scikit-learn, how do i load my own dataset? Unable to understand it in scikit-learn.org where they talk about loading from external datasets. Any help would be appreciated __eou__	User

TP	Anyoone?? My dataset is this format:  Day Outlook Temp Humidity Wind Play 1 Sunny Hot High Weak No 2 Sunny Hot High Strong No 3 Overcast Hot High Weak Yes 4 Rain Mild High Weak Yes 5 Rain Cool Normal Weak Yes 6 Rain Cool Normal Strong No 7 Overcast Cool Normal Strong Yes 8 Sunny Mild High Weak No 9 Sunny Cool Normal Weak Yes 10 Rain Mild Normal Weak Yes 11 Sunny Mild Normal Strong Yes 12 Overcast Mild High Strong Yes 13 Overcast Hot Normal Weak Yes 14 Rain Mild High Strong No __eou__	User

TP	Hi, in case your data is in a CSV or test file, you can use [`numpy.load_txt`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) and [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) and then use the scikit learn DecisionTreeClassifier for classification. Hope it helps. __eou__	User

TP	Hey guys, I hope you can help me out __eou__	User

TP	I want to train a random forest regressor, and my features are mainly categorical The thing is, the amount of possible labels per example is variable, so I can't easily dump it in a CSV I tried creating a 1-hot encoded CSV but that would result in a ~50GB file So what I want is saving my label-encoded values in some data format (maybe JSON, which supports arrays), and one-hot encoding on the fly during training I'm not sure where to start though, any clues? __eou__	User

TP	Hmm I guess scipy sparse matrices could help me here, then I just need to find a good file format __eou__	User

TP	Hello everyone! My name is Samriddhi Sinha. I am interested in contributing to this project. I have been working with various Machine Learning algorithms an have participated in a few Data Analytics competitions. Can some one help me getting started? __eou__	User

TP	@djokester You can check out the [contributors guide](http://scikit-learn.org/stable/developers/contributing.html) __eou__	User

TP	Hello, what's the feature importance implemented in trees / ensembles? Is it Gini-importance or what? __eou__	User

TP	At least DecisionTreeClassifier documentation says Gini importance __eou__	User

TP	@mkoske depends on the criterion ;) hm I can't build master because of some C++ linking issues :-/ ah right, I had that issue before on another box https://github.com/scikit-learn/scikit-learn/issues/7869 __eou__	User

TP	Does anyone know what is the precision used in metrics calculation in sklearn. I'm asking because I see significant differences in calculated metrics between sklearn and xgboost which might be occuring due to precision issues. __eou__	User

TP	Hi, is #7319 (Move to py.test) still under consideration as gsoc '17 project ? I pinged there earlier to work on it but realised that it is not that small to do within a week or two. I migrated joblib's testing framework from nose to py.test and am willing to take up the same here :smile: __eou__	User

TP	Hi all. Is there a plan for making `cross_val_score` accept multidimensional scoring metrics (i.e. a scorer that returns an array rather than a float)? I see that #7388 may be related, but IIUC it mainly to compare different metrics. __eou__	User

TP	@kingjr There is an intend. I'm not sure I'd say there is a plan. I haven't caught up with #7388 and I'm not sure whether it will include returning arrays, probably not @karandesai-96 yes, it's still a possible project. I'm not sure who would be mentoring, though. @nareshshah139 which metric? __eou__	User

TP	I posted a question on Stack Overflow related to preprocessing-scaling my features taking the logarithm but column-based as with the `MaxAbsScaler` -- Question: http://stackoverflow.com/questions/41600349/scale-apply-function-sparse-matrix-logarithmically Any help is much appreciated <unconvertable> __eou__	User

TP	@amueller Loic Esteve reviewed 20 of my PRs on joblib. :) We would probably complete by this weekend. __eou__	User

TP	Thank you to scikit-learn contributors -- It seems to be a great community __eou__	User

TP	@karandesai-96 sweet! Good you two! __eou__	User

TP	@amueller ok thanks __eou__	User

TP	Anybody recommend any starter guides for scikit-learn? Whats the best way to get into this? lol. What book? Ok awesome thanks. CS __eou__	User

TP	@aquan9 my book ;) or sebastians book? https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413/ref=sr_1_1?ie=UTF8&qid=1479485017&sr=8-1&keywords=introduction+to+machine+learning+with+python https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130/ref=pd_bxgy_14_img_2?_encoding=UTF8&psc=1&refRID=HA8X4W6W4KAK3RCZ7G85 (if you get mine, make sure it's the second print that just came out) There's also a bunch of free tutorials linked on the website: http://scikit-learn.org/stable/presentations.html what's your background? my book is "no math". sebastians is "some math" I recommend reading mine alongside bishops book or elements of statistical learning if your math-minded thanks. it's mostly all the worst typos. And obviously someone pointed out something real bad just after it got finished lol __eou__	User
TP	:)__eou__	Agent

TP	Just saw your update on the book. Congrats  @amueller __eou__	User

TP	@aquan9 there's also a bunch of free tutorials on my website: amueller.github.io __eou__	User

TP	@amueller Thanks for the advice __eou__	User

TP	In the past I have also found the following book to be very helpful "Machine Learning- The Art and Science of Algorithms that Make Sense of Data". @amueller . Whats your opinion about the same @amueller ? __eou__	User
TP	@pramitchoudhary haven't read that one. I like Max Kuhn's book though__eou__	Agent

TP	**\[shubham4060\]** Hi, __eou__	User

TP	**\[shubham4060\]** I am a 3rd year engineering currently pursuing Computer Science and Engineering department, IIT Kharagpur. i am really interested in this project and i really want to contribute. it would be very helpful if i could get some guidance on how to start. really looking forward to hearing from you. __eou__	User

TP	hello, everybody, i'm coming __eou__	User

TP	**\[madan96\]** Can anyone please explain me the reason for this error? Ref: https://travis-ci.org/sympy/sympy/jobs/192867768 **\[madan96\]** @jksuom I think the `elif` condition might resolve the issue. __eou__	User

TP	@yhaddad, why are you echoing chats in the sympy room here? __eou__	User

TP	Hi all ! I have some trouble with the MinMaxScaler, I m using it together with a keras model with input with 3 features and 1 output . my xdate is of shape (XX, 3) and output (XX,1) , I dont have any issue when calling scaler.fit_transform on both input and output training data but when I try to call inverse_transform on my predictions I get errors like this : ValueError: non-broadcastable output operand with shape (18,1) doesn't match the broadcast shape (18,3) or ValueError: operands could not be broadcast together with shapes (18,) (3,) (18,) or ValueError: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,3)  (I tried so many reshape but nothing works as excepted :( ) Using a different scaler for input and output fixes the issue but is it the only solution? __eou__	User

TP	@amueller is there  a reason that the new gaussian process classifier doesn't provide an error estimate on the fitted probability, similar to the gaussian process regressor?  I believe this can be done, similar as to how one would use the covariance matrix/Fisher information of logistic regression model to confidence bounds upon the fitted probabilities?  Thx in advance. __eou__	User

TP	@amueller Are there any immediate plans on inproving clustering performance?  Thanks! __eou__	User

TP	Hi, is there any one with experience with Gaussian processes. Are 20 data points enough to fit a GP? Is there any way to overfit a GP? __eou__	User

TP	iirc, I thought the "magic minimum" for gaussian  distributions was a sample of 30 __eou__	User

TP	How fast or slow is SpasePCA? I have a dataset of size about 15000 x 500 and it seems to take quite a while __eou__	User

TP	Hey Folks, [Core question], is there a specific reason why `RandomForests` dont inherit from `BaseBagging`? __eou__	User

TP	Does scikit have any ideas page for gsoc 2017? __eou__	User

TP	not yet __eou__	User

TP	@Djabbz things inherit when there's shared functionality. if there's not, there's no reason to inherit __eou__	User

TP	Thanks @amueller. But if you compare the code, there are a lot of similarities (not surprising given that RF are bagged random trees). My question is: is it done on purpose or is it a consequence of different persons working on different algorithms? __eou__	User

TP	Hey guys, I have a question regarding Gaussian process. It seems to me that is taking for ever to converge even with the n_jobs=-1 option. The data is very high dimensional. Any suggestions to speed up the convergence? Any tricks I should be aware of? Last but not least I've noticed that the option of `n_jobs=-1`is included only in a number of algorithms. Why was that the case? What is the logic behind it to include it only in a subset of the algorithms that scikit offers? __eou__	User

TP	are there no newcomer issues to solve at https://github.com/scikit-learn/scikit-learn/issues i want to start contributing here __eou__	User

TP	check the "easy" tag __eou__	User

TP	Hi guys is there anyone know C# ? __eou__	User

TP	Are there any 13 year olds on __eou__	User

TP	`@ThatGeoGuy:matrix.org` Hey all, was wondering if anyone had any thoughts regarding https://github.com/scikit-learn/scikit-learn/issues/4682. I've been thinking about  the issue more and the more I think about it, it seems the appropriate choice would be to change the covariance scaling in `MinCovDet` from `n` to `n-1` `@ThatGeoGuy:matrix.org` Or, barring that, rename `mcd.covariance_` to `mcd.scatter_`, since covariance usually implies that you did `n-1` scaling __eou__	User

TP	hey! quick question about LSTMs if someone has a minute __eou__	User

TP	Hello friends, first time on gitter so introduction: I am Shubham Bhardwaj 2nd year computer science undergrad from VIT Vellore. I mostly contributed on back end tasks using python in Google Developers Group-VIT. But I made a transition to Machine Learning this Winter after taking a course on UDACITY. Looking forward to working and learning from all. Thanks. __eou__	User

TP	Hello everybody, I am new here! It is nice to be here ! Cheers __eou__	User

TP	I have a question regarding applying the standardaization of the training set to a test set , for classification procedures. Do we repeat this 10 times for 10CV ? @shubham0704 Thank you, do we apply the mean/std of the train set to the test for each fold ? Sure, I read that a prefered practise in pre processing is to standardize the train set and apply the configuration used for standardization to the test set before we test with the classifier. I wonder if we need to do this for each fold in 10CV. __eou__	User

TP	@faprz  5CV is also fine but 10CV feels like standard. You can claim your accuracy better. Lets dive in: If you have k=5. that is 5 folds what you do is to divide the data set into 5 sets of equal length say s1,s2.....s5. Now you take s1 as test s2...s5 as training set and check accuracy. Each of the set gets a chance to become a test set.Now you would get 5 accuracies- average it and get the mean accuracy. Usage: sometimes using a particular split you can get better results but in real world you model doesn't performs good. So try some splits and get the average it helps you see the truth. Yes 10 times. __eou__	User

TP	cannot get what you are trying to ask @faprz  can you describe what you are trying to say a little more elaborately. __eou__	User

TP	@amueller I was reading the docstring of the f-regression and I am wondering if what is written is not very misleading @amueller I would have to read the code to check, but I was very confused with the docstring, so I googled and I arrived on stack exchange: http://stats.stackexchange.com/questions/204141/difference-between-selecting-features-based-on-f-regression-and-based-on-r2 @amueller To me, the docstring suggests that the code actually does p linear regressions, and looks for the significance of the parameter (usually, this is done by converting the  t-score/t-test), but it mentions and f-score and f-test which is usually used to compare nested models disclaimer: I have no clue what I talking about :D __eou__	User

TP	So I think the stack exchange link is misleading as well. In the specific case of scikit-learn's implementation, the f-score and t-score are identical (as we are comparing one model to the null), so it indeed boils down to selecting the features that are significantly correlated with the target. If the K-top features are selected, it is exactly what sure independance screening is about __eou__	User

TP	Hi everyone. I have a problem that I would love to solve with scikit-learn, but cant seem to figure out how to crack it. Im hoping someone who knows a bit more about the package might have a quick answer.  I have a matrix $$X$$ that I want to factorize with sparse NMF as $$X = WH$$. I would like to use cross-validation to tune the hyperparameters (to wit, level of sparsity and # of components). Since the only place that $$X$$ shows up in the objective function for NMF is $$||X-WH||_F$$, I would imagine doing the cross validation in random folds, each time leaving out a random subset of the elements of  $$X$$ from the Forbenius norm, fitting the model, and then computing the reconstruction error on those elements. However I cant seem to find any way or think of a trick to do the fit on just a subset of the matrix (scikit-learns NMF algorithm doesnt seem to like it when I try to <unconvertable> leave out <unconvertable> some of the elements by setting them to `nan` :-P ). Is there any other route to accomplishing this within the package? __eou__	User

TP	@NelleV I'm probably not the right person to ask this either.  It's just an F-test, right? https://en.wikipedia.org/wiki/F-test @jwittenbach have you checked out GridSearchCV? that does that automatically you can use ShuffleSplit if you want random splits of the data What's the objective that you want to use for selection? If you want to do it manually, you shouldn't leave them out by setting them to NaN but by just subsetting the data and throwing out those rows. __eou__	User

TP	@amueller I am definitely not the right person to ask this, but f-tests are used to compare different models, while t-tests used for significance testing @amueller I *think* that in that specific case, the code compares 1 model (univariate regression model) versus the null (mean == 0), and thus it is identical @amueller now, as f-tests are often use to compare models, they can be use for feature selection by comparing linear models with covariate X1 and linear models with covariate X1 and X2. @amueller in practice, our f-regression does not do this, and thus I think the Stack exchange answer is wrong (though I would have to look at the code): our f-regression just fit univariate linear models, and rank them with the significance of the regression parameter (with is computed with a t-test) am I making any sense? now, I have recently realized that sure independance screening and our f-regression is the same. I think that might be worth mentioning somewhere, considering how widely used SIS is. __eou__	User
TP	yes that makes sense. I haven't checked the code but sounds plausible__eou__	Agent
TP	the f_regression and f_classif docs are pretty bad imho__eou__	Agent
TP	I have not heard of SIS but that means nothing__eou__	Agent
TP	yep... I might work on that during the docathon :)__eou__	User
TP	cool :)__eou__	Agent
TP	the sure independence screening paper is quite interesting. They don't make the link between the cross correlation and the significance of the linear regression of X on y (but it might be just that it is trivial for this community), but it gives a good intuition on why this works better for feature selection than lasso__eou__	User
TP	it has over 1000 citations__eou__	User

TP	@amueller I guess my issue  is that I dont want to throw out entire rows, because that will change the shape of the factors ($$X = WH$$). I just want to hold out random elements during the fit (analogous to k-fold CV for regression) and then use the reconstruction error, i.e. $$\sum_{i, j \in S} (X_{ij} - (WH)_{ij})^2$$ (where $$S$$ is the subset of held-out elements), for the cross-validation __eou__	User

TP	That's matrix completion with is not really easily supported by sklearn, because the algorithm doesn't deal with missing values. You could try fancyimpute or any from this list: https://www.quora.com/What-is-the-best-open-source-package-to-build-a-recommender-system-in-Python/answer/Xavier-Amatriain?srid=cgo __eou__	User

TP	@amueller ok, thanks; that answers my question. I was just curious as to whether it  might be straightforward to do this in `scikit-learn` but I was just missing something obvious :) __eou__	User

TP	I noticed scikit-learn is taking part in GSoC. I was wondering if https://github.com/scikit-learn/scikit-learn/issues/5736 would make a nice projecet? it has the advantage not being too technical I think @glemaitre  OK thanks __eou__	User

TP	@lesshaste The projects should be in line with the proposals specified there: https://github.com/scikit-learn/scikit-learn/wiki/Google-summer-of-code-(GSOC)-2017 __eou__	User

TP	@glemaitre  I have implemented a DecisionTree version in order to gain some insight regarding your fix #8458 .My approach during split is to remove the feature we split on and each child node doesn't contain that feature. But again we are going to have to look at say n-1 features .How did you overcome that. Your help can go a long way into helping me. Also I can make the code available to you. its actually from a book. Thanks. thanks @glemaitre . __eou__	User

TP	@shubham0704 The idea is to keep a list of splitter and when scanning a feature, each sample is distributed to the given splitter to evaluate if this is a best split. Once the feature is scanned, all the potential splits have been evaluated with a single scan. __eou__	User

TP	how much memory I need with PolynomialFeatures? I have data with shape (10374, 500) and I use interaction_only=True, but it gives me memory error I  think I have 8G limit for memory on that machine where I'm running it if I use 32 bit floats, would that be half of it? like ~5GB? __eou__	User

TP	@mkoske Well, `PolynomialFeatures(degree=2, interaction_only=True).fit_transform(np.ones((1, 500)))`  generates an array with 125251 features, so if you do that for all of your 10374 rows, it would produce a ~10.4 GB array (64 bit floats) and 8 GB RAM wont be enough.. __eou__	User
TP	@rth ok, thanks :)__eou__	Agent
TP	@mkoske True that could work..__eou__	User

TP	Hi all, I am having issues installing scipy for scikit-learn. Is winPython a good workaround? *thumbsUp Thanks a million __eou__	User

TP	@Ij888 I personally opted for conda. __eou__	User

TP	Okay thanks @glemaitre! Can I safely use pip and conda side by side? __eou__	User
TP	pip is included in conda__eou__	Agent
TP	I never add problem with up to now__eou__	Agent

TP	I'm persisting a random forest classifier using joblib, and in another process loading it. If I use sample_weights in fit, when I load the model it produces all 0 predictions. If I turn off sample_weights and train, everything is fine. During kfold validation, both settings produce expected results. Any ideas? __eou__	User

TP	@ccarter-cs If this is something that you can reproduce with a simple snippet you can open an issue __eou__	User
TP	@glemaitre Alright, I'll try to minimize. Was just wanting a spot check.__eou__	Agent
TP	you can use a simple `iris` dataset or something like that.__eou__	User
TP	it always easier to find out when there is code to check.__eou__	User
TP	if this is really a bug, this is a win-win :D__eou__	User

TP	ty for your help __eou__	User

TP	@glemaitre It was a problem with my predict code. I was not imputing values for a feature that doesn't matter in the non sample weighted case, but apparently does with the weighted samples. I've learned my lesson on using the same processing on both sides. =) __eou__	User

TP	@ccarter-cs :+1: __eou__	User

TP	Hey is sklearn not in GSOC 2017? __eou__	User

TP	Please all, what DIY level apps can I build with scikit-learn? So far I feel as if I have answers but no problems! __eou__	User

TP	@SatyaPrakashDwibedi it is if we find good students ;) @NelleV have you opened an issue on the f_regression issue we discussed? Could be good for the sprint tomorrow. __eou__	User

TP	@amueller  after attending your talk I was really looking forward to it. __eou__	User

TP	Which one? Yesterday? Or PyCon India? If you don't mind and don't want to do it yourself next week, it would be great if you could open one. __eou__	User
TP	doing it right now__eou__	Agent

TP	We expect students to having contributed before applying for doing a GSoC with us. I haven't been around much the last two month, have you contributed so far? __eou__	User

TP	@amueller I have not yet __eou__	User

TP	if it is not done by next week, I'll try to tackle this during the docathon __eou__	User
TP	thanks :)__eou__	Agent

TP	I'll try to review some of the pull requests of the sprint btw so don't hesitate to ping me on PR that you think I could help review __eou__	User
TP	thanks, that would be great :)__eou__	Agent
TP	it will mostly be "easy fix" issues, I think__eou__	Agent
TP	I tagged a bunch of stuff "sprint"__eou__	Agent
TP	as you might have seen on the right here lol__eou__	Agent
TP	yep, I saw__eou__	User

TP	FYI thanks for being in the top 100 scikit-learn contributors and not making us look like the worst possible ;) For an article about the sprint tomorrow I was asked about the state of women contributing to sklearn. Since counting contributors in a meaningful way is hard, I settled for the top 100 reported by github..... which has a 99:1 ration men:women unless I miscounted :-/ well let's hope we can do something about that tomorrow __eou__	User
TP	using an infinite timeframe?__eou__	Agent
TP	I think that's what the default is.__eou__	User
TP	yep, I am surprisingly still in the top 20 :)__eou__	Agent

TP	I'm hoping to contribute more, but matplotlib is still taking a lot of my time __eou__	User
TP	2013 till now you are still in the top 20 ;) I'm not sure any other timeframe makes us look more diverse__eou__	Agent
TP	matplotlib might need contributors even more, somehow it's not as "cool" as sklearn I fear. I wish I could contribute more there, but I can't even keep up with sklearn__eou__	Agent

TP	Hi, If interested, there's a semantic segmentation problem waiting to be solved here : https://github.com/chromosome-seg/DeepFISH __eou__	User

TP	Started the sprint :) as you might have notices ;) __eou__	User

TP	Hi, I am new to the community.. and still a beginner. I would appreciate if you could share with me any documentation on the development process and ways to involve.. that information would be very helpful thanks Andreas :) __eou__	User

TP	http://scikit-learn.org/dev/developers/contributing.html __eou__	User

TP	is codecov still commenting? __eou__	User

TP	hello __eou__	User

TP	while I am trying to fix an issue using the the inspect.signature method with python3, I am getting this error: /sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8 Have I built something wrong , or do you know how I can build again so as to be python3 compatible? https://github.com/scikit-learn/scikit-learn/issues/8194 I am running the rcheck.py to scan the modules I managed to build successfully with make using python3. Everything looks good now, I 'll get on with the fix Thank you Andreas! __eou__	User

TP	do you want to use the development version? Otherwise I'd suggest you just use anaconda or the wheels provided by pip What's the error you're trying to fix? how did you build scikit-learn? and how are you running rcheck.py? Make the installation procedure matches the python environment you are running __eou__	User

TP	Question: how large are typical parameter sets to `GridSearchCV`? And how large are larger-than-typical parameter sets? @jcrist and I are trying to decide how much we should care about overheads in Dask+sklearn work __eou__	User

TP	@amueller Any idea on when this pull request can be accepted:  https://github.com/ja9harper/scikit-learn/pull/1/files __eou__	User

TP	@reshama You have made a pull request to your fork... You should instead [raise one at the scikit-learn/scikit-learn](https://github.com/scikit-learn/scikit-learn/compare/master...ja9harper:crossdecompostionmethods)... __eou__	User

TP	Hello! how many slots does scikit-learn usually gets every year? __eou__	User

TP	Reinforcement learning __eou__	User

TP	hi would this room fall under wanting to learn about neural networks? __eou__	User

TP	Hello I am new to machine learning __eou__	User

TP	hello world __eou__	User

TP	Hi guys i'm new to scikit learning i want to learn more about data science i hope you can help me with that. __eou__	User

TP	@El-moro hi there! A first step is to take a look at the [tutorial](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) for scikit-learn. [This GitHub repo](https://github.com/hangtwenty/dive-into-machine-learning) might also be a good start for places to get your hands on data science and machine learning. __eou__	User

TP	hello guys __eou__	User

TP	Hey!!  Would implementing dropout in current Neural Network module count as proposal for GSOC?   "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" __eou__	User

TP	@geekSiddharth You should probably address your proposal to the mailing list instead of the gitter. You have more chance to get the attention of the core developer. Check the wiki page also regarding the topic which have an higher chance for the GSOC __eou__	User

TP	 __eou__	User

TP	Hi I have answered it __eou__	User

TP	Hello guys ! I'm a beginner in Machine Learning, do you recommend me a tutorial for  working on a dataset with sikit-learn ? __eou__	User

TP	@aniked we have two tutorials on our website, there is some on kaggle.com and there's videos on my website amueller.io __eou__	User

TP	@aniked Try intro to ML by udacity. __eou__	User

TP	@aniked You can also got with the [book](http://shop.oreilly.com/product/0636920030515.do) of @amueller __eou__	User

TP	Do you guys have any online courses/ bootcamp recommendation for someone that have been learning the basics of machine learning? I just finished the Machine Learning Coursera course and I would like to learn more deeply about the more advanced topics in machine learning. __eou__	User

TP	@amueller  @geekSiddharth @glemaitre  thank you so much guys :) __eou__	User

TP	@tonyvanhain check out the replies to @aniked above. I would rather not recommend bootcamps, as you'd have to pay for them usually, and some of them give me money ;) There's really a whole bunch of good free material online, though __eou__	User

TP	The docs for StratifiedShuffleSplit and StratifiedKFold are very similar. One says "This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class." and the other says "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."   I might not be the only person who finds this confusing what is the difference? is the only difference that the first one is randomized? __eou__	User

TP	there are no folds in StratifiedShuffleSplit the number of iterations is independent of the training set size __eou__	User

TP	hi all :) __eou__	User

TP	does sklearn have tests that you can run? __eou__	User

TP	@lesshaste Do you mean https://github.com/scikit-learn/scikit-learn#testing __eou__	User

TP	yes thanks __eou__	User

TP	are some errors expected? For example "bagging.py:747: RuntimeWarning: divide by zero encountered in log " __eou__	User

TP	Sometimes, you can have error or warning raised by scipy and numpy which will be catch I think the important things is that you get no output as ``` Ran 20 tests in 0.238s  FAILED (errors=2) ``` or with failure __eou__	User
TP	@glemaitre  OK__eou__	Agent
TP	I think it is all works fine. I was trying to get TPOT to work which stalls mysteriously on OS X and I wanted to check it wasn't a scikit learn problem. It isn't.__eou__	Agent
TP	Then if you fill this is a bug you probably want to put it back in the issue tracker in github__eou__	User
TP	it's a TPOT bug I think, not a scikit-learn one__eou__	Agent
TP	probably in fact a pathos bug__eou__	Agent

TP	Hello, I am willing to get the frequency of keywords + ngrams over a Russian text. I am having troubles using the CountVectorizer, here is what I did so far:  ```python corpus = open("russian-text-file").read() corpus = nltk.regexp_tokenize(corpus, r'(?u)\\b\\w+\\b', gaps=True) cv = CountVectorizer(ngram_range=(1, 10), vocabulary=["list of russian keywords + ngrams"], token_pattern='(?u)\\b\\w+\\b') results = pd.DataFrame(cv.fit_transform(corpus).toarray(), columns=cv.get_feature_names()) results_sum = results.sum() ```  `results_sum` shows that none of the keywords or ngrams are present in the text. However when I searched manually I could find them. Also, this code snippet worked with english text. Any help is appreciated, thanks! __eou__	User

TP	would someone mind interpreting a learning curve for me? Im not quite sure whether this represents an issue with my data. [![Screen Shot 2017-03-31 at 15.06.27.png](https://files.gitter.im/scikit-learn/scikit-learn/QgsH/thumb/Screen-Shot-2017-03-31-at-15.06.27.png)](https://files.gitter.im/scikit-learn/scikit-learn/QgsH/Screen-Shot-2017-03-31-at-15.06.27.png) it looks like were getting a 7% CV error, which is a bit high, but tolerable for this use case? my p/r values sort of suck __eou__	User

TP	@sbromberger You can read more about bias and variance at http://cs229.stanford.edu/materials/ML-advice.pdf __eou__	User

TP	I understand (or at least I think I do) about bias and variance. What I dont quite have down is whether or not my actual results above indicate excessive variance. __eou__	User

TP	The overlap of the shaded curve hulls around 150 an 175 training samples would bring doubt to me, if wether this presentation is useful to judge the parameters impact - but I am alien to machine learning ;-) __eou__	User

TP	Hy guys, I'm trying to learn scikit clustering, but can not get into final step before giving data into clustering algorithms, hope someone will be able to point me out direction of next step to get two dimmensional array from dataframe so it can be used by algorithms like MeanShift or may be DBSCAN or something else  ``` import pandas as pd df = pd.read_csv('http://sandbox.mac-blog.org.ua/sample.csv') # C1..C5 - categorical, D1..D10 - dates, B1..B28 - binary, 100K rows df = df.drop(['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10'], 1) # do not understand how to deal with this df = df.fillna(False) # looking around, all categorical data has values, treating NaN for all binaries as false  # going to convert all binaries into 0..1 ints for c in df.columns:     if c.startswith('B'):         df[c] = df[c].astype('int')  # totally not sure should such things be done print('Before:', len(df)) # 100000 df = df.drop_duplicates() print('After:', len(df)) # 35944  # not sure is it good idea at all # but after that I have reduced number of columns from 33 to 12 from sklearn.feature_selection import VarianceThreshold sel = VarianceThreshold(threshold=(.8 * (1 - .8))) sel.fit(df) labels = [df.columns[x] for x in sel.get_support(indices=True)] print('Before:', len(df.columns)) # 33 df = pd.DataFrame(sel.fit_transform(df), columns=labels) print('After:', len(df.columns)) # 12  # not sure do I need something like StandardScaler or LabelEncoder?  # every example of clustering algorithms like https://www.youtube.com/watch?v=EQZaSuK-PHs expect 2 dimensional array - kind of stuck here  df.head() ``` Seems that have found one way:  step 1 looking around on data   ``` from sklearn.decomposition import PCA      pca = PCA(n_components=2) pca.fit(df) existing_2d = pca.transform(df) plt.scatter(existing_2d.T[0], existing_2d.T[1], c='b') ```  step 2 clustering  in my case i definitely see 4 clusters so using kmeans  ``` from sklearn.cluster import KMeans km = KMeans(n_clusters=4) km.fit(df) df['Cluster'] = pd.Series(clusters, index=df.index) # append cluster column to dataframe ```  step 3 get usefull data  ``` desired = [] for col in df.columns:     if col != 'Cluster':         vals = [] # will contain top 1 value from each cluster         for cluster in list(set(km.labels_)):             vals.append(df[df['Cluster']==cluster][col].value_counts().head(h).reset_index().rename(columns={'index': col, col: 'Count'}).iloc[0][col])         if len(np.unique(vals)) > 1:             desired.append(col) # we are looking only for columns that are changing between clusters  xx = [] for cluster in list(set(km.labels_)):     x = {'Cluster': cluster}     for col in desired:         h=1         z = df[df['Cluster']==cluster][col].value_counts().head(h).reset_index().rename(columns={'index': col, col: 'Count'})         x[col] = z.iloc[0][col]     xx.append(x)      pd.DataFrame(xx) ```  not sure if this is a right way but got answer dataframe with 6 columns (5 categorical and 1 binary) describing top1 from each cluster (4 rows)  hope that may be helpful __eou__	User

TP	Looking at the digits dataset, the `DESCR` says ":Number of Instances: 5620" but when I examine the shape of the `data`, `target`, and `images`,  they all have 1797 instances.  Is this a documentation error or could I be missing something when I loaded the data? __eou__	User

TP	A curious observation: ``` from sklearn.feature_extraction.text import HashingVectorizer v = HashingVectorizer() v.transform(["a","b","c"]) ``` The result is  ``` <3x1048576 sparse matrix of type '<class 'numpy.float64'>'  with 0 stored elements in Compressed Sparse Row format> ``` It seems that HashingVectorizer will transform any single word into zero vector. Do I miss anything? look like the default "word" analyzer will transform single character into empty list ``` analyzer = v.build_analyzer() analyzer("a") ```  ``` [] ``` __eou__	User

TP	Hi any good book for Keras __eou__	User

TP	Folks, whats the easiest way to figure out the top features used by RandomForestClassifier for prediction?  Im using DictVectorizer to extract features. __eou__	User

TP	so, `class_weight` doesnt do what I expected it to do. __eou__	User

TP	OHE makes me feel like a handicapped person __eou__	User

TP	is there a sklearn preprocessing function/class that removes data points (examples) with values that are out of range? here is a snippet, i.e. remove all data x<lo or x>up. here is an example https://gist.github.com/ulf1/6810883a985ef464ae9d26833966b4fa __eou__	User

TP	@ulf1 you can use [`numpy.clip`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html) __eou__	User

TP	@glemaitre thank you for the tip. however it isnt exactly what i was searching for. numpy.clip substitute values that are out of range. my plan is to remove the row (or column) that contain out-of-range values. __eou__	User

TP	There is not transformer which remove samples in scikit-learn I think you could probably use the [`pandas.query`](http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.query.html) to make such request __eou__	User

TP	welcome to sonnet from DeepMind         https://gitter.im/TF_Sonnet/Lobby?utm_source=share-link&utm_medium=link&utm_campaign=share-link __eou__	User

TP	I think Ridge normalized=True is broken __eou__	User

TP	In [44]: Ridge(normalize=True).fit([[1000],[1], [500]], [1000,1,500]).predict([[1000],[1], [500]]) Out[44]: array([ 750.16666667,  250.66666667,  500.16666667]) In [45]: Ridge(normalize=False).fit([[1000],[1], [500]], [1000,1,500]).predict([[1000],[1], [500]]) Out[45]: array([ 999.99899867,    1.00100066,  500.00000067]) 'scikit-learn==0.19.dev0' __eou__	User

TP	@kootenpv Use the issue tracker from GitHub if you have a bug report __eou__	User

TP	I am getting a major error in Dictionary Learning.  The error is Segmentation fault (core dumped). It is working fine for 100 Images to train a Dictionary but If I use 200 Images to train a Dictionary then I am  getting Segmentation fault (core dumped). I debuged my code and got this one : :0x00007ffff3059f50 in ATL_dJIK0x0x48NN0x0x0_aX_bX () from /usr/lib/libblas.so.3 __eou__	User

TP	hello, i am trying to use sklearn.svm.svc with l1 or l2 regularization and can't seem to find how to, can anyone please help with me some pointers? please see stackoverflow question  http://stackoverflow.com/questions/43407896/python-sklearn-non-linear-svm-penalty __eou__	User

TP	Guys need some advice on Networkx. What's the best way to learn it. Need to perform some clustering on stackoverflow dump. __eou__	User

TP	@Pratyush3196_twitter the networkx docs are really good, but be aware that its not memory efficient AT ALL so youre not going to be able to use it at scale. __eou__	User

TP	Should I prefer Gephi over Networkx? @sbromberger __eou__	User

TP	@Pratyush3196_twitter networkx is good. How big are your graphs? __eou__	User

TP	@sbromberger. I am using it on the 200gb stackexchange dump. Around 180345 __eou__	User

TP	how many nodes/edges? __eou__	User

TP	nodes? how many edges? and have you tried loading this into networkx? I dont believe it can. __eou__	User

TP	But all of them won't be mapped. Lots of tags contain null so wont map them. Networkx has pretty straightforward documentaion. I read through it but gephi looks quite exquisite. @sbromberger It's my first DS project. Learning by doing so could use some good advice. __eou__	User

TP	gephi is a different package with different goals. It really depends on what you want to do. if you want to visualize data and play around with it in a gui, gephi is better suited. If you want to run advanced graph analysis, networkx is probably better. but both have limitations. __eou__	User

TP	Can I apply kmeans on the data on the gephi graphs? __eou__	User
TP	I dont know.__eou__	Agent
TP	Okay. It can be done in Networkx. Isn't it?__eou__	User

TP	but there are many clustering algorithms available for each. http://stackoverflow.com/questions/40602158/how-to-draw-networkx-graph-based-on-k-means-cluster-label may help you. __eou__	User

TP	Thanks! That was a great help. Is this feature also available in Gephi in some way? __eou__	User
TP	I dont know.__eou__	Agent

TP	Is there a way to combine LevenbergMarquardt algorithm with Stochastic Gradient Descent? __eou__	User

TP	~~~ Testing ~~~ __eou__	User

TP	Hi everyone,  (it seems that this channel changed its purpose...? anyway)  I don't want to bother the group but I recently posted [an issue when running kNN's](http://stackoverflow.com/questions/43284115/sklearn-knn-sklearn-neighbors-kneighbors-function-producing-unexpected-result)? It is in stackoverflow with the suggested changes by other users - still downgraded though.  Does anyone can have a look? If not, which other place would be the best one to post my question? Also: what other tests should I try before posting anywhere else to be sure I did all I could to verify the problem was completely evaluated? __eou__	User

TP	^^^  --- Problem above was solve. Thanks for the help. __eou__	User

TP	Can i talk C# in here? I am new to here __eou__	User

TP	@HamunSunga It does not seem the right gitter room since this is a scikit-learn room __eou__	User

TP	Can i use datetime variable as independent variable in case of building a classification model like Random Forest.? I'm a new to data science. Great if someone can help. Thanks __eou__	User

TP	since your datetime variable will be unique per observation, what do you expect to gain by making it a feature? __eou__	User

TP	I agree to with ur point. In my training dataset i see a relationship among the transactions happened with in a less span of time and in morning & evening are classified as suspicious . __eou__	User

TP	so you want to create a set of features based on the timestamp: perhaps the hour of the timestamp only (to determine morning/evening), and then some duration engineered from the rest of the data. __eou__	User

TP	Ok. So new features like hour and minute bin like 5 or 10 minute size will have to create. __eou__	User

TP	Hi I need to implement a multiclass text classification using python How? __eou__	User

TP	what sites do you recomend for learning c or c++ __eou__	User

TP	Is CountVectorizer's vocabulary_ attribute sorted by their occurences in the documents? __eou__	User

TP	hi everyone, i was tasked with setting up a dev environment for our teams projects and I accidentally installed the latest version of sklearn instead of .17      is there a way to downgrade myself? or do I have to uninstall and reinstall? __eou__	User

TP	@kaufmak2 with pip or conda or how? __eou__	User

TP	It says on the PyParis page that the scikit-learn dev sprint is going to happen during the conference - but this [link](https://github.com/scikit-learn/scikit-learn/wiki/Upcoming-events) mentions it happening a week before. Will there be 2 sprints in Paris? __eou__	User

TP	DataScience Digest, Issue #7 - http://bit.ly/2p9NaRc  Please share;) __eou__	User

TP	Hi __eou__	User

TP	@b0bcup_twitter  hello __eou__	User

TP	@bhargavvader The sprint will be the week before as mentioned in the wiki. __eou__	User

TP	Hello @theslothhermit  you may consider the new boston tutorial in youtube. C++ Programming Tutorials Playlist: http://www.youtube.com/playlist?list=PLAE85DE8440AA6B83 C Programming Tutorials: http://www.youtube.com/playlist?list=PL6gx4Cwl9DGAKIXv8Yr6nhGJ9Vlcjyymq __eou__	User

TP	hi all, i'm trying to run n_jobs>1 with a OneVsRestClassifier(LinearSVC()) and am seeing the following... is this due to lambda's creeping in somewhere recently? ``` Traceback (most recent call last):   File "main.py", line 24, in <module>     train(df, Y)   File "/home/ubuntu/model.py", line 90, in train     mod.fit(docs_train, labels_train) #mod to use grid search or "model" for the pipeline only   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py", line 216, in fit     for i, column in enumerate(columns))   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 768, in __call__     self.retrieve()   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 719, in retrieve     raise exception   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 682, in retrieve     self._output.extend(job.get(timeout=self.timeout))   File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py", line 608, in get     raise self._value   File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py", line 385, in _handle_tasks     put(task)   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py", line 371, in send     CustomizablePickler(buffer, self._reducers).dump(obj) AttributeError: Can't pickle local object 'train.<locals>.<lambda>' ``` is there a way to set dill or some other serializer instead? __eou__	User

TP	nm, found a lambda hidden away in my code somewhere... :) __eou__	User

TP	I need some help Means I write codes in pycharm and it will execute in Windows cmd prompt Plzzzzzzzzz Help me __eou__	User

TP	hello __eou__	User

TP	anybody here with experience handling EEG data ?  I'll be very grateful if you could answer a few questions __eou__	User

TP	@fel-mazo You have maybe more chance to find people on the mailing list of MNE: http://martinos.org/mne/stable/index.html __eou__	User

TP	from sklearn.decomposition import PCA  pca = PCA(n_components=2) pca.fit(df) existing_2d = pca.transform(df) plt.scatter(existing_2d.T[0], existing_2d.T[1], c='b') After doing the pca now I want to know which are the columns that have been selected by pca and only use those in my dataset df Any idea how to get the column names selected by pca? *pca = PCA(n_components=29) say I have a large value of columns 400 and I want only 29 of them but I want the name of those 29 columns selected by pca from df can anybody help please? Thanks __eou__	User

TP	Hi, in case I understood the use case right, you might want to look at feature_selection http://scikit-learn.org/stable/modules/feature_selection.html for selecting features since PCA doesnt select particular features but converts to n_components dimensions where each component is a weighted sum of the features. Hope it helps. __eou__	User

TP	@maniteja123 :+1:  @puneetmathurDS each component is a "bit of all columns" __eou__	User

TP	@glemaitre thanks ! __eou__	User

TP	I have a doubt  in my code ``` from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_breast_cancer from sklearn.cross_validation import train_test_split cancer = load_breast_cancer() X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify=cancer.target, random_state=42) tree = DecisionTreeClassifier(random_state=0) tree.fit(X_train, y_train) print(tree.score(X_train, y_train)) print(tree.score(X_test, y_test)) ``` The first random_state is in test_train_split is used for  getting the same result next time when we are gonna run the code. But I don't get why is there a random_state in DecisionTreeClassifier line ? And how does that work ? @amueller  .. __eou__	User

TP	@ashiskriz There is some randomness in the decision tree. For instance a subset of feature can be taken at each note with a randomization. Having this in mind the random_state allows to have this part deterministic as well __eou__	User

TP	@glemaitre  - Thank you very much . Thats helpful  And I  want to add that the book I was following  written by @amueller  had a sentence saying "We fix the random_state in the tree, which is used for tiebreaking internally"  . I was wondering about the mechanism of tie breaking thing . How is the tie breaking thing happens internally? __eou__	User

TP	 __eou__	User

TP	@punitaojha you have posted the exact same message in https://gitter.im/Machine-Learning-Group/chat too. That's I believe a spammy behavior... __eou__	User

TP	Hey Guys, I am planning to build a ML and probabilistic modelling libray in Python. Would like to know if anybody is interested to start the project with me? __eou__	User

TP	@ashiskriz at each node, a random set of feature will be used to find a best split. If there two features leading to a split with the same impurity improvement, you get a tie. Therefore, the first feature which was randomly picked up will be selected. If you try multiple times, you will select one feature or the other which will lead to different trees architectures. Therefore, random_state allows you to pick up always the same feature in case of a tie. __eou__	User

TP	@anisnouri how will it be different from current existing libraries? __eou__	User

TP	@jmschrei  would be much focused on quantifying uncertainty. __eou__	User

TP	Have you looked into PyMC3 and PyStan, and the libraries built on top of those? __eou__	User
TP	@jmschrei  yeah. I find them a bit compound and complex to use. Would be nice to have something with the same APIs design as sklearn__eou__	Agent

TP	You should probably reach out in those communities then, if you'll be building on top of it. I think that learning Bayesian models like that is much more niche than classical machine learning. __eou__	User

TP	@glemaitre   Thank you very much  .Really appreciate your help. Great explanation there __eou__	User

TP	@punitaojha <unconvertable> You know why we are here. __eou__	User

TP	pbppppppppppapoppppp __eou__	User

TP	Hello, I'm looking for some help with a code I wrote. It seems straightforward enough but it takes forever (its not even run it yet and I've had it running for hours) to perform a 5-fold cross validation bit on an 800 x 18 dataframe. This seems to be where my code hangs: __eou__	User

TP	``` kf = KFold(800,5) score = [] for i in range(1,1001):      clf = SVC(C=i/100, kernel='linear')      error = []      for train_index, test_index in kf:           X_train, X_test = train_data[train_index], train_data[test_index]           y_train, y_test = train_label[train_index], train_label[test_index]           clf.fit(X_train,y_train)           error.append(1 - clf.score(X_test, y_test))      score.append(sum(error)/5) ``` __eou__	User

TP	you can use the [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to do what you try to do Also I think that you are using the `cross_validation` module which you should move away from since it is deprecated Use the `model_selection` where you have the refactored cross-validation classes long story short, you can write your code as Then it seems that you try to actually find the best `C` parameter probably. To do that you should use the [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class. ```python from sklearn import svm, datasets from sklearn.model_selection import GridSearchCV  parameters = {'C': [1, 10, 100, 1000]} svr = svm.SVC(kernel='linear') clf = GridSearchCV(svr, parameters) clf.fit(X, y) ``` It will search and select the best `C` parameter __eou__	User

TP	```python from sklearn.model_selection import KFold from sklearn.model_selection cross_val_score from sklearn.svm import SVC  clf = SVC(kernel='linear') print(cross_val_score(clf, X, y))  ``` __eou__	User

TP	OK. Thank you so much @glemaitre. I'd try that Yes, I am trying to find the best C parameter __eou__	User

TP	In your example you are trying all the 1000 possible `C` values and it could be pretty slow. You can also use `n_jobs=-1` to take advantages of all the CPU cores this is actually related to an SVM with different kernel and C values __eou__	User

TP	Oh. Sorry, I'm still new at it so, I don't know how to use the n_jobs yet __eou__	User
TP	`clf = GridSearchCV(svr, parameters, n_jobs=-1)`__eou__	Agent
TP	is enough__eou__	Agent
TP	clf = GridSearchCV(svr, parameters)__eou__	Agent
TP	you can check the examples in the doc__eou__	Agent
TP	OK. Thanks a lot!__eou__	User

TP	The task was to check for the best C value between 0.01 and 10 __eou__	User

TP	`GridSearchCV(cv=None, error_score='raise',        estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',   max_iter=-1, probability=False, random_state=None, shrinking=True,   tol=0.001, verbose=False),        fit_params={}, iid=True, n_jobs=1,        param_grid={'C': [1, 10, 100, 1000]}, pre_dispatch='2*n_jobs',        refit=True, return_train_score=True, scoring=None, verbose=0)` I guess this implies the best estimator is C = 1.0 ? __eou__	User
TP	yep__eou__	Agent
TP	Wow! Thank you. You are a life saver!__eou__	User
TP	Cool__eou__	User

TP	the returned classifier is this one __eou__	User
TP	I've been trying to do this for days!__eou__	Agent
TP	you can increase the number of C to visit if you want as well__eou__	User
TP	OK__eou__	Agent
TP	How do I do that?__eou__	Agent
TP	By changing param_grid?__eou__	Agent
TP	yep__eou__	User

TP	`parameters = {'C': np.logspace(-1, 2, num=20)}` Don't hesitate sometimes to check the tutorial http://scikit-learn.org/stable/tutorial/ __eou__	User
TP	Ok. Thank you__eou__	Agent
TP	http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html__eou__	User
TP	was actually the one you wanted ;)__eou__	User

TP	I'm new at sci-kit and machine learning. So sometimes, my solutions are not optimal and my system is rather slow Awesome. I'd go through it Yes, it is :D __eou__	User

TP	Please does it make sense to find the optimal gamma for a polynomial kernel? Most of the examples I've seen searching for gamma have the kernel as linear or rbf I tried using grid search, but it doesn't seem to be the right function for getting the gamma in this case (it seems slow in processing) __eou__	User

TP	 __eou__	User

TP	 Hi all,  Can I get some indepth resource on RFE(Recursive features elimination) ? I have went through the documentation but it will be good if I could get any detailed  example which uses RFE. __eou__	User

TP	[sklearn-porter 0.5.0](https://github.com/nok/sklearn-porter) has been released :sparkles:. The [MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) is the first supported regressor! That's one small step for a man, one giant leap for scientists. :smile: __eou__	User

TP	thats pretty cool, @nok __eou__	User

TP	Thanks :smile: __eou__	User

TP	Itd be nice to get Julia support into this :) __eou__	User

TP	does anyone happen to know if scikit-learns implementation of coordinate descent (for stuff like Lasso, ElasticNet, etc.) is stochastic? i.e., does sample order matter? (assuming random seeds are set and all that other stuff) __eou__	User

TP	I have been trying SVC and have a doubt. C is referred to the penalty term, therefore with increase in the value of C, the algorithm try's to reduce the number of wrong classified and with very high value of C it may overfit the data. Can we say that with overfitting, the number of support vectors would increase. And if we use a lower value of C the number of support vectors will be less ? But the results I got were different, with small value of C, the model was underfit and the number of support vectors were huge __eou__	User

TP	Can anyone explain, what is wrong with my thinking :P __eou__	User

TP	no don't you know anything? __eou__	User

TP	@sbromberger Currently Im not familiar with Julia, but it seems to be easy. I will have a look at the necessary parts. __eou__	User

TP	@amitmanchanda1995 https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel __eou__	User

TP	@amitmanchanda1995 Furthermore  the parameter `n_support_`  of an estimator gives you the number of used support vectors for each class: https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/svm/classes.py#L484-L485 __eou__	User

TP	@nok - its a very cool language, and it has bindings to sklearn :) __eou__	User

TP	@nok thanks. Previously I thought only the vectors present on the boundary lines were only considered to be support vectors, but all the misclassified and the vectors on the boundary lines are support vector so if we increase C the number of support vectors will decrease. __eou__	User

TP	S __eou__	User

TP	@nelson-liu sample order shouldn't matter but feature order. Some pick the coordinates at random, I think. @amitmanchanda1995 C is an upper bound on the dual coefficents, and if you restrict the dual coefficients more (make C smaller) you'll have less zero alphas, i.e. more support vectors __eou__	User

TP	Thanks @amueller. Can you also help me the following *In predict_proba of SVC* > Notes > The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.  What do we mean by cross validation in case of prediction, what I get by cross validation is that the training data is divided into K sets, and K-1 is used for training while the 1 set is used for testing. And it is repeated K times __eou__	User

TP	check out the paper by platt it's linked to in the description predict_proba uses platt scaling internally __eou__	User

TP	Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods, J. Platt, (1999) __eou__	User
TP	Thanks__eou__	Agent

TP	maybe also Predicting Good Probabilities with Supervised Learning, A. Niculescu-Mizil & R. Caruana, ICML 2005 __eou__	User

TP	`DBSCAN` takes a long time relative to K-means. __eou__	User

TP	Hello!   Am I right that https://github.com/scikit-learn/scikit-learn/pull/6015 is also within the sprint scope? Can I take it? __eou__	User

TP	@n0mad review it you mean? __eou__	User

TP	Anyone tackling this already https://github.com/scikit-learn/scikit-learn/issues/8899 ? Seems easy and would be useful for hmmlearn as well. __eou__	User
TP	@amueller I had impression it did not fully address https://github.com/scikit-learn/scikit-learn/issues/5879  so I thought I could continue working on it?__eou__	Agent

TP	@superbobry You can go ahead if there is not PR yet __eou__	User

TP	@jnothman : do you want to do a video hangout at some point, mostly to say hi and join the excitement? @jnothman I believe that it's 8:30PM your time. We are currently having the lunch break. So maybe in a little while will be good. We can put you on a big screen __eou__	User

TP	I'm okay with small screens. Sounds like fun. I'm currently having my dinner too. __eou__	User

TP	OK, after dinner? Or will you be busy? If you want: ping me on google hangout __eou__	User

TP	Should be alright when all the feasting is over __eou__	User

TP	Everybody is gone to take a break outside now :) __eou__	User

TP	No hurry on my part! There's washing up to do. __eou__	User
TP	When you want, everybody is back__eou__	Agent
TP	Child calls.__eou__	User

TP	I'll be with you in a couple of mins __eou__	User

TP	@tguillemot also https://github.com/scikit-learn/scikit-learn/pull/9030 https://github.com/scikit-learn/scikit-learn/pull/9029 ;) __eou__	User

TP	@jnothman I think @mblondel is going to ICML __eou__	User
TP	Haha I think__eou__	Agent

TP	All sprinters: we are having a sample-props discussion from 15:00 to 16:00, and a group picture at 16:00 hopefully on the roof __eou__	User

TP	if you want you can find a spec for sample-props  I have done several month ago to launch the discussion : https://github.com/tguillemot/sample_props_spec/blob/master/sample_props_spec_v2.md __eou__	User

TP	thanks @tguillemot, I'll reread __eou__	User

TP	Update: group picture: 16:30 sharp: Ibrahim will be there to bring us up to the roof __eou__	User

TP	simple fix for making BaseSearchCV (more) API compatible: https://github.com/scikit-learn/scikit-learn/pull/9038 __eou__	User

TP	hi all, I wish to calculate the confidence of an `SVC` prediction, but I have a hard time understanding the result I have fitting data like this ```python X = np.array([   [0, 0, 0, 0],   [1, 1, 1, 1],   [2, 2, 2, 2],   [3, 3, 3, 3]  ])  y = np.array([  0, 1, 2, 3  ])  clf = svm.SVC(C=1, kernel='rbf', probability=True) clf.fit(X, y) ``` Then I predict the class for input `x = np.array([1, 2, 3, 4])`, and I the output makes sence ```python clf.predict([x]) # array([1]) ```  But the probability does not make sense at all ```python clf.predict_proba([x]) # array([[ 0.2550524 ,  0.16488868,  0.25497241,  0.3250865 ]]) ```  According to the probabilities, class `1` has the lowest possibility. Why is that? __eou__	User

TP	what's x? @nlhkh check out the docs, it tells you that there can be inconsistencies between the probability estimate and the prediction, because of the platt scaling that is used to create the predictions you can also look at decision_function and that should be consistent __eou__	User
TP	oh, sorry__eou__	Agent

TP	```python x = np.array([1, 1, 1, 1]) ``` I tried the `decision_function` too Could you tell me a bit more what it means? __eou__	User

TP	I can explain to you what happens: the probability estimate uses cross-validation to create the probability estimates, but there's only one data point for each class, so the models will be pretty useless because as soon as you leave out some points you leave out the whole class if you're trying to understand the svm algorithm, don't look at the probabilities what is your goal? on this dataset? ok. well then it will work well the example is done in a way that breaks the method that is used to get the uncertainty __eou__	User

TP	my goal is to determine how confident a prediction is so as to declare that a prediction is not reliable no __eou__	User

TP	on a much bigger dataset this is just for example __eou__	User

TP	so when I do this `clf.decision_function([x])`, I get this  ``` array([[-0.0296443 , -0.22257708, -0.22257708, -0.19293278, -0.19293278,         0.        ]]) ``` how should I interprete these numbers? __eou__	User

TP	predict_proba they are estimated probabilities of the classes for decision_function they are unnormalized, so higher means better but there is no absolute scale __eou__	User

TP	```python import numpy as np from sklearn import svm X = np.array([         [0, 0, 0, 0],         [1, 1, 1, 1],         [2, 2, 2, 2],         [3, 3, 3, 3]     ]) X = np.repeat(X, 10, axis=0)  y = np.array([     0, 1, 2, 3     ]) y = np.repeat(y, 10)  clf = svm.SVC(C=1, kernel='rbf', probability=True) clf.fit(X, y) print(clf.predict([[1, 1, 1, 1]])) print(clf.predict_proba([[1, 1, 1, 1]])) ``` __eou__	User
TP	ohhh, now it makes better sense__eou__	Agent
TP	so I do not have enough data for cross-validation__eou__	Agent

TP	in your example, yes __eou__	User
TP	so in this case__eou__	Agent
TP	the decision_function is `array([[-1.00000001,  0.        ,  0.3495638 ,  1.00000001,  1.        ,         0.5530018 ]])`__eou__	Agent
TP	the evaluation for class 1 is 0__eou__	Agent
TP	so close to 0 is better?__eou__	Agent
TP	isnt that the distance to the hyperplane?__eou__	Agent
TP	no__eou__	User
TP	with this code I get__eou__	User
TP	[[ 1.89159397  3.5         0.9255003  -0.31709426]] In [ ]:__eou__	User
TP	print(clf.decision_function([[1, 1, 1, 1]]))__eou__	User
TP	higher is better__eou__	User
TP	I dont get the same output as yours__eou__	Agent
TP	no no__eou__	Agent
TP	I copied your code__eou__	Agent
TP	it returns an array of 6 elements__eou__	Agent
TP	then you didn't copy my code__eou__	User
TP	ohhh wait__eou__	User
TP	sorry__eou__	User
TP	set decision_function_shape='ovr'__eou__	User
TP	if you have scikit-learn 0.17__eou__	User
TP	you should upgrade to 0.18__eou__	User
TP	SVC.decision_function is / was really weird as well__eou__	User

TP	there's a 3.5 for class 1, that's why class 1 is predicted it's always the argmax of the decision function you're running different code then ;) __eou__	User

TP	it does say this "The decision_function_shape default value will change from 'ovo' to 'ovr' in 0.19. This will change the shape of the decision function returned by SVC. <unconvertable> :) __eou__	User
TP	yeah__eou__	Agent
TP	I wrote this ;)__eou__	Agent
TP	make it ovr__eou__	Agent
TP	then it'll have 4 elements__eou__	Agent
TP	I'm on 0.19-dev__eou__	Agent
TP	sorry about that__eou__	Agent
TP	I see :)__eou__	User
TP	it is an honor__eou__	User
TP	ok starbucks is closing and I have to write a keynote__eou__	Agent
TP	good luck__eou__	Agent
TP	have fun!__eou__	User

TP	Hello, I need some help with C++ Coding __eou__	User

TP	@MoreToDo have you tried stackoverflow? __eou__	User

TP	I've requested some very minor changes at #8096 and gather that @dalmia won't complete it. A volunteer to take it over? __eou__	User

TP	Need any reviews anyone? __eou__	User

TP	@jnothman  do you still need someone for #8096 ?? __eou__	User
TP	Yes__eou__	Agent

TP	@raghavrv I will need one soon https://github.com/scikit-learn/scikit-learn/pull/9058 he it goes __eou__	User

TP	Aye master on it :) __eou__	User

TP	@jnothman see #9059 __eou__	User

TP	hey all, please point me to a "Definitive g" hey all, don't mean to bother but please I'd appreciate if I could be pointed to a "Definitive guide to using Scikit-Learn" -style tutorial walkthrough. Peace <unconvertable> __eou__	User

TP	Did you miss this http://scikit-learn.org/stable/ ? __eou__	User

TP	@Ij888  There is http://scikit-learn.org/stable/tutorial/index.html or you can buy a book such as: http://shop.oreilly.com/product/0636920030515.do __eou__	User

TP	thanks @dohmatob and @ogrisel . I tried working through the docs on  http://scikit-learn.org/stable/tutorial/basic/tutorial.html but it got rather dense at the **Learning and predicting** section especially at `clf.fit(digits.data[:-1], digits.target[:-1])`. I am at a loss to what some of the parameters in `SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)` represent. __eou__	User

TP	you don't need to know the meaning of them all at first but you should check the API documentation of the SVC class at some point http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html __eou__	User

TP	thanks for the tip @ogrisel  <unconvertable> __eou__	User

TP	Hey guys! I have an idea for an app and I was hoping i could find rediculously passionate people who could come work with me. it has machine learning integrated to bring progress to mankind. __eou__	User

TP	I think people in this room are ridiculously passionate (and incredibly busy) in building tools for other people to build apps to bring progress to mankind. ;) __eou__	User

TP	ajajajajaaj __eou__	User

TP	@ogrisel Haha yeah that's pretty evident :') so you wouldn't be able to help me? __eou__	User

TP	I am sorry, no. You should probably setup a proof of concept of your own if you want to recruit others to join your project. __eou__	User

TP	Hi, I am a veteran R/Rcpp developer wanting to transition to being a sklearn contributor. There are a lot of classes and I think a consistent coding style in the sklearn code base - I'm wondering if there are any docs or presentations on the overall architecture (class hierarchy, the way helper functions should be organized, etc) that could help me understand the flow and be able to contribute? __eou__	User

TP	@jeffwong I would check the tutorial http://scikit-learn.org/stable/tutorial/index.html The introduction will give you the convention and bases regarding the estimator __eou__	User

TP	@jeffwong and the contributors doc: http://scikit-learn.org/stable/developers/contributing.html __eou__	User

TP	Is there any way to incorporate cost matrix in SVM classification as is available in weka https://weka.wikispaces.com/CostSensitiveClassifier __eou__	User

TP	@axay possibly the class_weight parameter, though that doesn't allow specifying a full matrix, only relative weights It also depends a bit on whether you want to do that for training or for predicting with a trained model or both. It looks like weka influences the training. false positives for that label? I think it would make most sense to calibrate the prediction thresholds for that, but scikit-learn doesn't have tools for that yet. you can try to change the class-weights, but the control they give is a bit indirect do you know how weka implements this? the scikit-learn class_weights for SVC are multipliers to C so that you get one C per class, as described in the libsvm paper __eou__	User

TP	weka supports both afaik. My aim is to prevent misclassification of other labels as a target label for example if i have 3 labels A, B and C, I don't want A and C to be predicted as B yeah no, haven't gone through their code yet. __eou__	User

TP	Hi, this is probably not the right forum to ask it, but not sure where else to go: How do I get permission to use the scikit-learn machine learning map (ML-map.png) in a published document? Obviously with full citation!  Thanks. Available from this link: http://scikit-learn.org/stable/tutorial/machine_learning_map/ __eou__	User

TP	Hi, is there any way to incorporate self-learning in SVC. By self-learning I mean if a datapoint is being predicted class A with accurary 0.7, then if I add same datapoint again in the corpus, then the prediction accurary should increase from 0.7 like in case of Neural Nets. But looking at the mathematics, if the datapoint is not a support vector then it is not considered in cost function. Then what should I do to increase the accuracy? predict_proba of SVC gives me a confidence score of 0.7. Can I seed it back into your model and hope that the next time confidence score for the same datapoint increase __eou__	User
TP	that's not the same as accuracy. and SVC is not probabilistic. And was it in the original training set? self-learning is for semi-supervised learning__eou__	Agent
TP	I think SVC uses sample_weight that you can increase__eou__	Agent
TP	hm...__eou__	Agent

TP	Self learning is a meta-algorithm for semi-supervised learning and should work with any supervised model. I'm not sure what you mean by a point being predicted with accuracy 0.7 __eou__	User

TP	yeah sorry that was a mistake. what I want is once I deploy a model in production, it somehow keeps on improving. I was thinking that if the confidence score of an unseen data point is above a certain threshold (say 0.7) i add it to the training set and retrain . Is it a good idea to use this with SVC ? __eou__	User

TP	yeah that can work. self learning can have problems with concept drift __eou__	User

TP	Hello, I need some assistance with C++ homework.  let me know if anyone can assist! __eou__	User

TP	hey __eou__	User

TP	anyone seen an error like this when building from source? I get this with conda Cython but not pip Cython (0.25.2 in both cases, Python 3.6.1) ``` [ 2/39] Cythonizing sklearn/_isotonic.pyx Traceback (most recent call last):   File "setup.py", line 267, in <module>     setup_package() ... "/Users/brettnaul/miniconda3/envs/py361/lib/python3.6/copy.py", line 169, in deepcopy     rv = reductor(4) TypeError: can't pickle Cython.Compiler.FlowControl.NameAssignment objects ``` __eou__	User

TP	hi guys ,i m coding in c ,and i want to store values from a txt file to an array any idea ? __eou__	User

TP	@Krikbov  Bad chance that scikit-learn is in Python __eou__	User

TP	Hello my name is priyam and i am interested in contributing to scikit-learn.Can anyone help me i am completely new in open source world. __eou__	User

TP	@bnaul that's really weird. Do you have the full Traceback? It seems that the interesting part is under the setup_package function. __eou__	User

TP	@satishjasthi Hi all, Anyone knows how to visualize high dimensional data in 2d image We have 25 instances each of dimension 1800. We want to visualize each instance separately and then build a classifier on it. Please help can We visualize each instance separately in t-SNE __eou__	User

TP	You can do a scatter plot of a t-SNE embedding to get some intuition on the structure of your data.  But you should not train the classifier on the 2D project. It's very likely that you will get much higher predictive accuracy by training the classifier on the original high dimensional data. There is a TSNE class in scikit-learn but it has many bugs. We would like to solve them for the next release but it's not ready yet. I would advise to have a look at https://github.com/lvdmaaten/bhtsne . The scatter plot will put on dot in the 2D plane for each instance. What you visualize is the neighborhood structure of your data. http://distill.pub/2016/misread-tsne/ You should also try with 2D PCA of your data. This is much faster to compute alhough generally the neighborhood structure can be much blurier. I don't understand what it means to "visualize a 2D image or a single 1800 dimensional vector". what are you features ? which data types ? what is the physical meaning of each of them ? you already said that, please answer my question if you want me to be able to help you. There is no generic way to visualize a single feature vector of a CNN. __eou__	User

TP	No. We dont want a dot for each instance. We want an altogether different 2D image for each instance __eou__	User

TP	We have 1800 features for each instance. We then need to visualize these 1800 features in a single image. Repeat this process for each instance. And then build a classifier on it we got these 1800 features after doing CNN on the image RGB image, we performed CNN, and then extracted features are of dimension 1800 We have 25 such images so it becomes 25X1800 __eou__	User

TP	If you want to visualize the distribution of your 25 images embedded in the 1800 dimensional CNN space,  then you can use PCA or TSNE, for instance using http://projector.tensorflow.org/ (online web interface) or https://github.com/lvdmaaten/bhtsne to do that programmatically. Also if you want to train an image classifier, I would advise you to build a training set with at list 100 images per class. 25 is probably far too few even if you do binary classification. __eou__	User

TP	Hi all, has anyone in scikit-learn tried to import some of the surrogate model methods used by pyKriging or inspyred? __eou__	User

TP	Hi all, So we have 25 images of dimension 400X488. We have labels assigned to each image as 1/0 . There are 14 1s and 11 0s. What we need to do is we need to find pixels that differentiates 1 from 0. Also along with these differentiating pixels we also need to allot ranking to them based on their frequency in images with label 1.  Any thoughts on how to do it? @ogrisel Thank you Hi all, So we have 25 images of dimension 400X488. We have labels assigned to each image as 1/0 . There are 14 1s and 11 0s. What we need to do is we need to find pixels that differentiates 1 from 0. Also along with these differentiating pixels we also need to allot ranking to them based on their frequency in images with label 1.  Any thoughts on how to do it? __eou__	User

TP	@satishjasthi please do not spam this channel with repeated questions. The way you pose your image classification problem is very non standard. What does ranking pixels mean? Why do you want to do that in the first place? Is this a natural image classification problem? In that case you will need much more than 25 labeled images to do anything useful as I already told you earlier. If those are not natural images then you should probably give more details on the nature of the images. Which objects are in the images? What are the two labels 0 and 1 (what do they mean)? Have the images been recorded with a traditional camera with three color channels or is it a very specific recording device? __eou__	User

TP	Hi all, I want to revisit the question regarding how to interpret the output of `decision_function`into something meaningful. With the output from `decision_function`, how can I determine if a prediction is trust worthy or not, given some threshold `alpha`? __eou__	User

TP	There is no generic method, it depends on your estimator and the kind of multiclass reduction used by the underlying model (e.g. One vs One for SVC, One vs All for most other models such as LinearSVC). You might also have heteroschedastic prediction errors: some regions of your feature space might lead to a higher error rate and this heteroschedasticity might not be part of the model assumptions (or maybe cannot be handled properly by the model capacity). __eou__	User

TP	Which model family are you interested in? One interesting thing to consider is using a calibrated classifier (especially for binary classification): http://scikit-learn.org/stable/modules/calibration.html This way you can better interpret the output of `clf.predict_proba`. This won't solve any heteroschedasticity issues though. What you can also do is plot the precision / recall curve, select an admissible precision level (e.g. 0.8) according to business considerations and select the model and threshold that maximizes the recall at that precision level. We currently don't have a high level API to do this but the http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html function of scikit-learn returns the thresholds for each point along the PR curve. Once you have fitted your best model, you can compute its precision and recall on a held out test set to check that this is still fine, and further analyse that you don't have strong heteroschedasticity in your precision / recall metrics. For instance if you use your classifier to build a recommender systems for users, you can check that you get approximately similar performance for various ways to split your user base, e.g.: male vs female users, age groups, geography, very engaged users vs casual users... __eou__	User

TP	@ogrisel thanks Olivier :smile: I'll give it a try I am doing facial recognition, so it's multiclass __eou__	User

TP	you should definitely try to label your test dataset with additional info such as facial hair, glasses, long hair vs short air, gender, maybe ethnicity so as to compare the accuracy of your model for different groups of people. __eou__	User

TP	i have  some customers data what information can i get from there using machine learning hi there @ogrisel  hi there __eou__	User

TP	Lol wut __eou__	User

TP	when I use `GridSearchCV`, how can I find the actual parameters being used for a classifier? the `get_params` method seems to print everything in the search grid thanks Olivier :) __eou__	User

TP	`grid_search.best_params_` after fit __eou__	User
TP	ah ok__eou__	Agent

TP	test #9032 __eou__	User

TP	I'm using sklearn '0.15.0b1'.  I think I found a bug - when using lm.Ridge, if XtX is singular, the problem is still solved but sample_weight is ignored.  Is this worth filing an issue for?  I tried searching to see if this has been reported/resolved but I didn't find anything __eou__	User

TP	@hhuuggoo that's a *really* old version and a beta release. Can you please update to master and see if the problem persists? __eou__	User
TP	yea I'll check__eou__	Agent

TP	ok cool looks like it's correct in sklearn 0.18.2 __eou__	User
TP	great :)__eou__	Agent

TP	I'm close to defeating my arch enemy, dot: https://user-images.githubusercontent.com/449558/27715089-d81ef462-5d02-11e7-806b-87ac0753ecbe.png (pure matplotlib) __eou__	User

TP	:) __eou__	User

TP	Maybe you should make all boxes have the same width and left align the content:  ```text petal width(cm) split: <= 1.65 entropy: 0.041 samples: 48 value: 0, 47, 1 class:  versicolor ``` __eou__	User

TP	cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccp __eou__	User

TP	Hello __eou__	User

TP	l __eou__	User

TP	Hello good people! If I may enquire,  is there any advantage for using one-hot encoding over label encoding with GradientBoosting algorithm? __eou__	User

TP	Hi, Is there a particular clustering algorithm that works best with time series? __eou__	User

TP	@arnawldo, LabelEncoder is not intended for features. __eou__	User

TP	@jnothman so given my feature is categorical,  should I just replace the levels with integers, or spread it across columns? __eou__	User

TP	We have had discussions on this topic on the mailing list. Either should work alright with trees. Integers will require deeper trees to select for (or against) a single category. So the model parameters need to be tuned to whichever approach you take. __eou__	User

TP	@jnothman great. How can I see this discussion and learn more. I'm not on this list __eou__	User

TP	https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/ suggests you are better off with integers, fwiw __eou__	User

TP	@jnothman Thanks a lot __eou__	User

TP	Hello __eou__	User

TP	Docs for nearest neighbors says "The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset...". Why it is so? __eou__	User

TP	Because BallTree and KDTree require true metrics and dense data; and because brute force pairwise distances may be faster for small datasets. __eou__	User

TP	I mean, why it is pairwise (O(dn^2))? Why you have to compute distances between all pairs of points? I'm trying to understand the time complexity of the kNN, that's why I ask. isn't it then like O(knd), not O(dn^2) ? __eou__	User

TP	hmm, it is hard to say. it is the nature of kNN. you can find some visualization of kNN on the Internet and you will understand well, let's say for  one new data point how do you know the k-nearest neighbors of this new one? exactly so for one data point you need n calculation __eou__	User
TP	yes__eou__	Agent
TP	so ?__eou__	User
TP	:D__eou__	User
TP	cool__eou__	User
TP	let me check again__eou__	User
TP	:D__eou__	User
TP	I suppose it means you find kNN for all N__eou__	User
TP	I can understand that, but why I would like to do that?__eou__	Agent
TP	hmm, I have no answer for this in fact__eou__	User
TP	:D__eou__	User
TP	https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity__eou__	User
TP	https://nlp.stanford.edu/IR-book/html/htmledition/time-complexity-and-optimality-of-knn-1.html__eou__	User
TP	yep, they say the same thing__eou__	User

TP	calculate distance from this new point to all training points and keep k closest? __eou__	User
TP	yep__eou__	Agent

TP	Yes, I was just reading that... the example I gave was from that (crossvalidated I mean) __eou__	User

TP	It seems that people just think it different ways. And probably different implementations. That's why they differ Well, the answer at the Cross Validated says it depends on "algorithmic choices" Doesn't it mean different implementations? __eou__	User

TP	I wouldnt say so. brute-force kNN is very straightforward __eou__	User

TP	O(knd) is referring to the cost *per query*. If you have n queries, then the cost is O(kn^2d) and by per query I mean per instance in your query With a binary tree index, the k neighbor search should be O(k log n) *per query* (ignoring some factor related to d) __eou__	User

TP	Ok, it's just confusing when the complexity of brute force is stated to be O(DN^2) i.e. N queries in O(DN) time but for KD-tree it's stated to be O(log N) i.e. only for one query. Or am I missing something here? Should it be made more clear in the docs that it refers to computing nearest neighbors for N query points? __eou__	User

TP	Also, I looked at code and if I'm not mistaken, it  calculates pairwise distances between query points and training points using pairwise_distances() function. In this case, if there's M query points, it would be O(DMN) and only if M is close to N in size, then it's O(DN^2), right? __eou__	User

TP	sure. a pull request is welcome which removes the ^2. I think the current implementation is actually something like O(knd log k)... __eou__	User

TP	Ok. Where does the log k come from? __eou__	User

TP	we sort the k neighbors so nearest is output first. which we perhaps need not be doing. __eou__	User
TP	Oh, I saw that on the code... thanks for helping__eou__	Agent

TP	Hi All! Is there an option to run k-median incremental algorithm with the library? Thanks __eou__	User

TP	It is not provided in the library, but there might be a compatible implementation outside. There is an implementation of K Medoids under review, but this is not the same thing. __eou__	User
TP	thanks__eou__	Agent

TP	Hi all, I have a program to run pipeline with featureunion infinitely by putting into fit_transform function and training data. So may I know how to know program running status? I am using anaconda Jupiter notebook to run that python script. Million thanks Any verbose features available on fit transform and pipeline? __eou__	User

TP	@bryanleekw it depends on the models in the pipeline there is a pull request for verbosity in pipelines, but I don't think it's merged yet __eou__	User

TP	Hi, does anybody know how should I go if I want to work on an open PR from another person? __eou__	User

TP	you can add them as a remote and pull their branch and open your own pr @gsmafra __eou__	User

TP	Thanks Andreas, may I know any links in sklearn that i can know detail? __eou__	User

TP	hi guys, i am new to gitter. Not sure if this is right way and channel to post the doubt. but if it is, here is my doubt: I have to create a model to assign sales representatives in like 105 territories across united states based on sales-and other business rules like geo location - in each territory. But i am having hard time finding a solution to limit the size of each cluster based on sales, vicinity and few other parameters. So my question is how do i stop the cluster from growing when it meets a certain defined criteria/conditions. __eou__	User

TP	If you truly want to dynamically "stop growing a cluster" based on custom criteria, I don't think any of the scikit-learn algorithms have an API for that. Moreover, it would only apply to algorithms that incrementally grow clusters, e.g. hierarchical clustering. If your dataset is not huge and you don't need to fit many different models, you could try implementing your own hierarchical clustering with simple python logic, and add your own constraints. It's a fairly straightforward algorithm. __eou__	User

TP	As its agglomerative, you dont lose a great deal by not stopping. You should just process the output of linkage_tree to adhere to your criteria. __eou__	User

TP	Its is possible to retrain a Sklearn classifier once loaded back from a pickled filed __eou__	User

TP	Yes but it will forget everything from the previous training set. __eou__	User

TP	So its again a fresh training? __eou__	User
TP	Yes.__eou__	Agent
TP	Is there a way to persist that memory__eou__	User

TP	There is no generic way, it depends on the model. Some models have a `partial_fit` method, it might or might not do want you want. Read the documentation, the reference papers of the literature (usually referenced in the docstring of the class) and the source code of the model to determine that whether it can help accomplish what you are looking for. __eou__	User
TP	@ogrisel thank you for this information__eou__	Agent
TP	My advice would be to always snapshot the training sets of your models so that you can retrain later, possibly with an extended / enriched training set.__eou__	User
TP	you can discard or down weight older samples if you think that the distribution of the data drifts over time.__eou__	User
TP	Ok that sounds good__eou__	Agent

TP	So you mean to keep the training set as a back up and letter merge the new dataset with old and fit __eou__	User
TP	yes, that's the safest thing to do.__eou__	Agent
TP	Perfect__eou__	User

TP	@ogrisel did you tag at some point? We should definitely tag before the sprint looks like no branch yet ok that also good with me @ogrisel __eou__	User

TP	sir can i ask You something hi there __eou__	User
TP	hi__eou__	Agent
TP	if i have few doubt__eou__	User
TP	@amueller can i ask u something__eou__	User
TP	sure__eou__	Agent

TP	@amueller no, not yet I got busy on other thing and now I am back at trying to debug a precision issue in the error computation of the TSNE model that has an impact on the stopping criterion. __eou__	User
TP	for interviw__eou__	Agent
TP	can any one help me__eou__	Agent
TP	@ogrisel so branch now and backport TSNE once it's merged?__eou__	Agent
TP	@amueller  In any case, if I cannot find the bug before tomorrow, I will cut the branch 0.19.X anyway.__eou__	User

TP	@amueller  I have an interviw  in a startup @amueller I clear technical part but if somebody ask about is there any questions for me  what should i ask __eou__	User

TP	This is not really a forum to discuss interview practices __eou__	User

TP	Hey! Did someone deal with this preprocessing dilemma when working with timeseries? https://stackoverflow.com/questions/45080001/how-to-preprocess-timeseries-test-data-to-make-a-classification-prediction __eou__	User

TP	if anyone wants to help the sprints run smoothly, please help tag issues appropriately with "easy" "need contributor" and "sprint" __eou__	User

TP	looks like all "easy" "need contributor" issues already have PRs awaiting review... so review sprint? ;) __eou__	User

TP	If you have experienced macOS users who know a thing or two about numerical stability, clang, Accelerate / OpenBLAS. https://github.com/scikit-learn/scikit-learn/issues/9351 this is blocking the 0.19b1 wheels on macOS there is also a 32 bit linux issue in the feature importance test the test is probably too strict, I am currently investigating with docker the saga solver on  macOS is a real bug but it's probably not easy to debug. __eou__	User

TP	sprint will be on saturday ;) hm yeah it would be cool to have some easy issues but it looks like the issue tracker doesn't really have a lot right now :-/ __eou__	User

TP	I will add one to improve CI on master to test 32 bit linux and another to test macOS __eou__	User
TP	"easy"__eou__	Agent
TP	;)__eou__	Agent
TP	it's easy in the sense that you don't need to know python programming or machine learning__eou__	User
TP	but you need to know system and travis stuff__eou__	User
TP	I won't have time to work on those tomorrow, I need to review a long journal paper... + other admin stuff to do over the WE__eou__	User
TP	https://github.com/scikit-learn/scikit-learn/issues/9352__eou__	User
TP	I gave some boilerplate docker commands and scripts + references to docs to get started__eou__	User
TP	random failure I think__eou__	User
TP	yeah true__eou__	Agent

TP	Hi @amueller just saw your tweet about advocating the need for fair and un-biased prediction. We at datascience.com have been looking into this topic and open sourced our first step in that direction. https://github.com/datascienceinc/Skater. If possible checkout our roadmap. How do others feel about the idea ? __eou__	User

TP	@pramitchoudhary so that builds upon lime? what functionality are you adding to it? __eou__	User

TP	@amueller so the idea is to balance between global and local interpretation. At global level: model agnostic pdp, model agnostic variable importance as of now(one flavor of it, work is in progress to add other flavors); local level: its currently our forked version of LIME(_some improvements in the way local samples are generated_). There is also support for InMemoryModel(_one has access to the environment in which model is build_) and DeployedModel(_model is deployed in the wild_) here is a nice example of DeployedModel to evaluate third part models  https://github.com/datascienceinc/Skater/blob/master/examples/third_party_model/algorithmia_indico.ipynb. __eou__	User

TP	InMemoryModel example: https://github.com/datascienceinc/Skater/blob/master/examples/credit_analysis/Credit%20Analysis.ipynb __eou__	User

TP	@ogrisel did you break master ;) __eou__	User

TP	@amueller it's green: https://travis-ci.org/scikit-learn/scikit-learn/branches __eou__	User
TP	@ogrisel https://circleci.com/gh/scikit-learn/scikit-learn/12068?utm_campaign=build-failed&utm_medium=email&utm_source=notification__eou__	Agent
TP	it's plot_stock_market__eou__	User
TP	ugh__eou__	Agent

TP	Then couldt we start the rebuild again? __eou__	User

TP	@nok yeah but it doesn't matter it will restart with the next commit __eou__	User

TP	Okay :smile: __eou__	User

TP	Consider if i am having a column of integers is there any strong suggestion whether to do classification or regression. @ogrisel @amueller normally i use pandas dtypes to differentiate the problem if its object or int64 - classification if its float64 - regression is that right? @ogrisel i can understand. Is there any specific automation in such cases I wrote a function to see uniques of integer columns counts if its above 75% then i consider is regression is it a right thing __eou__	User

TP	it depends : if the integers encodes target classes / categories, then classification, if they represent a target quantity (e.g. ratings, prices...),  then a regression model should work it depends on what they mean, not their physical types __eou__	User

TP	pandas has a categorical datatype that you could use to make the distinction instead of using ambiguous integers. __eou__	User
TP	Perfect__eou__	Agent
TP	your heuristic should work most of the time, but then it can fail__eou__	User

TP	e.g. predicting ratings, you might get 80% of "4" in your training set and treat the probleme as classification instead of regression. __eou__	User
TP	Exactly__eou__	Agent
TP	Thank you @ogrisel__eou__	Agent

TP	but that should work well enough anyway. But using classification metrics might not be the best metric. __eou__	User

TP	Hi, I have a quick question for you experts: I'm trying to use the function "model_selection.train_test_split()" but I need to make sure that only there are no transactions whose "coreID" appears both in the test and in the train set.  I generally have 100 transactions per core and I have a coreID for every transaction.  I basically want each "core" (I have thousands of them) to be either in train or in test. Any ideas to help? Thanks!! (my dataset is at the single transaction level though) __eou__	User

TP	@mario_avevo_twitter can you make it with the parameter **group** of split ? http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html __eou__	User

TP	thank you @massich I'll look into it!! __eou__	User

TP	If after 6th epoch of perceptron ,  our perceptron is getting converged but I have set n_iter parameter as 10 , will the  values of weights that are obtained at 6th epoch as well as 10th epoch  be optimal or Is it like the weight values at 6th epoch be more optimal than 10th epoch? Need some clarifications . Ok I'll share the code soon... __eou__	User

TP	I don't know the perceptron implementation of scikit-learn but convergence with Delta rule is guaranteed only for linearly separable problem __eou__	User

TP	<unconvertable> I will take a look, I'm dev a perceptron in go (on github) and I want also to implement a multilevel + backprop framework (so guys, if you are interested, please join :D :D) About convergence: I don't remember maths related to "is monotonous descent or not" But definitely is guaranteed under the assumption of linearly separability (I have demonstration, if you want) __eou__	User

TP	@ogrisel I have pickled my model using Joblib. Now i am pickling the LabelEncoder used to build the dataset also. So i can use it when prediction. Is it a right path? __eou__	User

TP	How did you use the LabelEncder? To transform input features or the target variable? Most scikit-learn classifiers will automatically use a label encoder internally so you don't need to do it externally. If it's used to transform  input features it's better to use a pipeline. there is a PR for a ColumnTransformer under way. You can copy the code in your own project if you want it to be compatible with sklearn 0.18.2 and the future 0.19. ColumnTransformer will be part of 0.20. __eou__	User
TP	oh thats great__eou__	Agent
TP	you can follow progress at: https://github.com/scikit-learn/scikit-learn/pull/9012__eou__	User

TP	Only for couple of categorical variables __eou__	User
TP	an pickle the full pipeline__eou__	Agent
TP	like State__eou__	User
TP	@ogrisel i dont get u__eou__	User

TP	I am currently planning serving prediction via RESTapi So mostly ppl will upload their dataset my models automatically are preprocessed (imputing, labelEncoding for Object types, Scaling) as its encoded i need to have those encoding instance for cross validating my new predictions right Thats perfect Thank you @ogrisel  for instant reply :) exactly.. thank u :) __eou__	User

TP	Then write all the preprocessing logics in a transformer (as done in ColumnTransformer for instance) and use a pipeline: http://scikit-learn.org/stable/modules/pipeline.html to combine it with the supervised classification or regression model. Then you can pickle the full pipeline for deployment. __eou__	User

TP	Don't forget to snapshot the training for a given version of the model to be deployed. This way you can make sure you can retrain a similar model from the same data when you decide to upgrade the scikit-learn version. __eou__	User
TP	Yes i do remember it :)__eou__	Agent
TP	model pickles are not guaranteed to work across different scikit-learn versions.__eou__	User

TP	@BastinRobin BTW if you deploy your model with several python processes running on the same host (e.g. gunicorn workers), you should use `joblib.dump(pipeline, '/path/to/store/model.pkl')` to save the model and `joblib.load('/path/to/store/model.pkl', mmap_mode='r')` to load the model parameters in read-only shared memory and save memory usage on your production servers. __eou__	User
TP	Yes i am doing the same__eou__	Agent
TP	its a perfect suggestion__eou__	Agent

TP	@amueller Noticing your latest issue, there is another malformed class at: http://scikit-learn.org/stable/modules/cross_validation.html#group-k-fold __eou__	User

TP	@made2591  .. Thanks for the help . I did the maths manually and got it cleared .. __eou__	User

TP	@ashiskriz actually I didn't anything ^^ u r welcome! __eou__	User

TP	@SebastinSanty it's fixed in the master branch: http://scikit-learn.org/dev/modules/cross_validation.html#group-k-fold :) __eou__	User

TP	@ogrisel Yes, I fixed it! :-) __eou__	User

TP	Hi guys are there any c++ projects I can get involved in if yes please link me below __eou__	User

TP	@PriyaChincholikar lol this is a python package. We have a bunch of Cython though. But you can check out Shogun or MLPack or xgboost __eou__	User

TP	@ogrisel  Is there a better recommendation on using timeseries data in realtime. Is it good to store it or use any tools to get realtime inputs __eou__	User

TP	Hi all, any open easy issues need to be solved? __eou__	User

TP	Hello does anyone know if there are any plans to add factor rotation to FA ? __eou__	User

TP	@ibrahimsharaf, https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+label%3A%22Need+Contributor%22+label%3AEasy includes: #9341, #9336, #9325. #4458, among other stalled pull requests, should not take much work to finish up... __eou__	User

TP	Hi all, I was wondering if it is somehow possible to include a model into the mean of a Gaussian Process Regressor (just like one can choose a kernel) a fit for the mean as well? __eou__	User

TP	No, I don't think so. That's rarely done in GPs afaik you can remove the global mean beforehand if you like __eou__	User

TP	Oh ok, thanks for the quick reply. I dont want to subtract the mean because sometimes subtracting things out of a dataset somehow changes its random nature... __eou__	User

TP	A toy for anyone who uses grid search, and particularly @amueller: https://github.com/jnothman/searchgrid very much in alpha status __eou__	User

TP	@jnothman nice! __eou__	User

TP	What I'm thinking about right now / talking about with some people is to make it easier to implement ask/tell interfaces with BaseGridSearch. Maybe not in sklearn, but I think it should be possible to use a coroutine approach that allows implementing arbitrary parallel search strategies with a common interface __eou__	User

TP	I implemented a callback-based parameter search variety of grid search years ago. It wasn't hard then, but it might be now ;) Not sure if that's all you mean... __eou__	User

TP	I'm looking at our GMM and the BIC derivation looks weird never mind... It's the score that is weird, and the BIC function corrects for that. __eou__	User

TP	I hope thats a good thing ;) __eou__	User

TP	I recently built a rudimentary classifier that identifies the news topic of a given article. I scraped lots of news articles - as training data - off a website, with their labels. In terms of text-processing, I removed stopwords, punctuations, lemmatized all words, then computed the tf-idf values of the remaining words. I represented each article as a doc of its 25 words with the highest tf-idf values. Then I decided on adopting a knn approach to the problem: upon getting the input article and determining its most frequent words (after the text-processing), I find the 5 articles that have most words in common with the input article - and then I perform a majority vote. If a majority of the 5 articles are tech, then the input article is tech. What are some ways I can improve upon this approach? __eou__	User

TP	would there be a way to include some of the changes suggested by https://stackoverflow.com/questions/26851553/sklearn-agglomerative-clustering-linkage-matrix into agglomerative clustering to plot dendrograms of the model? __eou__	User

TP	@jzf2101 can you open an issue, or check if there's already one? I think there were previous requests to draw dendrograms, which I think would be a great feature I haven't read the question in full but there were some issues in supporting this in the past __eou__	User

TP	Hey all, chipping away at #5653 and was wondering if there's any places in the codebase that check if fitted on any input estimator (where the est type is unknown) ? https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/partial_dependence.py#L125 from #7464 is a little tricky to generalise __eou__	User

TP	hrmm. i think i can sneak it into the logic for ensembles and then check for other estimators in the fit stage. never mind! __eou__	User

TP	hi __eou__	User

TP	Hi, I've generated a big set of very high-dimensional embeddings (7300 verbs with 1700 dimensions each) from a QnA dataset. I'm trying to visualize them and what I do is to apply [`truncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD) (a PCA which centers the data) to obtain 50 dimensions and then pass those to [`TSNE`](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) in order to visualize them in 2d. The problem is that those embeddings include lots of verb conjugations and synonyms, so I would like to apply some type of algorithm to those high-dimensional embeddings (like kNN or cosine similarity) in order to obtain clusters or groups of similar embeddings represented by a single vector for each of those groups (without reducing their dimensionality since that is done later through SVD and TSNE). Does anybody know how to obtain that? __eou__	User

TP	@amueller issue already exists but pr is still left open __eou__	User

TP	Can anyone help me with a calculus issue using the SVM lost function __eou__	User

TP	hi everyone in SGDClassifier http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html how can I determine the baseline model? the doc said: "Linear classifiers (SVM, logistic regression, a.o.) with SGD training." how can I choose between SVM and logistic regression? __eou__	User

TP	it depends in your data and what you want to predict continous data __eou__	User
TP	I mean__eou__	Agent
TP	in the code__eou__	Agent
TP	how can I choose__eou__	Agent
TP	check the documentation :D__eou__	User
TP	http://scikit-learn.org/stable/modules/svm.html__eou__	User
TP	http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html__eou__	User
TP	now ?__eou__	User

TP	hi yes, but I couldn't find that's why I ask here __eou__	User

TP	it is SVM? but how can I use SVM as a base model with SGDClassifier? or suppose SVM is by default, how can I switch to logistic regression? no problem, thanks for your help __eou__	User

TP	or sorry , i didn't have depth knowledge in machine learning how to choose the algorithm i'am begineer i just follow some moocs @vinhqdang tell me your experience with machine learning ? __eou__	User

TP	http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier @vinhqdang you have to set the `loss` properly `loss=hinge` correspond to the SVM while `loss=log` to logistic regression __eou__	User
TP	great, thanks__eou__	Agent
TP	do you know what functions correspond to other loss__eou__	Agent
TP	say, "modified_huber"__eou__	Agent

TP	modified_huber is a smooth hinge loss __eou__	User
TP	is it possible to pass a custom loss to SGDClassifier__eou__	Agent
TP	?__eou__	Agent
TP	it does not seems so since it does not accept callable__eou__	User
TP	great thanks__eou__	Agent

TP	@glemaitre hi can you contact you in private ? @vinhqdang __eou__	User

TP	@vinhqdang If really you want to play with the loss they are defined there https://github.com/scikit-learn/scikit-learn/blob/ef5cb84a805efbe4bb06516670a9b8c690992bd7/sklearn/linear_model/sgd_fast.pyx in case that you want to implement your own __eou__	User
TP	yes, thanks__eou__	Agent
TP	https://github.com/scikit-learn/scikit-learn/blob/ef5cb84a/sklearn/linear_model/stochastic_gradient.py#L901__eou__	User
TP	then you can register it there__eou__	User
TP	I see__eou__	Agent

TP	@vinhqdang I wouldn't use SGDClassifier for custom loss functions for learning. Just implement your own in pure python. It will not be fast but it's a good learning experience and much easier to understand __eou__	User

TP	Anyone have experience predicting multiple time series with similar inputs and outputs? My problem is as follows: I have a bunch of products, and a bunch of locations at which the products are sold. Each of these locations has a sales history, so essentially I have a time series for each product at each location. My goal is to forecast future sales of each product at each location. I have tried approaching this problem as a time-series forecasting problem however I can't wrap my head around how to build a generalized single model from multiple time series. Would love to chat with someone if they've had experience with this before. thanks! __eou__	User

TP	@tblazina you should probably consider it as a time series data for each location. that way you will be only considering sales as a function of time and thus make your initial model simple. once you are able to make predictions on that  model and are able to understand it probably you can scale this model to encompass addtional dimensions like geography. point is to take only one location at a time. that should help you __eou__	User

TP	@infinite-Joy  thanks for the feedback, yes that was my general approach before having a model for each location but unfortunately then I'm reducing the amount of data significantly,  from >300,000 data points in all for all locations, versus having sometimes <<1000 for newer locations, I was wondering if there was somehow a way to reframe the problem so as to only have one model which generalizes to all locations. cool, i'll have a look at it thanks! __eou__	User

TP	this one seems a close match. not sure if this will help though. also not in python https://stats.stackexchange.com/questions/23036/estimating-same-model-over-multiple-time-series __eou__	User

TP	The #9551 merge seems to be causing fails in a lot of new PRs __eou__	User

TP	Hmmm really do you have examples? __eou__	User

TP	#4197 https://travis-ci.org/scikit-learn/scikit-learn/builds/270779099?utm_source=github_status&utm_medium=notification #9147 sorry... feeling dlysexic this evening #5653 will fail shortly too I'm guessing (for a variety of reasons lol... but this complex check will be one of them) __eou__	User

TP	:confused: #9147 has not been merged yet, right? I think I understand what you are saying. I'll fix it. __eou__	User

TP	the fails aren't related to the PRs that are failing... #9551 seems to be the source unrelated to the new code __eou__	User

TP	Should be fixed now: https://github.com/scikit-learn/scikit-learn/commit/deaa96452a981e3e54dc302fc14cb1c83cb2e399 __eou__	User

TP	Looking good now  @lesteve  :beers: __eou__	User

TP	hi.. I want to do cross-validation but my data comes in groups. I have 10,000,000 rows in total and the sizes of the groups vary a lot. So I would really like to sample 100,000 rows first in proportion to the group sizes and then do the group cross-validation.  Is that possible? __eou__	User

TP	Is scikit-learn taking part on the Google Summer of Code this year? (I couldnt find anything for this year) __eou__	User

TP	@mirca not this year __eou__	User

TP	@glemaitre  And what about next year? Is scikit-learn taking part next year? __eou__	User

TP	@thechargedneutron no idea. I think that the core devs will keep the community informed. __eou__	User
TP	@glemaitre Thanks :)__eou__	Agent

TP	@glemaitre thanks __eou__	User

TP	I am confused by scikit-learn's support for pandas dataframes.  Can you just use them like 2n numpy arrays in scikit learn and if so, is there still any need for sklearn-pandas? __eou__	User

TP	@lesshaste you can use pandas dataframes with numeric values as input to most scikit-learn estimators and model selection tools (e.g. cross_val_score and parameter search tools). sklearn-pandas can still be useful to do per-column feature preprocessing although this also should be improved by default in future scikit-learn versions. __eou__	User

TP	@ogrisel  thanks!  I can't tell by looking at the docs which parts of sklearn will work with dataframes and which won't. For example http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html should I just assume all classifier and regression functions will work? __eou__	User
TP	yes, whenever there is `array-like` as the type for X in fit and predict, numerical dataframes should work.__eou__	Agent
TP	thanks so much__eou__	User
TP	@ian-flores  It's the only sensible way I know to measure a classification  where the number of items in each class is very different__eou__	User
TP	@amueller  I would like to know how much each feature contributes to the auc . Does that make sense?__eou__	User
TP	It would also be great if sklearn had multi-class auc but that PR seems to have stalled a while ago__eou__	User
TP	@lesshaste feel free to pick it up ;)__eou__	Agent
TP	it makes sense if you can define it? ;)__eou__	Agent
TP	@amueller  I wish I had the skills!__eou__	User
TP	@amueller  what do you mean by "it makes sense if you can define it"?__eou__	User
TP	if you mean the multi-class AUC, isn't there a more or less standard definition by Hand and Till?__eou__	User
TP	there are two standard definitions__eou__	Agent
TP	I'm not sure what's currently to do__eou__	Agent
TP	is there a more efficient way of doing this?__eou__	User

TP	Is there any way to get feature importance (for a random forest classifier) based on auc (area under the curve)? __eou__	User

TP	why is auc important? __eou__	User

TP	@lesshaste what do you mean by that? you could do sequential feature selection for any arbitrary metric, but that requires fitting multiple models __eou__	User

TP	ah ok.  Looks like it just stalled on that observation https://github.com/scikit-learn/scikit-learn/pull/7663#issuecomment-307566895 __eou__	User
TP	What does it mean for a feature to contribute to the auc? how much will AUC change if you leave it out? That is just one step of sequential forward selection__eou__	Agent
TP	yes I think that makes sense. You just want to avoid rebuilding the model n times when you have n features__eou__	User
TP	I often have 10,000 features__eou__	User

TP	Hey I am a newbie. Would love to contribute to the organization. Can anyone guide me on this? __eou__	User

TP	You may start by solving easy issues. Its available at https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue+is%3Aopen+label%3AEasy Solve the ones which is not currently being attempted by anyone else. (I'm a newbie too :P ) __eou__	User

TP	Alright. Thanks :smile: __eou__	User

TP	hi i am new here can anyone guide me..! __eou__	User

TP	Hi, i am quite new to machine learning and am thinking about a good approach for my ML analysis. I have several tables with data in a DB. Some of this data are name, adresses and email and so on and some are 'other' data, mostly numbers and configuration data. Both kind of data can be in the same table. What I want to analyse now is, has a column address like data or number/config data. As I undersatand it with most ML examples you have a number of test data in tables and each table row is a data point. When I want to predict something, I take data that has the data structure of one table row and see that I predict the wanted data based on the test data training. In my case I have the table + column metadata (column name, length, etc) plus the actual content of the table column. My first idea is now to create a new table with (table name, colunm name, length, ctable column content as a string ,lets say) and then copy the column content column by column into this structture. This would give me individual rows to analyse. It just feels a bit clonky as the meta data (table/comuln name, ..) would be naturally the same for a whole column and only the column content differs and so there is not much variation in this table. Is there a more clever way of prepading the data ? __eou__	User

TP	Could I have Kmeans example convert data frame and output cluster prediction with Plot visualization? __eou__	User

TP	like this? https://beta.gryd.us/notebook/published/fJevxtDmjFfo5nGYs4A4bC/ __eou__	User

TP	and how about convert string data frame to vector? Thanks anyway __eou__	User

TP	anyone here have authority to update the topic in the IRC channel? (#scikit-learn on freenode) __eou__	User

TP	@mirca @thechargedneutron aboug Google Summer of Code, the consensus amongst the core developers seems to be that we need all of the three conditions: a good student, a good focussed project with accomplishable goals that will produce something within the time constraints, and one mentor (ideally two mentors actually) who has enough spare bandwidth during this time. It just was not the case last year. It's hard to tell what will happen next year for sure. Potentially interested GSoC students are more than encouraged to get involved in scikit-learn so they get to know the project and we get to know them. __eou__	User

TP	@lesteve  maybe add something to the scikit front page about this? It's always good to advertise :) __eou__	User

TP	@lesteve  Thanks for the information. I am continuously trying to understand the codebase by solving bugs. It's more comfortable for me now. Thanks :) __eou__	User

TP	@lesshaste yeah I could update the topic if I find the time. Maybe the update should be "check gitter instead" __eou__	User

TP	hi __eou__	User

TP	@Rebaiahmed hi __eou__	User

TP	i need some help for scholarship project just i'm searching about sickit learn with django i want to implement machine learning into web project but i didn"t have any idea for the application or the avalaible dataset __eou__	User

TP	you can post your question here, but I recommend going to stackexchange and tag it with sklearn __eou__	User
TP	wait__eou__	Agent
TP	see http://scikit-learn.org/dev/faq.html#what-s-the-best-way-to-get-help-on-scikit-learn-usage__eou__	User

TP	for an introduction, check out @jakevdp's free book: https://github.com/jakevdp/PythonDataScienceHandbook __eou__	User

TP	i have an experience with django and sickit-learn they are separated i want an idea o combine them good idea ;) why ? what are the advantages ? ok good :D __eou__	User

TP	Build a recommender system for suggesting movies based on the movie lens data as a web application or a book recommender system using https://www.kaggle.com/zygmunt/goodbooks-10k I would recommend you to use [lightfm](https://github.com/lyst/lightfm) or [spotlight](https://github.com/maciejkula/spotlight) instead of scikit-learn though :) Those libraries are dedicated to building recommender systems. Factorization machines and neural networks with categorical embeddings are known to be very good for building recommender systems but are not implemented in scikit-learn. (yet) __eou__	User

TP	there is any other suggestion for other dataset for example client -ecommerce dataset something ike this __eou__	User

TP	what do you think about Twitter Sentiment Analysis ? __eou__	User

TP	e-commerce: I don't know any out the time of my mind. For twitter you can use the sentiment140 dataset but I find sentiment analysis pretty useless personally. I have to go. Good luck for your project. __eou__	User

TP	> Build a recommender system for suggesting movies based on the movie lens data as a web applicationLove  this one! __eou__	User

TP	@Rebaiahmed Sentiment analysis is iffy at best. Twitter is particularly bad. Training data is typically nice and clean, but Twitter users are are an insanely sarcastic bunch which makes this so unreliable. Plus it's pretty worthless. The move to 280 characters might improve this in the future but we'll need to get new training data.  What could be interesting is to do questionaries, and demographics and other data, then build a model that predicts opinions and responses based on that data. You could through in sentiment analysis here for longer paragraph answers if it really interests you. __eou__	User

TP	is it possible to use  the pearson correlation coefficient as the loss function when doing regression in scikit-learn? __eou__	User

TP	To look at feature importance, I extract it from the `feature_importance_` attribute, sort, and visualize. Is there a way to pretty print / plot automatically? In R, `plot(random_forest)` does the trick If there's not a way to do this in scikit-learn, is there a reason why and does the community welcome PR in this regard? __eou__	User

TP	@LaDilettante you need to use matplotlib to plot your results __eou__	User

TP	Sentiment analysis is good for articles, but yes I do agree it's bad for Twitter. __eou__	User

TP	perhaps use bayesian classifier if predicting something from tweets it worked in some works ive come across __eou__	User

TP	@rishavroy1264bitmesra Hey guys ! Can someone help me to find a way to find similar meaning sentences sentence-A is known and I have to develop an implementation to find all sentences meaning same to sentence-A in coming input data(paragraphs) __eou__	User

TP	@rishavroy1264bitmesra  I am not sure how you can exactly achieve this but what you are trying to achieve is a very hard problem. I feel WordNet shall be used for calculating the word distances for similarity/dissimilarity and even use tf-idf for sentence scoring. I know this is very vague for you've got some work to do.. __eou__	User

TP	Does someone have experience with using SE(3) poses as inputs? I am trying to find a good representation for the orientations. Some pros/cons, along with usage guidelines would be much appreciated as well. Thanks in advance __eou__	User

TP	Hey guys anyone here familiar with University of Edinburgh's grad school program? Looking for recommendations for European graduate schools __eou__	User

TP	Hi. Was wondering if anyone knows of any academic papers on marketing attribution modelling with offline source (assigning a website visitor a probability of having come to the site due to a TV ad)? __eou__	User

TP	HI, I am trying to prepare training data for hand written recognition for Tibetan language. As i have to prepare thousand of it. I want to make sure i am making the correct format of pixel values for Tibetan character for machine learning. I tried a single Tibetan alphabet image of size 32*32 pixel. I got the pixel values but i am not able to plot its back in python using matplotlib.pyplot. __eou__	User

TP	hey guys I have X_test and X_train data. Does anyone know how I can divide it so I can have two different two of each for two different models? trying to make a pipeline nope I didn't use test_train_split to make X_test and X_train __eou__	User

TP	@angelotc  you got one data file ? __eou__	User

TP	Hi, a recent ML enthusiast trying to contribute. I just forked the scikit-learn repo and got a very easy PR merged. __eou__	User

TP	I set up a dev environment using virtualenv, so that it does not interfere with my existing scikit-learn installation. But I see that the docs does not mention it at all. Is there any particular reason? Or am I missing something obvious here? __eou__	User

TP	@angelotc  : First import  from sklearn.model_selection import train_test_split then  X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42) X is a dataframe having only features and y is a dataframe having only target from your original dataset __eou__	User

TP	Are you guys familiar with C++? I need some help... __eou__	User

TP	[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/yuhz/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/yuhz/image.png) Hello everybody , I have a doubt - Why is the number of nodes in hidden layer always 1 more than number of nodes in input layer ? How do we set that number of nodes in hidden layer . In most cases I see this type of representation as shown in diagram I mean in most cases that I came across   @mikegraham __eou__	User

TP	@CaptainAshis "Always"? __eou__	User

TP	@CaptainAshis I am not aware of this being a standard practice. Do you mean in actual cases or just diagramatically? __eou__	User

TP	I mean diagrammatically .In most youtube videos I see this type of representation @mikegraham __eou__	User
TP	@CaptainAshis It's probably what's convenient for the creator to draw.__eou__	Agent

TP	@mikegraham  . So is there any standard rule to know how many nodes we can set for the hidden layer ? __eou__	User

TP	@CaptainAshis It varies wildly what ends up working well. There is an old rule of thumb for single-hidden-layer models of 'mean of input nodes and output nodes', but that is even rougher than most rules of thumb. __eou__	User
TP	ok , Thank you @mikegraham__eou__	Agent

TP	@CaptainAshis hot damn! How can I implement a hello world style for this ![alt](https://files.gitter.im/scikit-learn/scikit-learn/yuhz/image.png)... __eou__	User

TP	https://www.youtube.com/watch?v=bxe2T-V8XRs @Ij888 __eou__	User

TP	yo @tms1337 woot woot __eou__	User

TP	@Ij888 e.g. https://github.com/rushter/MLAlgorithms __eou__	User

TP	`<SPAM>` In case someone is interested in visiting Colombia and presenting something, or maybe just attending ;-)  https://www.pycon.co And sorry for the spam `</SPAM>` __eou__	User

TP	@lj888 hope you got your answer  .. :) __eou__	User

TP	I am doing this ``` In [1]: from sklearn.datasets import fetch_olivetti_faces In [2]: import pandas as pd In [3]: ol_faces = fetch_olivetti_faces() In [4]: ol_faces_df = pd.DataFrame(ol_faces.data) In [5]: pd.tools.plotting.scatter_matrix(ol_faces_df, c=ol_faces.target, figsize=(8, 8)) ``` But this programs tends to keep increasing memory usage, is something wrong that I am doing? I am not trying to do anything in particular on my own. I was just trying to 'replicate' (on self exercise type question) for what Alexandre Gram tried to do in SciPy 2017 tutorial. I am doing this on pandas version 0.19. I think we have `pd.plotting.scatter_matrix` in versions higher than 0.19 (i.e. development version I guess) __eou__	User

TP	Hello, I am quite free after my semester ending examinations. I want to contribute more to scikit-learn. Apart from solving bugs (which currently I am doing with 6-8 merged PRs), how else can I contribute to scikit-learn? __eou__	User

TP	Hi, is there a way to remove samples using a pipeline? I was looking at FunctionTransformer but I don't see how it will modify the y values __eou__	User

TP	@cs_hanes_twitter  `imbalanced-learn` has a `Pipeline` object that lets you remove samples. Also, there is WIP for a `FunctionSampler` that lets you use arbitrary functions confirming the `scikit-learn`'s object oriented API. __eou__	User

TP	How do I run tests (unit tests) locally after making some changes? Would running `make` at the top-level directory be sufficient? __eou__	User

TP	Though I did that now. But I would still like to know how to do it cleanly. __eou__	User

TP	make test __eou__	User

TP	Hi all, could you tell what you think of this approach  : https://www.kaggle.com/jankoch/scikit-learn-pipelines-and-pandas/notebook ? __eou__	User

TP	@ncouturier I didn't go through your notebook in detail, but I think you might be interested in looking at https://github.com/scikit-learn/scikit-learn/pull/9012/ That certainly relates to those transformers you implemented to apply certain transformers to certain columns (you call it DataFrameFeatureUnion) Regarding the dummy encoder transformer, you also might want to look at https://github.com/scikit-learn/scikit-learn/pull/9151 that implements a CategoricalEncoder Further, you might be interested in https://github.com/scikit-learn-contrib/sklearn-pandas (but I am also not very familiar with that, so not sure how the functionality there relates to the transformers you implemented) (so as a summary, I agree with you that it currently is hard to do pandas-like preprocessing in sklearn pipelines, but we are working to improve that. Feedback on those linked PRs / project is always welcome!) __eou__	User

TP	ok. Thx for your answer. __eou__	User

TP	<unconvertable> Hi, this is the link to my blog: http://dhrubajitdas44.blogspot.in/ <unconvertable>  It contains my machine learning/ deep learning projects, few as of now. More coming. Any kind of criticism/suggestions/corrections are very much welcome, as it will help me learn. I am a beginner. If any experts/instructors would like to give a review on the projects, that would be great. And the students, follow the blog if you find it useful and for future updates. Thank you __eou__	User

TP	Hi everyone. I'm a statistics graduate student and I'm interested getting some FOSS (and programming) experience. Would it be a worthwhile endeavor to implement Dirichlet Process mixture models in sklearn? Yikes, sorry just noticed it's already implemented! __eou__	User

TP	what would probably be useful on the other hand is a python framework for generalized finite mixture models eg. where you specify a set of distributions and parameters to estimate and run the model without hardcoding the maximum likelihood equation yourself __eou__	User

TP	@VHRanger as an extension to sklearn? __eou__	User

TP	@VHRanger you mean pomegranate? __eou__	User

TP	@ssequeira if your interested in DPs, maybe check out our LDA? I feel it could use some love, and I also think having a gibbs sampling version would be nice not sure if we also want a gibbs sampling version of the GMM __eou__	User

TP	Are the tags 'help wanted' tagged issues for beginners? How do they differ from 'Easy' tags on issues? __eou__	User

TP	Ok __eou__	User

TP	if you use minmaxscaler like this: scaler = MinMaxScaler(feature_range=(0, 1)) train = scaler.fit_transform(train) test = scaler.transform(test) are we guaranteed that test is in the range 0 to 1? __eou__	User

TP	@lesshaste the training set yes, the test-set  not. __eou__	User
TP	@amueller  ah right.. so what is the right thing to do here?__eou__	Agent
TP	the right thing to do for what?__eou__	User
TP	I need to scale the data to use keras (LSTM)__eou__	Agent
TP	so I train on some set and the test on another__eou__	Agent
TP	how should I scale the test set?__eou__	Agent
TP	if this method doesn't guarantee it will be in the right range__eou__	Agent
TP	is there a technical reason why it needs to be in this range?__eou__	User
TP	it will be in this range unless there are samples in the test set that are outside of the range of the training set__eou__	User
TP	hmm.. I don't actually know.. it's just that all the docs says you should scale everything to be in the range 0 to 1__eou__	Agent
TP	yeah, then Using MinMaxScaler in this way is exactly the right thing to do.__eou__	User
TP	ok thanks.. It occurred to me that you could just scale the test set independently too__eou__	Agent
TP	which wouldn't be "cheating" but would give you the guarantee__eou__	Agent
TP	but maybe that is wrong__eou__	Agent
TP	that will make it harder for the algorithm to generalize, because the meaning of the values in the training and test set will be different__eou__	User
TP	true and good point__eou__	Agent
TP	thank you__eou__	Agent

TP	in a related question.. how would you do CV for time series data?  The only obvious method that I can think of is to randomly split the data into the first x% of samples and the last (100-x)% of samples and train and test accordingly. But that clearly has much less randomness than a normal CV and the different folds overlap hugely I am confused by this example to visualize a decision tree. http://scikit-learn.org/stable/modules/tree.html#tree In a script, how do you get to see the picture? The example just finishes with >>> graph = graphviz.Source(dot_data)   >>> graph __eou__	User

TP	in jupyter you can just put the graph object in a cell it'll show as image __eou__	User

TP	@amueller  right.. it would be nice to have an example that worked in a script though as many newbie people will just copy and paste into their code @amueller  I got it work myself so this is a suggestion for others __eou__	User

TP	@lesshaste you mean for the tree plotting? The real solution is here: https://github.com/scikit-learn/scikit-learn/pull/9251 __eou__	User

TP	@amueller  Thanks. Should this work for regression trees too? __eou__	User
TP	yes__eou__	Agent

TP	great! I look forward to it's being merged __eou__	User

TP	@amueller  When i ingest data, set sharing to true, what is shared? @amueller The distributed ingest data in geomesa is the MapReduce method, why not use spark? __eou__	User

TP	Ah, never mind. I found about 'help wanted' here: http://scikit-learn.org/stable/developers/contributing.html#issue-tracker-tags __eou__	User

TP	Using `make html` in the `doc` directory I get the following error (on master branch) ``` Exception occurred:   File "/home/gxyd/anaconda3/lib/python3.6/site-packages/sphinx_gallery/gen_gallery.py", line 322, in sumarize_failing_examples     "\n" + "-" * 79) ValueError: Here is a summary of the problems encountered when running the examples  Unexpected failing examples: /home/gxyd/dev/scikit-learn/examples/ensemble/plot_feature_transformation.py failed leaving traceback: Traceback (most recent call last):   File "/home/gxyd/anaconda3/lib/python3.6/site-packages/sphinx_gallery/gen_rst.py", line 450, in execute_code_block     exec(code_block, example_globals)   File "<string>", line 15, in <module> ImportError: cannot import name 'CategoricalEncoder'   /home/gxyd/dev/scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py failed leaving traceback: Traceback (most recent call last):   File "/home/gxyd/anaconda3/lib/python3.6/site-packages/sphinx_gallery/gen_rst.py", line 450, in execute_code_block     exec(code_block, example_globals)   File "<string>", line 40, in <module> TypeError: __init__() got an unexpected keyword argument 'validation_fraction' ``` __eou__	User

TP	Are you sure you installed the development version? __eou__	User

TP	It seems as you are building the master docs but with a released version of sklearn for running the examples __eou__	User
TP	Well that is what I thought initially.__eou__	Agent
TP	But how can I force I make it to use the development version.__eou__	Agent
TP	Would I need to modify the `PATH` variable?__eou__	Agent

TP	I get things like ``` /home/gxyd/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:docstring of sklearn.model_selection.GridSearchCV:166: WARNING: Undefined substitution referenced: "param_kernel|param_gamma|param_degree|split0_test_score". /home/gxyd/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:docstring of sklearn.model_selection.GridSearchCV:166: WARNING: Undefined substitution referenced: "...". ``` While running `make html` which made me inclined towards the thing you are saying. As I can see for `-e` option: ``` Install  a  project  in editable mode (i.e.  setuptools "develop               mode") from a local project path or a VCS url. ``` __eou__	User

TP	@gxyd I typically have a 'development' environment in which I install the master version (with `pip install -e .`) __eou__	User

TP	@jorisvandenbossche thanks. That seems to be working just fine for me right now. __eou__	User

TP	hello __eou__	User

TP	Does running tests using `make test` differs from tests run using `pytest`? __eou__	User

TP	@gxyd Check the Makefile: https://github.com/scikit-learn/scikit-learn/blob/master/Makefile#L39 __eou__	User

TP	So that would mean `make test` actually runs `pytest` with different parameters (or options)? __eou__	User
TP	`make test` run `make test-code` `make test-sphinxext` `make test-doc__eou__	Agent
TP	therefore it is equivalent to__eou__	Agent

TP	``` pytest --showlocals -v sklearn pytest --showlocals -v doc/sphinxext/ pytest $(shell find doc -name '*.rst' | sort) ``` __eou__	User

TP	Yup, I understand now. Thanks. __eou__	User

TP	Hi, I want to pickup https://github.com/scikit-learn/scikit-learn/pull/7694 again - which branch should I merge to the my code -  0.19.X, master ? __eou__	User

TP	I think 'master' branch should be the one to go with. __eou__	User

TP	That's what I try, but make seems to fail: https://gist.github.com/Kornel/11e69ef9fd2e9380a21991029fbecaf9#file-gistfile1-txt-L9473 I'm using python 3.6.3, ``` python -c "import numpy as np; import scipy as sp; print(np.__version__); print(sp.__version__)" 1.13.3 1.0.0``` and running simply make __eou__	User

TP	Hi everyone, simple question here: is the prior in, e.g., Ridge Regression, also applied to the intercept? See here: http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression By "prior" I mean the L2 regularization term. __eou__	User

TP	Hello all, I want to take up a moder Hello all, I want to take up a moderate issue or something significant not currently being worked upon by anyone. Is there an issue or feature which needs quick attention? __eou__	User

TP	@thechargedneutron have you worked on anything before? if not, go with those that say "good first issue" oh actually you did, sorry your name looked familiar lol __eou__	User

TP	Yeah, I have like 8-9 issues solved. And currently working on a moderate, but almost done. Hence looking for something more than just a bug __eou__	User
TP	sorry I gotta run, I'll think about it__eou__	Agent
TP	feature additions are usually pretty long-term__eou__	Agent
TP	I am like free for a month. So willing to take if there's a need__eou__	User

TP	[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/7foy/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/7foy/image.png) __eou__	User

TP	Does anyone know if you need to be a repo owner to access CircleCI artifacts? Supposedly you should be able to access the full documentation built by CircleCI, but I don't see the tab in the build results... __eou__	User

TP	need some help please __eou__	User

TP	Hi,  Have you used Sklearn TImeSeriesSplit before?  I was wondering how you'd actually implement it?  # Splitting Time-series dataset into Training set and Test set using TimeSeriesSplit from sklearn.model_selection import TimeSeriesSplit tscv = TimeSeriesSplit(n_splits=10) print(tscv) for train_index, test_index in tscv.split(X):     print("TRAIN:", train_index, "TEST:", test_index)          X_train =     X_test =     y_train =     y_test =  So after you have the indexes built, how do you actually use this to get your X_train/test and y_train/test? From this point onwards, do you fit it to your model the same way?  Thanks, Joe ahh no worries, solved it misread documentation __eou__	User

TP	Hi everyone, I'm classifying an 8Million of pixels image using supervised classifiers in scikit-learn. I observed that SGD is that only classifier that converges in a reasonable time; the other classifiers tend to either run endlessly or to get stuck somehow. Is it true what's stated in this page (https://datascience.stackexchange.com/a/996/19222) that above 200.000 observations, one should stick with linear learning (i.e. those that implement partial_fit, although I'm really using it and the SGD is still working correctly)? __eou__	User

TP	quick poll: did you all notice the conda compiler package update and have you trashed all your legacy environments? Am I the only one that didn't hear about that? are you mixing old and new packages by any chance? __eou__	User

TP	I have had a lot of problems lately with conda, but I don't know if they are related to the new compiler packages. Basically whatever small change I want to do, it wants to downgrade/upgrade a whole set of other packages (eg it always wants to downgrade my conda-forge numpy 1.13 to defaults numpy 1.11 when installing a package that is in no way depending (also no through its deps) on numpy) __eou__	User
TP	huh__eou__	Agent
TP	at a certain point it even just wanted to remove numpy__eou__	User
TP	https://github.com/conda/conda/issues/6283__eou__	User
TP	possibly, but as a user I should not be needed to be aware of that__eou__	User
TP	these are all "old" packages. I guess conda-forge only uses old packages?__eou__	Agent
TP	that's what I though, but I was just told in the conda issue tracker that I need to discard any old environment__eou__	Agent
TP	or at least not try to upgrade anything or compile anything__eou__	Agent
TP	they can say that maybe for environments that depend on defaults, but I have environments that are mainly conda-forge (because they eg depend on geospatial things that are (or were) broken on defaults), and there just updating a conda-forge package should still work, without having it pull in defaults (conda-forge should take priority)__eou__	User
TP	yeah, true__eou__	Agent
TP	that sounds possibly unrelated ;)__eou__	Agent

TP	 I am trying (X,y) = make_classification(n_features=20, n_samples=1000, n_classes=3) but it says ValueError: n_classes * n_clusters_per_class must be smaller or equal 2 ** n_informative I just want to make a random classification problem with 3 classes. How can I do that? __eou__	User

TP	How about ``` make_classification(n_classes=3, n_redundant=0, n_informative=20) ``` (I am also a beginner, but just trying here. You can ignore if you want someone more experienced to comment) There are quite a few other (default)arguments which probably would need to be changed. See http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html __eou__	User

TP	thanks.. (trainX, trainY) = make_classification(n_informative=20, n_redundant=0, n_samples=50000, n_classes=120) works! this is slightly less intuitive than normal for scikit learn because (trainX, trainY) = make_classification(n_informative=20, n_samples=50000, n_classes=120)  does not work __eou__	User

TP	To be true, I am new to scikit-learn and to machine learning as well. This is what intrigues me a lot that for every function we find a lot (a lot) of arguments. I would have thought to simply use `**kwargs` and extract out the important information from it. But I think may be developers would have already given a good thought into this. __eou__	User

TP	Even if scikit-learn is intuitive, @lesshaste do not forget to read the doc: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html __eou__	User

TP	so if we still wish to have 2 clusters per classes you need at least `n_informative=6` minimum __eou__	User

TP	I really want 120 thanks __eou__	User

TP	> To be true, I am new to scikit-learn and to machine learning as well. This is what intrigues me a lot that for every function we find a lot (a lot) of arguments. I would have thought to simply use `**kwargs` and extract out the important information from it. But I think may be developers would have already given a good thought into this.  You can refer to [this talk](https://www.youtube.com/watch?v=MQMbnhSthZQ) to see ONE of the problem of the kwarg :). __eou__	User

TP	I haven't seen the video completely (but in between I liked to point out). But I have spent quite sometime with SymPy, and I definitely agree I used a lot of `*args, **kwargs`. I think I could say, that for a library having other libraries as dependencies it is better to use hard-coded arguments instead of `*args, **kwargs`. Thanks for the video, I can sleep tight now :) __eou__	User

TP	Apologies for being slightly OT but...does anyone know what I am doing wrong with xgboost? It seems amazingly slow when you have a large number of classes.  https://bpaste.net/show/ef817a256658 shows the problem thanks to the wonderful scikit-learn make_classification whereas as RandomForestClassifier is still super fast __eou__	User

TP	Hi all. First of all sorry this question since it is not a sci-kit learn related, but I thought someone can help me out. I have some pet projects that Id like to start, but I am having some troubles to start. Can anyone point me out? or bring some ideas? Both involve unsupervised learning.  1. Id like to classify students using some economic information such as income, number of family members, etc. The classification should the economic status, e.g. low resources, medium, rich. The problem is that I dont have any labeled data. Can I approach this problem in a unsupervised way? (I tried with k-means)  2. I want classify academic papers using a taxonomy. Many librarians tag their documents in a specific way. In most cases, this tagging is different for each library. Tagging documents with a specific taxonomy will help search engines to retrieve documents. However, I found a taxonomy called Unesco nomenclature, but I dont know a way to match a document with an element/elements of the taxonomy. __eou__	User

TP	Hello everyone. Anyone can point me to a mathematical proof of the objective function used in sklearn for the logistic regression? http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression Im trying to demonstrate it starting from the Bernoulli likelihood in which the probability of a success is a sigmoid function, however, the final likelihood expression that I arrive is not equivalent to sklearns. __eou__	User

TP	@mirca there's nothing sklearn specific about this, this is *the* objective for binary logistic regression (with penalty) Elements of statistical learning, chapter 4.4 https://web.stanford.edu/~hastie/ElemStatLearn/ __eou__	User

TP	The objective function for binary classification logistic regression stems from the negative log likelihood of a linear parametrization the log odd ratio of the Bernoulli model. the linear parametrization (w.T . x + b)  is for the log odd ratio log(p / (1 - p)) instead of p directly, where p is the parameter of the Bernoulli function. I edited my comment as I made a mistake :) __eou__	User

TP	@amueller Im arriving exactly at expression (4.20) of the book you mention. Am I mistaken or (4.20) is different from http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression ? (disconsider the regularization/prior term) __eou__	User

TP	@ogrisel exactly, through these assumptions I arrive at (4.20) ^, which, to me, looks a little different from http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Perhaps I am missing out something? __eou__	User

TP	I think you can simplify the 2 class case further.  let me catch up with the notation __eou__	User

TP	IIRC y_i take values in {-1, 1} for the negative and positive classes respectively. This is not clear in the scikit-learn doc. __eou__	User
TP	why do you need to sum up the y_i and 1 - y_i cases? in 4.19 we are only looking at the probability of gi__eou__	Agent
TP	ah right one of them is always zero... hm yeah could be the different encoding of y_i__eou__	Agent
TP	did they use p(x, theta) as the top of 4.18 or the bottom of 4.18? I guess if they are consistent, they use the top, but we might use the bottom one__eou__	Agent
TP	but that just replaces classes 0 and 1 I guess__eou__	Agent
TP	Eh, I should be grading__eou__	Agent
TP	:)__eou__	User

TP	its not clear to me how (4.20) and sklearns docs are equivalent Hum, ok! That looks to be the answer thank you! Will do! =) __eou__	User

TP	have you tried replacing y by -1 and 1 like ogrisel said? or maybe easier, try to expand sklearns doc for  y=-1 and y=1 and then try to match that to the two terms in 4.20 __eou__	User

TP	Feel free to submit a pull request to improve the doc :) https://github.com/scikit-learn/scikit-learn/blob/master/doc/modules/linear_model.rst?utf8=%E2%9C%93#logistic-regression Just mentioning explicitly the -1 and +1 values for y_i might be enough. __eou__	User

TP	I am new to open source development but have worked with python for many year. I have found an issue I would like to solve. https://github.com/scikit-learn/scikit-learn/issues/10279 I have read a fair bit on how to submit pull requests but I hesitate to do so because of my inexperience. Is anybody interested in doing a review of my code before I submit so that I do not burden the community at large? __eou__	User

TP	@DrEhrfurchtgebietend until your are fixing the issue and try to follow the contributing guide as much as possible, you can submit the PR. The community will review the code such that it follows the scikit-learn standard :) __eou__	User

TP	OK thanks. I will do my best. I am trying to get up to speed on something small then I plan to tackle the more complex issue I discussed with you before https://github.com/scikit-learn/scikit-learn/issues/9947 __eou__	User

TP	Hello Everyone,I am new to scikit-learn.I followed the advanced install instructions but when I run pytest sklearn. I get module not found error.So can anyone help me. __eou__	User

TP	@DrEhrfurchtgebietend omg that name lol __eou__	User

TP	 By far my favorite word. German has most of my favorite words __eou__	User

TP	@DrEhrfurchtgebietend I know, I'm german ;) __eou__	User

TP	@amueller I had guessed based on your name. I use it in place of "awesome" just to get a reaction __eou__	User

TP	Hello from Berlin! __eou__	User

TP	How can I debug using the simple 'print' commands? This is what I tried:, I first put a `print( (blaaa, bllllaaaa))` in my code. Then if I run the tests using `pytest path/to/file.py` then I don't get output from the `print` statement, I get whatever was expected without the `print` statements. (I'm sure that running those test would definitely reach those `print` statements).  Has this got something to do with `*.pyx` files? __eou__	User

TP	Hello developers! Am a newbie. Can someone provide me instructions link to build & run scikit-learn from source code. Am messing up with things. nope sklearn/tests/test_docstring_parameters.py:150: AssertionError ``1 failed, 8361 passed, 16 skipped, 5024 warnings in 293.63 seconds`` m newbie. understanding issues and working is still tough despite working in python for long __eou__	User

TP	I guess this http://scikit-learn.org/stable/developers/advanced_installation.html should probably work. __eou__	User

TP	Am not getting how to put the cloned code to work. Help __eou__	User

TP	are you able to do `make test` (without errors) on root scikit-learn directory? __eou__	User

TP	if you could paste into a pastebin or somrthing that might help __eou__	User

TP	https://pastebin.com/raw/1Q8PfyRf __eou__	User

TP	@ai-coder don't worry about that, that looks like ups messing up. we need to fix that, though though master if working hm... so might be that you're mixing different installations? meant "us messing up" though I double checked, and it's correct __eou__	User

TP	working now __eou__	User

TP	It says: `1 failed, 8361 passed, 16 skipped, 5024 warnings in 293.63 seconds`, it does contain 'passed' non-zero value, does it indicate ruling out mixing installations? __eou__	User
TP	lol__eou__	Agent
TP	Oh cool.__eou__	User
TP	What is 'ups messing up'?__eou__	User

TP	what time will it take to get familiar with things over here? __eou__	User
TP	?__eou__	Agent

TP	well depends on what you mean with "getting familiar with things over here"... I've been doing machine learning for err.. 8 years and learn new stuff most days ;) __eou__	User
TP	I meant the open-source terms :smile:__eou__	Agent
TP	I think you need -s -v ?__eou__	User
TP	but I'm relatively new to pytest ;)__eou__	User

TP	@amueller since you are here. Can you please answer this query of mine https://gitter.im/scikit-learn/scikit-learn?at=5a3024da540c78242db7aaaf ? (I think otherwise it might get lost upward) __eou__	User
TP	same applies, I guess?__eou__	Agent
TP	@gxyd how do you run pytest?__eou__	Agent
TP	yeah, sure. doing my best__eou__	Agent
TP	`pytest path/to/file.py`__eou__	User
TP	Nope I don't use `-s -v` arguments.__eou__	User
TP	Aah.__eou__	User
TP	It does work accordingly to what is expected.__eou__	User
TP	passing arguments is important I guess.__eou__	User
TP	I'm sure, I am even new (newer?) than you on pytest.__eou__	User

TP	pytest -s -v name-of-file-to-test.py should be enough __eou__	User

TP	Is this page for development using scikit or contributing to the project or both? __eou__	User

TP	For any type of queries and suggestions related to scikit-learn. There's a special dev room for developers of the project. __eou__	User
TP	Can I get a link for the room?__eou__	Agent
TP	Never mind__eou__	Agent
TP	Got it__eou__	Agent
TP	Thanks for the response though__eou__	Agent

TP	:smile: __eou__	User

TP	Hi guys! I want to contribute to the scikit learn open source. I am new to this. Can anyone give me any direction as to where to start with? nice! a lot of information!! thanks  @ogrisel  ! __eou__	User
TP	The best way to contribute is to actually use scikit-learn for a project of yours to identify which part needs improving from your users point of view.__eou__	Agent
TP	I have been reading the documentation and want to use it for kaggle projects.__eou__	User
TP	have not used it extensively besides from some classification and regression tasks__eou__	User

TP	@ashish-ram have a look at the contributors guide: http://scikit-learn.org/stable/developers/contributing.html __eou__	User

TP	@gxyd  `pytest  -s -v ` will display something only if the test fail. You can always put a `raise`after your printing :) or use a proper debugger maybe __eou__	User

TP	No, I think it worked. None of the test failed, but I simply put a `print` statement and it worked just fine. I am thinking you confuse that with something else. __eou__	User

TP	ups right, this is `-l` that does show only at `raise` __eou__	User

TP	Seeing in documentation? (Refer me to it if you are). __eou__	User

TP	`-l, --showlocals      show locals in tracebacks (disabled by default).` __eou__	User
TP	That's also seems useful, thanks @glemaitre .__eou__	Agent

TP	How can I run just doctests using pytest? Searching over the web I reached https://github.com/scikit-learn/scikit-learn/pull/9697, but still it isn't clear as how to do that. Perhaps it is better if documented in http://scikit-learn.org/stable/developers/advanced_installation.html#testing  also? __eou__	User

TP	is there  a suggested replacement for the deprecated http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html ? or http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLasso.html ? __eou__	User

TP	@lesshaste I think that you can check the discussion: https://github.com/scikit-learn/scikit-learn/issues/9657 __eou__	User

TP	Hello everyone, how can I help with this project? __eou__	User

TP	Here in `make_blobs` http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html , what is centers? It says: ``` centers : int or array of shape [n_centers, n_features], optional      (default=3) The number of centers to generate, or the fixed center locations. ``` The number of centers to generate. I don't understand this. @MartinEliasQ see http://scikit-learn.org/stable/developers/contributing.html __eou__	User

TP	I think I got some idea using its different values in plots. (centers of normal distribution) __eou__	User

TP	Yep. The center of each blob (cluster) __eou__	User
TP	Thanks @DrEhrfurchtgebietend__eou__	Agent

TP	@jorisvandenbossche can you do me a favor and explain https://github.com/pandas-dev/pandas/issues/18801 to me? or maybe @jnothman understands and can enlighten me __eou__	User

TP	I wonder if Joel ever comes here, always busy with issues/PR. :) __eou__	User

TP	I sometimes forget to open gitter and dont see its notifications... __eou__	User

TP	hey __eou__	User

TP	Does scikit learn have support for sparse Gaussian processes, like with variational learning? Is that a feature that's on the list? *to be implemented list __eou__	User

TP	Hi everyone.  I joined this room first time today,  nice to meet you all __eou__	User

TP	hi, __eou__	User

TP	Hello __eou__	User

TP	hey, __eou__	User

TP	i have some doubt in estimator_check can anyone plz explain the purpose of that file __eou__	User

TP	I think it contains various routines to check the correctness of input to the various estimators. In particular see: http://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.check_estimator.html __eou__	User

TP	sklearn/utils/estimator_checks.py includes assertions run on each estimator to check that it behaves according to Scikit-learns conventions. __eou__	User

TP	@ssequeira, I dont think we have any planned enhancements to gaussian processes except for easier access to a linear kernel in #8373 __eou__	User

TP	@jnothman (as you are aware) I'm currently working on https://github.com/scikit-learn/scikit-learn/pull/10083 and the other remaining PR is https://github.com/scikit-learn/scikit-learn/pull/10273 (has been inactive for sometime, but I'll get to that), do you think this would be a good time for me to get to medium-level issues?  If the answer to above question is somwhat yes, then, I've been searching for a medium-level issue (non-documentation issue), one which will require a few months of work. Better if there are a series of medium-level issues on the same topic that you guys need someone to contribute on? __eou__	User

TP	Sorry, I cant think of anything right now. I think you would benefit from a few more easy issues. https://github.com/scikit-learn/scikit-learn/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+-label%3A%22good+first+issue%22+ might help, but as you know, were much better at adding the <unconvertable> help wanted <unconvertable> tag than removing it. Do you want to help me build a bot to manage that?? I have ideas... Ping @gxyd __eou__	User

TP	Yes, I'll am willing to work on that. Will you open an issue for that? __eou__	User

TP	Please let me know if any assistance is needed __eou__	User
TP	Sure. Thanks.__eou__	Agent

TP	https://travis-ci.org/scikit-learn/scikit-learn/jobs/327310192 Why can't I use scipy.sparse.random? Works fine locally __eou__	User

TP	scipy.sparse.random was not there in scipy 0.13.3 which is the minimal version required by scikit-learn __eou__	User

TP	Alroght thanks I'll change it with something else Alright * __eou__	User

TP	#9099 is also a fairly large, non-core project... Designing a better way to print out estimators. __eou__	User

TP	Is github down? __eou__	User

TP	 @maykulkarni nope __eou__	User

TP	It's working now, it was d os n It was down a while ago __eou__	User

TP	@maykulkarni - Were you trying to access using Jio internet because I faved the same problem *faced __eou__	User

TP	No github was down https://status.github.com/messages __eou__	User

TP	guys does anyone have a cool idea for a simple machine learning project? using sciket-learn which is never done before? __eou__	User

TP	Do you mean something which has never been done with scikit-learn or something that has never been done in general? If the former then just look for feature request. If the latter I think it is safe to say all simple things have been done long ago. __eou__	User

TP	Yeah its all done __eou__	User

TP	Well the intersection of novel and simple is nearly nonexistant in most developed fields __eou__	User

TP	am i supposed to know the internal functioning of all the scikit learn models? because there are too many models i just did the andrew ng course on coursera he taught a few basic things. __eou__	User

TP	Supposed to by who? __eou__	User

TP	I'm currently getting some errors in my scikit-learn branch (made some changes). Running tests via pytest, produces some output on terminal, but since numpy arrays contains large number of elements, they are printed with a threshold. ``` $ pytest sklearn/metrics/tests/test_common.py ============================================================= test session starts =============================================================  sklearn/metrics/tests/test_common.py:917: in check_averaging     y_pred_binarize, is_multilabel) sklearn/utils/testing.py:311: in wrapper     return fn(*args, **kwargs) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  metric = <functools.partial object at 0x7fa62c5d5b50> y_true = array([0, 1, 0, 1, 1, 2, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 0, 1, 0,   ...0, 2, 0, 1, 1, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0,        1, 1, 2, 0]) y_pred = array([0, 1, 0, 1, 2, 2, 0, 1, 1, 1, 1, 2, 2, 2, 0, 2, 1, 0, 1, 2, 0, 0, 2,   ...0, 0, 0, 2, 0, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 2, 0,        2, 0, 1, 2]) y_true_binarize = array([[1, 0, 0],        [0, 1, 0],        [1, 0, 0],        [0, 1, 0],       ...0, 0],        [0, 1, 0],        [0, 1, 0],        [0, 0, 1],        [1, 0, 0]]) ``` Now the problem is how can I print the complete array's (i.e with `threshold=np.nan`).  I tried to put `np.set_printoptions(threshold=np.nan)` in concerned test file as well as in skearn/utils/testing.py but with no success. Do I need some pytest option? See here `y_true` is printed with some threshold size. Well, one thing I can do is, use the `print` statement and then use `pytest` with `-s - v` option. But I don't think this is a good solution. __eou__	User

TP	hi __eou__	User

TP	#10443 Ready to MRG, needs review. __eou__	User

TP	Is scikit-learn going to take part in GSoC 2018? If yes, I saw a couple of projects in GSoC 2017 wiki. Since, there were no participant last year, is that project list still valid or are you guys going to come up with some other projects? I would be willing to explore available/possible projects. __eou__	User

TP	@gxyd, try `np.set_printoptions(threshold=np.inf)` rather than `np.nan`, or perhaps `np.set_printoptions(threshold=np.iinfo(np.int).max)` __eou__	User

TP	@thechargedneutron, our problem with GSoC is assuring mentor availability, as well as developing well-defined projects and students we are confident will complete the work without too much hand-holding. We have much less core dev availability than a few years ago, and many fewer well-defined, coherent project options that do not require substantial expertise. For example, we have a few big API things that could do with attention (see https://github.com/scikit-learn/scikit-learn/projects/9), but they mostly require a lot of familiarity with Scikit-learn API design issues. At this stage of maturity we have less need for an <unconvertable> implement this kind of algorithm <unconvertable> or <unconvertable> optimise that <unconvertable> kind of project. If a compelling student/project candidate approached, I think we would consider it. __eou__	User

TP	@jnothman  Thanks for the reply. I am indeed interested in working in API things but need a good understanding of it. Since there's some time before the GSoC, I would like to orient myself in the direction of the current requirements. Can you advise me on how to acquaint me with the current issues and possible solutions. Thanks :smile: __eou__	User

TP	@jnothman neither of them work for me. BTW, which files should I put them in? I tried to put it both files sklearn/utils/testing.py and sklearn/metrics/tests/test_common.py. __eou__	User

TP	Modify conftest.py if youre doing this for testing?? __eou__	User

TP	Is there a contributor who wants to work on an Easy issue with a moderate amount of work? #9726: create a new sklearn.impute module for imputation __eou__	User

TP	I am relatively free. I have currently #10206 which may get completed by the end of this week. So, I am willing to take it. __eou__	User

TP	For a longer project (potentially a GSoC), contributors might be interested in resampling API: https://github.com/scikit-learn/scikit-learn/issues/3855#issuecomment-357949997 __eou__	User

TP	@jnothman I can work on #9726. __eou__	User

TP	[![Screenshot from 2018-01-17 22-43-24.png](https://files.gitter.im/scikit-learn/scikit-learn/aUpR/thumb/Screenshot-from-2018-01-17-22-43-24.png)](https://files.gitter.im/scikit-learn/scikit-learn/aUpR/Screenshot-from-2018-01-17-22-43-24.png) __eou__	User

TP	Hi everyone. an SVM model i created returns an error when i fit with my x and y data. below is a print out of my x and y values and the error message. I transformed the y values to onehotencoder values as well. the same error is generated when i use random forest and naive bayes __eou__	User

TP	Don't OneHotEncode `y` and it will work __eou__	User

TP	Just curious, Should every bug fix / enhancement go into what's new? __eou__	User

TP	@glemaitre: i the data is a multiclass data so i labeled the classes 0,1 ,2. will is it okay to use them as it is without onehotencoding? __eou__	User

TP	i encoded them as [001],[010] and [100] and that is what generated the error __eou__	User

TP	http://scikit-learn.org/stable/modules/multiclass.html @maykulkarni most of the time yes @jotes35 SVM does not handle multi-label. But I am not sure that you have to hot one encode __eou__	User

TP	#10478 #10443 completed, needs review __eou__	User

TP	@maykulkarni I put 2 additional tests to do because I am almost sure that the solution in #10443 is not working if we don't introduce a dtype to the transformer as proposed [here](https://github.com/glemaitre/scikit-learn/commit/4e5e1f06ed1b90c0ffb00584db81a4e8c77e1dff) __eou__	User
TP	Thanks, will check it out__eou__	Agent

TP	Anybody know if there is a wrapper or python equivalent to RankLib ? __eou__	User

TP	@DrEhrfurchtgebietend  It seems to be LEROT in Python. You have more details here : https://www.quora.com/What-are-the-alternatives-to-RankLib __eou__	User

TP	Guys, the GaussianMixture model's score_samples() method return log-probabilities, I'm not sure how to calculate regular/percentage-wise probabilities from these, can anyone enlighten me? __eou__	User

TP	how to make spacemacs work behind https_proxy? __eou__	User

TP	I tried `(setq url-proxy-services         '(("no_proxy" . "^\\(localhost\\|127.*\\)")           ("http" . "127.0.0.1:1087")           ("https" . "127.0.0.1:1087")))   )`,but it didnt work __eou__	User

TP	Sorry,i send my message to wrong room,sorry again __eou__	User

TP	hi what are you doing now __eou__	User

TP	@JVanloofsvelt np.exp is the inverse of np.log... Just apply that to get probabilities. __eou__	User

TP	hello world __eou__	User

TP	@jnothman thanks! __eou__	User

TP	using GaussianMixtureModel.predict I'm able to guess what cluster/distribution the sample(s) belong(s) to, but how do I get the probability of the given sample occurring given that distribution (not a weighted average of all clusters)? __eou__	User

TP	I would like to include data loading/augmentation as the first step of a pipeline, so I can optimize parameters. But I see no way of doing so because of the signature of TransformerMixin (no place for class labels to be returned) . Any ideas? __eou__	User

TP	Hi, anyone replicated IDL SMOOTH function in python successfully ? I infact want to use smooth2 function adopted by JHUAPL (https://github.com/callumenator/idl/blob/master/external/JHUAPL/SMOOTH2.PRO) which uses smooth function of IDL. Eagerly waiting ! __eou__	User

TP	As  a statistics graduate student, I'd love to add inferential tools to scikit-learn. __eou__	User

TP	Would it be a worthwhile effort to implement conformal inference (http://www.stat.cmu.edu/~ryantibs/papers/conformal.pdf) ? __eou__	User

TP	Hello, I'm trying to understand the RCV1 dataset, so I can run some analysis on it. Please has anyone worked with it before? I'd also be grateful to receive help in understanding how to work with CSR __eou__	User

TP	Any good book for machine learning with python? __eou__	User

TP	hello __eou__	User

TP	i want to ask which is more advantageous to use dummy variables or one hot encoding? __eou__	User

TP	@itsmegaurav check out https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn __eou__	User

TP	@xidorn5 I can't speak for the sklearn team, but generally sklearn is intended to target the most widespread, widely-cited, well-established methods for the core package, and to allow third-party packages a lot of freedom in implementing less-established advancements from the literature. __eou__	User

TP	@Pimp_Fada_twitter https://books.google.com/books/about/Introduction_to_Machine_Learning_with_Py.html __eou__	User

TP	Introduction to Machine Learning with Python: A Guide for Data Scientists https://www.amazon.com/dp/1449369413/ref=cm_sw_r_cp_apa_47QGAbZNXX58Z __eou__	User

TP	hi i am working on ml that determine the emergency situation in vehicle(cars) by getting the input from the sensors of the vehicle . so i need a real time OBD2 logged csv file for training in the ml. any help ? __eou__	User

TP	Hello, I tried to use PredefinedSplit as CV for GridSearchCV. I have two sets, first is for training and second is for validation. I made indices array so that for training set indices I put -1 and 0 for testing set. I get reasonable results to grid.cv_results_, but when I test it with the second (testing) set, I get .99 which is clearly not correct. Why is that? __eou__	User

TP	I get warning that some columns are collinear. Is there any utility to find out which columns they are? __eou__	User

TP	Hello, I am considering applying for a [GSoC project](https://github.com/rstats-gsoc/gsoc2018/wiki/SAGA-sparse-linear-models) with the aim of adapting the SAGA algorithm for R, possibly by porting existing code from scitkit-learn. The project, however, needs another co-mentor and I am wondering if there is any scitkit-developer here that would interested in participating? You can message me directly or contact Toby, who is signed up as mentor. __eou__	User

TP	Hello, I want to use the http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html but with the decision boundary always going through the origin. The API doesn't allow this so I wonder if there is a way around this. __eou__	User

TP	I guess I can just use the normal SVM and mirror one example along the origin. __eou__	User

TP	I would like to cluster about 100,000  vectors according to the Pearson distance https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#Pearson's_distance  . Which clustering method would support that? many of them seem to want the Euclidean distance __eou__	User

TP	@lesshaste __eou__	User
TP	hi__eou__	Agent

TP	@guyome80  I am hoping you might be about to tell me about clustering... __eou__	User
TP	Oups, sorry, wrong message...__eou__	Agent

TP	You can try k-medoids __eou__	User

TP	I was thinking about a preprocessing method (Standardscaler) instead of clustering... __eou__	User

TP	K-medoids isnt released yet, but it support any distance metric you wish. you might copy paste the class or build from the branch. The complexity is worse then k means, for 100k should be good __eou__	User

TP	hi __eou__	User

TP	Hi, further on my spell correct algo: Is there a function that gives you the number of occurences of a word (BoW-style)? I would expect that frequent words (preposition) are often in the neighbourhood of other words (nouns) @aph61 Further to my question  before, can you also work with word vectors, like, count the number of vectors for a given word? Makes actually more sense __eou__	User

TP	Hi guys __eou__	User

TP	What's the... eh, correct way to test C extensions without exposing the original class? oops, wrong room, sorry :worried: __eou__	User

TP	Hello all! I just finished compiling a survey with more than 350 open source software project members which, together with some communication theory, we used to define a set of **best practices and guidelines for using Gitter** (in fact, it probably applies to *any chat-like platform*).  If anybody would be interest on it, or in applying them in this community, or your other projects, there is a [small survey](http://bit.ly/survey-and-guidelines) you can fill to get the guidelines. The survey takes something between just *10 seconds to a maximum of 2 minutes to fill* and it is intended to help us validate the guidelines in the future.  There is also a small [article](https://medium.com/@fabiomolinar/validating-chat-communication-tools-guidelines-and-best-practices-fb8852f319da) I wrote with a really short description about the study; in case you would be curious about it.  Of course, feel free to send the [survey link](http://bit.ly/survey-and-guidelines) to anyone you would like to share these guidelines with. __eou__	User

TP	Guys I'm new to contributing to open source projects and I'm trying to resolve issue #10689, the issue is about replacing a depreciated function with new function But when I try to replace the depreciated issue with new issue, and create a PR, my commits are facing merge conflicts because of circle ci and lgtome tools  Can anyone please help me? __eou__	User

TP	try a rebase __eou__	User

TP	Why are partial dependence plots not in the units of the prediction variable? https://stackoverflow.com/questions/49247796/understanding-partial-dependence-for-gradient-boosted-regression-trees __eou__	User

TP	Hi everyone! I have been looking in to multi-output regression the last few days. I know that the scikit-learn package offers a class called multioutput regressor. This class will give every algorithm multi-output support by fitting model for every variable. My questions : Does this package take the relationship between the input variables into account? Which algorithm would work better , a model with a multi-output regression or without (some models support it natively ? __eou__	User

TP	@satishjasthi , I would use Pycharm if the command line version of git is giving you trouble.  Then, do the [visual merge conflict resolution](https://www.jetbrains.com/help/idea/resolving-conflicts.html). __eou__	User

TP	How many documents scikit k means clustering can process at a time? __eou__	User

TP	Just discover gitter. Nice room to join :-) __eou__	User

TP	Do "train" and "fit" mean exactly the same thing or do they have different connotations? __eou__	User

TP	@bsheline they mean the same. I think train mostly comes from the neural net community, while fit comes more from statistics but I'm not sure __eou__	User

TP	i know this is python but does anyone know how to do C#? __eou__	User

TP	hi all __eou__	User

TP	hi send help __eou__	User

TP	Hi guys __eou__	User

TP	<unconvertable> <unconvertable> <unconvertable> <unconvertable> __eou__	User

TP	Hey there __eou__	User

TP	Hey there __eou__	User

TP	hello guys im new in data science can any one please guide me with some books or anything that i should learn __eou__	User

TP	hi guys, I got some question about dimensional reduction using LDA __eou__	User

TP	@idahmed One of my latest favourites is Hands on machine learning with Scikit-Learn and Tensor flow by Aurelien Geron @hndr91 there is an issue opened at Github with some info to take into account __eou__	User

TP	@idahmed : also try this https://developers.google.com/machine-learning/crash-course/ __eou__	User

TP	I have to classify docs into two categories and if the docs does not belong to these two, then i have to identify them in other category i have training data for two. Classes What algo or approach i can use? __eou__	User

TP	Hi , I am a beginner and I try to predict an anomaly in different systems, I have different parameters, which I can use to do more precise prediction. I have a general question, is there a way to do prediction without know with parameters should I use ? sometimes, I don't have all parameters values, so I am thinking if there is a away to do prediction with a minimum parameters __eou__	User

TP	Google feature selection __eou__	User

TP	@naaioa @DrEhrfurchtgebietend more like imputation? __eou__	User

TP	@idahmed the "python data science handbook" is a great introduction and available online for free and in notebook format. @mcasl if I may ask, have you looked at mine as well? I feel the two are somewhat similar in scope, but tbh I haven't looked into Aurelien's book in that much detail __eou__	User

TP	@amueller __eou__	User

TP	@amueller Sure! I have it on my to do list. Just browsed it and seems an impressive work. Eager to read it @amueller Besides of books, I would recommend anyone visiting the materials you have posted regarding your University course on machine learning. I specially liked  the fact of introducing unitary tests, continuos integration engines and git as essentials. Keep up the good work! __eou__	User

TP	hi all __eou__	User

TP	I am doing some testing with gradient descent and labeled data. For X, I have selected two features: one sort of nonsense, and one the EXACT (binary) label. I train on a 66% subset of this data my precision/recall for `y = 1` is 0.00. how is this possible? ```              precision    recall  f1-score   support            0       0.94      1.00      0.97      4194           1       0.00      0.00      0.00       247  avg / total       0.89      0.94      0.92      4441 ``` __eou__	User

TP	Hi,all I am using scikit 0.19.1 I generated a training model using random forest and saved the model. These were done on ubuntu 16.01 x86_64. I copied the model to a windows 10 64 bit machine and wanted to reuse the saved model. But unfortunately i get the following Traceback (most recent call last): File "C:\Users\PC\Documents\Vincent\nicholas\feverwizard.py.py", line 19, in rfmodel=joblib.load(modelfile) File "C:\Python27\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py", line 578, in load obj = _unpickle(fobj, filename, mmap_mode) File "C:\Python27\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py", line 508, in _unpickle obj = unpickler.load() File "C:\Python27\lib\pickle.py", line 864, in load dispatchkey File "C:\Python27\lib\pickle.py", line 1139, in load_reduce value = func(*args) File "sklearn\tree_tree.pyx", line 601, in sklearn.tree._tree.Tree.cinit ValueError: Buffer dtype mismatch, expected 'SIZE_t' but got 'long long'  What could be happening? Is it because of a switch from ubuntu to windows? However i am able to reuse the model in my ubuntu. __eou__	User

TP	@jotes35 i think the issue isn't fixed.  the only feasible solution is retrain it over there in the new architecture, and yes it seems to be because of different architectures, not because it's windows and ubuntu, it happens even on ubuntu and another version of ubuntu have a look at this https://github.com/scikit-learn/scikit-learn/issues/7891 __eou__	User

TP	in general, what are good starting points for `min_samples_leaf` in random forest classifiers? I find that `0.005` gives me decent results but Im afraid Im overfitting. __eou__	User

TP	Thanks @greed2411 . Retraining on the new architecture works. __eou__	User

TP	I have a few thousand points on a line and most of them are in a dense part with a few hundred spread out more widely. What is a good way to find the dense part? This looks like a clustering problem with one cluster plus noise __eou__	User

TP	How to get weight for signs(Perceptron, binary classification)? In what function they are? __eou__	User

TP	hello everyone __eou__	User

TP	`D3XT3R` HoL `D3XT3R` a __eou__	User

TP	did everyone really stop talking  as soon as I showed up? __eou__	User

TP	@quant12345 you can look into the class's attributes. Say if you are using an instance called `mlp` belonging to ` MLPClassifier` class, you can check them at `mlp.coefs_` and `mlp.intercepts_` after fitting/training the model. More on this here : http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier __eou__	User

TP	@greed2411                                                                                                                                          Thank you, for the answer. That is, the "mlp.coefs_ " weights for sign, "mlp.intercepts_ " predicted class labels. There is an example with three classes. I multiplied the signs by weight. The values of each class were obtained: 1. 0.40-043 2. 0.47-0.51 3. 0.24-0.35 Is it possible to get from the function thresholds, which determine the belonging to each class?  regards __eou__	User

TP	I don't think mlp works that way. You can't determine which neuron made it predict that output @quant12345 . But try googling it.im not entirely sure either. __eou__	User

TP	@lesshaste if you dimension is low enough (e.g. max 300, the lower the better), then density based clustering such as DBSCAN or https://github.com/scikit-learn-contrib/hdbscan should work well for that case. __eou__	User

TP	@ogrisel  Thanks.. in my case the dimension is 1 I will see what dbscan gives you. Is it really happy to find just one cluster and ignore the outliers? @ogrisel  ok thanks. dbscan in 1d it is :) __eou__	User

TP	it's worth a try with DBSCAN. You do not choose the number of cluster but the typical distance between two points that are to be considered as "core points" that is point in high density regions. There are also a bunch of other hyperparams. Read the scikit-learn docs and the hdbscan docs to learn more about this family of estimators. __eou__	User
TP	will do__eou__	Agent
TP	just shape you data as `(n_samples, 1)` that is `n_features=1`.__eou__	User
TP	thanks__eou__	Agent

TP	hello guys anybody here ? __eou__	User

TP	hey __eou__	User

TP	I'm trying to understand when `fit_params` would be used in `_fit()` in a `Pipeline`.  ```python def _fit(self, X, y=None, **fit_params):     ... ```  Are there any example Pipelines that make use of this functionality? Is the point of this to allow passing additional data to a `fit`? If the point is to pass parameters what is the use case over adding these parameters to the estimators init? It seems like passing parameters would kind of go against https://github.com/scikit-learn/scikit-learn/issues/1975 no? __eou__	User

TP	I was also curious why `BaseEstimator` supports recursive `get_params`? Are there times when an estimator will have another estimator as a parameter? __eou__	User

TP	 __eou__	User

TP	Is there any way to get the uncertainty on the fit parameters out of a `RANSACRegressor` or an underlying `base_estimator`? __eou__	User

TP	Is there Any existing function for computing partial correlation coefficients ? __eou__	User

TP	hi __eou__	User

TP	Hi guys __eou__	User

TP	I'm getting an error when I try to train a MultiOutputRegressor using sparse (csr) numpy matrices for both X and y I get: ``` AttributeError: 'MultiOutputRegressor' object has no attribute 'fit_transform' ``` __eou__	User

TP	HI __eou__	User

TP	@gonesbuyo_twitter it is a regressor. The meaningful method is `predict` __eou__	User

TP	Hi! Read all that I found about RandomForestClassifier. Is it possible in any way from 100 trees to make 1 common? The fact that I want to move the logic of the tree with cut-off thresholds in another program. With one tree it is easy to do. And here is how to be with 100 trees from a random forest? Only  if  operator will get a few hundred. Or maybe to calculate importance of characteristics using random forest and then build 1 the  decision tree with relevant signs? What can you advise on this problem in General? __eou__	User

TP	@quant12345 numirate your questions, please [Answer](https://stackoverflow.com/questions/7152470/how-i-can-merge-two-binary-trees) on the first question Maybe with one tree it's incomplicated problem, but I think after concationations, your tree will be so big __eou__	User

TP	@istom1n_twitter  Looked graphs of all 100 trees. They have different architectures. Do even, if possible to unite them, the tree will be huge. regards __eou__	User

TP	1.  To take advantage of a random forest, for later use in a single decision tree graph(diagram). Getting significant features from random forests. Then I use only these features in one solution tree. What do you think about this? 2.  Is there a need to balance classes in the random forest and decision tree(the number of signs in class 1-two times more than in the 2nd class)? If so, what features should I use? __eou__	User

TP	@quant12345 2. Balabcing, [first in search](https://www.geeksforgeeks.org/how-to-determine-if-a-binary-tree-is-balanced/) 1. I don't understand your questions, if you mean reduce your tree after connections, I think you can get Dijkstras Algorithm, Minimum Spanning Trees I found good [explanation](https://web.eecs.umich.edu/~akamil/teaching/sp03/041403.pdf) __eou__	User

TP	@istom1n_twitter  Thank you! On the second issue was meant another. Already found a solution: class_weight =balanced __eou__	User

TP	depends about what tree we're talking, if binary, them balanced is in the left and in the right, we have equal amount of nodes Im only now get your message, strange Alright, thats correct, you are welcome __eou__	User

TP	Question in that, if we connected 100 tree, for example, we get a graph, not tree obviously, and our problem is to find minimum spanning tree __eou__	User

TP	Hello everyone, __eou__	User

TP	hii __eou__	User

TP	anyone use Named Entity Recognizer ?? __eou__	User

TP	anyone active here? Hello? Hello Prady. __eou__	User

TP	Yes __eou__	User

TP	Hello Dark Knight (Bruce Wayne) <unconvertable> By the way I love Dark Knight __eou__	User
TP	Thanks__eou__	Agent

TP	So are you working on machine learning? __eou__	User

TP	Yes I am __eou__	User
TP	What exactly?__eou__	Agent

TP	@pradyumnad   you kid me? __eou__	User

TP	I am here to start journey __eou__	User

TP	[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/VVFY/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/VVFY/image.png) __eou__	User

TP	As anybody encountered PicklingError when trying to use pipeline.predict with pool.apply_async ? PicklingError: Can't pickle <function Pipeline.predict at 0x1c655cd08>: it's not the same object as sklearn.pipeline.Pipeline.predict The code that I'm using is -  result_train = pool.apply_async(       estimator.predict,       (train_data['X'], ),     ) I tried wrapping it into a function. It still throws the same error. def wrapper(*args):  func = args[0]  func(*args[1:]) result_train = pool.apply_async(      wrapper,      (estimator.predict, train_data['X']),    ) Okay, let me try that. Yes, it works. Thanss @amueller. Where can I read more about this error ? __eou__	User

TP	what's the error? __eou__	User

TP	yeah you can't pickle instance methods like that write a new function that is not a method and does the prediction. make estimator the argument, not estimator.predict __eou__	User

TP	I don't know.  google pickling instance methods, and you'll see that's not possible sklearn.pipeline.Pipeline.predict is a bit of a wild beast and not a normal class method, so you can't pickle it __eou__	User

TP	Is there a reason Scikit doesnt parallelize predict ? __eou__	User

TP	Since estimator wont mutate as a part of the computation, would it be safe to use it across the threads ? __eou__	User

TP	it's often slower, depending on estimator and dataset size __eou__	User

TP	I am working with text data. I'm using TfIdf right off the shelf. I noticed that MaxAbsScaler gives a better performance than StandardScaler(with_mean = False). I found this link: http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py. This prompted me to try out different scaling transformers. Is there anybody who has worked with text data before ? I wanted to know if "selecting scaler" is worth time spending on ? If it is, the link uses feature distributions, outliers to select a scaler ? Do the above concepts make sense in the space after TfIDF transforms the text data ? __eou__	User

TP	@AMaini503 I think we should deprecate StandardScaler(mean=False). It seems weird to me but I would just try a bunch and see what makes sense. There's no general rule __eou__	User

TP	Hello __eou__	User

TP	@kelux19 Hey people! ***   string[] name = {"Malin", "Manar", "Stefan", "Ali", "Alexandra", "Robert","Suzana" }; string[] property = { "kind", "bad", "talentfull", "helpfull", "nice", "melancholy", "egoistic" }; string[] role = { "teacher", "system developer", "student", "musician", "programmer", "actor", "doctor"};  *** how can i pick from these three arrays randomly, for example "Ali is a kind teacher"? its c# btw __eou__	User

TP	@kelux19  I don't think so there is any api to scikit-learn in c#. But you might want to wait for others' answers as well. __eou__	User

TP	I'm using  a custom tokenizer for the TfIDF. I trained the model and saved the pipeline object to a pickle file using joblib. However, when I try to load the pickle file in a different script, it throws an error: module '__main__' has no attribute 'MyTokenizer'. Am I pickling the model correctly ? __eou__	User

TP	You should put the code for the MyTokenizer class in an importable module that his installed somewhere in the python path in the Python where you load the model. Alternatively, you can use `cloudpickle.dump / load` instead of joblib. It will be slightly less efficient if your model has large numpy arrays as attributes but this is probably not a problem in practice. __eou__	User

TP	The first solution that you mention works. I read that joblib pickles by remembering the paths to objects So, if I create a function in an interactive session, there is no path to that function. Putting that into an importable module makes it work __eou__	User

TP	@ogrisel Coming back to the pickling issue. The way I was using it in another script was by redefining the function. __eou__	User

TP	Now, I'm trying to use the pipeline object inside a class. In the __init__, I try to load the model, it throws the same error again. I tried defining the function inside the __init__, but still the pickle doesn't see the function. Any workaround for this ? Here is the paste: https://pastebin.com/7rLmaxxJ. Open to suggestions from others. __eou__	User

TP	@AMaini503  please provide a Minimal Complete Verifiable Example in your paste (http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) __eou__	User

TP	I'm trying to get cluster assignments from the linkage matrix. So far, I know that I can use the fcluster to do that. But, how do I specify the n_clusters ? __eou__	User

TP	hello __eou__	User

TP	I'm facing an issue in importing svm __eou__	User

TP	__eou__	User

TP	Is there a reason sklearn.ensemble.partial_dependence only works on gradient boosting? __eou__	User

TP	@ucalyptus https://stackoverflow.com/questions/15274696/importerror-in-importing-from-sklearn-cannot-import-name-check-build __eou__	User

TP	Can anyone help me with this: https://stats.stackexchange.com/questions/350520/evaluation-of-machine-learning-model-in-production ? TL;DR - I'm trying to evaluate a model in production. I need advice on what metrics to use ( excluding explicit evaluation on a data set ). __eou__	User

TP	Hi all, do the kernel density estimators in scikit-learn allow for diagonal (D class) bandwidths, and do any of them also use a leave-one-out bandwidth estimation? __eou__	User

TP	@tanimislam you can use gridsearchcv for that. (bandwidth estimation) there's no efficient implementation if that was the question __eou__	User

TP	 __eou__	User

TP	Hi everyone, I am Florian and I am a co-organizer of the Python sprints meetup in London (at least I hope so as this is my first time next week). We are planning on doing a small documentation sprint on a few open source libraries. The main organiser is planning to address some issues and structural changes in the pandas documentation and I wanted to help a separate group work on a different library. I thought that maybe https://github.com/scikit-learn/scikit-learn/issues/10453 would be a good issue to work on with a small group of people for about 2-3 hours since it overlaps nicely with the other group. Is that something that would be helpful and is there someone working on it already (did not see anyone on github picking it up so far)? __eou__	User

TP	I don't see anyone working on the issue for the moment and it seems a good opportunity. It would be beneficial for the project since we start to have some feature (e.g. ColumnTransformer) which can benefit from the pandas. __eou__	User

TP	ok great, I see where we can get to on Thursday __eou__	User

TP	What is the difference between logloss and cross entropy? Sorry I am a newbie in this field __eou__	User

TP	hey guys, I am trying to visualize a high dimensional RNA dataset with TSNE [![image.png](https://files.gitter.im/scikit-learn/scikit-learn/6rZd/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/6rZd/image.png) However I get the error ValueError: could not convert string to float: 'sample_800' Any inputs on this? __eou__	User

TP	@ziweiwu which line is the error on? __eou__	User

TP	Apparently you have string in your data which you try to convert into numeric __eou__	User

TP	I see. I have solved this issue, what I did is to convert my df to df.values. so only numerical datas are inputted into the algorithm __eou__	User

TP	Hey all, Is there a way to pass a sentence like play some music to an AI model and after that chrome tab will open with a random YouTube song Ive been trying to figure out a way to do this for a while and I appreciate any advices or help Just curious if anyone has a way or approach how to wrap this up, advice is highly appreciated __eou__	User

TP	Hi guys trying to analyze a data set for predicting employee absenteeism Can you please suggest that linear regression is good or I need to use a time series model. My dataset is https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work __eou__	User

TP	Based on its description, the dataset you linked does not look like timeseries data: it's a table of feature data and one can probably make the i.i.d assumption. This is not a series of events with a timestamp for each with a progressing time dependency.  For supervised regression of the number of hours of absenteism, I would indeed try linear regress ion (+ some feature engineering) and compare with a more complex model such as RandomForestRegressor __eou__	User

TP	Hello! I have taken machine learning class from udacity and now there's a problem while loading a pickle file. The file is just 15MB and I've 4GB RAM. Anyone who can get me through this problem? __eou__	User

TP	@loginofdeath Hi, I am using this pipeline object pipe = Pipeline(   [    ('fresh',     FeatureAugmenter(column_id='index', column_sort='tick', default_fc_parameters=MinimalFCParameters())),    ('scaler', StandardScaler())    ]) If I fit_transform it on the train data and get some features, will I get the same features on the test data too so that I can give it  to a classifier? __eou__	User

TP	I apologize for what I'm sure is a common question, but is there a schedule for a 0.20 release?  "No" or "a long time from now" are both useful answers.  I don't want this to be interpretted as "please do a release", I'm just planning some activities and knowing this would help __eou__	User

TP	As fast as possible :) __eou__	User

TP	We are trying to finish up couple of issues and we would like to have a release candidate for SciPy However we might have some delay depending on how it will go :) __eou__	User

TP	Thanks for the response.   Given the "trying to have a release candidate for scipy" statement I'm interpretting this as "weeks away", not days and not months __eou__	User
TP	Yep__eou__	Agent

TP	Hi folks, excuse my ignorant question. I was wondering if it is possible to pass a tensorflow model into scikit OneVsRestClassifier, if I expose fit and predict methods in the tensorflow model? I couln't find an example when I googled it. __eou__	User

TP	@kirk86  I don't know about tensorflow. But keras has wrappers to make models a part of sklearn workflow. I think you can take a look at the source for these wrappers. Here's the link: https://keras.io/scikit-learn-api/ __eou__	User

TP	can knn be used for One Class classification? __eou__	User

TP	Would it make sense for http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html to have a transform method? Currently you can't train it on a training set and then use it on a test set it seems __eou__	User

TP	Hey guys, is there a nice book or a course online to understand how the MLP and the SGD classifiers implemented in Scikit-learn work? I'm looking at the code, but I can't understand it, without having a basic understanding of the theory behind __eou__	User

TP	https://chat.whatsapp.com/EY44e81jzzCFLXEkY0tjFc __eou__	User

TP	I have some 2d data which I think would be well classified by two straight intersecting lines. One straight line is logistric regression I believe but how can you get two straight lines as the decision boundary? __eou__	User

TP	oh verbosity = 2 does that is it possible to get tpot to optimize the ROC when doing classification? oh that is there too sorry __eou__	User

TP	If you have this code: tpot = TPOTClassifier(generations=20, population_size=50, scoring='roc_auc', verbosity=2, n_jobs=-1) tpot.fit(X_train, y_train) print("TPOT score", tpot.score(X_test, y_test)) is the score being printed the roc_auc score? __eou__	User

TP	I am going to guess it is __eou__	User

TP	Hello, I'm just not getting anywhere. I have a regression ANN with four inputs and want to draw a contour plot.  So two variable variables and two fixed. Does anyone have an example or can you give me a hint how I do this? __eou__	User

TP	> is the score being printed the roc_auc score? accuracy for classifier and r2 for regressor __eou__	User

TP	@glemaitre  sorry I was really asking a  a tpot question. I think it prints whatever you used when defining TPOTClassifier __eou__	User
TP	so it depends what TPOTClassifier from which class is it deriving but I assume `ClassifierMixin`__eou__	Agent
TP	as in "TPOTClassifier(generations=20, population_size=50, scoring='roc_auc', verbosity=2, n_jobs=-1)"__eou__	User
TP	where scoring is defined  as roc_auc__eou__	User
TP	but I could be wrong of course__eou__	User
TP	TPOTClassifier is not something in scikit-learn so I am not aware of how it works or implemented actually__eou__	Agent

TP	of course.. sorry for asking a slightly off topic question here __eou__	User

TP	no problem but looking quickly at their base class, score is calling the associated string score from sklearn so I think that your guess was good __eou__	User
TP	great, thanks!__eou__	Agent

TP	on another topic, I have lots (1000s) of pairs of vectors. For each pair on the left there is exactly one pair on the right that it should be associated with I could make a classification problem out of it by concatenating the vectors and labelling them with 0 or 1 but 1/1000s of the labels would be 1.  That is the fraction of 1 labels would be tiny is there a standard way to approach this sort of problem? oh that sounds interesting thanks.. I will see if that is appropriate  The main restriction is that I need to do out of sample prediction I did also look at scikit learn's kernel pca might that work? you would need to feed in a full set of distances which would only ever be 0 or 1 I don't know if that is generally a bad idea for kernel pca http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html me neither! __eou__	User

TP	I am not really familiar but it could something linked to metric learning in which you want to know the relative distance between pairs and know which pairs have the minimal distance https://github.com/metric-learn/metric-learn I don't have enough background there :) __eou__	User

TP	I would presume that having a continuous distance is what you would need __eou__	User
TP	I need to find a nice expert online as I don't know anyone in real life who could help__eou__	Agent
TP	That's my intuition too__eou__	Agent

TP	```  def get_age(df):     return df[['age']]  preprocess_pipeline = Pipeline([             ('get_cols', FunctionTransformer(get_age, validate=False)),             ('median_impute', Imputer(strategy='median')),             ('min_max_scale', MinMaxScaler())         ])  with open('./preprocess_pipeline.pkl', 'rb') as handle:     preprocess_pipeline = pickle.dump(handle) ``` I want to dump and load this pipeline but it requires me to define `get_age` function again when I want to load. I don't want to duplicate my `get_age` function in 2 different files, how can I include only 1 column (`age` in this case) in the pipeline dump/load friendly way? I've tried using lambda function and joblib function of sklearn, didn't work __eou__	User

TP	Anyone knows on which paper are based the formulas in: http://scikit-learn.org/stable/modules/sgd.html#id1 for the `weights` and the `learning rate` of the `SGDClassifier`? I couldn't find the exact same formulas in the papers linked at the bottom of those sections. __eou__	User

TP	@h4k1m0u probably bottou's sgd code __eou__	User

TP	@AMaini503  thanks for the pointers I was trying to achieve the same outcome but with pure tf. Not luck so far :( __eou__	User

TP	I am fitting several SVRS to images and get ~3MB pickled model file size for ~20MB image data. Does the pickle contain any redundant data (training or otherwise) or is that a reasonable size? __eou__	User

TP	Sprints!!! __eou__	User

TP	Hi all, I would like to know whether there is interest in adding a least absolute deviation regression with L1 penalty to sklearn. The optimization problem is very similar to the Lasso one, but it uses a L1 likelihood rather than L2. __eou__	User
TP	@mirca that's probably a better question for the mailing list. what's the application? robust regression? How does it behave differently from huber in practice?__eou__	Agent

TP	@amueller yes, thats mostly for robust regression. I dont have the answer on how it compares to huber right now, but I would guess least absolute deviation would work better on sparse settings. I will investigate that and follow up on the mailing list. Thanks! __eou__	User
TP	:)__eou__	Agent

TP	we're in 105 for now __eou__	User

TP	A list of issues/stalled PRs are tagged "sprint" which could be good selection for the sprint. https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3ASprint __eou__	User

TP	also if you're new the ones tagged "good first issue" https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22+sort%3Aupdated-desc __eou__	User

TP	thanks @amueller and @glemaitre ! __eou__	User

TP	I have a question about the issue here: https://github.com/scikit-learn/scikit-learn/issues/9352 Where should docker be run? https://github.com/scikit-learn/scikit-learn/blob/master/.travis.yml#L62 I assume that we should copy over install and testing scripts and run in a docker container, is that the right path? __eou__	User

TP	ran tests in the CentOS32 bit docker image. 13 failed, investigating why first __eou__	User

TP	@jrmlhermitte yeah I think travis @jrmlhermitte olivier is at the back of the room ;) __eou__	User

TP	@jrmlhermitte you can report the errors you get in that issue about 32bit bits in any case-- it might be helpful for future reference.. __eou__	User

TP	thanks i posted them here : https://github.com/scikit-learn/scikit-learn/pull/11515 waiting for travis to build but i thikn i can start trying to debug locally __eou__	User

TP	I'm hungry food? where should we go? __eou__	User

TP	I agree this is time :) Whatever works where there is a vegetarian option __eou__	User

TP	the vietnamese place or the indian place? or the food carts if they are there? __eou__	User

TP	can someone merge this please? https://github.com/scikit-learn/scikit-learn/pull/11289#pullrequestreview-137246243 __eou__	User

TP	done __eou__	User

TP	https://sci-hub.tw/https://doi.org/10.1002/sim.1822 __eou__	User

TP	Can someone merge this one https://github.com/scikit-learn/scikit-learn/pull/11391 :) __eou__	User

TP	@amueller if you are OK with https://github.com/scikit-learn/scikit-learn/pull/11431 should we merge it? __eou__	User

TP	Hi all, quick question: how do I run a single test locally? For example the docttests? Thanks! __eou__	User
TP	make test-doc__eou__	Agent
TP	or pytest file__eou__	Agent
TP	Great, thanks!__eou__	User

TP	To run a single test you can also run, ``` pytest sklearn/file_path..  -k part_of_test-name ``` providing both the path to the file and a part of the test name will make test collection faster. See http://scikit-learn.org/dev/developers/tips.html#useful-pytest-aliases-and-flags __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/11469 I push some changes @amueller @rth __eou__	User

TP	James Bourbeau (@jrbourbeau) would like to chat with us about how to make GridSearchCV play nicer with dask-ml. If you have a preferred time today, let me know! Otherwise we'll probably just drop by at a random time :-).  To sum up, in GridSearchCV something check that the score returned by the metric is a number, so they dask-ml needs to call `.compute` explicitly which has a bit of friction with the rest of their API (everything is lazy). __eou__	User

TP	Hi all, I'm afraid I am still a bit test-confused (newbie here). When running `make test`, I can see errors (like `scikit-learn/sklearn/impute.py:547: DocTestFailure`) related to the docstring in the Imputation, but `make test-doc` does not surface these. __eou__	User

TP	@lesteve we you want __eou__	User

TP	Ok, `pytest sklearn/impute.py::sklearn.impute.ChainedImputer`  does the trick! I guess that was in the docs, thanks again! __eou__	User

TP	FYI we'll drop by in 5 -10 minutes __eou__	User

TP	Sorry I missed you @lesteve . I merged the Python 3.7 ABC warning  fix in 0.19.X and triggered a new build on the MacPython/scikit-learn-wheels repo if it goes well I can push those wheels to pypi.org __eou__	User

TP	I'm getting a doctest failure when building from source on master It's getting raised from modules/compose.rst Has anyone else here run into this? __eou__	User

TP	sprint will end in 10 minutes also: if you want to ask me something, you have 10 minutes because then I'll f off __eou__	User

TP	does anyone understand what's happening with lgtm? seems like pypi timeouts? @jnothman ? __eou__	User

TP	@ogrisel do you have an opinion on https://github.com/scikit-learn/scikit-learn/pull/11469#discussion_r202540743 ? @lesteve is there an issue tracking the joblib pickle thing? __eou__	User

TP	Yep, let me find it. https://github.com/scikit-learn/scikit-learn/issues/11408 __eou__	User

TP	There is to pending PR waiting for an extra review to be merged https://github.com/scikit-learn/scikit-learn/pull/11469 https://github.com/scikit-learn/scikit-learn/pull/11391 __eou__	User

TP	Anyone interested in a reviewing a PR on PyPy support ?  https://github.com/scikit-learn/scikit-learn/pull/11010 it's mostly a CI setup + very light changes to make tests pass .. Maybe @lesteve ? :) __eou__	User

TP	I gave my +1 __eou__	User

TP	FYI I disabled the lgtm webhook (or tried to?) because it's confusing for the sprinters __eou__	User

TP	Any additional review on the MissingIndicator is welcomed https://github.com/scikit-learn/scikit-learn/pull/8075 @amueller It seems it was done on purpose https://github.com/scikit-learn/scikit-learn/commit/b4561f09e6e0ff8a2e16f09be21b3202012bbdd7 oh right __eou__	User

TP	Anybody can review this one https://github.com/scikit-learn/scikit-learn/pull/9616 __eou__	User

TP	does anyone know what happened here? https://github.com/scikit-learn/scikit-learn/pull/11504 there's something funky going on with the pooling_func deprecation __eou__	User

TP	@amueller __eou__	User

TP	@amueller  When I post a future warning, am I to assume we are  currently on version 0.20 of sci-kit learn? __eou__	User
TP	@annaayzenshtat right__eou__	Agent
TP	ok, thanks!__eou__	User

TP	fixed in #11537 it's unused in agglomerative clustering but not feature agglomeration which inherits the fit... __eou__	User

TP	(and that's why I don't want deprecation warnings in the examples and tests ;) __eou__	User

TP	@amueller  When I do a pull request, should it be against the master branch? __eou__	User
TP	It should be against master__eou__	Agent
TP	thanks!__eou__	User

TP	An update from Paris: we have a dozen of people working on a variety of small issues or reviewing (they have marked the issues that they are working on). I think that some of us are going to prioritize issues needed for the release. __eou__	User

TP	great __eou__	User

TP	we will in the office in 30 minutes or so ready to light up __eou__	User

TP	Awesome. I am busy finishing details on small PRs that are important for the release. I would like to move a bunch of issues to the next milestone. I think that we should not delay the release. If needed, we can discuss this quickly. I think that we should delay KMedoids It would be good to have it, but this release is already a big one __eou__	User

TP	it will not be in 30 min, Guillaume was a bit optimistic :) __eou__	User

TP	It's good to be optimistic! __eou__	User

TP	anyone looking for a simple issue that is pure doc: https://github.com/scikit-learn/scikit-learn/issues/9196 __eou__	User

TP	Anyone able to review a small PR using BLAS? ( https://github.com/scikit-learn/scikit-learn/pull/11420 ) __eou__	User

TP	but we eventually got in the office now __eou__	User

TP	OK. Looking at the list of issues for the next release: https://github.com/scikit-learn/scikit-learn/milestone/24, I would like to postpone a few. Things like #11520 , #8642, #9098. My goal is to move us forward to the release. Any objections? Also, #11408 needs a review. It fixes one of the problems of the build @amueller : OK, but this list is too long. We are going to have to make choices. The delay in the release is not good for our users. I am happy focusing on things to get them done (including #11520), but we will not be able to address all this. Great. I'll have a look at #11520 right now @amueller is adding issues to the milestone faster than we can close them :) Yes, but it's quite depressing to see the percentage going down as we work. I really worry about feature creep. I was expecting this release to be out a couple months ago. __eou__	User

TP	I don't want to postpone the Yeo-Johnson because it will be the default for PowerTransform __eou__	User

TP	basically if we don't do #11520 then we'll release with power transform only working on non-negative data and we need a deprecation cycle to change the default to yeo-johnson __eou__	User

TP	I'm also untagging some things right now, and tagging other things as blockers yes I know it is I think joris is creating a github project board right now __eou__	User

TP	Hi . please help me about visualization of SVM, cross validation and its performance measures __eou__	User

TP	what' about #9723 ? __eou__	User
TP	If we consider it as a bug fix then this is ready to be merged.__eou__	Agent
TP	I really think that it was actually a bug__eou__	Agent

TP	yeah looks like it from a glance @GaelVaroquaux I kinda wanted estimator tags in, but I guess we might not be able to do that :-/ KMedoids? #11099  I'd say delay #10058 NCA I'd say delay as well __eou__	User

TP	I do think that NCA is delayed :( __eou__	User

TP	we need a second review on https://github.com/scikit-learn/scikit-learn/pull/11464 __eou__	User
TP	ok most PRs that are tagged either are small and have +1 or are bug fixes or blockers__eou__	Agent
TP	ok apart from KMedoids I think tagged PRs are relatively reasonable right now__eou__	Agent
TP	anyone have oppinions on KMedoids?__eou__	Agent

TP	We also need to fix the MICE/ChainedImputer/IterativeImputer https://github.com/scikit-learn/scikit-learn/pull/11350 basically the remaining question would be link to the default imputer to use (RidgeCV or RandomForest) __eou__	User

TP	I commented on this choice directly in the PR, to keep the discussion on github. __eou__	User

TP	@GaelVaroquaux do you have opinions on https://github.com/scikit-learn/scikit-learn/issues/11536 ? there is tons of convergence warnings everywhere currently also, the examples and tests have tons of warnings about iid. Should we do something about that? not sure if it's worth in the examples, in the test we should probably catch @ogrisel also would love to hear your thoughts on https://github.com/scikit-learn/scikit-learn/issues/11536 __eou__	User

TP	@GaelVaroquaux sorry :-/ only thing that I'm noticing now that we broke __eou__	User

TP	what feature creep? you mean scope of sklearn or scope of this release? I'm just going through examples and dogfooding all the changes we did and make sure the user experience is sane like the iid change can easily lead to a wall of deprecations. I guess the user can always catch them... @glemaitre the one you just posted lol this is related but not the consistency: #6425 this is the actual issue #7242 __eou__	User

TP	Opinions of what should be done about the blocking  euclidean_distance in 32 bit issue would be welcome https://github.com/scikit-learn/scikit-learn/issues/9354#issuecomment-405314164 __eou__	User

TP	this is ready for reviews: https://github.com/scikit-learn/scikit-learn/pull/11561 __eou__	User

TP	This could be merged (MRG+2): https://github.com/scikit-learn/scikit-learn/pull/9616 __eou__	User
TP	I was trying to press the green button on #9616 but got many unicorns__eou__	Agent
TP	what is #9616__eou__	User
TP	because gitter says that it is not found__eou__	User

TP	OK, we are wrapping up here in Paris. I am signing off, as I have "real work" to do for tonight. Sorry __eou__	User

TP	thanks! __eou__	User

TP	@amueller https://hub.github.com/ __eou__	User

TP	or this without dependencies: git push https://github.com/lmcinnes/scikit-learn pr/8554:sparse-LLE-Isomap -f __eou__	User

TP	@amueller You seem to be interested in merging this one since the test are passing https://github.com/scikit-learn/scikit-learn/pull/8075 __eou__	User

TP	what about https://github.com/scikit-learn/scikit-learn/pull/8760 ? anyone working on that? is that a regression? __eou__	User

TP	I'm working on it (PR related to issue #8720 which I commented) __eou__	User

TP	@rth do you wanna review the openml loader? #11419 ? anyone else have a quick review for #11561 ? __eou__	User
TP	I'll do that while waiting for some benchmark__eou__	Agent
TP	thanks :)__eou__	User

TP	@amueller : feature creep = scope of release. Not scope of scikit-learn. I think that we are doing good on this. __eou__	User
TP	ok__eou__	Agent

TP	@amueller https://github.com/scikit-learn/scikit-learn/pull/10198 (OneHotEncoder.get_feature_names, was not tagged apparently) __eou__	User

TP	someone look at #11567 ? ;) (still opening issues and PRs quicker than people can close them) __eou__	User

TP	Hey Austin, while both y'all and we all are awake, how about we schedule a hangout for tomorrow I miss you all! __eou__	User

TP	Alright, which time would be fine with you? __eou__	User

TP	jan: https://github.com/scikit-learn/scikit-learn/pull/11570 __eou__	User

TP	@rth https://github.com/scikit-learn/scikit-learn/pull/11561 merge this since your comments have been addressd __eou__	User

TP	@rth do you know about this? 11572 #11572 it's about AUC reorder __eou__	User

TP	@amueller https://github.com/scikit-learn/scikit-learn/pull/10495/files __eou__	User

TP	can someone change the working in https://github.com/scikit-learn/scikit-learn/pull/10495 from interpret to convert? __eou__	User

TP	I'll do a PR, rewording it __eou__	User

TP	Second review needed on the SimpleImputer which was buggy for sparse matrix https://github.com/scikit-learn/scikit-learn/pull/11496 __eou__	User

TP	Sorry, I missed the reply about the hangout. Let's do it when you guys are up, and maybe an hour or so after you have started. That gives you time to get organized. __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/issues/11536 is blocker and needs a discussion __eou__	User

TP	scikit-learn/scikit-learn#11557 ready for review __eou__	User

TP	@amueller #11577 __eou__	User

TP	@ogrisel  could you have a look at a small BLAS related change in https://github.com/scikit-learn/scikit-learn/pull/11420 by @jakirkham Thanks. __eou__	User

TP	quick reviews on #11593 please? __eou__	User

TP	@amueller https://github.com/scikit-learn/scikit-learn/pull/11592 __eou__	User

TP	@amueller  maybe you can merge #11124 (or anyone who can click the green button !0 ) __eou__	User

TP	review https://github.com/scikit-learn/scikit-learn/pull/11535 ? __eou__	User
TP	@massich : done for #11124__eou__	Agent
TP	@agramfort : done__eou__	Agent

TP	@GaelVaroquaux can you have a look at #11589 ? __eou__	User

TP	The travis backlog is starting to be a major problem. Does someone have a contact at travis to ask them if they could bump our plan for the day? @jbschiratti : on it __eou__	User

TP	Can I get comments in #11563 maybe it can be merged without CI __eou__	User
TP	I don't have a contact. I had talked to them in the past, sometimes it works. You're in the same time zone as them @GaelVaroquaux ;)__eou__	Agent
TP	btw I think https://github.com/scikit-learn/scikit-learn/pull/11570 should basically be good t ogo__eou__	Agent

TP	@GaelVaroquaux are you contacting travis? __eou__	User

TP	What contact should I use? For travis? __eou__	User

TP	By the way I'll like to have discussion regarding Stacking and the issue of fit.transform != fit_transform such that we can unlock the PR I know this is not for the coming release thought __eou__	User

TP	they have an email thing on the website @GaelVaroquaux support@travis-ci.com well but this sounds like we need a slep so the outcome of this discussion is: we disagree so far I'll write a slep, we can discuss it and we can vote __eou__	User

TP	If we want to do a hangout, it's in the next hour or so. @amueller : on it Good with the call. Do you want to call me When you do, I'll run in the room next door, and whoever wants will join me __eou__	User

TP	are there particular topics for the hangout? the stacking fit_transform? pandas column names? the governance doc? __eou__	User
TP	All these seem important__eou__	Agent

TP	Done for travis. We'll see what they reply __eou__	User

TP	whether we should backport estimator-tags to the RC branch if I can finish it within a week lol ;) (semi-kidding) should we do the call? __eou__	User

TP	I'll try to call you. __eou__	User

TP	we can call you by phone on a free mobile __eou__	User

TP	> The travis backlog is starting to be a major problem.  There is an auto-cancellation feature for PRs with outdated commits  ref: https://blog.travis-ci.com/2017-03-22-introducing-auto-cancellation There's a similar feature for PRs on AppVeyor HTH __eou__	User

TP	I think that we have that activated __eou__	User

TP	yes, both in Travis and AppVeyor, but not circle ci.. __eou__	User

TP	@amueller The PR regarding the n_estimators=100 in Forest is ready for review https://github.com/scikit-learn/scikit-learn/pull/11542 __eou__	User

TP	@GaelVaroquaux and other people there: are you OK with switching the default in `ColumnTransformer` from `remainder='passthrough'` to `remainder='drop'` ? __eou__	User

TP	The motivation is that silently passing through uniquely identifying columns (e.g. a user id) will lead to catastrophic  overfitting that beginner users will have a hard time to debug. __eou__	User
TP	also changing the mode of one-hot-encoder to sparse=False in the "new mode"__eou__	Agent
TP	Furthermore, it would also make most of our examples a bit more concise.__eou__	User

TP	appveyor is failing very quickly (less than 1min) for a lot of PRs __eou__	User

TP	We have a script for fast cancelling old builds in c-f. Here's a [usage example]( https://github.com/conda-forge/numpy-feedstock/blob/0d8089f7d346fabaa8bf1ea948d89a9c42e9f9c8/.circleci/fast_finish_ci_pr_build.sh#L3-L4 ). This works on Travis CI, AppVeyor, and CircleCI as long as the proper environment variables are supplied  ref: https://github.com/conda-forge/conda-forge-ci-setup-feedstock/blob/master/recipe/ff_ci_pr_build.py __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/11583 __eou__	User

TP	scikit-learn/scikit-learn#11585 on sparse PCA needs review __eou__	User
TP	I will look at it now__eou__	Agent

TP	https://github.com/scikit-learn/scikit-learn/pull/11599 __eou__	User

TP	Thanks @jakirkham , that's very useful. I'll look into it. __eou__	User

TP	here's how appveyor kills all but last commit https://github.com/conda-forge/staged-recipes/blob/master/.appveyor.yml#L19-L23 https://github.com/conda-forge/staged-recipes/blob/master/.circleci/fast_finish_ci_pr_build.sh here's for CircleCI __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/11526 is also ready for a last review/merge __eou__	User

TP	Travis bumped our resources!! They rock We'll need to thank them __eou__	User

TP	I'm off until tomorrow guys.  You can have a look at scikit-learn/scikit-learn#11596 and continue if you wanna merge quickly. __eou__	User

TP	Hey, does anybody understand what the problem is on appveyor: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.23610/job/fab9oqavus2wy8sa ? It's happening on several PR. I don't love the smell of it. __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/11578 is almost green on tests, ready for a second review __eou__	User

TP	looks like travis bumped up to 10 according to @jorisvandenbossche @GaelVaroquaux did you tweet about it? we should also tweet to enthought maybe? __eou__	User

TP	Yes, travis did bump up to 10. It makes a big different I didn't tweet, but I'll thank them in the blog post __eou__	User

TP	can someone riddle me this? https://github.com/scikit-learn/scikit-learn/pull/11542 SAG test failure on python2.7 in the n_estimators=100 branch?! @GaelVaroquaux you mean the appveyor, not the tranvis I mentioned, right? ok cool __eou__	User

TP	No idea about the AppVeyor problem but it does seem to happen quite often. __eou__	User

TP	@GaelVaroquaux the appveyor issue is recent. One possibility might be that have changed something in their API and that breaks and that broke our trick to automatically cancel / skip builds on PRs that have received new push events in the mean time. __eou__	User
TP	OK, but it's a random failure?__eou__	Agent
TP	I'll merge the PRs despite of it.__eou__	Agent

TP	I don't know if it's random with a very high likelihood or if it's a deterministic failure. It might be a temporary outage. __eou__	User
TP	Yes, I mean appveor__eou__	Agent

TP	Somebody to open the door of Enthought ;) __eou__	User

TP	I have AppVeyor enabled in my fork and I have some green builds recently: https://ci.appveyor.com/project/lesteve/scikit-learn/history This could well be the HTTP request we use which is problematic. __eou__	User

TP	I'll look at the AppVeyor problem more in details. __eou__	User

TP	Thanks Loic! __eou__	User

TP	my current reading is that we shouldn't have done the error_score deprecation in ``_fit_and_score`` but in BaseSearchCV. But it depends on what we want to do in learning_curve, cross_validate and cross_val_score (and other places I haven't thought of) https://github.com/scikit-learn/scikit-learn/issues/11576 __eou__	User

TP	@amueller column transformer remainder change: https://github.com/scikit-learn/scikit-learn/pull/11603 __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/11604 try to fix appveyor ! __eou__	User

TP	@amueller https://github.com/scikit-learn/scikit-learn/pull/11570/files#r203161922 __eou__	User

TP	so @rth made the good point that the deprecation warning thing should be removed in the release __eou__	User

TP	my PR is green now if someones wants to be the 2nd reviewer: https://github.com/scikit-learn/scikit-learn/pull/11578 __eou__	User

TP	We looked at AppVeyor with Sik. There is a work-around in master and all the current running AppVeyor builds have been killed. Just push a new commit in your branch if you want AppVeyor to run again or ask me while I have a sklearn-ci tab opened. __eou__	User

TP	I will also cancel all the circle ci builds as the queue is far too long. We can restart new builds on new PR (preferably recently opened ones, or those with a recent master merge commit). __eou__	User

TP	any comments here https://github.com/scikit-learn/scikit-learn/pull/11213 ?? __eou__	User

TP	@lesteve https://github.com/scikit-learn/scikit-learn/pull/11213 sorry https://github.com/scikit-learn/scikit-learn/pull/11552 __eou__	User

TP	@lesteve https://github.com/scikit-learn/scikit-learn/pull/11563 __eou__	User

TP	Hum, CI is a bit in a mess. There is going to be some work there __eou__	User

TP	In the lastest commit on master there is just 1 test failing in one build.  Attempting to fix that in https://github.com/scikit-learn/scikit-learn/pull/11617 Appveyor and Circle CI should be hopefully OK, the issue is mostly a very long queue.. __eou__	User

TP	Great! I was about to look at this face issue! Thanks! OK, @rth: I reviewed it. Travis will be done with it in 20mn. It needs a second review  scikit-learn/scikit-learn#11617  is green and needs a second review __eou__	User

TP	Travis is green in master :fireworks: ! https://travis-ci.org/scikit-learn/scikit-learn/builds/405208107 __eou__	User

TP	It seems that travis is failing again on one of the configuration. I have made a PR that should fix that: https://github.com/scikit-learn/scikit-learn/pull/11625 It fixes travis indeed. It would be good to have reviews +merge, as many PRs are red because of this small issue __eou__	User

TP	OK, I now realize that it was fixed in the mean time by downgrading the joblib requirement on travis :). Still good to have! __eou__	User

TP	Hi, is it possible to use different distance metrics for `kmeans`? I noticed that `cosine` and `manhattan` are already implemented in `metrics/pairwise.py`.  Is there any quick dirty way to somehow specify these metrics instead of euclidean distance for `kmeans`? __eou__	User

TP	Is there a way to check what BLAS scikit-learn is built with? Maybe like an info function or something? :tada: Thank you __eou__	User

TP	@jakirkham there's a pr for that ;) but it's whatever numpy is build with, I think __eou__	User
TP	Really, where's that :)__eou__	Agent
TP	#11596__eou__	User
TP	merged actually!__eou__	User
TP	so you have it in master__eou__	User

TP	If it says `cblas`, does that mean scikit-learn's internal BLAS? __eou__	User

TP	I think we stopped shipping blas, didn't we? __eou__	User

TP	@amueller  not merged yet :) __eou__	User

TP	@jakirkham the method you are looking for is `sklearn._build_utils.get_blas_info()` __eou__	User

TP	Great thanks! __eou__	User

TP	Hello - for BallTree can I use a user defined metric as great circle distance calculated from PyProj ? I believe the answer is YES but just confirming it here on gitter as shown in this SO answer - https://stackoverflow.com/questions/21052509/sklearn-knn-usage-with-a-user-defined-metric __eou__	User

TP	We still ship a bunch of cblas and actually use it in the windows and linux wheels. The easiest way to get rid of that would be to use scipy cython BLAS functions API but this won't be possible before we bump up the dependencies. __eou__	User

TP	Sounds like issue ( https://github.com/scikit-learn/scikit-learn/issues/11638 ) :wink: __eou__	User

TP	Hello peoplw I want to begin with scikit, how should I begin ? Please suggest __eou__	User

TP	@geekyharshal https://jakevdp.github.io/PythonDataScienceHandbook/ __eou__	User

TP	Hey guys, are there any prebuild Docker images to work with sagemaker for sklearn? They support Apache MXNet or TensorFlow directly but not sklearn. https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb This is an option if not https://github.com/jupyter/repo2docker __eou__	User

TP	that example shows you what to do, doesn't it? Don't think anyone here has used sagemaker __eou__	User

TP	I can build it myself but its more a question of if there are standard images. Maybe this is more or a conda question since it is about making a consistent collection of packages. I would suggest you guys take a look at sagemaker. There are not any good exampke for scikit learn for the whole train test deploy. Would be good for adoption of sklearn. Althoughj, i am sure you have no shortage of projects __eou__	User

TP	I was visiting the sagemaker team like a month ago __eou__	User

TP	Cool. My team uses sklearn a lot and we are just getting started with sagemaker for deployment. We do everything locally now but will move to AWS for train and predict. We will try to come up with a good example for the deployment and maybe add it to their existing examples. __eou__	User

TP	Hello! My name is Tushar.  I recently finished a course on machine learning and now I'm making some projects. One of them is object classification, I have googled all I could.  The problem is, I am confused on how to preprocess the images to work. I have worked on text. Now I am having troubles with images. All the things  I saw were using deep learning and tensorFlow. Articles with sklearn were using the dataset already provided in datasets. And that wasn't helping. If anyone can guide me, I'd appreciate it. Thanks __eou__	User

TP	Typically each pixel is encoded as rgb so 3 features per pixel. Sklearn can try to classify this with a number of algorithms but I suspect you will not get far. The pooling and covolutional layers in a NN are what gives the power for image classification. Sklearn does not have any NN beyond an MLP so you lose that ability. __eou__	User

TP	hey i am here to contibute pls assign me some task __eou__	User

TP	Thank you very much Keith! __eou__	User

TP	I actually finished a machine learning course and I'd like to make a few projects before I move on to deep learning and neural networks. __eou__	User

TP	Then don't do image recognition __eou__	User

TP	@Ishaan28malik Tasks are more claimed than assigned. Go to the open issues on github and find one you can do. Ask if anybody is activly working on it. If not then you can give it a try __eou__	User

TP	Hello - are we allowed to put links to SO questions here ? @amueller Ok then here you go - https://stackoverflow.com/questions/51627721/typeerror-with-scikit-learns-balltree __eou__	User

TP	@winash12 you can but I wouldn't encourage it, I guess? Generally it's better to ask on stackoverflow __eou__	User
TP	@amueller  yes understood__eou__	Agent
TP	i had an issue with BallTree__eou__	Agent
TP	Can I ask here ?__eou__	Agent
TP	I mean if you're about to ask the same question again then post the SO link ;)__eou__	User
TP	what's the type of bt.data ?__eou__	User
TP	and what's the type of ``matches``?__eou__	User
TP	no the type__eou__	User
TP	type(bt.data), type(matches)__eou__	User

TP	So like in scipy it must be the coordinates of point in this case x and y hang on let me run my toy example type(matches) is a list __eou__	User
TP	you can use "points" instead of "bt.data"__eou__	Agent
TP	So in kdtree the bt.data equivalent is numpy ndarray__eou__	User
TP	yeah here it's not. I opened https://github.com/scikit-learn/scikit-learn/issues/11728__eou__	Agent
TP	but you already have the numpy array, it's just points__eou__	Agent
TP	``points`` that is__eou__	Agent
TP	together with like 500 other people or something ;)__eou__	Agent
TP	I'm the person that opened the most issues, I think. Dubious honor.__eou__	Agent

TP	ok that's what I thought. bt.data is a memory view, the docs are wrong :-/ __eou__	User

TP	@amueller  An honor to converse with the author of scikit-learn ! Thank you for opening the issue on github !! __eou__	User
TP	I'm not "the author" by any means__eou__	Agent
TP	co-author ?__eou__	User
TP	I guess__eou__	Agent
TP	Thank you for opening mine :)__eou__	User

TP	you can just do np.array on bt.data to get a numpy array for now, but you already have the array as ``points`` so there's really no need... __eou__	User
TP	yes thanks. I have accepted your answer on SO as well :) Hopefully I will be notified that the problem has been fixed so I can reinstall scikit-learn__eou__	Agent
TP	I had another question. I had a offline conversation with another core developer of  scikit-learn__eou__	Agent
TP	I want to use BallTree with https://en.wikipedia.org/wiki/Great-circle_distance and that core developer warned me against it and asked me to use Haversine instead.__eou__	Agent
TP	Any reason why ?__eou__	Agent
TP	PyProj provides great circle distances and I can define a custom metric. What is the objection in doing this ?__eou__	Agent
TP	Not sure what pyproj is doing - https://jswhit.github.io/pyproj/pyproj.Geod-class.html#fwd__eou__	Agent
TP	but the author told me it is the  most accurate__eou__	Agent

TP	that wiki pages says the harvesine formula is more stable? not my area of expertise but look at the wiki page that's probably what pyproj is using or should be using? __eou__	User

TP	Can I raise this as a issue on github as people on my project want to use pyproj ? __eou__	User

TP	you can ask on pyproj? who did you talk to as sklearn? Jake? __eou__	User
TP	yes you are right. I did ask  Jake__eou__	Agent
TP	No I mean I do not understand how asking pyproj would help. The question is why I cannot use pyproj's great circle distance with scikit-learn__eou__	Agent
TP	I think you can__eou__	User
TP	ask jake what he meant lol__eou__	User

TP	jake is unlikely to reply to an issue on github __eou__	User

TP	By the way I used points in that code example and I get another error -` File "testballtree.py", line 23, in main     x1,y1 = bt.points[matches].T AttributeError: 'sklearn.neighbors.ball_tree.BallTree' object has no attribute 'points'` np.array gives the same exception as the one I showed on SO __eou__	User

TP	not bt.points. your array points __eou__	User
TP	aah OK__eou__	Agent

TP	I think PyProj uses the Vincenty formula __eou__	User

TP	hello __eou__	User

TP	@amueller I got BallTree to work with PyProj's inv() __eou__	User

TP	@amueller As mentioned by you in this issue - https://github.com/scikit-learn/scikit-learn/issues/6256 you are suggesting that the custom myfunc can be written in Cython which I am willing to do. But we are shipping a product. What happens when there is a new version of scikit-learn and what will happen to my Cython extension ? __eou__	User

TP	The second question I had is the following - in my field the requirement is that the Nearest Neighbors search for a particular grid point of say 2D data must be include contributions from above and below the surface in question. Supposing i have three 2-D surfaces. top and below the current surface. Then I do a radius of influence query for the three surfaces separately. The interpolated value for the grid point in the current surface must include contributions from all three surfaces from above and below the current surface as well as the current surface __eou__	User

TP	Hey, I notices something strange in the documentation. For the GradientBoostingClassifier is says the criterion=friedman_mse for the default. I would have expected criterion=gini like in RandomForestClassifier since mse is typically used for regression not classificaiton. The text is the same as in GradientBoostingRegressor. Is it possible that it was copied by accident? __eou__	User

TP	Has anyone played with memory pools or custom allocators in the context of scikit-learn? __eou__	User

TP	Hey guys. I'm dealing with an older version of sklearn (0.13.1) (not able to upgrade at the time) where the class attribute `_label` for `svm.SVC` object is used instead of `classes_`. So where I'd have `'classes_': array(['compatible', 'incompatible'], dtype=object)` in my fitted svm object, how would I replace that with `_label`? Simply exchanging the key names doesn't do the trick. I get the error message: `dtype mismatch, expected 'int32_t' but got Python object`.  If you guys could help me figure this out, you're be even more awesome than you already are. __eou__	User

TP	@jakirkham not to my knowledge. What approaches were you considering and why do you think it might be beneficial? __eou__	User

TP	Is there an estimate on a release date of 0.20 ?  No pressure or expectations, I'm just doing some planning.  "No estimate" is also a fine response. __eou__	User

TP	@mrocklin See https://github.com/scikit-learn/scikit-learn/pull/11838 for the 0.20.rc1 , probably several weeks later for the 0.20 final . __eou__	User
TP	Thank you for the pointer @rth .  I should have been able to find that :)__eou__	Agent
TP	That shouldn't take long :)__eou__	Agent

TP	hope to do the RC today or tomorrow but I'm an optimist ;) I need to convince some more people that I'm right and they are wrong before we can do that, though __eou__	User

TP	> not to my knowledge. What approaches were you considering and why do you think it might be beneficial?   There are cases where we seem to be allocating very similar sized blocks for arrays repeatedly (e.g. enforcing Fortran order on inputs, applying LASSO repeatedly on similar sized arrays). The time for the allocations here is larger than I would naively expect, which makes me think that whatever default memory allocation scheme is running into issues.  Am debating the value of allocating a larger array that includes N such blocks and distributes sliced chunks of it for these allocation operations. This may be too naive, but this is my first thought. WDYT? __eou__	User

TP	does anyone get what I'm doing wrong with appveyor? https://ci.appveyor.com/project/sklearn-wheels/scikit-learn-wheels/build/job/8ij4qa5mcayg8xr8 __eou__	User

TP	the appveyor thing is mostly what's preventing me from doing the RC right now :-/ __eou__	User

TP	@amueller there seem to be a few different issues. Very strange. There are unicode differences. 'foo' vs. u'foo' and there are also a lot of `, dtype=int32)`vs  `, dtype=int64)` __eou__	User

TP	@bgruening that's actually not the problem I was having, @jnothman has fixed my problem :) We're supposed to be skipping doctests on 32bit and possibly on py27. or maybe there was a fix in the doctests for py27 changes? Anyone remember lol? __eou__	User

TP	If anyone has time to look at PR ( https://github.com/scikit-learn/scikit-learn/pull/11896 ) and/or PR ( https://github.com/scikit-learn/scikit-learn/pull/11898 ), that would be greatly appreciated. The former is just a slimmed down version of PR ( https://github.com/scikit-learn/scikit-learn/pull/11507 ) __eou__	User

TP	Ok thanks @DrEhrfurchtgebietend  any link for the issues .pls __eou__	User

TP	@DrEhrfurchtgebietend  any link for joining the org at github like any invitation or anything .. __eou__	User

TP	Is there a reason why LinearSVC doesn't have predict_proba? oh!  Hmm... http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html has predict_proba @amueller  I am confused why one is a probabilistic model but the other isn't I mean there is no randomness in the decision tree classifier __eou__	User

TP	@lesshaste yeah it's not a probabilistic model __eou__	User

TP	randomness has nothing to do with whether it's a probabilistic model. there's no randomness in logistic regression __eou__	User

TP	@amueller  Yes sorry my mistake. Could you give a short reason why one is a probabilistic model and the other isn't? Please __eou__	User

TP	not really? There is a way to interpret a tree model in a probabilistic way but there is no way to interpret an SVM in a probabilistic way __eou__	User

TP	@amueller  Thanks, that's interesting. I noticed that you can always use  http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html to make probabilities __eou__	User

TP	yes you can __eou__	User

TP	I tagged my pull request with WIP. However,  right now, Id like if someone would have a look over it. Im not sure if my approach is the desired one. I could do some stuff on documentation of course, but it might be that my approach is declined entirely. So should I change the name of the request? For completeness: this is the pull request (https://github.com/scikit-learn/scikit-learn/pull/11891). Of course I know that time is precious for all of us. __eou__	User

TP	I know this is out of topic here but trying to get as much as data we can. Can you please help me understand your use of agile methods by completing this one minute survey https://www.surveymonkey.com/r/98JMTJ2 __eou__	User

TP	Hi all! What is the current recommended was to save models and redistribute them. We tried https://github.com/uchicago-cs/deepdish and similar concepts, but all do pickle the object at some point. This is not relocatable and breaks with different python versions etc ... is there any emerging standard. Any hint how the ML community is tackling this at the moment or in the future? @DrEhrfurchtgebietend I consider this a very bad hack :) Is there no emerging standard :( __eou__	User

TP	@bgruening Use pickle from joblib and build in a docker container. It seems to be the standatd method but is not without flaws. In a standard local server deployment i rarely use the docker containers and just keep track of package versions. __eou__	User

TP	I would refer to the talk of Alejandro Saucedo at EuroSciPy couple of days ago https://axsauze.github.io/scalable-data-science/#/ and more precisely https://github.com/axsauze/awesome-machine-learning-operations However, I did not check all the solution and I cannot ensure you that you will not get the pickling issue that you mentioned. __eou__	User

TP	thx for sharing Guillaume __eou__	User

TP	Much of that has to do with code versioning not really the deployment method. PKL + docker is a common method mentioned in the Guillaume's links __eou__	User

TP	Hi. I'm a student and am new to sklearn. One of my peers told me that once I get a job, I can't and shouldn't use sklearn for professional projects. Is this true? Is not sklearn completely open source and free to use? which license does it come under? __eou__	User

TP	If the response is too detailed to be mentioned in a chat, please respond on my email : ujjawalpanchal32@gmail.com Thanks for your help. Looking forward to replies. __eou__	User

TP	@Ujjawal-K-Panchal just look at github: https://github.com/scikit-learn/scikit-learn/blob/master/COPYING __eou__	User

TP	Thanks! __eou__	User

TP	interesting question: https://stats.stackexchange.com/questions/367051/how-to-learn-new-clusters-on-residuals-of-kmeans given you have 40k frozen centroids, how to learn 40k more? `partial_fit` and `_mini_batch_step` don't feel very modular __eou__	User

TP	hello __eou__	User

TP	question: does it make sense to go through the docstrings and make sure the default values are mentioned for each optional parameter, preferably with a consistent format? Or should we just try to enforce it for new PRs and leave the rest alone? There are some funny cases though (http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor) @amueller why would it create a conflict with existing PRs, if those PRs haven't change the line corresponding to the parameter? And if they have, it does make sense for them to follow whatever convention we choose anyway, doesn't it? __eou__	User
TP	I think it's enough if there's changes around the line for it to create a conflict__eou__	Agent
TP	hmm, fair enough.  So no easy way to fix. Although I'm not sure how much it would bug PR submitters to fix those conflicts. Well, I guess then we'll leave it as is. Unless you have a better solution.__eou__	User

TP	@adrinjalali making sure defaults are there would be great. consistent formatting has the issue that there's gonna be a lot of changes creating lots of conflicts with existing PRs. but it'll be hard to search for missing defaults without a consistent format :-/ __eou__	User

TP	Hello everybody! I just wrote an article about Machine Learning. Let me know what you think about it :wink: https://medium.freecodecamp.org/how-to-predict-likes-and-shares-based-on-your-articles-title-using-machine-learning-47f98f0612ea __eou__	User

TP	@flaviohenriquecbc nice report! Two suggestions. One, it would be nice to explain what the red and blue colors mean. I didn't get a strong sense of what those were when reading the Medium post or your final report PDF. And two, if you were to improve this accuracy of your models, maybe consider feature selection to narrow which words you use in your model, instead of using all of them. From your final report, it appears you used all of them, but if I missed that part of your analysis, forgive me. Thanks for sharing your work and results! @flaviohenriquecbc And feel free to share in the [freeCodeCamp DataScience](https://gitter.im/FreeCodeCamp/DataScience) room as well :wink: __eou__	User

TP	thank you for your feedback, @erictleung .. i will try to implement what you said :) __eou__	User

TP	actually the colors didn't mean anything.. it was a style that i used for the graphs __eou__	User

TP	@amueller checked Bottou's sgd code in C++ (https://leon.bottou.org/projects/sgd), I'm not sure where that same formula used for the learning rate can be found there. __eou__	User

TP	@h4k1m0u which one? __eou__	User

TP	@amueller I cannot find this one $\eta = \frac{1}{\alpha (t_0 + t)}$ (http://scikit-learn.org/stable/modules/sgd.html#id1) in Bottou's source code __eou__	User

TP	Isn't that the same as svmsgd? __eou__	User

TP	@amueller I've opened `svm/svmsgd.cpp`, I can't locate that same formula. I found this one which is different: `double eta = eta0 / (1 + lambda * eta0 * t);` __eou__	User
TP	@h4k1m0u open an issue or ask the mailing list maybe?__eou__	Agent
TP	@amueller Thanks I'll open an issue on Github__eou__	User

TP	:+1: __eou__	User

TP	For demonstration purposes, I would like to track the kmeans centers during each iteration of the fit. Is that possible with sklearn kmeans, e.g. with a callback? __eou__	User

TP	I will try to run kmeans for one iteration at a team, initializing the centers with the centers from the previous kmeans. That should do the trick. __eou__	User
TP	yeah that should work. at that point it's easy enough to implement it yourself as well though ;)__eou__	Agent
TP	:plus1:__eou__	User

TP	Hello guys, I wish to start contributing to sci-kit learn. As I am a beginner, so can anybody suggest me how to start contributing. __eou__	User

TP	There are issues on GitHub marked with "easy"/"good first issue" https://github.com/scikit-learn/scikit-learn/issues __eou__	User

TP	Number of features of the model must match the input. Model n_features is 13 and input n_features is 2 What does it mean? __eou__	User

TP	@Praful-cs can you please say what's unclear about that so we can improve the error message? The model was trained with 13 features and you give it data with 2 features __eou__	User

TP	@nicolashug you here? so in lbfgs alpha = 1/C I'm trying to figure out the loss computation for lbfgs first because we can most easily access the function __eou__	User

TP	yup __eou__	User

TP	@NicolasHug https://pastebin.com/5Vv72rLc these two are identical and identical to what's internally used __eou__	User

TP	and if you want SGD to be equivalent to LogisticRegression, you have indeed to set alpha = 1/(C  * n_samples) __eou__	User

TP	Hi. Can you anyone tell me if there is any tutorial for ML ? __eou__	User

TP	@KoulickS https://jakevdp.github.io/PythonDataScienceHandbook/ https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners __eou__	User

TP	Greetings everyone, anyone knows about bitcoin price prediction algorithms ? __eou__	User

TP	Hi, i will try to predict customer lifetime period. My dataset has information about customer lifetime period. Should i set this column as target and do regression for predict it? __eou__	User

TP	@MahmoudElsayad you probably would need algorithms which deal well with time series challenges, such as HMM or RNN (mostly LSTM related) models, which are not included in scikit-learn. You may find more information in the following places: - https://hmmlearn.readthedocs.io/en/latest/ - https://machinelearningmastery.com/make-predictions-time-series-forecasting-python/ - https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/ @talatccan that would be a start. But please note that issues such as preprocessing and scaling your inputs and/or outputs, and hyperparameters of your models, may substantially affect your results. You can refer to the resources posted above your message for more information. __eou__	User

TP	What all do you think is the future scope of ML and AI in the field of data science __eou__	User

TP	https://docs.google.com/viewerng/viewer?url=https://s3.amazonaws.com/acadgildsite/course/masteringdatascience/session23/ACD_MDS_V2_Session_23_Project_1_Main.pdf Someone help with this assigment Ir could be pais __eou__	User

TP	Not cool __eou__	User

TP	  Write a function to return the intercept as a float (rounded to the nearest 3 integers) of a linear regression model  def lin_reg_intercept(X_train, y_train): __eou__	User

TP	@GONZALORUIZR_twitter what help do u need? __eou__	User

TP	I need somebody do the assigment and send me via open repository __eou__	User

TP	Is it for course credit? Are you going to pay? __eou__	User

TP	Hi guys.  I just have a doubt.  Is it better to scale down the target values before using it as ground truth for training a model or can we use the target values as such? My target values are in the range of 100's currently. __eou__	User

TP	@MVenkat_28_gitlab if you have one target that you are predicting, scaling should have no real effect. What models/approaches are you using ... maybe there is something I am not thinking about? so rounding error would be a concern if you had massive range in your targets And it is generally easier to start with a classifier rather than regressor ... __eou__	User

TP	I have only one target.  I'm currently using a regression neural network. @cottrell how do you say that for one target scaling will make no effect? __eou__	User

TP	hi, everyone i'm new here so anything i need to know? __eou__	User

TP	@MVenkat_28_gitlab well, I guess I mean for unregularized regression. Linear scaling mathematically should not have a direct impact for most models and methods. I think the linear scaling will just factor out of everything. Of course, it might help with numerical stability if the target has extreme values.  Basically, intuition is that for regression methods, min F(X) - y is same problem as min F(X) - \alpha * y but with a modified X. For example, you could just scale the weights of the last layer to get the different Y.   With SGD or whatever method you are using to solve, you could of course get different results with scaled y.   This is not a proof of course ... just found this: https://roamanalytics.com/2016/11/17/translation-and-scaling-invariance-in-regression-models/#Scaling-does-not-affect-unregularized-regression  which is potentially a good illustration of the details. @nisnt2411 Hi, I dunno I jumped in recently without even asking anything :). Seems like a mix of questions from people starting and a few technical discussions. (human) Latency is pretty high. Just try to help people who are stuck if you know some answers to questions I guess. __eou__	User

TP	@cottrell thanks! I thought there might be some prerequisites . __eou__	User

TP	@nisnt2411 wanting to contribute. Python and ML knowledge is helpful ;) start with the contributors guide and "good first issue" tag if you want to start helping and yes, review latency is terribly high unfortunately __eou__	User

TP	Hi all, i see a bunch of `unused variables` alert  in lgtm. https://lgtm.com/projects/g/scikit-learn/scikit-learn/alerts/?mode=tree&ruleFocus=6780086 Would this be a useful PR? __eou__	User

TP	@whiletruelearn not if they are in externals or backports. otherwise yes. __eou__	User

TP	How wants to code in a kaggle team! __eou__	User

TP	Hi all! How can I see a error in this build?   https://circleci.com/gh/gilbertoolimpio/scikit-learn/11?utm_campaign=workflow-failed&utm_medium=email&utm_source=notification  Can you help me? __eou__	User

TP	Hi all I am a newbie here only have coding experience but I would like to contribute in this project so where should I start with __eou__	User

TP	@dwibedis the contributing guides (http://scikit-learn.org/dev/developers/contributing.html) and the "good first issue" ones on github (https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) are not a bad place to start, if you haven't checked them already. @OlimpioGilberto_twitter you can download the build log there, and look for errors (if there's any). But I'm not sure why you're looking in that build log. those are mostly for the docs, and that particular one is for python2, which is deprecated in master (it's still there cause future 0.20.xxx releases still support python2). If you're trying to see some build logs and to check the tests, you probably need to check the travis-ci logs, not circle-ci. __eou__	User

TP	@adrinjalali Thank you! I am looking in the circle-ci because my PR was labeled with an error, because of this problem and I did not find which was the error that was triggered. But now, I suppose this problem is not so serious! Tks! __eou__	User
TP	@OlimpioGilberto_twitter if you want, and the problem still exists, I can try and have a look if you paste here the PR number.__eou__	Agent
TP	@adrinjalali Very tks! I'll fix my code! Your help was very important!__eou__	User

TP	@adrinjalali here is my PR number #12524 Tks! __eou__	User

TP	Hi all, I am trying to update the DNS of scikit-learn.org to enable https, and it broke. Working on fixing it. For reference I was trying to follow the instructions from https://github.com/scikit-learn/scikit-learn/issues/12278 It works again \o/ I did nothing more, it just took a bit of time apparently. __eou__	User

TP	woohooo now computer says yes :P __eou__	User

TP	Now I won't be able  to use scikit-learn.org with Airports wifi one of the remaining websites that doesn't use https to get redirected to the network sign-in. So this can break some use cases :) Great that it happened, can confirm it works! __eou__	User
TP	http://8.8.8.8__eou__	Agent
TP	It should trigger sign-in pages on open wifi networks.__eou__	Agent
TP	Yeah, but scikit-learn.org was smoother to type on a French keyboard  :)__eou__	User
TP	Just kidding, thanks for making it happen!__eou__	User

TP	sweet! example.com walso works @rth __eou__	User

TP	hi folks, sorry to bother. I've came across an old scikit example on GPs for which I can't seem to find any documentation or example on how to translate it to the current version: ``` from sklearn.gaussian_process import GaussianProcess gp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1E-1,                      random_start=100) xfit = np.linspace(0, 10, 1000) yfit, MSE = gp.predict(xfit[:, np.newaxis], eval_MSE=True) ``` I understand that's GPR but the parameters have changed, so `corr=cubic` and `thetas` don't really exist. Anyone has any idea how to translate this to version 0.20? __eou__	User

TP	@kirk86 you need to find the appropriate kernels from `sklearn.gaussian_process.kernels` and construct the equivalent kernel to construct your covariance matrix for the GP. I haven't looked into them to know if there's a one to one mapping between the old ones and the new implementations. Alternatively, you can try and construct the solution to your usecase from scratch using tutorials such as: https://scikit-learn.org/dev/modules/gaussian_process.html#gaussian-process __eou__	User

TP	@amueller in less than two weeks we're going to have a meetup for people who are new and interested in open source development. I'll be introducing them to scikit-learn's development process, for which it'll be nice to have the issue/pr labels updated. The main issue which also troubles myself whenever I'm looking for something to work on, is that the "help wanted" labels are very rarely removed from the issues once they're put there. Going through the first 25 issues tagged with "help wanted", only 7 actually really need help, and for most of them there's already an open PR to fix the issue. I'm not sure how it can be improved in a sustainable way, but it'd be nice if somebody could update the labels at least for now :) this is our meetup: https://www.meetup.com/opensourcediversity/events/255369540/ __eou__	User

TP	@adrinjalali I second that. Is there any proposed date for this? @adrinjalali ? __eou__	User

TP	@adrinjalali thank you for taking time to respond. I was thinking the same think that you're suggesting but I can't find any `cubic` kernel in the list of available kernels: ``` >>> sklearn.gaussian_process.kernels. sklearn.gaussian_process.kernels.ABCMeta(                sklearn.gaussian_process.kernels.PairwiseKernel(         sklearn.gaussian_process.kernels.kv( sklearn.gaussian_process.kernels.CompoundKernel(         sklearn.gaussian_process.kernels.Product(                sklearn.gaussian_process.kernels.math sklearn.gaussian_process.kernels.ConstantKernel(         sklearn.gaussian_process.kernels.RBF(                    sklearn.gaussian_process.kernels.namedtuple( sklearn.gaussian_process.kernels.DotProduct(             sklearn.gaussian_process.kernels.RationalQuadratic(      sklearn.gaussian_process.kernels.np sklearn.gaussian_process.kernels.ExpSineSquared(         sklearn.gaussian_process.kernels.StationaryKernelMixin(  sklearn.gaussian_process.kernels.pairwise_kernels( sklearn.gaussian_process.kernels.Exponentiation(         sklearn.gaussian_process.kernels.Sum(                    sklearn.gaussian_process.kernels.pdist( sklearn.gaussian_process.kernels.Hyperparameter(         sklearn.gaussian_process.kernels.WhiteKernel(            sklearn.gaussian_process.kernels.signature( sklearn.gaussian_process.kernels.Kernel(                 sklearn.gaussian_process.kernels.abstractmethod(         sklearn.gaussian_process.kernels.six sklearn.gaussian_process.kernels.KernelOperator(         sklearn.gaussian_process.kernels.cdist(                  sklearn.gaussian_process.kernels.squareform( sklearn.gaussian_process.kernels.Matern(                 sklearn.gaussian_process.kernels.clone(                   sklearn.gaussian_process.kernels.NormalizedKernelMixin(  sklearn.gaussian_process.kernels.gamma( ``` __eou__	User

TP	A question for the developers, how can someone add a custom kernel to gaussian process, I have custom method but keep getting error when adding it directly do we need to wrap it with some base estimator because it complains about the `get_params` method ``` (Pdb) gp.fit(np.random.randn(10, 3), np.random.randn(10)) *** TypeError: Cannot clone object 'array([0.99946371, 0.99859908, 0.99818002, 0.9994673 , 0.99954341,        0.99969851, 0.99905772, 0.99932515, 0.99906479, 0.99983097])' (type <class 'numpy.ndarray'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods. ``` I guess this is still an issue https://stackoverflow.com/questions/49188159/how-to-create-a-custom-kernel-for-a-gaussian-process-regressor-in-scikit-learn __eou__	User

TP	@kirk86 can you please open an issue? I don't think there's an issue open on that __eou__	User

TP	@amueller just did #12558 thanks __eou__	User

TP	Another question for the developers. It seems that `sklearn.feature_extraction.text.CountVectorizer` is lacking an option to pass a 2d numpy array of strings or objects. It seems that it's impossible at the moment. It would be nice because at some point someone might need to do text wrangling and cleaning on another tool such as pandas or spark and be able to dump that as 2d numpy array in order to further analyze. __eou__	User

TP	@kirk86 I'm not sure I understand the usecase. You want several separate text-fields and have each vectorized separately? Or together? __eou__	User

TP	Apologies for not clearly explaining the usecase. Let's say that our dataset is like this: ``` X = np.array([['this is a text'], ['the brown fox jumped over the fence'], ['This is a longer string for just showcasing an example'], ['Dummy text here'], ...]) X.shape = (1000, 10) ``` Each sample or row from `X` creates a vector representing the bag of words. Each vector is of variable size because it depends on the length of the sting a.k.a how many times a word is present in the string. One could pad with zeros all of those vectors in order to have the same dimensions. Ideally it would be nice to give `X` to `sklearn.feature_extraction.text.CountVectorizer ` and get back a matrix `X_new .shape = (1000, m)`  where `m` represents that each row vector in `X_new` has the same length. Is that a bit more clear? __eou__	User

TP	sorry, what's the 10 in your example? Is each example a single string or multiple strings? __eou__	User

TP	the 10 is the number of columns and each of them may or may not contain multiple stings __eou__	User

TP	Can you give an example of such an X? Sorry I'm being slow. The X example you gave above has shape (n_samples, 1) __eou__	User

TP	What are the typical approaches used for time series based classification decisions. Appreciate any pointers. I am planning to start with decision trees to somehow learn features based on class transitions. ( I only expect a couple of class transitions amongst Classes which got renamed over time but Essentially carry similar features - say classes a1, a2,a3) besides the vanilla classification problem against classes b1,b2 and c1. __eou__	User

TP	@amueller here's an example: ``` array(['dsny', 'bcc - brooklyn south', 'sanitation condition',        '15 street cond/dump-out/drop-off', 'street', '218 31 street',        '31 street', 'brooklyn', 'closed', '07 brooklyn', 'brooklyn'],       dtype=object) ``` [![example.png](https://files.gitter.im/scikit-learn/scikit-learn/LYOA/thumb/example.png)](https://files.gitter.im/scikit-learn/scikit-learn/LYOA/example.png) here `X` is (n_samples, 11) __eou__	User

TP	these two are different examples, though?! Ok so do you want one vectorizer for each column? ``make_column_transformer(*[(c, CountVectorizer()) for c in X.columns])`` should do that. but really status here is probably a categorical variable - thought I guess CountVectorizer also works for those Well unless there's a space in one of the category names - so explictly saying these are categorical might be better __eou__	User

TP	> these two are different examples, though?  They are the same in the sense that the picture is `X` and the array above corresponds just to the first row of `X`.   > but really status here is probably a categorical variable  True, its a categorical value. Thanks! __eou__	User

TP	yeah well treating a single column is different from treating the table, and the single column example is exactly what CountVectorizer does __eou__	User

TP	[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/2epS/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/2epS/image.png) [![image.png](https://files.gitter.im/scikit-learn/scikit-learn/l8Ri/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/l8Ri/image.png) [![image.png](https://files.gitter.im/scikit-learn/scikit-learn/HwWK/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/HwWK/image.png) __eou__	User

TP	@Ngamlana this seems entirely unrelated. Please don't spam the channel __eou__	User

TP	Hello. I am new to this community. I would love to contribute. Can someone help me start __eou__	User

TP	@Naman9639 the contributing guide (https://scikit-learn.org/dev/developers/contributing.html) is a very good place to start, and then you can try some of the "good first issue" (https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) ones. __eou__	User

TP	Thanks I will start __eou__	User

TP	How to access groups under NetCDF4 files? __eou__	User

TP	Hi, i get following error when im trying to apply one hot to categoric columns. I didnt understand what is problem exactly. Error is: TypeError: Wrong type for parameter `n_values`. Expected 'auto', int or array of ints, got <class 'numpy.ndarray'> __eou__	User

TP	and here is the my one hot code: col_index = ([train.columns.get_loc(c) for c in train.columns if c in temp_cat_cols]) print('OneHot Uygulanacak Columns: ', col_index) ohe = OneHotEncoder(categorical_features=col_index, handle_unknown='ignore') X_train = ohe.fit_transform(X_train).toarray() X_test = ohe.transform(X_test).toarray() __eou__	User

TP	@talatccan can you give a minimum example to reproduce? and what version of sklearn are you using? __eou__	User

TP	master's circle-ci badge says failing, but I fail to find the build which fails, is the badge wrong? Or am I looking at the wrong place? __eou__	User

TP	There is an issue with the ssh key in the deploy step in the master branch: https://circleci.com/gh/scikit-learn/scikit-learn/tree/master Here is the end of the log of the last failure:  ``` ...  rewrite dev/searchindex.js (73%) + git push ERROR: The key you are authenticating with has been marked as read only. fatal: Could not read from remote repository.  Please make sure you have the correct access rights and the repository exists. Exited with code 128 ``` __eou__	User

TP	The sklearn-ci github user has a user ssh key named "sklearn-docbuilder" that should be able to push to the scikit-learn.github.io repo. However I don't understand how the cicleci job is supposed to have access to this ssh private key. The recent changes in the `scikit-learn/scikit-learn/.circleci/config.yml` do not seem to be related in any way to the ssh key configuration. __eou__	User

TP	shouldn't that be inside a section in circle-ci holding "secrets"? And then giving access to those values to jobs via environment variables or something? I have no idea how circle-ci works though. __eou__	User

TP	yes I believe so but I did not see anything related to this in the circle CI settings menus and I am not the one who configured it in the first place so I am not sure what has changed and what should be done to restore the push. __eou__	User

TP	On circle-ci, when I look at our jobs, it says "Projects currently running on CircleCI 1.0 are no longer supported. Please migrate to CircleCI 2.0." up there, and the migrate thingy is a hyperlink. We already use circle-ci 2 in our pipeline scripts, but it'd be nice if we upgraded there as well I suppose. __eou__	User

TP	Just posted in pydata/pandas but someone suggested I ask here too:   Does anyone where the joblib folks hang out? I am trying some custom store backend stuff with pyarrow serializers and want to see if anyone else is messing around in this space. Feels like am between dask, joblib and pyarrow communities and want to make sure I'm not missing something someone else is already doing. __eou__	User

TP	@david-cottrell_gitlab joblib folks are here ;) I think it's primarily @lesteve and @ogrisel ? pretty sure they haven't thought about pyarrow though (but @ogrisel is probably the best person to know these three communities) __eou__	User

TP	@lesteve @ogrisel @amueller basically just wondering if there is some hidden trove of custom store backend hackers out there, there are some interesting use cases with using something like hyperdb (decentralized) as a store_backend for sharing the cache globally but it means swapping out all the pkl for something safer.  I am just playing around but I would suprised if someone hadn't gone down this route before. __eou__	User

TP	guys need some help __eou__	User

TP	You would be better of posting on SO and putting a link here __eou__	User

TP	@david-cottrell_gitlab I am not aware of any arrow-serializer aware store for joblib. As far as I know there is only pkl based things. __eou__	User

TP	@ogrisel Thanks. __eou__	User

TP	Hi, sorry if this is a silly question, very new to SVM in general. Was wondering if it's possible to create a model for one-class classification (ie. training on normal data, testing on normal and novel data), where there are more than 2 features for each example. All the code I've seen online seem to only consider 2 features __eou__	User

TP	Those are hand-made minimal examples mostly for presentation purposes. I've applied SVMs to 22k+ features and have gotten good results. I guess that answers the question :) You may find the "Learning with Kernels" book useful if you decide to get deeper into the topic. __eou__	User

TP	Also if you interest is novelty detection, don't restrict yourself to OneClassSVM. You should also give IsolationForest and LOF a try: https://scikit-learn.org/stable/modules/outlier_detection.html __eou__	User

TP	Hello everyone , I am a noobie with scikit , interested in datascience , can anyone give me trusted quick reference and reading material link , Thanks Thanks @amueller __eou__	User

TP	https://jakevdp.github.io/PythonDataScienceHandbook/ @Ritzing (free online) __eou__	User

TP	In scikit-learn, where `gpr` is a GaussianProcessRegressor object. The dimension of the $$Y$$ variable is $$N$$. $$X$$ consists of $$M$$ points.  In this code below ```python y_pre, y_cov = gpr.predict( X,  return_cov=True ) ``` Why isn't `y_cov` an array of $M$ covariance matrices of size $$N \times N$$, where $N$ is the dimensionality of the $$Y$$ variable? Why is `y_cov` an array of $$M$$ scalar values? __eou__	User

TP	Hey... why is `DecisionTreeClassifier/Regressor` using a `random_state` from my understanding  greedy tree building algorithms dont need random? __eou__	User

TP	@yupbank there can be some randomness in choosing the splits in the trees as you can see [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) __eou__	User

TP	@g_abhishek10_twitter probably doing some research on named entity recognition would give you some good hints. __eou__	User

TP	why would there be any randomness in choosing splits? since decision_tree is  a greedy algorithm, find the best feature to split @adrinjalali ^ __eou__	User

TP	@yupbank  I suggest you look for `random_state` in [here](https://github.com/scikit-learn/scikit-learn/blob/7389dbac82d362f296dc2746f10e43ffa1615660/sklearn/tree/_splitter.pyx) to better understand how it's working. __eou__	User
TP	Thanks man, i understand how the code works...  but i dont understand why are we doing this__eou__	Agent

TP	https://gist.github.com/yupbank/1e0c2f50d5ed571e10559a681e7bb76f should be fun for some people __eou__	User

TP	Any pull requests or changes/additions/stars to this repository would be very much beneficial, please help https://github.com/gautam1858/python-awesome __eou__	User

TP	Hi, I have been seeing some conversation in the mailing list regarding an upcoming Dev sprint. I have made a couple of contributions at the tail end of last year and it's one of my goals this year to see if I can contribute more. Will this be an event which I will be able to participate remotely as well or do I need to be France to participate? __eou__	User

TP	I have a minor question on making open-source contributions.. Traditionally, before I decide to change a file, I ALWAYS FIRST do a `git pull` to get the updated version. I then switch to the branch I created initially, and then make the changes, and do the `git add .`,  commit and push, and then submit a pull request. Correct? __eou__	User

TP	Is the SAME procedure followed for making contribution to this library? I do 'git pull' everytime? __eou__	User

TP	Is this a bug? https://stackoverflow.com/questions/54098749/how-can-i-make-gradientboostingregressor-work-with-a-baseestimator-in-scikit-lea __eou__	User

TP	[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/gww6/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/gww6/image.png) Can someone help me on why words of 1 characters are not available as features here? __eou__	User

TP	Hello World,I am new.Can someone link me up on how I can start contributing. __eou__	User

TP	@sameshl https://scikit-learn.org/dev/developers/index.html __eou__	User

TP	@whiletruelearn Yes, of course it's is possible to participate in the sprint remotely.  I guess list of participants on the wiki is mostly there for organization reasons (booking the room of the right size etc). If you want to participate just comment on gitter (here or on the dev channel) during the sprint and someone will help get you started. @rbhambriiit Word of one character are ignored because typically they are stop words (i.e. have no predictive power). If you want to keep them you can change the regexp in the `token_pattern`. @DrEhrfurchtgebietend Thanks for the heads up. Yes, it should be fixed by https://github.com/scikit-learn/scikit-learn/pull/12436 I think __eou__	User

TP	Thanks @rth . Looking forward to it __eou__	User

TP	Hello Everyone! I am New here, My name is Vedang and currently studying at IET-DAVV,Indore. Can someone please tell how can I start contributing. Thanks __eou__	User

TP	@vedangj044 https://scikit-learn.org/dev/developers/index.html __eou__	User

TP	@adrinjalali thanks, i would look into it. __eou__	User

TP	Hi everyone, I'm currently doing a pixel-based supervised classification of an image with the SGDClassifier. I want to include the spatial context between pixels in the classification besides the pixel intensity. So, I just found out about `sklearn.feature_extraction.image.grid_to_graph()`, and was wondering if there was a way to include this information (or the graph adjacency matrix) in the classification? __eou__	User

TP	A good source for data set for malware detection using ML? __eou__	User

TP	[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/CO5c/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/CO5c/image.png) why in the lst  line this throws an error "value eror" string to float? [![image.png](https://files.gitter.im/scikit-learn/scikit-learn/Jl8c/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/Jl8c/image.png) ?? __eou__	User

TP	@farman32 the OneHotEncoder should be used on numerical features. So, for instance, use it on your r LabekEncoder, but reshaped : OneHotEncoder.fit_transform(r.reshape(-1,1).toarray(= Oups... toarray()! __eou__	User

TP	@ronitneve_twitter interesting http://arxiv.org/abs/1802.10135 __eou__	User

TP	Salaam everyone! I am currently working on my final year project which includes 3 modules out of which one is 'Topic Generation'. Right now i am stucked in the results. I am using LDA mallet model but the results are not accurate. Help needed! __eou__	User

TP	Can anyone recommend a pretrained model for text summarization with for instance Apache license for commercial use? __eou__	User

TP	hi all __eou__	User

TP	Good sources to learn scikit for beginners? __eou__	User

TP	@Akash-Sharma-1 try intro to machine learning course on udacity. Its free and uses skikit library. __eou__	User

TP	@anandvimal thanks for help I am gonna jump straight into it __eou__	User

TP	It also has a full project in the end to practice the complete ml flow. __eou__	User

TP	There's also Jake Vanderplas' data science handbook __eou__	User

TP	just fun __eou__	User

TP	Hi together, I like to write custom transformers which should not rely on sklearn as a dependency but should be usable in sklearn pipelines anyway (if necessary). Is that possible by simply providing `fit` and `fit_transform` methods? I assume not looking at [BaseEstimator](https://github.com/scikit-learn/scikit-learn/blob/7389dbac82d362f296dc2746f10e43ffa1615660/sklearn/base.py#L129). Thanks for any help. __eou__	User

TP	@Akash-Sharma-1 also look into hands on machine learning with scikitl-learn and tensorflow __eou__	User

TP	@mansenfranzen you need to provide `transform` as well, since there are cases where `transform` is called on different data than `fit`; and otherwise make sure you follow the custom estimator guidelines, if you have any parameters to set: https://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator __eou__	User

TP	sprint Paris: I am continuing #5862 __eou__	User

TP	@vene Thanks alot for the link to the documentation - it's exactly what I was looking for :-) __eou__	User

TP	oecd_bli = oecd_bli[oecd_bli["INEQUALITY"]=="TOT"] can anyone explain the last line of code? I can't understand this line properly. ''' ''' def prepare_country_stats(oecd_bli, gdp_per_capita):     oecd_bli = oecd_bli[oecd_bli["INEQUALITY"]=="TOT"] ''' can anyone explain the last line of code? I can't understand this line properly. __eou__	User

TP	@Pritom-Mazhi `oecd_bli[oecd_bli["INEQUALITY"]=="TOT"]` selects the all the lines of a pandas dataframe named `oecd_bli` where the `INEQUALITY` column has value `"TOT"` All the other lines are dropped. __eou__	User

TP	I'm having no luck with this building. Can someone tell me which contineny were in? Second floor, tight? __eou__	User

TP	7th floor should I come down to get you? african continent __eou__	User

TP	Doing a presentation on ONNX right now for people at the sprint __eou__	User

TP	@adrinjalali, what is the difference between `min_samples` and `min_cluster_size` in optics ? __eou__	User

TP	@assiaben by convention (I think dbscan uses the same thing) `min_samples` is the parameter used to do the nearest neighbor part, and `min_cluster_size` is used when _extracting_ the clusters from the reachability distances. we probably need to find a way to better explain these __eou__	User

TP	hello guys! Im working on a movie dataset for item-item collaborative filtering. Im able to get my cosine_similarity function working with for loop on a trunkated part of the dataset. I hear that utilizing apply(lambda x: <func>) is much faster. Anyone familar with converting? __eou__	User

TP	@troykirin a minimal example copypasta would be helpful to get us started; usually its as simple as you described `apply(lambda x: func(x))` assuming all records in the loop are independent __eou__	User

TP	@tylerkontra-system1 this is the loop im trying to work with ```  for pair in combinations(np_array_movieandratings[:,],2):     x1 = pair[0]     x2 = pair[1]     np.append.(cos_sim(x1,x2) ``` __eou__	User

TP	it might make more sense to do something like this ``` import numpy as np from itertools import combinations  def my_func(p):     return np.sum(p) my_array = np.array(range(100)) my_combin = np.array(tuple(combinations(my_array, 2))) my_result = [my_func(pair) for pair in my_combin] ``` since the biggest performance boost is likely to be from simply using numpys data structures (re: https://stackoverflow.com/questions/38709313/numpy-vectorize-multidimensional-function) __eou__	User

TP	scikit-learn 0.20.3 is online! https://scikit-learn.org/stable/whats_new.html#version-0-20-3 __eou__	User

TP	why do we use random numbers in machine learning ? I don't quite get it __eou__	User

TP	@Devosource there can be many uses of random numbers. For example if you are working on a dataset of a hundred thousand images and you need to check the result randomly, then you might use a randInt() function i guess.   Also, that was just an example. I didnt quite understood the context of your question. Hope you got your answer but if not then you could be more specific and someone would give you the answer you are looking for. __eou__	User

TP	i was going through keras beginner's tutorial and abit of CNN model tutorial. I found that the first thing to do after importing modules is to fix random seed for reproducibility , for random numbers. That's what I don't understand ``` from keras.layers import Dense import numpy # fix random seed for reproducibility numpy.random.seed(7) ``` __eou__	User

TP	@Devosource  So basically when you are watching a tutorial and coding along with it, you can use fix seed for the same result as in the tutorial you are watching. Just for the sake of same random number generation throughout the code whenever a random number is generated. __eou__	User

TP	@algo-circle thanks __eou__	User

TP	@Devosource You are welcome sir! <unconvertable> __eou__	User

TP	Anyone know anything about why the keras.io room is blocked? Says github users only. I think there is a glitch? __eou__	User

TP	Yes, it is a glitch with gitter. They link accounts but sign-in still matters. __eou__	User

TP	Hello everyone, I have a quick question regarding normalization (min-max-scaling) of output values. Usually you are supposed to use normalization only on the training data set and then apply those stats to the validation and test set. But for instance, my prediction variable is a single percentage value ranging [0, 100%]. And I know for sure that in the "real world" regarding my problem statement, that I will get samples ranging form 60 - 100%. But my training set is to small and does not contain enough data points including all possible output values. So here comes my question: Should I stay with my initial statement (normalization only on training data) or should I apply the maximum possible value of 100% for my output value to max()-value of the normalization step? __eou__	User

TP	Are you doing regression? Typically minn max scaling is used on the feature vector values that are inputs into the classifier. Not necessarily on the outputs of the classifier __eou__	User
TP	yes, regression__eou__	Agent

TP	Another silly idea was to squash my output value into the range [0,1] by applying target/100. Does that make sense? __eou__	User
TP	thats just chaning how you look at it, not really changing the properties of it__eou__	Agent
TP	just checking I understand your problem though. You are saying in your training set, you have min max output of  something like  .2 - .6 and so you are scaling that to 0-1.__eou__	Agent
TP	It depends on the problem you are trying to solve. You'll have to decide if it is appropriate for your problem or not.__eou__	Agent
TP	what min max scaling does in that case is stretch the predictions between .2 and .6 to a higher resolution, and then make evertying outside .2 or .6 equal to each other__eou__	Agent
TP	if that makes sense to do will be application specific__eou__	Agent

TP	Maybe I'm just dump. So, my target value ranges from something like 60% - 100% and I'm trying to predict it by using MLP/CNN. My understanding is that I have to scale my output value that it matches my output activation function (ReLU). But I got cought up with normalization in general and the correct usage on training,validation and test data. That's how I got confused __eou__	User

TP	ok, yeah so any scaling that you do on your training data. You will want to apply those same normalization to the validation and test data you can also set the normalization of the training to be what you expect from your real data __eou__	User

TP	Exactly, but what if my training set doesn't include all possible outcomes for my output value due to the small sample size. Usually that would just mean, I would have to include more samples. But that is not possible at this point. So furthermore this leads to the situation that my test sample will have output values which are not present in the training set.  One solution would be to give the model the information in advance that the max possible range of the output value will be 100%. But this idea contradicts the Literature __eou__	User

TP	jupyter notebook crashes when running opneAI's gym env any fix for this ? __eou__	User

TP	Hello Guys and Girls DO you Wanna Meme Based Group Only Of Data Science and Machine Learning I have Created One Dumbily For You People To Join https://www.facebook.com/groups/2084179105032029/ Join in With your Facebook Account __eou__	User

TP	Hi anyone can please assist me to Install anaconda on Ubuntu __eou__	User

TP	have you tried an online tutorial and failed @SadiaAman ? __eou__	User

TP	Can i join in with my github account? __eou__	User

TP	@wwf6688 just login with github account __eou__	User

TP	I can't open the website in China https://www.facebook.com/groups/2084179105032029/ __eou__	User

TP	Try using vpn Shadowsocks or any other way __eou__	User

TP	ok __eou__	User

TP	I'm looking to take a set of attributes about an object in the real world, and then determine a set of products that would best fit it based on these attributes. Would this be a good fit for using a decision tree? these set of products would be values calculated from the attributes themselves I am not looking for any type of prediction though, just a method of representing logic branches nicely in some pretty complex business rules __eou__	User

TP	Hi, what's the reason why none of the CV iterators support single splits (i.e. `n_splits=1`)? __eou__	User

TP	n_splits is the number of chunks produced. So n_splits=1 means not to split the data at all, leave it in one chunk __eou__	User

TP	That's not right if I understand you correctly, running  ``` cv = KFold(n_splits=2) len(list(cv.split(np.arange(10)))) ``` returns 2, i.e. 2 tuples of two arrays, not  a single tuple. __eou__	User

TP	Take a look at the list itself and it'll become clear. You have 2 chunks. First you use the first chunk as train and second chunk as test set. Then, you use the second chunk as train and first chunk as test set. I should not call them chunks. They're folds. Two folds :) __eou__	User

TP	Yes, I understand that, but why can it not give me a single train and a single test set (`n_splits=1`) as `split_train_test` does? Because it's then not really folds anymore? __eou__	User

TP	I guess `n_splits` is a bit of a misnomer. `n_folds` would've been clearer maybe. __eou__	User

TP	Yes, okay, but still why not return a single split? Any underlying design choice/complication? __eou__	User

TP	I know this is going to sound like I am complaining, but I am not. In 2015 I suggested adding Gower similarity/dissimilarity to our metrics. https://github.com/scikit-learn/scikit-learn/issues/5884 . It is now April 2019 and the PR is still in the works https://github.com/scikit-learn/scikit-learn/pull/9555 It feels like someone  could have coded up the dissimilarity score in a day and written tests in a week. Is this sort of 4 year period normal or something worth exploring? In fact it was implemented in 2017 I see in the issue as a jupyter notebook __eou__	User

TP	It's easy for PRs to get buried. __eou__	User

TP	Hi, I have a question about what `GridSearchCV` (or, in my case, `RidgeCV`) passes to the scorer for evaluation. I created a custom scorer that takes in `y_true` and `y_pred` and returns the correlation (see below). However, I stuck a `print` statement in there to verify the shape of what's being passed to the scorer. I'm using `KFold` cross-validation with e.g. _k_ = 10, which for 200 samples will return splits with shapes `y_train` = 180 and `y_test` = 20. It seems that both arrays of shape 180 and 20 are being passed to the scorer for evaluation. As if the scorer is being run on both the test set (expected) and the train set (unexpected, for me). Is this actually what's going on? If so, is this the desired behavior? Maybe my implementation of the scorer is screwed up, but couldn't figure this out from the documentation. Thanks, and sorry for any confusion! ```python def correlation_metric(y_true, y_pred):         y_true_demean = y_true - np.mean(y_true, axis=0)     y_pred_demean = y_pred - np.mean(y_pred, axis=0)     numerator = np.sum(y_true_demean * y_pred_demean, axis=0)     denominator = np.sqrt(np.sum(y_true_demean ** 2, axis=0) *                           np.sum(y_pred_demean ** 2, axis=0))     print(f"True shape: {y_true.shape}; Predicted shape: {y_pred.shape} "           f"Correlation: {numerator/denominator}")     return numerator / denominator correlation_scorer = make_scorer(correlation_metric) ``` [![image.png](https://files.gitter.im/scikit-learn/scikit-learn/vOzF/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/vOzF/image.png) __eou__	User

TP	Hello, is there a simple way how to make a polynomial regression of given degree? __eou__	User

TP	@Borda take a look at  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html @snastase yes GridSearchCV has a `return_train_score` attribute (the default will change in the next version) __eou__	User

TP	Ah got itthanks! @NicolasHug __eou__	User

TP	hi anyone here? while using librosa I 'm getting this error ImportError: cannot import name '_inplace_contiguous_isotonic_regression' I tried googling it but I got nothing please help hmmm ok thanks let me try it on librosa can u plz give a one liner pip code to downgrade it ? __eou__	User

TP	This is private API that is not guaranteed to stay stable across scikit-learn releases, maybe librosa was meant to be used with a specific version of scikit-learn. Please check with its documentation or asking its developers and make sure that you have the correct version of scikit-learn. __eou__	User

TP	https://github.com/librosa/librosa/blob/master/setup.py#L50 could be that > 0.19 could be problematic __eou__	User

TP	pip install scikit-learn==0.18 __eou__	User
TP	thanks a lot__eou__	Agent

TP	now I m getting another error ImportError: cannot import name 'astype' plz help I have python 3.8.Does it matter by that too ? __eou__	User

TP	hi all __eou__	User

TP	*Hi all* __eou__	User

TP	Hi, I have a doubt on `TfidfVectorizer` in the `sklearn.feature_extraction.text` package. I see that stopwords like the which occur frequently in all documents I am trying, have an IDF value of 1 Shouldnt the IDF value be 0 (because log of 1 is 0)? The example documents I am using are: > ["The quick brown fox jumped over the lazy dog.",   "The dog.",   "The fox"] I suspect the log is not being taken, how do I configure the vectorizer to take the log and get an IDF of 0? __eou__	User

TP	I think I figured out he IDF calculation. It seems to be `ln(N/df) + 1`. Where `N` is the total number of documents and `df` is the number of documents a particular term appears in So for the word the it is `ln(3/3) + 1` = `0+1`, which is why the value is 1. How do I configure the vectorizer not to add the 1? __eou__	User

TP	can you tell me what is the api name for congressional voting records datasets? as i have to import the dataset what i should write? data = datasets.load_????? __eou__	User

TP	please tell me how to find the right dataset name api __eou__	User

TP	@aarck you could try to see if that dataset is uploaded on OpenML in which case it would be `datasets.load_openml`. If not, it's up to you to write a loader for this dataset (or upload it to OpenML). __eou__	User

TP	hello everyone is there someone who knows about scikit-multiflow library ??? __eou__	User

TP	@Praful-cs  , Hiii, let me help you with this. __eou__	User

TP	Hello all, I have a question regarding joblib's "vendor" distribution present in scikit-learn `0.20.X` : is it up to date with the latest release of joblib? I know that the latest version of scikit-learn (`0.21.X` and above) are now using joblib as a dependency directly but I need to use scikit-learn `0.20.X` for python 2.7 support. Thank you! :) Thank you @ogrisel for this quick answer! __eou__	User

TP	@jjerphan scikit-learn 0.20.3 is embedding joblib 0.13.0: https://github.com/scikit-learn/scikit-learn/blob/0.20.X/sklearn/externals/joblib/__init__.py#L109 __eou__	User

TP	if we release 0.20.4 we should think of upgrading the joblib version as well. You can also set the `SKLEARN_JOBLIB_SITE=1` environment variable to use an independently installed version of joblib instead of the vendored version. Note however that future versions of joblib might stop supporting python 2 as well, so better start thinking of upgrading to Python 3 in any case :) __eou__	User
TP	Thanks for the tips ; I would like to upgrade but I sadly can't in my setup <unconvertable> sigh. :)__eou__	Agent
TP	Are there any significant changes between joblib 0.13.0 and 0.13.2 ?__eou__	Agent
TP	Potentially important bugfixes: https://github.com/joblib/joblib/blob/master/CHANGES.rst__eou__	User
TP	Yes, I just came across this.__eou__	Agent

TP	@zzj0402_gitlab What is cross correlation? What is the relationship between it and convolution? I am not sure if it's cross product between data and convolution is just flipping the signal from both data sets to negative? __eou__	User

TP	Hey guys!!! Can someone suggest me any dataset available on kaggle which contains both numerical and textual data? I have to apply machine learning as well as nlp concepts on it. __eou__	User

TP	Quick question, when using TfIdf vectorizer, would you consider it beneficial to lemmatize the body of the document before fitting? __eou__	User

TP	Lemmatization is always a good idea. Maybe you can run the vectorizer first and see in how many cases you got a wrong prediction because of no lemmatization and then take a call __eou__	User
TP	I will check that in practice then I guess as suggested. Thanks @srniranjan__eou__	Agent

TP	[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/DQTb/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/DQTb/image.png) how to handle with this error? __eou__	User

TP	Hey guys!!! Can someone suggest me any dataset available on kaggle which contains both numerical and textual data? I have to apply machine learning as well as nlp concepts on it.  refer this----->>>>https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling @mukul09 __eou__	User

TP	Can anybody please explain me batch decent gradient and the procedure to calculate it I have just started machine learning In python __eou__	User

TP	Batch gradient descent uses the whole dataset to calculate the gradient vector unlike mini-batch or stochastic gradient descent, thats the key point. The procedure in common words is: calculate partial derivatives for a cost function with respect to each coefficient using the whole dataset, make a gradient step, update coefficients __eou__	User

TP	Hi there. Try this link for batch gradient descent in python https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f __eou__	User

TP	Thanks @isaacaugustus  and @gyrdym __eou__	User

TP	@harshchaplot this is a great resource too: https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU __eou__	User

TP	Thanks @srniranjan __eou__	User

TP	Say you have over 9000 features which you would like to significantly reduce, what outside of PCA you can use to put down that number? __eou__	User

TP	You may use lasso regression, for instance __eou__	User
TP	What is PCA?__eou__	Agent

TP	https://en.wikipedia.org/wiki/Principal_component_analysis __eou__	User

TP	PCA is a linear way of reducing your dimensions to a few Principal components. You can use Neural Networks for a non-linear approach for the same Basically the last hidden layer of your neural network is a representation of your input in much the same way projecting to principal components is The number of dimensions of this representation will be the number of neurons in the last hidden layer __eou__	User
TP	Ohk thanks @srniranjan__eou__	Agent

TP	@gyrdym does it work with sparse matrices so I can use it with tfIds vectorized text? tfidf* __eou__	User

TP	As far as I know, it works with sparse matrices, but I myself havent use it for such situations __eou__	User
TP	I see TruncatedSVD which uses LSA underneath for the decomposition of sparse matrices suggested by sklearn team__eou__	Agent
TP	but this method lets me go down only by about 200 features which is just not enough.__eou__	Agent
TP	and I need to cut it down say to 2000 features so I don't spend days training one model__eou__	Agent
TP	yes it is, not entire vocabulary, I have stop words filtered out and lemmas extracted__eou__	Agent
TP	What would be the recommended way?__eou__	Agent

TP	SVD is not using LSA underneath: LSA is the application of (truncated) SVD to text data represented as bag of words (e.g. TF-IDF vectors). SVD is a generic mathematical tool, LSA is one specific application of SVD to text mining.  9k features features is pretty low number of features for bag of words vectors. Bag of Words is very very sparse. SVD on TF-IDF / bag of words is a good fast preprocessing used as baseline for text classification / clustering and information retrieval / text mining. __eou__	User

TP	@piotr-mamenas 9k features is huge! Are you having such a big feature set because youre considering each word in the vocabulary as a feature? __eou__	User
TP	PCA is SVD on centered data.__eou__	Agent
TP	Thats not the recommended approach when doing NLP__eou__	User
TP	Look at word2vec to get dense vectors__eou__	User
TP	Glove is also recommended to get dense vectors__eou__	User
TP	Depends on your use case.. what are you trying to solve?__eou__	User

TP	I have a set of classes with labelled data which I want to build several binary output models from, each model would just output 0, 1 to highlight whenever the article belongs to a class or not (1 class per model) __eou__	User

TP	LogisticRegression on TF-IDF vectors should be a good and fast baseline. You can also try: TF-IDF => TruncatedSVD => LogisticRegression or RandomForestClassifier and TF-IDF => NMF => LogisticRegression / RandomForestClassifier as alternatives. __eou__	User

TP	but this stays with the TfIdf approach, how would it handle the 9600 or so features? __eou__	User

TP	You can also try to include features derived from pretrained word vectors (e.g. word2vec or glove) and some fancy neural networks with keras or pytorch but I would try the above baselines first. > but this stays with the TfIdf approach, how would it handle the 9600 or so features?  How is this a problem? LogisticRegression works fine on high dimensional sparse data About the first question: lemmatization is not always a good idea depending on what you are try to predict. __eou__	User

TP	I asked that question the other day and wrote about it, lemmas might lose the context of whole sentences so in theory they will lower the accuracy but that's also a way to lower the amount of data to process Lets see the logistic regression approach then THanks __eou__	User

TP	@ogrisel this looks suprisingly good, it was training just for a few seconds on a set of only 3500 articles and the confusion matrix looks like this: array([[1262,   19],        [   0,    0]], dtype=int64) Accuracy:  0.985167837626854 if it looks to good, there must be something wrong __eou__	User

TP	Is your data well balanced? Maybe you need to shuffle the original dataset before do logistic regression. It would be good also to use cross validation to get the answer if your model is really so good __eou__	User

TP	Yeah, I am checking it. __eou__	User

TP	array([[1262,   19],        [   0,    0]], dtype=int64) means that the mode always predict the class 0. It's the constant predictor (probably caused too much bias / regularization) Your test data is actually imbalanced with  19 to 1262 ratio. 1262 / (12 + 1262) == 0.99 accuracy which is weird because this does not match your reported accuracy. @piotr-mamenas I hope you did a proper train test split :) You should use a balanced accuracy or ROC AUC or precision / recall / f-beta score to evaluate your model instead of the raw accuracy. __eou__	User

TP	``` UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.   'recall', 'true', average, warn_for) ``` the train test split was 0.75:0.25 ratio __eou__	User

TP	It can't predict 0.98 with constant class because you got 1200/5000 samples class 1, and the TP is 1262 and train test split takes into account balance of classes from what I know @gyrdym __eou__	User

TP	the overall class balance of the entire dataset is a difficult topic, the type of class I am trying to detect may appear in every 1/100 articles but as said I have 1200 samples of "1" and 3800 samples of "0" so I am guessing increasing the "0" sample would prove beneficial generally __eou__	User

TP	check the content of your `y_test`: the recall warning seems to indicate that you only have negative samples (samples from the majority class) in your test set. __eou__	User

TP	ok got it @ogrisel ``` Accuracy:  0.8227946916471507 Precision:  0.9347826086956522 Recall:  0.28013029315960913 ``` ``` array([[968,   6],        [221,  86]], dtype=int64) ``` looks different now __eou__	User

TP	I shuffled the dataset on the train test split and it changed completely as you can see but the score is anyway pretty impressive considering the sample and no hyperparameter tuning thanks __eou__	User

TP	You may also try stratified shuffled split instead of the regular one - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html __eou__	User
TP	I will check it out, thanks @gyrdym__eou__	Agent
TP	You're welcome__eou__	User

TP	Anyone has 2$ million dollars by the way? : p __eou__	User

TP	oh and that's what I like, I added 6000 articles to the 0 class and I already have 0.92 ``` array([[2492,    7],        [ 235,   47]], dtype=int64) ``` __eou__	User

TP	Accuracy is rather meaningless for unbalanced problems. Look at precision and recall and plot the precision recall curve. Here a recall of 0.2 might be too bad for your classifier to be useful. It depends on the application of your classifier If you do parameter tuning of your text classification pipeline, use scoring="balanced_accuracy" or "f1_score". See the end of this tutorial on how to build a pipeline and do parameter tuning: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#evaluation-of-the-performance-on-the-test-set __eou__	User

TP	That will be useful, thanks @ogrisel __eou__	User

TP	hi guys does scikit sdk or articles available for go language __eou__	User

TP	https://www.quora.com/Go-vs-Python-which-is-better-for-AI __eou__	User

TP	anyone on windows? I am trying to run a script with just "python script.py" but I am getting missing module errors and I am sure I should have this installed these* __eou__	User

TP	Check the spellings of the modules you have imported And also check whether the modules you imported contain that specific module And just as shortcut run pip install on all the modules to ensure everything has been properly installed It also depends on the version of python you are using __eou__	User

TP	Python 2.7 does not support many modules that are supported by python 3.6 and other newer versions __eou__	User

TP	@piotr-mamenas on windows I would recommend you to use conda environments: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html (or alternatively Python 3 builtin venv module) to get a fine control on the list of packages installed in the currently active environment __eou__	User

TP	*IIT BHU* conducting Coding fest (online as well as offline also) which is *free of cost* + lots of goodies  <unconvertable>  Some information regarding codefest 2019  Just registered yourself .   https://codefest.tech/login?referral=qbK0w2UF   1.Annual coding fest of CSE Department ,IIT BHU 2. 8 events covering almost every domain of Computer Science and Engineering 3. Global participation from more than 100 countries 4. *Cash prize* of nearly *500k* ,goodies an other merchandise as well 5. No registeration cost, *certificate to all participants* 6. *Onsite events,HaXplore,accomodation,goodies and food <unconvertable> will be provided to everyone ,travel <unconvertable> reimbursement as well*  *Share with your buddies also* __eou__	User

TP	@ogrisel that's what I am doing locally, I work with a jupyter notebook with a separate tensorflow + scikit environment installed and used with "activate" and conda I was asking because I've got a separate lightweight service running inside of container and I wanted to keep it limited to just pip and relevant libs on production In the container I don't want to use conda __eou__	User

TP	@piotr-mamenas have you considered Miniconda as a lightweight alternative? https://docs.conda.io/en/latest/miniconda.html I think there is also the odd dockerhub image to build of __eou__	User

TP	@piotr-mamenas use a venv and pip install everything you need in it. You can also prepare a lightweight conda environment with some tricks: https://twitter.com/jiminy_crist/status/1135637901457395712 __eou__	User

TP	I have an input sparse matrix in the csr format. Any suggestions on how to generate mini batches for training from this input? __eou__	User

TP	U can use the sklearn.model_selection.train_test_split https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html __eou__	User

TP	does it work with sparse matrices? Great, thanks __eou__	User

TP	I think it will __eou__	User

TP	you can use `sklearn.utils.safe_indexing` that will return a matrix given a set of indices and will work with dataframe, array, sparse matrix depending what you are doing with mini-batch, we have some generator of mini-batch in imbalanced-learn http://imbalanced-learn.org/en/stable/api.html#module-imblearn.keras you can check the implementation if you want to bypass the resampling stage __eou__	User

TP	hi everyone can i ask you something is there autocomplete C# codes in komodo __eou__	User

TP	@akil101 please ask on a channel related to komodo or C# development. __eou__	User

TP	I wonder what you guys think. __eou__	User

TP	Say you have a terrible unbalanced text data set that you vectorize, 1:50 ratio binary classification would you consider it a bad practice to lower the imbalance by duplicating proportionally the first class? so practically speaking, if I have 1000 articles with class 0 and 50000 with class 1, just copy each one of the class 0 to get another set of 1000 articles and push it into the training set so I have a 1:25 ratio instead? Similar to how its done with image classification __eou__	User

TP	Another thing, how would you measure quality of scrapped data provided from 3rd party data science firm? Text. I would assume if i do cluster analysis on the data and get word frequency per cluster I should be able to see more or less but that doesn't give me a full picture __eou__	User

TP	@piotr-mamenas https://www.youtube.com/watch?v=EUiIydNBIbE&list=PL_pVmAaAnxIQGzQS2oI3OWEPT-dpmwTfA&index=10 and https://www.youtube.com/watch?v=Eix70D-H5ag&list=PL_pVmAaAnxIQGzQS2oI3OWEPT-dpmwTfA&index=11 are relevant to the first question For imbalanced data I would worry about evaluation first __eou__	User

TP	@piotr-mamenas yeah as long as you are randomly sampling, you can either upsample from the smaller class or down sample..  or use an algorithm that can tolerate unbalanced data.. you could even turn the problem into an anomaly detection one.. if the smaller class has only very few data points.. There are other techniques like SMOTE .. that you could look into.. as well. __eou__	User

TP	Thanks for the answers @amueller and @rahulunair , I wasn't aware you have models for working with imbalanced data sets specifically, how do they fare against large sparse matrices, I am talking word vectors? Thanks @rahulunair your response is much appreciated, I will take a look at it. __eou__	User

TP	@piotr-mamenas I would consider the word vectors as the input embeddings? and if you are looking to classify something, you can look into SVMs that deal with unbalanced classes, essentially it weights the unbalanced class differently.. scikit-learn has a section for that: https://scikit-learn.org/stable/modules/svm.html or try a tree based algorithms to see how your accuracy numbers and ROC curve is for accuracy , check out: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html __eou__	User

TP	if you are doing image classification using scikit learn, do you have to convert the images into 1d arrays first? @glemaitre  hmm... that seems to lose some vital information i..e that pixels next to each other are related HoG features? Histogram of Oriented Gradients ? __eou__	User
TP	Histogram of Oriented Gradients__eou__	Agent
TP	yes__eou__	Agent
TP	all very interesting thanks. It seems a weakness somehow in the general non-NN classification model that it can't take advantage of 2d data__eou__	User
TP	I suppose even in 1d random forests etc are invariant to permutations of the input array__eou__	User
TP	@glemaitre  thanks__eou__	User
TP	I wonder if random forests could be changed to take arrays of pairs, say, as inputs__eou__	User
TP	@ogrisel  that's true but I am also thinking of time series data__eou__	User
TP	where it makes a big difference if two values are from successive times or not__eou__	User
TP	@ogrisel  you said No but I read your answer as yes :)__eou__	User
TP	@ogrisel  that would be good to see__eou__	User

TP	yes __eou__	User

TP	you can look at an example of `load_digits` to see that the 8x8 images are transformed to 1d 64 arrays > you can use a pre-trained convolutional neural network to extract interesting features  which a much better approach https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html I think that I have 2 quick examples showing a bit how things can be connected: https://scikit-image.org/docs/dev/auto_examples/applications/plot_haar_extraction_selection_classification.html#sphx-glr-auto-examples-applications-plot-haar-extraction-selection-classification-py https://github.com/scikit-learn/scikit-learn/pull/6509/files __eou__	User

TP	you can use a pre-trained convolutional neural network to extract interesting features or you can use scikit-image HoG features for instance. Depending on the kinds of images, it might be enough. __eou__	User

TP	yes, you have to do feature engineering first. You can consider the 2D conv layers before the final flatten / global average pooling as a feature extractor and the last fully connected layers as a standard classifier. It's just that both the feature extraction and the classifier are trained end-to-end together __eou__	User
TP	it's only really the convolutions that  take advantage of the neighborhood of pixels I suppose__eou__	Agent
TP	@ogrisel  right.__eou__	Agent
TP	but nowawdays, (convolutional) neural networks are almost always the good solution for image classification, unless you have very specific prior knowledge on the image you want to classify.__eou__	User
TP	> it's only really the convolutions that  take advantage of the neighborhood of pixels I suppose  No: if you have deep conv layers with downsampling (strides or max pooling for instance) the conv layers can capture large high level complex patterns that span a large receptive field.__eou__	User
TP	we need an example of some standard feature engineering you can do on time windows for time series forecasting / classification.__eou__	User
TP	I misread the original quote, then yes.__eou__	User
TP	But what I meant is that deep conv net can model non-local patterns__eou__	User
TP	if you really want to use decision trees for image classification you might be interested in https://arxiv.org/abs/1905.10073 but this is not (and will not) be implemented in scikit-learn ;)__eou__	User

TP	@ogrisel yes . What I meant is that without any convolutions you don't get to see local patterns on an NN topic, is there software to give you a good guess at a reasonable architecture for a classification task? I saw autokeras but it's pretty heavy. @ogrisel thanks! Why won't it be implemented? Because it doesn't work or coding resources? @ogrisel  got you image classification was just interesting because the data is in 2d but even in 1d it seems unclear to me what the right thing to do is @ogrisel  I have read those guidelines! They seem very sensible to me I greatly admire how scikit learn is run in general @ogrisel  I read that second link too :) __eou__	User

TP	I don't know what is the practical state of the art for architecture search for image classification __eou__	User
TP	really I am secretly interested in time series__eou__	Agent
TP	because it is not a standard, established method.__eou__	User
TP	https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms__eou__	User
TP	https://scikit-learn.org/stable/faq.html#why-is-there-no-support-for-deep-or-reinforcement-learning-will-there-be-support-for-deep-or-reinforcement-learning-in-scikit-learn__eou__	User

TP	hi __eou__	User

TP	What version of OpenMP does scikit-learn require? Is 2.5 sufficient? __eou__	User

TP	Probably, we use OpenMP via the `prange` construct of Cython. __eou__	User

TP	Hey guys, so I'm new to scikit, so please bear with me. I have a pandas dataframe that looks like this: [email, businessId, manager, app1, app2, app3, ... , app170] So essentially one row defines one user that has either a 1 or NaN on each of the appX columns specifying if they have that app or not. What I want is a classifier that given email, businessId, and manager ....would return a list of apps should have I've got the data in that format as i specified, what are the models do you guys think would b good for creating this type of classifier? And how would i go about this in general? __eou__	User

TP	How I get the individual components from classification_report? I found this: https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format sorry, this: https://stackoverflow.com/questions/48417867/access-to-numbers-in-classification-report-sklearn but the output_dict=True doesn't seem to work, I am receiving an error stating this parameter does not exist on the classification_report function, I also don't trust precision_recall_fscore_support, plus it misses accuracy __eou__	User

TP	@piotr-mamenas Please check the version of sklearn you are using. I believe `output_dict` was added in 0.20. __eou__	User

TP	@thomasjpfan yup, I figured it out yesterday and after some fight with tensorflow dependencies I got it running __eou__	User

TP	hi everyone __eou__	User

TP	Hello from the SciPy sprints! __eou__	User

TP	Hi All, also from the SciPy sprints :) __eou__	User

TP	Welcome everybody :) __eou__	User

TP	@thomasjpfan wanna look at https://github.com/scikit-learn/scikit-learn/pull/14326 ? anyone wanna look at https://github.com/scikit-learn/scikit-learn/pull/14320 ? __eou__	User

TP	how can i use yolo to detect numbers in sudoku puzzle? I want to read those numbers but Canny, Hough, contour aren't working any good __eou__	User

TP	Please let me know your opinion on this https://github.com/scikit-learn/scikit-learn/issues/4450#issuecomment-512681856 __eou__	User

TP	Hello People, Do we have any package like NLTK to support your languages other than English __eou__	User

TP	@venkyyuvy this looks Like a good feature to have in LabelEncoder. Would this make sense as a feature @amueller __eou__	User

TP	Hi __eou__	User

TP	@adityap31 we choose to only officially support English in our documentation, to avoid having to maintain different versions __eou__	User

TP	Thanks @NicolasHug __eou__	User

TP	Please the best c# tutorial online Give me ideas __eou__	User

TP	@Emoruwa since you're not the first one asking this here: what gave you the idea of asking about C# in a channel about a Python library for machine learning? __eou__	User

TP	Hi, everyone. My name is Manish and It's nice to meet you all. I used SK learn for one of my projects this summer and  I really love this library. I want to start contributing to it. I'm new to open source stuff and I don't know how to get started. I checked issues under good first issue label but I'm not able to understand anything. Can anyone plz guide me with this?? __eou__	User

TP	@ManishAradwad welcome! the easiest way is probably to ask directly on the issue. Have you checked out the contributors guide? __eou__	User

TP	Yes, I'm now going through the repo first. I'll then go for the issues. Thanks for the reply! __eou__	User

TP	I wouldn't try going to the repo, it's a lot. I would start with the contributor docs even understanding how we set up and run tests would probably take me a week to understand __eou__	User

TP	is there something in scikit learn for 4000 dimension regression where I know I only one or two of the coefficients to be non-zero? something like forward stepwise regression? __eou__	User

TP	not yet. mlxtend has it and there's a PR #8684 __eou__	User

TP	@amueller  Thanks! I will take a look at mixtend which I didn't know about __eou__	User

TP	Can anyone please provide a good source of how to deal with categorical data? It's very helpful and thanku __eou__	User

TP	@amueller  Hi!! As you said I've gone through the contributing guides and set up the development environment. Can you plz tell me what should I do next. Thanks for the help!! __eou__	User

TP	@ManishAradwad look at things tagged as "good first issue" and "help wanted" as outlined in the contributing guide __eou__	User

TP	Hello guys, maybe anyone can help me out here. I am running following validation code: ``` train_scores, valid_scores = validation_curve(estimator=pipeline,  # estimator (pipeline)                                               X=features,  # features matrix                                               y=target,  # target vector                                              param_name='pca__n_components',                                              param_range=range(1,50),  # test these k-values                                              cv=5,  # 5-fold cross-validation                                              scoring='neg_mean_absolute_error')  # use negative validation ```  in the same `.py` file on different machines, which I would name `#1 localhost`, `#2 staging`, `#3 live`, `#4 live`  localhost and staging have both i7 cpus, localhost needs around 40s for the validation, staging needs around 13-14 seconds  live (#3) and live (#4) need almost 10 minutes for executing the validation - both of these servers have intel cpus with 48 threads.   In order to get more "trustworthy" numbers I dockerized the images and run them on the servers. Anyone has an idea why the speed is so different? __eou__	User

TP	how many cores do you have in localhost and staging? could be that you're overallocating processes in the estimator and parallelization actually hurts you what's pipeline? so the number of cores is the likely difference, right? __eou__	User

TP	@amueller localhost and staging are both with i7 (4 cores and 8 threads) yeah, live 3 and live 4 have 48 threads, 24 cores. Pipeline: ``` from sklearn.linear_model import LinearRegression model = LinearRegression() from sklearn.preprocessing import PolynomialFeatures poly_transformer = PolynomialFeatures(degree=2, include_bias=False) from sklearn.pipeline import Pipeline pipeline = Pipeline([('poly', poly_transformer), ('reg', model)]) ``` __eou__	User

TP	After profiling, I saw this (slowest time on bottom, sorted by 3rd column): ```      4150  208.706    0.050  208.706    0.050 {built-in method numpy.dot}       245   13.112    0.054   13.360    0.055 decomp_svd.py:16(svd)      2170  142.567    0.066  143.360    0.066 decomp_lu.py:153(lu) ```  Just executed `python -m cProfiler validation.py` __eou__	User

TP	can you try to benchmark just calling svd directly without any sklearn around it? if that's a pure scipy issues that would be good to isolate __eou__	User

TP	how can I isolate it, make a separate `.py` and run `cProfiler` on it? __eou__	User

TP	make a py file that calls scipy.linalg.svd without using sklearn well that should work __eou__	User

TP	lol I am killing the sorting in the pull requests in the issue tracker with adding tags. sorry lol __eou__	User

TP	I will try this and report here. Any ideas what could be the reason? Localhost and staging are intel i7, live3 and live4 are xeon cpus, do you think mkl would improve speed or setting up the environment in another way? (Tensorflow recommends custom compile for speed for example) __eou__	User
TP	how did you install numpy and scipy? if you did custom compilation that might be a reason. if you install binaries they will use mkl or openblas, either of which should be quite fast__eou__	Agent

TP	Using pipenv, numpy 1.16.x i think They are using openblas __eou__	User

TP	@amueller I don't know if this helps: I ran ``` from scipy import linalg import numpy as np m, n = 9, 6 a = np.random.randn(m, n) + 1.j*np.random.randn(m, n) U, s, Vh = linalg.svd(a) print(U.shape,  s.shape, Vh.shape) ```  `cProfile` says: ```       394    0.004    0.000    0.017    0.000 <frozen importlib._bootstrap_external>:1233(find_spec)       900    0.004    0.000    0.004    0.000 {built-in method posix.stat}         1    0.006    0.006    0.006    0.006 lil.py:23(lil_matrix)     81/24    0.007    0.000    0.011    0.000 sre_compile.py:64(_compile)   402/399    0.011    0.000    0.022    0.000 {built-in method builtins.__build_class__}     212/1    0.023    0.000    0.222    0.222 {built-in method builtins.exec}       190    0.024    0.000    0.024    0.000 {built-in method marshal.loads}     39/37    0.038    0.001    0.043    0.001 {built-in method _imp.create_dynamic} ``` (sorted by second column)   ```         9    0.000    0.000    0.000    0.000 __future__.py:79(__init__)         9    0.000    0.000    0.000    0.000 _globals.py:77(__repr__)         9    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}         9    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}         9    0.000    0.000    0.000    0.000 os.py:742(encode)         9    0.000    0.000    0.001    0.000 abc.py:151(register)         9    0.000    0.000    0.001    0.000 datetime.py:356(__new__)       900    0.001    0.000    0.005    0.000 <frozen importlib._bootstrap_external>:75(_path_stat)       900    0.004    0.000    0.004    0.000 {built-in method posix.stat}       936    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:321(<genexpr>)        96    0.000    0.000    0.000    0.000 enum.py:630(<lambda>)     39/37    0.038    0.001    0.043    0.001 {built-in method _imp.create_dynamic}         1    0.002    0.002    0.002    0.002 __init__.py:259(_reset_cache)         1    0.006    0.006    0.006    0.006 lil.py:23(lil_matrix) ``` (sorted by third column) this is on the 24 core machine __eou__	User

TP	@amueller when I run this code: ``` train_scores, valid_scores = validation_curve(estimator=pipeline,  # estimator (pipeline)                                               X=features,  # features matrix                                               y=target,  # target vector                                              param_name='pca__n_components',                                              param_range=range(1,50),  # test these k-values                                              cv=5,  # 5-fold cross-validation                                              scoring='neg_mean_absolute_error')  # use negative validation ```  directly on the host (with 24 cores) I get ~30 seconds. When I run it directly on localhost (4 cores, 8 threads) I get around 30-40 seconds as well. When I run inside docker with cpu limit of 6 cores and 6GB RAM, it needs almost 10 minutes. Inside a VirtualBox with 2 cores.. around 30 seconds, seems scikit does not play well with docker limitations which uses the CFS Scheduler: [link](https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler) Also found out that if I adjust `param_range` to `range(1,5)`the code runs much faster (I am no data scientist) this saved my life @amueller __eou__	User

TP	It seems `validation_curve` does not really profit from multithreading/multiprocessing. I get almost same results on intel i7 (4 cores) and intel xeon (24 cores). The problem is that if the validation curve runs on the xeon machines.. it uses all cores and the machine is overloaded, which makes no sense, really :) `cv=3` makes it faster as well it reduced my validation curve from 500s to 15 seconds @amueller this is a life saver __eou__	User
TP	what did?__eou__	Agent
TP	https://stackoverflow.com/questions/30791550/limit-number-of-threads-in-numpy__eou__	User
TP	these envs__eou__	User
TP	ah__eou__	Agent
TP	It's good for performance tweaks__eou__	User

TP	How should I install the dependencies for local development of scikit-learn? __eou__	User

TP	@sameshl https://scikit-learn.org/stable/developers/advanced_installation.html#install-bleeding-edge __eou__	User

TP	I'd recommend using conda and doing ``conda install numpy scipy cython matplotlib pytest flake8 sphinx sphinx-gallery`` or something like that __eou__	User

TP	@amueller by the way, numpy and scipy from conda perform somehow faster than from pip but I still haven't found out why @amueller how can I reconfigure numpy and scipy to use max threads e.g. 6? I have no `mkl` (from conda or pip) https://pypi.org/project/mkl/ __eou__	User

TP	@katsar0v thats mkl vs openblas possibly but could also be how they are configured by default i.e. how many threads they use etc pip has no mkl ;) (so far) https://stackoverflow.com/questions/30791550/limit-number-of-threads-in-numpy @katsar0v I don't think that helps given that numpy and scipy will not be linked against it __eou__	User

TP	well in your script n and m are way too small to show anything useful well stackoverflow saved your live *life __eou__	User

TP	How should I build the docs for harversine_distances in my local repo? I ran `python setup.py install` but still I can't find it under `doc/modules/` @lesteve Sure. Thanks for the help. As a beginner contributor to this organisation, the arrangements of the docs did feel a bit tough to navigate. I will put my thoughts about it more concisely and then open a issue and PR for the same __eou__	User

TP	The documentation is another command line ``` cd doc make html ``` should work all OS I think then it will create a `_build/html` folder and you can search for the `index.html` __eou__	User

TP	@sameshl note this part of the contributing scikit-learn doc: https://scikit-learn.org/stable/developers/contributing.html#documentation If you see ways the contributing doc can be improved while you face this "setup" issues, let us know or/and open PRs to improve the contributing docs! __eou__	User

TP	We're working on improving our contributing docs @sameshl, there's some discussion under #14582 __eou__	User

TP	Thats great. Would love to contribute on https://github.com/scikit-learn/scikit-learn/issues/14582 __eou__	User

TP	I am working on https://github.com/scikit-learn/scikit-learn/issues/14575. So I found the corresponding example under `sklearn/metrics/pairwise.py`.  My question is, are the examples run in the doc building process and output is generated or I am supposed to manually write the output of the example in the docstring of a function? __eou__	User
TP	you should write the output in the example. The doc build will run the code and check if the generated output is the same as the one you put there. See https://docs.python.org/3.5/library/doctest.html for more info__eou__	Agent
TP	Thanks @adrinjalali !__eou__	User

TP	Does anyone here knows a good source to learn rnn structure ? Is it like replacing every hidden node with a rnn cell? __eou__	User

TP	I am working on https://github.com/scikit-learn/scikit-learn/issues/14131 . So, I thought that I could append a note in the docstring of `KDTree` regarding the issue. But  I looked into `sklearn/neighbors/kd_tree.pyx ` and it looks like `KDTree` is inheriting its docstring from `BinaryTree`. So can someone tell me an elegant way to append my note docstring to the inherited docstring of `KDTree` or if I could do something else to solve this issue. __eou__	User

TP	Currently working on #14081.  I am supposed to create a pitfalls section which includes practices not to be followed by users. Quite confused about how should I approach it, should I create a whole new section in documentation.html or is there another way to do this?? Thanks for the help!!! __eou__	User

TP	Hey channel, ive being working on vectorizing regression tree with Numpy, and i have achieved some speed up against the cython version of sklearn.  in case anyone is interested, here is the link https://github.com/yupbank/np_decision_tree#regression-with-mae on median data(10000*100), with MAE criteria, achieved 20 times speed up :) lol, you are right, actually with max_depth=10, i only get 5 times faster. __eou__	User

TP	still haven't checked the code in depth. But it's definitely interesting @yupbank . What do you think @NicolasHug ? __eou__	User

TP	i havent clean the code yet, and also working on a blog post explainning what i did, and add some CI to it.  But i would love to have some extra inputs before i proceed, e.g. reviews. __eou__	User
TP	I don't think it'd be easy, but I'd love to see if it actually passes our tree tests, and if it doesn't why not and which tests. Feel free to ping me when you write the blog post.__eou__	Agent
TP	sure.. that would be nice,__eou__	User

TP	@yupbank pretty cool stuff! I took a quick glance at the tree grower and the `greedy_split` function and it looks good as far as I can tell. I wouldn't advertise benchmarks with only `max_depth=1` though ;) Please definitely ping us when you write the blog post!! __eou__	User

TP	@NicolasHug  @adrinjalali  hey.. i have a draft version here.. comments are very welcome :) https://yupbank.github.io/learning/2019/08/08/faster-regression-tree.html __eou__	User

TP	omg omg omg, For L2 loss, if i replace `import numpy as np` with `import cupy as np`, i get another 10x Speed up for 1 split, but i would lost the edge when i have too many depth.. i need to refactor my code... but i really like the fact that, switching to GPU is so trivial ... __eou__	User

TP	+1 __eou__	User

TP	Have a question maybe someone can answer. Trying to use a simple model on a set of data. About a couple thousand rows and only a dozen features, most are binary. I'm training on Logistic Regression, and found my model overfits. So when I try to tune my hyperparameters, my accuracy remains entirely unchanged. Has anyone seen this before or know why this is happening? __eou__	User

TP	Do you have imbalanced classes? __eou__	User

TP	I want to rebuild the 'scikit-learn' project. I tried running `pip install --editable .` as stated in the docs https://scikit-learn.org/stable/developers/advanced_installation.html#building-from-source but I am getting this error. Can someone help me out. ``` ERROR: Cannot uninstall 'scikit-learn'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall. ``` __eou__	User

TP	@sameshl See https://github.com/pypa/pip/issues/5247#issuecomment-381550610 probably best to reinstall in a new virtual environment. __eou__	User

TP	@thomasjpfan you like puzzles, right? https://github.com/scikit-learn/scikit-learn/pull/14704 __eou__	User

TP	I can't seem to get to make the virtual environment with sphinxgallery  conda create -n sklearndev numpy scipy matplotlib pytest sphinx cython ipykernel sphinxgallery  or   conda create -n sklearndev numpy scipy matplotlib pytest sphinx cython ipykernel sphinx-gallery never mind! the solution is in the other gitter chat! __eou__	User

TP	Hi team, I am new to Cpython but really wants to play with the internals of sklearn. I want to test out some of the cdef classes in the pyx file but looks like the methods are inaccessible within Python. Any thought?   For example:  ```python from sklearn.tree import _utils ph = _utils.PriorityHeap(100) dir(ph) ```   And I cannot find call methods like pop, push.  Usually how does the workflow look like if I want to play with the internals of sklearn within Jupyter notebook. __eou__	User

TP	hello everyone. I'm really new to Machine learning in general and i have been working with some sklearn Regressors. I need some help :). My question is how do i know if the RMSE i have is minimum enough for good predictions. To what do i compare this RMSE to? __eou__	User

TP	I was able to create a model by curve fitting a set of data that has 5 variables using GaussianProcessRegressor.    The problem is I am unable to export/load this model into an older version of python (version 2.5.2).  Is there a way to dump the equation/formula into mathematical terms in relations to these 5 variables so that I can use this prediction on the older python?  Thanks __eou__	User

TP	@enoch-sun We don't really support those Python versions anymore. You can try and figure it out with some other persisting models such as ONNX or PMML, but you'll be mostly on your own __eou__	User

TP	@biwa7636 The PriorityHeap functions `pop` and `push` are cdef, which means they are not available in python. __eou__	User

TP	Is there a scikit-learn preferred way to store a vector using Cython?  I've seen libcpp.vector, array.array and numpy used in the code base.  @NicolasHug @amueller __eou__	User

TP	The way we do it now is to allocate numpy arrays (in python or in cython), and then use a memory view for pure cython parts. You can take a look at how we do it in e.g. `ensemble/_hist_gradient_boosting` __eou__	User

TP	Hi, does apply in df.apply(fun) iterate over each columns in 'df' data-frame and pass them to 'fun' function as a series? __eou__	User

TP	@thomasjpfan, you are right, however, I also tried to execute the above code too using `%%cython` magic also from `sklearn.tree cimport _utils` but still did not work. Was it supposed to be like that? ```python %%cython # requires numpy headers from sklearn.tree._utils cimport Stack s = Stack(10) print(s.top) >>> AttributeError: 'sklearn.tree._utils.Stack' object has no attribute 'top' ``` I found the source code so well written, fascinating and really want to be able to get the development environment up and running. __eou__	User

TP	Weird, the above code will work if I replace `s = Stack(10)` with `cdef  Stack s = Stack(10)`, I believe this must have something to do with static type declaration. __eou__	User

TP	Does anyone know why the base estimator for `ExtraTreesClassifier` is `ExtraTreeClassifier`, instead of `DecisionTreeClassifier` with splitter='random'?   I am working on adding a new type of tree. @NicolasHug @amueller __eou__	User

TP	No idea. It doesn't make much sense for `ExtraTreeClassifier` to allow for a splitter that isn't 'random' IMO. Would you want to submit a PR to deprecate the parameter? __eou__	User

TP	Hi All, I`m getting the following error while executing the python setup.py install  error: Command "cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT -IC:\Users\Moti\Anaconda3\envs\motidevs\lib\site-packages\numpy\core\include /EHsc /Tpsklearn\svm\src\libsvm\libsvm_template.cpp /Fobuild\temp.win-amd64-3.7\sklearn\svm\src\libsvm\libsvm_template.obj" failed with exit status 127  Do you have any idea? Thanks! __eou__	User

TP	Any scikit devs who can shed some light on why `calibration_curve` is only for binary estimators? __eou__	User

TP	how can i start committing to the open source __eou__	User

TP	@Anj-ali you can start by going through our contributing guides: https://scikit-learn.org/dev/developers/contributing.html#contributing __eou__	User
TP	thank you Sir, surely i will do that__eou__	Agent

TP	Heads up: if you use conda and upgrade your env, you might get a crash when using `n_jobs>=2`. This is caused by an updated version of intel-openmp in the default channel of conda. I reported the issue upstream as https://github.com/ContinuumIO/anaconda-issues/issues/11294 and the problem is tracked in this PR on the scikit-learn side: https://github.com/scikit-learn/scikit-learn/pull/15020 __eou__	User

TP	The error message is `OMP: Error #13: Assertion failure at z_Linux_util.cpp(2361)` reported by the dying worker process. Which in turns causes loky to raise: `TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6)}`. __eou__	User

TP	If someone is free to review, please take a look at https://github.com/scikit-learn/scikit-learn/pull/14993 and https://github.com/scikit-learn/scikit-learn/pull/15045. __eou__	User

TP	hm is there a pandas gitter? Or is @jorisvandenbossche around lol? For a pandas dtype, how do I get the closest numpy dtype to cast to? indeed it's for https://github.com/scikit-learn/scikit-learn/pull/15094 which is currently failing because np.result_type(pd.CategoricalDType) raises an error __eou__	User
TP	the issue that I rememered is https://github.com/pandas-dev/pandas/issues/22791__eou__	Agent
TP	ok. so no solution :-/ is there a work-around?__eou__	User
TP	like what does actually happen when you do the conversion?__eou__	User
TP	is it from the ``pd.DataFrame.__array__`` method or something?__eou__	User
TP	yeah it is, no way to figure that one out :-/__eou__	User

TP	yep there is pandas gitter actually (pydata/pandas) I don't think there is a typical way to do it If I remember correctly, there is an issue about it Basically, you would like to know the dtype of `np.asarray(obj).dtype` right? (but without needing to do the actual conversion?) __eou__	User

TP	 Hello all (I am new to Cython),  I am currently working on adding an augmented version of Brieman's forest-RC (similar to RandomForest) algorithm into my fork of scikit-learn: In short, the algorithm takes linear combinations of features and projects them with weights randomly selected in {-1,1} to form a new feature to split on. The number of features combined at each split is a random variable.    The current `SplitRecord` only holds one feature, I need something to store a vector of features and a vector to hold weights.   1. I tried initializing an np.ndarray and using memoryviews, but ran into GIL issues. 1. I tried to make an `ObliqueSplitRecord` class, but that can't be passed as a pointer into functions because it is a Python object. 1. I tried to augment the `SplitRecord` struct in [_splitter.pxd](https://github.com/scikit-learn/scikit-learn/blob/a47e914163c2dbecb4a80ec40d2d8fe313a83010/sklearn/tree/_splitter.pxd#L23-L32) but that didn't seem to work because vectors would then be of fixed length. 1. I tried to use something similar to the [`tree/_utils:Stack`](https://github.com/scikit-learn/scikit-learn/blob/3046990e76c7c90a1150c26770572c8d76ee00de/sklearn/tree/_utils.pyx#L81-L157) but fell into the same problem as it was a class and couldn't be passed as a pointer into a function.  I am looking into using cppclass, but am not sure if that will fix solve my problem.   Does anyone have suggestions on how to best implement this in a Cythonic way?  i.e. storing a vector of things while avoiding the GIL and not using python objects? __eou__	User

TP	@MrAE you can use a cpp vector in cython. But since you're changing the splitrecord struct, you'll need to change the code in quite a lot of places. __eou__	User

TP	Hi, I have some basic question about local docs build for scikit. I've been trying to modify docs inside API for some file in `sklearn/linear_model` and followed instructions in Contributors Guide. But after few attempts the `make` command inside `/docs` does not seem to modify local docs build inside `_build`. In the browser,  API docs didn't change although I modified the sources. Am I missing something? __eou__	User

TP	@mtsokol it seems that you're doing it right... maybe double check that 1. you're actually changing the sources, i.e. not anything in the _build folder, 2. the doc that you're changing is about a public estimators/tools (private tools aren't rendered in the doc anyway) and 3. that you're looking at the generated html in `doc/_build/html/stable/` __eou__	User

TP	@MrAE   re 1. you can't use (let alone allocate) numpy arrays when the GIL is released because these are Python objects. Is there a way for you to allocate the arrays somewhere where the GIL is held, and use memory views when the GIL is released? Memory views are safe to use without the GIL  re 2. is it still considered a Python object if you use a `cdef`ed class and all the attributes are `cdef`ed as well?  re 3. what vectors? can't you use a view as a field of the struct? Also @MrAE  I happen to have been writing about Cython over the weekend... maybe that could help http://nicolas-hug.com/blog/cython_notes __eou__	User

TP	could somebody share a good example for class docstrings in scikit-learn that we could use as a sort of template? thanks! __eou__	User

TP	@janjagusch https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/compose/_column_transformer.py#L37 ? __eou__	User

TP	Here is the issue search string "is:issue is:open examples class docs involves:adrinjalali" https://github.com/scikit-learn/scikit-learn/issues/3846 __eou__	User

TP	Hey guys who is veerlosar on Githib? just want to talk about OneVsRestClassifier example https://github.com/scikit-learn/scikit-learn/pull/15200/ __eou__	User

TP	@zioalex I can talk to veerlosar, we're at the same sprint __eou__	User

TP	> Hey guys who is veerlosar on Githib? just want to talk about OneVsRestClassifier example  @zioalex  what did you want to talk about? __eou__	User

TP	how to learn complete sk learn ? please give the resources? __eou__	User

TP	Andreas Muller's book, Introduction to Machine Learning with Python: A Guide for Data Scientists, is quite complete. You can also look at the user guides: https://scikit-learn.org/stable/user_guide.html __eou__	User

TP	there's also my lecture series: https://youtube.com/AndreasMueller   The only complete resource is the user guide though __eou__	User

TP	Hello there! __eou__	User

TP	Hey guys, me again: Regarding me previous message :point_up: [October 4, 2019 5:28 PM](https://gitter.im/scikit-learn/scikit-learn?at=5d97b97b0e67130aae15b693) I've gone through some more attempts that don't quite work.    @NicolasHug The blog post helped a bit with my understanding of memory-views, however I still have a few questions:  Can a memory-view be initialized `with nogil`? And no, a struct member cannot be a memory view.  I tried to make my own class but then got yelled at because it's not of type `Splitter`, so that was a bust.   I augmented the `SplitRecord` with 2 cpp vectors, but that caused things to go wonky requiring cpp in files that I'm not willing to touch.   I ended up augmenting `SplitRecord` with 2 Cython vectors with hard-coded length, but then can't seem to initialize a memory-view into them inside of the `node_split`.  I'm pretty much stuck (in my current view of things), because I'm trying to do as little modification as possible, but it seems that in order to accomplish my task I'll have to re-write a big chunk of ensemble methods. I'd have to add an input argument to the `node_split` method? That doesn't sound like a good idea.  Any ideas?  Much appreciated. __eou__	User

TP	Hi all, I'm trying to help my team reduce creating new code when leveraging existing libraries might get the job done. Does anyone have thoughts on how the following can be accomplished? https://stackoverflow.com/q/58533004/1566074  Basically finding the optimal subgroups for a dataset to then feed into an estimator to reduce noise. __eou__	User

TP	Hello the scikit-learn community! I'd like to have your thoughts on what I coded. It's a way to do automatic machine learning on scikit-learn pipelines. It allows for handling hyperparameter spaces as well as hyperparameters. Example: https://www.neuraxio.com/en/neuraxle/stable/examples/hyperparams.html#sphx-glr-examples-hyperparams-py __eou__	User

TP	any takers on https://github.com/scikit-learn-contrib/imbalanced-learn/issues/616? it's a good first issue. __eou__	User

TP	a first good issue? a find it a bit harsh :) __eou__	User

TP	lol, I'm just a messenger, Joel tagged it as such :D __eou__	User
TP	Basically, I was starting to solve the issue yesterday__eou__	Agent
TP	While making `master` work with `master` is easy (just change the import path), the challenging part is to make work out-of-date version with a newer scikit-learn.__eou__	Agent
TP	In the latest case, we need to make some `try except ImportError` as you suggested I think__eou__	Agent
TP	Yep. If you're already at it, please leave a comment so that others don't start working on it ;)__eou__	User

TP	Yep I just cross-reference my PR __eou__	User

TP	For the people joining the MAN-AHL sprint, you can find the instructions to install scikit-learn from source at the following documentation page: https://scikit-learn.org/dev/developers/advanced_installation.html#building-from-source In addition, you can find the contributing guide as the following address: https://scikit-learn.org/dev/developers/contributing.html Finally, if you are searching for an issue to work on, several issues have been tagged specifically for sprints: https://github.com/scikit-learn/scikit-learn/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3ASprint You can set some other tags if you want ("good first issues", etc.). You also free to search any issue that you are interested in on the issue tracker. :) __eou__	User

TP	One example of a "good first issue", particularly if you have never contributed to large open-source projects before is  https://github.com/scikit-learn/scikit-learn/issues/15440 aiming to improve docstrings. That would allow you to see how the contribution workflow works before tackling more complex issues. Sure, please comment about it in the issue __eou__	User

TP	Also it's useful to read the contribution guide at https://scikit-learn.org/dev/developers/contributing.html __eou__	User

TP	Hi @rth I just tried to run `test_docstrings` and looks like just 13 out of 1619 tests pass. I suppose I can pick any estimator to start to improve docstrings, am I right? is there any scale of priorities? __eou__	User
TP	@gbroccolo Yes, you can pick any estimator that fails :)__eou__	Agent
TP	thanks__eou__	User

TP	Hi @rth I'd like to pick RadiusNeighborsClassifier __eou__	User

TP	Would like to take care of one of the PR that has been labeled as `stalled` and `help wanted`. I suppose a new one can pick this and conclude the PR also taking into account the comments of the reviewers. What's the best here? Create a new PR that refers to the already existing one? __eou__	User

TP	Trying to take a look at https://github.com/scikit-learn/scikit-learn/issues/13045 -- Does this seem like a decent issue to tackle? I'm at the man hackathon __eou__	User
TP	ah :)__eou__	Agent

TP	@norvan sure please comment there. Are you part of the wimlds sprint? I didn't realize there were two today lol __eou__	User

TP	hi there any clue for identify text from one document to another? we are working on a prototype for fake news __eou__	User

TP	hi anyone there to help __eou__	User

TP	Hi @qazi1002 @eliseo looks like you need some NLP for this project...can you provide more info about which kind of help do you need? Are you meant to use specifically scikit-learn for this? Also, not sure that this is the proper place where to talk about this - topics should be strictly focused on scikit-learn development/bug fixing/etc. __eou__	User

TP	I don't think we have a strict policy for this channel being related to the dev only. But in the interest of the rest of the community being able to use the answers we give to your questions related to the usage, posting them on stackoverflow or other related forums may be more appropriate. __eou__	User

TP	@gbroccolo  I need help regarding software development...as i am beginner so I want to get some tips for developing softwares... I want to develop software that reads the smart ID cards using card reader. __eou__	User

TP	Hi there, is k-means clustering stochastic even when the initial centers are given? I'm noticing different results when I run my code multiple times __eou__	User

TP	@h4k1m0u I don't think it should be __eou__	User

TP	@amueller That's weird, I can't find out why in this short piece of code (https://bpaste.net/show/ORPVW) the centroids found by kmeans are sometimes located at the center of the 3 samples and sometimes not Oh sorry, I've completely forgotten the np.random above. Thanks a lot for reminding about that. __eou__	User

TP	you don't fix the random seed so the data changes __eou__	User

TP	 When using "random forest" and "gradient boosting". I add to the main signs, a sign that in the picture. [title](https://ibb.co/997YpMK) The data is clearly not stationary. To make the series stationary, I apply a one-time difference to the data(increments) After all, I normalize the data. [title](https://ibb.co/6tbdVzJ) Why, if I don't use increments(one-time difference), then classes are separated better? [title](https://ibb.co/dBJrnDf) Although in all textbooks they write that non-stationary data should be decomposed into increments. For training, I use the first 5000 characters. If you pay attention to the data, extreme values start after 5000. That is, the model does not even see that such large values were in the training sample. __eou__	User

TP	I have two files that contain Event Name, Event City, Event Venue, Event State but in both files it's written in different ways or you can assume both the files are from different source.  I want to create a Machine learning-based algorithm that can do the matching.  I have tried with fuzzy-wuzzy to get string similarity. Can anyone please tell me if I want to solve this with Deep Learning what would be the approach. Thanks @amueller __eou__	User

TP	Hi, what does it mean when `linear_model.Ridge`returns `n_iter_` = None? does it mean it didn't even perform one single iteration? __eou__	User

TP	@h4k1m0u the doc says     n_iter_ : None or array of shape (n_targets,)         Actual number of iterations for each target. Available only for         sag and lsqr solvers. Other solvers will return None.  you're probably not using sag or lsqr? __eou__	User

TP	Thanks @NicolasHug , you  were right I was actually not even setting that parameter (`solver='auto'`). With solver=sag, it returns the # of iterations __eou__	User

TP	Hi, semi random question but I can't find it in the docs - do y'all implement a consensus clustering evaluator that's not the bicluster one? __eou__	User

TP	Hi, I am the "maintainer" (more like caretaker) of hmmlearn (which was split out of sklearn a couple of years ago); I tried moving the CI to azure and realized that the macOS tests were failing (previously testing was only done on linux (travis) and windows (appveyor)) but can't test locally on macOS, would anyone be willing to have a look? https://dev.azure.com/anntzer/hmmlearn/_build/results?buildId=170  Thanks! __eou__	User

TP	@anntzer , running the test locally on my linux I get a bunch of  zero division warnings on the failing test, so you might be able to debug locally still __eou__	User

TP	I get a single warning running tests locally but they still pass... __eou__	User

TP	yeah they pass but they probably should not. Unless you do expect to get a zero division in the test, in which case you need to protect the call. the macOS CIs probably use different versions of numpy or scipy so that's why they fail while the others don't __eou__	User

TP	I'll look into it but it would be strange that different versions of numpy are being used __eou__	User

TP	wait, are you really getting zero division warnings from TestGMMHMMWithTiedCovars::test_fit_zero_variance (which is the failing test on osx)?  I get only get warnings on TestGMMHMMWithDiagCovars::test_fit_zero_variance (another test) __eou__	User

TP	Dear, I i tried to tune hyperparameters of scikit GradientBoostingRegressor model using the Hyperopt optimizer. I set search space for learning_rate parameter in the range [0.01, 1] by many ways (for example : ""'learning_rate': hp.quniform('learning_rate', 0.01, 1, 0.05)"" or as simple array ""[0.01, 0.02, 0.03, 0.1]"") but when I run the code hyperopt start to calculation and I get the error " ValueError: learning_rate must be greater than 0 but was 0".  I do not know what is problem in the code because zero value is not in the parameter's scope. How zero value come to function?  Please help me to solve this problem. __eou__	User

TP	This looks like a bug in hyperopt, no? Can you add print statements (or debugger breakpoint) in the hyperopt and scikit-learn code to check where this zero comes from? Actually hp.quniform is for rounding to integer values. You probably want `hp.loguniform(-3, 0)` or someting similar. __eou__	User

TP	@anntzer I get an underflow for `TestGMMHMMWithTiedCovars::test_fit_sparse_data` and indeed most of the zero div warnings come from `TestGMMHMMWithDiagCovars`, not the tied version  Maybe addressing the existing warnings would fix the one that's failing? __eou__	User

TP	that's interesting, I don't get any warning with Tied::test_fit_sparse_data and you don't see it on Azure either; what's your numpy/scipy/anythingelse relevant version? __eou__	User

TP	``` System:     python: 3.7.4 (default, Oct  4 2019, 06:57:26)  [GCC 9.2.0] executable: /home/nico/.virtualenvs/sklearn/bin/python    machine: Linux-5.3.1-arch1-1-ARCH-x86_64-with-arch  Python dependencies:        pip: 19.0.3 setuptools: 40.8.0    sklearn: 0.23.dev0      numpy: 1.17.1      scipy: 1.3.0     Cython: 0.29.10     pandas: 0.24.2 matplotlib: 3.0.0     joblib: 0.13.2  Built with OpenMP: True  ``` Here's my pytest output. I locally installed the master branch of hmmlearn  ``` lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithSphericalCovars::test_fit_zero_variance lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithTiedCovars::test_fit_sparse_data   /home/nico/dev/hmmlearn/lib/hmmlearn/hmm.py:849: RuntimeWarning: underflow encountered in multiply     post_comp_mix = post_comp[:, :, np.newaxis] * post_mix  lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithDiagCovars::test_fit_zero_variance   /home/nico/dev/hmmlearn/lib/hmmlearn/stats.py:47: RuntimeWarning: divide by zero encountered in log     + np.dot(X ** 2, (1.0 / covars).T))  lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithDiagCovars::test_fit_zero_variance   /home/nico/dev/hmmlearn/lib/hmmlearn/stats.py:47: RuntimeWarning: divide by zero encountered in true_divide     + np.dot(X ** 2, (1.0 / covars).T))  lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithDiagCovars::test_fit_zero_variance   /home/nico/dev/hmmlearn/lib/hmmlearn/stats.py:47: RuntimeWarning: invalid value encountered in add     + np.dot(X ** 2, (1.0 / covars).T))  -- Docs: https://docs.pytest.org/en/latest/warnings.html  Results (20.38s):       93 passed        3 xpassed       15 xfailed ``` __eou__	User

TP	that's... curiouser and curiouser.  I don't get the warnings with the exact same versions of everything (AFAICT, except that cpython is from conda), whether with pip-installed numpy and scipy or conda-forge ones. __eou__	User

TP	Hi, does anyone know about any plans that might exist involving the release cycle or timeline in moving  IterativeImpute package out of it's experimental version? Thanks, I'm hoping to use it and it looks great for my use case! __eou__	User

TP	[![Screenshot 2019-12-28 14.29.36.png](https://files.gitter.im/scikit-learn/scikit-learn/Xmzs/thumb/Screenshot-2019-12-28-14.29.36.png)](https://files.gitter.im/scikit-learn/scikit-learn/Xmzs/Screenshot-2019-12-28-14.29.36.png) Hi, I want to apply Multinomial Logistic Regression to compute winning probabilities for each contestant in my races. The Data I want to feed in my model look like the image above. I'm tring to understand how should I feed the target class to my model because every race can have a different number of runners, the target class for race A has 5 contestants, instead target class for race B has just 4 contestants.  Is there a way to model this using scikit-learn? __eou__	User

TP	@guptane6 we hope to fix some issues by the next release. But no guarantees __eou__	User

TP	Worth reading: https://www.neuraxio.com/en/blog/scikit-learn/2020/01/03/what-is-wrong-with-scikit-learn.html __eou__	User

TP	I would probably found cool to have an entitled the blog post with something "Limitations and Caveats ..." instead of "What's wrong ...". This said I think that there are some criticisms that should be discussed by opening issues to come up with adequate solutions. __eou__	User

TP	ugh they credit me as the creator of sklearn __eou__	User

TP	haha, yeah I saw :D THE creator :P to be fair, you're the sole maintainer contact on pypi (IIRC) __eou__	User
TP	that means nothing lol__eou__	Agent

TP	@amueller Thanks for the feedback haha! I'll edit the post soon to correct what you just pointed out. I sincerely thought you were the main creator of sklearn, as you are the top contributor, and also that you are very very involved. I'd love to know if there is anything I could do to help, or if you have any idea of things you'd like to see in Neuraxle to help with making sklearn more integrated in Deep Learning projects.   For instance, I think the following code snippet is really talkative as a way to do Deep Learning pipelines using the pipe and filter design pattern: https://www.neuraxle.org/stable/Neuraxle/README.html#deep-learning-pipelines  Would you have any ideas to share, or things you'd like to point out for me to work on next with Neuraxle? @glemaitre "Limitations and Caveats ..." sounds cool! I could rename the article. I wanted it to catch the eye, seems like it worked hehe. I love sklearn tho :)   On my side, I've already fixed 95% of the issues I listed, in Neuraxle (as per the links to the Neuraxle website documentation for each problem listed). __eou__	User

TP	Regarding deep learning pipeline, I think that we want to be conservative: https://scikit-learn.org/stable/faq.html#why-is-there-no-support-for-deep-or-reinforcement-learning-will-there-be-support-for-deep-or-reinforcement-learning-in-scikit-learn Issues regarding serialization and hyperparameter search could be discussed, however. I think that onnx-sklearn provide a nice way to deployed scikit-learn model in production __eou__	User

TP	yeah, that's the goal (onnx-sklearn), but it still needs a bit of work. I'm all in favor of focusing a bit on partial_fit (mini batches) though. __eou__	User
TP	but this is rather challenging to retrain models and update models across versions. This might not be in the scope of scikit-learn but having a third-library to manage those could be nice__eou__	Agent
TP	@adrinjalali Incremental learning, early stopping, and callbacks are things which would be nice__eou__	Agent
TP	they are in the roadmap I think__eou__	Agent
TP	yeah they are, they're just hard :P__eou__	User
TP	:) yes indeed__eou__	Agent

TP	Nice to confirm that you scope scikit-learn like that. I feared I'd play a bit too much in your backyard but it seems fine, I'm glad you have this opinion. I'm totally down to make Neuraxle a way to handle all those callbacks and things required for doing deep learning, + serialization. I don't know about Onyx, but there could be a way that I adapt to that to save every neural net usign that instead of building custom savers. For now I'm doing 2 other libraries already: Neuraxle-TensorFlow and Neuraxle-PyTorch to provide default neural net savers to allow serialization and checkpointing and have those models have their special callbacks. Might also do Neuraxle-Keras and so forth. __eou__	User
TP	you also have keras-onnx__eou__	Agent
TP	and pytorch-onnx__eou__	Agent
TP	which manage the same way than sklearn-onnx__eou__	Agent
TP	but this is for prediction only__eou__	Agent
TP	I'll need to look into that. For now, with Neuraxle, someone could do this using 3 tf functions that builds tf graphs:  ``` model = TensorflowV2ModelStep(     create_model, create_loss, create_optimizer,     has_expected_outputs=False ).set_hyperparams(hp).set_hyperparams_space(hps) ``` And I have savers that allows for saving and reloading and continue a fit (already!)__eou__	User
TP	Same API would work for TF v1 using a TensorflowV1ModelStep instead, also PyTorch (using some `nn.Module`s), and eventually Keras in some ways__eou__	User
TP	I also have a `ParallelTransform` class which uses the savers for parallelizing instead of using joblib. So all the pytorch, tf, and keras code is parallelizeable. I also am building right now a `ClusteringWrapper` which acts like the ParallelTransform using savers, but sends the saved wrapped pipeline over a worker that has a REST API. So the Clustering Wrapper can split a batch of data to N workers, by first sending the model, and then sending the data it splitted in parallel.__eou__	User
TP	The same concept applies to a new `StreamingPipeline` class I'm creating right now :D it has the ability to have some steps (e.g.: sub-pipelines) run in different threads, and to have queues between each thread like a consumer-producer design pattern. I also already have a [MiniBatchSequentialPipeline](https://www.neuraxle.org/stable/api/neuraxle.pipeline.html#neuraxle.pipeline.MiniBatchSequentialPipeline) that just like a single-threaded [Pipeline](https://www.neuraxle.org/stable/api/neuraxle.pipeline.html#neuraxle.pipeline.Pipeline), but that already uses mini-batches, meaning that it splits the batches into mini batches, and it's just like having a normal Pipeline but calling `.fit` many times in a row (sorry, I didn't name it `partial_fit`, my `fit` is already thought of as potentially always a partial one.__eou__	User
TP	@glemaitre You said:  > @adrinjalali Incremental learning, early stopping, and callbacks are things which would be nice  If you look closely [here](https://www.neuraxle.org/stable/Neuraxle/README.html#deep-learning-pipelines), I already have incremental learning (e.g.: if you CTRL+F for the `MiniBatchSequentialPipeline `). I'd love to add early stopping and other callbacks soon, good idea. I opened an issue [here](https://github.com/Neuraxio/Neuraxle/issues/228) for such things, I'd add callbacks to it!__eou__	User
TP	So in the issue [#228](https://github.com/Neuraxio/Neuraxle/issues/228) I just linked to, there is some example API code, but it might not be enough. I'd like to really discover the good design patterns for that, although I at least found something that seems like it would work properly.__eou__	User

TP	They are for serialization and deployment, though. I think @gulliaume-chevalier wants training as well lol ok you beat me to it ;) __eou__	User

TP	Does HistGradientBoostingRegressor have an equivalent of subsample and max_features in GradientBoostingRegressor? I have a GradientBoostingRegressor model with tuned hyperparameters and I want to see if HistGradientBoostingRegressor is better __eou__	User

TP	@DrEhrfurchtgebietend not right now. Do you want to open an issue as a feature request? I would recommend first comparing with the default parameters of HistGradientBoosting __eou__	User
TP	OK, I can try with the defaults. I will open a request if I remember__eou__	Agent
TP	There also is no min_samples_split__eou__	Agent

TP	no, but there is min_samples_leaf (which is not the same but similar) __eou__	User

TP	@amueller Yea, that is likely good enough. While I have you. I did send you and Gael an email about entity embedding as we discussed at NeurIPS __eou__	User
TP	oh that was you! sorry I didn't connect your handle to you in-person. I've been in grant-writing mode and just procrastinating ;)__eou__	Agent
TP	Yup, I tend to not use real name on the internet. Anyway, as I say in the email. It might be better as a scikit learn add on. A separate but dependant package. There are a number of them.__eou__	User

TP	I'm not sure when I'll have time to look at it tbh, in particular because it's unlikely we can directly integrate with sklearn. I didn't forget about it but my list of todos is pretty long... maybe I should add it to my class hand have my students do it ;) __eou__	User

TP	@DrEhrfurchtgebietend Are you thinking of using entity embedding for transfer learning? __eou__	User

TP	No, categorical encoding. If you have many level it is great. I saw like a 5% RMSE improvement __eou__	User

TP	For your use case, how did you learn the entity embeddings? __eou__	User

TP	yea, this is a must otherwise they are not tied to your target here is the proof it works https://github.com/entron/entity-embedding-rossmann and the paper https://arxiv.org/abs/1604.06737 __eou__	User

TP	@amueller so I am unable to run HistGradientBoostingRegressor because you use np.isnan '''  File "C:\Anaconda3\lib\site-packages\sklearn\ensemble\_hist_gradient_boosting\gradient_boosting.py", line 151, in fit     has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)''' There is a "bug" if you have mixed types. Which is pretty common.   https://stackoverflow.com/questions/59637976/potential-bug-in-np-isnan-for-mixed-types-on-pandas-dataframe Normally Pandas passes through with numpy functions fine so you do not have a dependence technically. It is pretty common in practice to use pandas in this way as it makes it more simple to keep your features organized clf.fit(X_train.values , y_train.values) works but clf.fit(X_train, y_train) does not. The issue seems to be that I am passing a dataframe I can just do clf.fit(X_train.values , y_train.values) __eou__	User

TP	Maybe switch to pd.isnull or do u npt want pandas dependance? __eou__	User

TP	@DrEhrfurchtgebietend we don't want dependence on pandas. However, I would have expected to have `X_train` to be a NumPy array at that stage. Uhm we used a `check_X_y` earlier in `fit`. Could you open a bug report with a minimal example. nop But a minimal example will help :P normally yest __eou__	User

TP	but `pd.isnull` will require an import of pandas? __eou__	User
TP	That is only one possible solution.__eou__	Agent
TP	Do you have a minimal code example because I am unsure of what is your input__eou__	User
TP	because the HistGradientBoostingClassifier will not support mixed type__eou__	User
TP	This is where I am confused because the array should be converted into float 64 when passing in `check_X_y`. So I would really like a minimal code example__eou__	User
TP	it should failed before the line that you pointed out which is interesting__eou__	User
TP	Basically this line should do the conversion: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py#L104__eou__	User

TP	OK, that is the crux of the issue. If the algorithm does not support mixed types then I will just cast it beforehand. Is there a preferred type? GradientBoostingRegressor supports mixed types Well GradientBoostingRegressor it runs with a mix of float64, int64, bool in a dataframe I am trying to upgrade to HistGradientBoostingRegressor and hit this error __eou__	User
TP	internally they will all be converted to float64__eou__	Agent
TP	all of them__eou__	Agent
TP	OK perhaps that conversion needs to be done in HistGradientBoostingRegressor before  np.isnan is called__eou__	User
TP	it is done in `check_X_y` which is called before__eou__	Agent
TP	but apparently it is not working as expected__eou__	Agent
TP	yea, it would seem that way__eou__	User
TP	I do not have one since I am just swapping out GradientBoostingRegressor for HistGradientBoostingRegressor in a huge package.__eou__	User
TP	Ill play around a bit and get back to you__eou__	User

TP	X_train.values will not give a numpy array necessarly oh sorry this is the way it works. so it should give you a numpy array you can always check by hand ```python X, y = check_X_y(X_train, y_train, dtype=[np.float64], force_all_finite=False) ``` to check what X is looking like and the same by passing `X_train.values` to spot difference __eou__	User

TP	yes there is a difference. For example with a date originally 2011.  ```sklearn.utils.check_X_y(X_train, y_train, dtype=[np.float64], force_all_finite=False)``` gives the original 2011 but  ```sklearn.utils.check_X_y(X_train.values, y_train.values, dtype=[np.float64], force_all_finite=False)``` gives 2.011e+03 which I presume is how it would show it as a float dtype('O') yes in the second dtype('float64') I have a fix with X_train.values so I am good but I would think this issue will come up a lot as HistGradientBoostingRegressor becomes popular Is this then a bug in sklearn.utils.check_X_y. Was it designed to handle being passed dataframes? __eou__	User

TP	`X.dtype` is `object` in the first case? Uhm this is really weird __eou__	User

TP	dtype=[np.float64] should force the conversion __eou__	User

TP	this would be in `check_array` in `sklearn.utils.validation.py` might be a side effect of this https://github.com/scikit-learn/scikit-learn/pull/15797/files ups we might have forgot to backport the fix in 0.22.1 uhm no it is fine can you check the version of scikit-learn are you using `0.22.0` because we corrected the bug in `0.22.1` __eou__	User

TP	yup hold on 0.22 0.22.1 not ready in conda yet This issue only happens if X_train has a float in it already I have a minimum example. __eou__	User

TP	```  import pandas as pd import sklearn import numpy as np  raw_data = {'Binary 1': [True, True, False, False, True],      'Binary 2': [False, False, True, True, False],      'age': [42, 52, 36, 24, 73],      'preTestScore': [4.4, 24.1, 31.3, 2.2, 3.1],     'postTestScore': [25.7, 94.5, 57.0, 62.2, 70.9]} df = pd.DataFrame(raw_data, columns = ['Binary 1', 'Binary 2', 'age', 'preTestScore', 'postTestScore'])    X_train = df[['Binary 1', 'Binary 2', 'age', 'preTestScore']]   y_train = df['postTestScore']  print(X_train.dtypes)  X, y = sklearn.utils.check_X_y(X_train, y_train, dtype=[np.float64], force_all_finite=False)  print(X.dtype)  X, y = sklearn.utils.check_X_y(X_train.values, y_train.values, dtype=[np.float64], force_all_finite=False)  print(X.dtype)   X_train = df[['Binary 1', 'Binary 2', 'age']]   y_train = df['postTestScore']  X, y = sklearn.utils.check_X_y(X_train, y_train, dtype=[np.float64], force_all_finite=False)  print(X.dtype)  X, y = sklearn.utils.check_X_y(X_train.values, y_train.values, dtype=[np.float64], force_all_finite=False)  print(X.dtype) ``` Sorry the markdown is not working as I would expect. Does this work for you? __eou__	User

TP	Jump a line after the 3 quotes you can install from conda-forge we upload the packages yesterday or via PyPI yes it was the bug __eou__	User

TP	so in 0.22.1 the last 4 print statements all give float64? __eou__	User
TP	let me try but it should__eou__	Agent
TP	OK sure. Thanks so much for the real time tech support. This is some high quality service__eou__	User
TP	Awesome. I will update.__eou__	User

TP	``` Binary 1           bool Binary 2           bool age               int64 preTestScore    float64 dtype: object float64 float64 float64 float64 ``` __eou__	User

TP	I cannot update ``` conda install scikit-learn=0.22.1 ```  does not work __eou__	User

TP	`conda install scikit-learn -c conda-forge` the package are only upload to conda-forge conda is managing directly the default channel and it can take a bit more time __eou__	User

TP	**Curious To Learn & Contribute To Scikit-learn At The 'Paris Scikit-learn Sprint Of The Decade' | Jan 28 - 31, 2020**  It was quite insightful listening to Reshama Shaikh's recent podcast: https://www.listennotes.com/podcasts/the-banana-data/bdn-15-finding-community-in-uK-yL2tf_S4/  It was quite helpful to broaden my horizon and perspective on open-source when I learned the challenges the organization faces in finding the sponsors and fundraisers for scikit-learn sprint events.  As an avid user of scikit-learn for my research projects in the recent past, Im excited about the potential of contributing and working alongside other attendees at the Paris scikit-learn sprint. Reshama's comments about funding & accessibility have made me even more eager to join the team.  Would you all mind letting me know if I could connect with the other participants remotely from Bengaluru, India?  Best, Sandeep Aswathnarayana __eou__	User

TP	@SandeepAswathnarayana, sprints are meant to allow people to meet in person, remote participation is not planned. There will be other sprints I'm sure you will be able to attend. In the meanwhile, thanks for your enthusiasm... if you check the contributors guidelines (https://scikit-learn.org/stable/developers/contributing.html) you could probably start helping already. __eou__	User

TP	@cmarmo, Thanks for reverting to my query. I was aware of the already existing ways to contribute. I was only curious to see if I could be a part of the scikit-learn sprint which allows me to do 'Pair Programming' with individuals from diverse backgrounds attending the event. __eou__	User

TP	@cmarmo, Any leads or inputs on future possibilities for remote participation are greatly appreciated. Thank you! __eou__	User

TP	@SandeepAswathnarayana  > Any leads or inputs on future possibilities for remote participation are greatly appreciated. Thank you!  noted: indeed, there is always room for improvemnts. __eou__	User

TP	hey folks. I have started my Data Science journey. In the process of completing the DataQuest online Data Science bootcamp . Is SciKit Learn & specifically Auto Sklearn a good set of tools to learn to help accelerate my journey and on the way to becoming an expert? __eou__	User

TP	@ScottHameed_twitter I know it's not a dev/Git related question, but appreciate the help __eou__	User

TP	@ScottHameed_twitter It's a necessary library used in machine learning. Learn it __eou__	User

TP	Hey, I opened the PR to add Neuraxle to the Related Projects page:  https://github.com/scikit-learn/scikit-learn/pull/16100  I've put it under the category for `Auto-ML` as it seems better suited here. I'm still developing the serialization plugin/extra libraries for TensorFlow and PyTorch as of right now, so those plugins could go into the `Model export for production` category later on (to allow saving / reloading / then continue training / partial_fit whenever after). I also corrected the thing about "the creator of scikit-learn" in my article :) srry again for the mistake haha __eou__	User

TP	I tried building scikit from source, its giving me import error for conftest.py ``` pytest sklearn/metrics/_classification.py  ImportError while loading conftest '/media/sid21g/Dev/github-dev/scikit-learn/conftest.py'. conftest.py:15: in <module>     from sklearn import set_config sklearn/__init__.py:81: in <module>     from . import __check_build  # noqa: F401 sklearn/__check_build/__init__.py:46: in <module>     raise_build_error(e) sklearn/__check_build/__init__.py:41: in raise_build_error     %s""" % (e, local_dir, ''.join(dir_content).strip(), msg)) E   ImportError: No module named 'sklearn.__check_build._check_build' E   ___________________________________________________________________________ E   Contents of /media/sid21g/Dev/github-dev/scikit-learn/sklearn/__check_build: E   setup.py                  _check_build.c            _check_build.pyx E   __init__.py               __pycache__ E   ___________________________________________________________________________ E   It seems that scikit-learn has not been built correctly. E E   If you have installed scikit-learn from source, please do not forget E   to build the package before using it: run `python setup.py install` or E   `make` in the source directory. E E   If you have used an installer, please check that it is suited for your E   Python version, your operating system and your platform. ``` __eou__	User

TP	@sid21g  try maybe `make clean` and start over following the build guidelines __eou__	User

TP	In order not to be verbose, I place a link to the question [stackexchange](https://ai.stackexchange.com/questions/17334/interpretation-of-feature-selection-based-on-the-model) no one answered me There, but there is a desire to understand. I apologize in advance for my poor English). __eou__	User

TP	@quant12345 as already replied to your post, have a read to this: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py  In RF, feature importance is affected by features able to overfit the model. __eou__	User

TP	@gbroccolo Thanks! __eou__	User

TP	Why does `GradientBoostingClassifier` use `DecisionTreeRegressor` instead of `DecisionTreeClassifier`? __eou__	User

TP	@NicolasHug Worked! __eou__	User

TP	@jacobcvt12 because gradient boosting tries to predict gradients which are always continuous targets, even in the case of classification. With a log loss (as used in sklearn) these gradients are homogeneous to a log-odds ratio, and are then passed through a sigmoid function to become a probability between [0, 1] self plug: http://nicolas-hug.com/blog/around_gradient_boosting __eou__	User

TP	Hello, I would be very grateful for help or hint. What I would like to do is to somehow find pattern in text. Lets say you have forum and people or posting on it. I would like to find pattern which would indicate me what are they talking about the most. Thank you for hint. __eou__	User

TP	Try TF-IDF __eou__	User

TP	Hello. How can I make the program better predict? When you enter the numbers 771, 322, 344, 632, 10, the program predicts 234168, but I need it to be 200000-210000. In linear regression, more than 1000 examples are already embedded. __eou__	User

TP	Thanks @NicolasHug . I'm trying to figure out why sklearn's GradientBoostingClassifier gives different estimates from R's GBM. I had thought it might be the criterion for splitting, but maybe not. Any suggestions? __eou__	User

TP	@jacobcvt12 I'm not familiar with R's gbm. The splitting criterion will definitely be a major factor. I'd suggest checking the parameters of each implementation and try to find equivalent settings. In a vanilla implementation of gradient boosting (ignoring the sub-estimator which is a tree in our case), the only parameters are the learning rate / shrinkage, the loss, and the number of iterations. __eou__	User

TP	Hi __eou__	User

TP	@jacobcvt12 gbm supports categorical variables, I think __eou__	User

TP	[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/6t1w/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/6t1w/image.png) __eou__	User

TP	Hello, I'm new to using Pipelines and getting the above error 'Last step of Pipeline should implement fit or be the string 'passthrough'. I can't figure out how to overcome this step. Please assist if you can. __eou__	User

TP	@wbadiah_gitlab you're passing a list, instead just pass the estimators directly as in the example: `make_pipeline(StandardScaler(), GaussianNB(priors=None), ...)` ` i.e. remove the brackets. you can also remove the redundant parenthesis around the estimators __eou__	User

TP	Thanks @NicolasHug It worked! __eou__	User

TP	This PR by me is also with the Paris Sprint https://github.com/scikit-learn/scikit-learn/pull/16256 __eou__	User

TP	I'll start by completing https://github.com/scikit-learn/scikit-learn/pull/11296/files at Paris Sprint __eou__	User

TP	I am working on this Issue https://github.com/scikit-learn/scikit-learn/issues/12542 as part of the Paris Sprint __eou__	User

TP	I'm working on the issue scikit-learn/scikit-learn#12730 at the Paris Sprint __eou__	User

TP	I'm working on GenericUnivariateSelect of https://github.com/scikit-learn/scikit-learn/issues/11000 __eou__	User

TP	I'm working on random_state descriptions for _weight_boosting of  scikit-learn/scikit-learn#16264 __eou__	User

TP	Please also comment in the issues to indicate the topic you are working on  @DatenBiene @ksslng @martinagvilas @maskani-moh so that other people don't work on the same things. __eou__	User

TP	I'm working on a documentation for _coordinate_descent scikit-learn/scikit-learn#16285 related to the issue scikit-learn/scikit-learn#15761 at the Paris Sprint __eou__	User

TP	Is there like a standard file format for storing the output of roc_curve? Maybe something that is easy to read, analyze, visualize, etc? __eou__	User

TP	Can you help me please !! __eou__	User

TP	How can i reach to each tree in random forest algorithm __eou__	User

TP	the estimators_ attribute, check the docs __eou__	User

TP	I want to extract the rules for every tree i get the following code put there is an error in it model=RandomForestClassifier(n_estamator=10) model.fit(iris.data,iris.target) estamator =model.n_estamator_[5] __eou__	User

TP	The error is ''RandomForestClassifier object has no attribute 'n_estimators_' how can i write this line please estimator=model.n_estimators_[5] __eou__	User

TP	Hello. I'm new to this community and I have got no prerequisites to get started, could someone please help me getting started. Thank you! :) __eou__	User

TP	Hey guys! New here. Got a question: why oob_score computing isn't done in parallel like the predict method from RandomForestRegressor? __eou__	User

TP	Hi there,  I'm new here... quick question, I'm recently using MLFLOW to manage model lifecycle... I've used it with some scikit-learn models and TensorFlow. Any opinion about mlflow framework? just wanted to know other experiences __eou__	User

TP	can GaussianProcessRegressor fail in subtle and obvious ways? I might have uncovered a bug. __eou__	User

TP	demonstration here: https://github.com/tanimislam/sharing-github/blob/master/DEMO%20GPR%20MISUNDERSTANDING.ipynb never mind, found the subtlety in scikit-learns GPR and possibly other regressors <unconvertable> scaling the variables made the problem disappear. __eou__	User

TP	@tanimislam hi any body Please, I am beginner ,how can i update 3.6 to 3.8 python version on window 10 , 64 bit ? _ __eou__	User

TP	Hi any body , I am beginner , can body can give me some basic assignments for python , just for beginners to learn Please? I am using PyCharm . _ __eou__	User

TP	@anjumuaf123_twitter  sir you can join cs50 or EDX Mit course on INTRODUCTION TO PROGRAMMING USING PYTHON __eou__	User

TP	@jhamlal Dear Sir, how to join this , is it free?  Please send me link  @jhamlal  @jhamlal  @jhamlal __eou__	User

TP	@jhamlal Sir, when will you online  acc. to your indian time ?? I need to discuss few things please @jhamlal I can explain  or tell you in details __eou__	User

TP	@anshu_bansal280_twitter  I am also new like for learning python __eou__	User

TP	> [![perceptron.png](https://files.gitter.im/scikit-learn/scikit-learn/Tyle/thumb/perceptron.png)](https://files.gitter.im/scikit-learn/scikit-learn/Tyle/perceptron.png)  can anyone explain line 6-7 update work , if example that would be great __eou__	User

TP	@jhamlal  hi sir google colab mutb?? __eou__	User

TP	Hello, why does `cross_val_predict` "eats" prints from `Pipeline`? When i set verbose on Pipeline i dont see any verbosity and when i have debug step in the pipeline that prints shape of the data it also doesnt print anything but when i execute the pipeline manualy it prints all the thing above __eou__	User

TP	Hello! new, hopefully soon to be contributor here, I am working with a team of fellow data science majors who have recently submitted a pull request for documentation. Would love some guidance or suggestions as we haven't heard anything back yet and are working on more as we speak! Thanks in advance! thank you so much, understandably so! we are PR #16417 and really just added a couple lines of documentation @adrinjalali thanks again for responding, even if someone has a minute to let us know if we are on the right track that would be super helpful! __eou__	User

TP	nice to have you on board @hansenallison . Review time is our main bottleneck, and therefore it may take some time before a reviewer can get to check your PR. What's your PR number? __eou__	User

TP	I am running the same version of sklearn but once the pipeline outputs the verbose when in cross_val_predict and the other time it doesnt, could it be something conserning jupyter? __eou__	User

TP	@EnyMan probably it'd be easier to track the problem if you open an issue with a reproducible bit of code __eou__	User

TP	Hey everyone I recently discovered the partial_fit method for certain regressors such as SGDRegressor, I wondered if I can partial_fit the model with a decreasing learning rate, save it, then reload it and further train with partial_fit but with a different learning rate schedule as before? Thank you in advance __eou__	User

TP	Can we use an unsupervised algorithm to perform sentiment analysis? If no , how to extract dataset on the customer support conversation transcripts ? as I dont' have any dataset I looking for unsupervised model Please let me know how to proceed ? __eou__	User

TP	Hi @vishu_rj_twitter it depends from what you need...which kind of sentiment analysis would you like to perform? Through unsupervised models you could in theory clusterise the conversations, and for this I'd suggest to have a look to bag of words techniques, word2vec, doc2vec, etc.  But I think you need to classify the conversation following some criteria that exploits the sentiment, right? In this case I don't see any alternative to a supervised classification. And like any supervised problem, find a godd training dataset it's not trivial at all.  But anyway, I'd be open to any further suggestion that could come from this chat. Hope it helped anyway. __eou__	User

TP	@gbroccolo  I am looking for happy / unhappy ( or positive /negative) criteria sentiments. I have searched for the  customer support conversation transcripts , but I did not got any datasets . I have searched in kaggle and google . let me know if any other repository for such datasets __eou__	User

TP	@vishu_rj_twitter have a look on NLTK package: there should be some basic implementation based on word's semantics (i.e. it extracts single words and check the presence of generally negative terms like "isn't", "aren't", etc.). but, again, it strongly depends from what you define "positive" or "negative" in your sentiment analysis. In most cases, you need supervised approaches, and generally you need your own labeled datasets. Machine learning stops to be nice at the moment you realise  you need to label your datasets by your own :) __eou__	User

TP	Hello, i'm ML student and i'm loving scikit-learn. Altough, i'm struggling using AdaboostClassifier. I'd like to set MLPClassifier as base_estimator but it says "MLPClassifier does not implement sample_weight function". Is there anyway i can customize this to accept this learner and others? I also tried to create a VotingClassifier of MLPClassifiers, and then using AdaboostClassifier with VotingClassifier as base_estimator, but no success oh @NicolasHug do you recommend another option to achieve this? I really need an Adaboost of MLP classifiers, even if it's not scikit-learn __eou__	User

TP	@vendrafilm you can't use AdaBoost with MLP because MLP does not support sample weights and AdaBoost requires the the `base_estimator` parameter to support sample weights (it's in the docstring). AdaBoost works by re-weighting some of the samples, so support for SW is mandatory. There's no way around it unfortunately. __eou__	User

TP	Hi all, I am working on revamping the [Keras](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras) [Scikit-Learn wrappers](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/wrappers/scikit_learn.py). This essentially requires implementing the entire Scikit-Learn API supporting multi-outputs, etc. I think I got everything working just by reading the API reference, but I would like to see if any of the Scikit-Learn developers are willing to take a look at the implementation and give me any pointers on things that might be issues. For example, it is not clear to me if ensembles of multi-output estimators are supported, and other edge cases of that nature. The PR is [here](https://github.com/tensorflow/tensorflow/pull/37201) if anyone wants to take a look. Thank you! __eou__	User

TP	@vendrafilm you should actually check out that PR I just linked! It should be easy to make an MLP in Keras that supports sample_weight and then wrap it to be Scikit-Learn compatible __eou__	User

TP	Hi, I've recently cloned the scikit-learn repository. But I'm having difficulty debugging the code in my Pycharm editor. I keep running into the infamous relative import error. Can someone pass me a reference that guides me on how to do it?  Also, please take a look at my debug configurations and let me know if something's wrong: Working directory: S:\Scikit-learn\scikit-learn\sklearn (I already marked sklearn as the source root) Python interpreter is the virtual environment I created for this. Running "pip show scikit-learn" in this env correctly shows 'Version: 0.23.dev0'. Script path: S:\Scikit-learn\scikit-learn\sklearn\ensemble\_iforest.py I even tried using Module instead of Script for debugging: "sklearn.ensemble._forest".  This just throws another runtime warning and fails. Pycharm is using "pydev debugger (build 193.6494.30)".  Am I wrong to try and debug this file? How can I debug the module as a whole? Some stack overflow discussions answer this but I couldn't figure it out for sklearn. Any help here would be greatly appreciated. Thanks in advance! __eou__	User

TP	@adriangb thank you, i will take a look. In fact, i need not only Adaboost to support MLP but also support different base estimators in one AdaboostClassifier.. not quite sure if it's possible __eou__	User

TP	Hello, I wanted to write a usage-question on github, but the issue tracker brought me here. I have a multilabel problem which i want to solve with a RandomForestClassifier. But I have so much data that I use a dataloader and a random forest with warmstart: https://stats.stackexchange.com/questions/327335/batch-learning-w-random-forest-sklearn . But I get an error after I fitted the model multiple times and try to predict testdata with multilabels. I broke it down to a minimal example (which fails too), but when I fit data, which i created with make_multilabel_classification, in one go the prediction of the random forest works. Any ideas? __eou__	User

TP	Hi, I want to write a new scikit-learn compatible estimator where the behaviour of fit/predict depends on a "strategy" hyper-parameter (similar to DummyClassifier but the behaviour is more complicated). Is there any other way/design pattern to implement this apart from making case distinctions within fit/predict? __eou__	User

TP	@mloning if the different "strategies" share common code, the conveniently named strategy design pattern might be worth using __eou__	User

TP	Hi,In the Scikit learn Website: https://scikit-learn.org/stable/testimonials/testimonials.html#who-is-using-scikit-learn. I know many other companies use scikit-learn package but do not see in the list, what is the process to list the testimonials in the website. __eou__	User

TP	@NicolasHug well I guess scikit-learn's Estimator class itself follows the strategy design pattern, but no, the fit/predict behaviour does not share code. I could encapsulate them into their respective classes (`StrategyARegressor`, `StrategyBRegressor`, etc) but then I could tune over the strategy parameter ... any other idea or example in scikit-learn that comes to mind? Thanks for the help! __eou__	User

TP	@mloning you can run parameter search over multiple estimators by wrapping them in a Pipeline object (slightly hacky, but it works: https://stackoverflow.com/questions/38555650/try-multiple-estimator-in-one-grid-search). Otherwise,  you can have both StrategyARegressor and StrategyBRegressor, and use these as instances in a MyRegressorClass. __eou__	User

TP	@NicolasHug "Otherwise, you can have both StrategyARegressor and StrategyBRegressor, and use these as instances in a MyRegressorClass." so how does this last suggestion work? Like a composition? So I'd have something like `if strategey="a": self.estimator = StrategyARegressor()` and then call `fit` by calling `self.estimator.fit()` __eou__	User
TP	yup__eou__	Agent

TP	Hello all, i'd be grateful if anyone could take some time to see my question:  https://datascience.stackexchange.com/questions/69788/valueerror-the-estimator-should-be-a-classifier Briefly, i need ELMClassifier from sklearn-extensions running with VotingClassifier and AdaboostClassifier. After some customizations, i am able to pass the ELM directly to the Adaboost, but i also need to pass a VotingClassifier (of ELMs) to the Adaboost as well. And that's where i'm struggling __eou__	User

TP	Hi everyone , Im a begginer and i want to contribute to scikit learn. I have worked on few Deep Learning projects in pytorch . I cant find a good issue to work as someone is already assigned to it or somebody has already fixed it . Can someone guide me to contributing in this repo __eou__	User

TP	`jonpsy` I can see the chat's packed already,anyway here's my two cents  I was going through the implementation of RandomFourierFeatures implemented in sklearn, under kernel_approximation.py. I noticed a part where you put  ``` class RBFSampler(..):     . . .. def transform(..): .. ..  np.cos(projection,projection) ..  return projection ``` Now np.cos doesn't take two arguments, so what's happening? I tried replicating this in my console and I got error, also why is no variable storing that information ?. I've created the corresponding [issue](https://github.com/scikit-learn/scikit-learn/issues/16746).   Thanks for the help :) `jonpsy` Nvm Issue solved thanks !! `jonpsy`  * Nvm Issue solved thanks Nicolas Hug (Gitter) . __eou__	User

TP	greetings! __eou__	User

TP	I have an assignment in which I am supposed to implement a neural network from scratch. My NN outputs a number between 0 and 1 to classify the input to either +1 or -1. so if the output if less than 0.5 then it belongs to -1, else +1. Unfortunately, my code only returns either +1 or -1, depending on how I initialize the weight matrix. I THINK I'm facing vanishing gradient problem. But I need an expert's opinion to make sure. anyone can please help me?? link to my code: https://www.kaggle.com/mowhamadrexa/kernel2ee3f07315 __eou__	User

TP	I made some machine learning models using Python scikitlearn library and I found some strange situation for me regarding real importance of some variables (features) to ML model. I found that variable which has smaller Pearson coefficient has higher importance on ML model (when exclude variable from model using backward elimination principle) than variables which has higher Pearson.  Below I send the real results of three models where first model includes all three variables and another two models excludes some variables (- means variable is excludes). Iuse Random Forest method.  Model Name     MAE  ModelV1V2V3 0.92 ModelV1V2- 3.86 ModelV1-V3 2.96  PearsonV1=0.99, PearsonV1=0.82, PearsonV3=0.02  When I exclude variables which has no importance based on Pearson (0.02) I got model with better performance comparing to model which includes another variable (V2) whic has far higher Pearson (0.82). Please, help me to explain this situation.  Please, answer me as soon as possible.  Thank You in advance.  Dusko Tovilovic __eou__	User

TP	Is it possible to create an adjacency matrix  with LDA? __eou__	User

TP	Hosting TFUG Mysore first meetup : TensorFlow JS - Show and Tell by Jason Mayes  - Senior Developer Advocate at Google. 8 presenters showing what they have #MadeWithTFJS with epic demos lined up + more. RSVP now  http://meetu.ps/e/HTwBV/jYwqF/a __eou__	User

TP	greetings __eou__	User

TP	Hello, I was using sklearn.svm.SVC and it took five hours.  I did get good results, but I am wondering if scikit-learn has an ETA (Estimated time to Arrival) setting, which shows the estimated time for the program to run. __eou__	User

TP	@luishrd  There is a verbose setting and a max_iter. __eou__	User

TP	@JohnPaulMSU15_twitter There are 2 interesting things I found after a bit of research . There is a parameter you can set for your SVM called `verbose` .  As such you can write: ``` cf = svc.SVM(verbose=2) cf.fit(X,Y) ``` Now , according to [this](https://stackoverflow.com/questions/22443041/predicting-how-long-an-scikit-learn-classification-will-take-to-run) stackoverflow answer, setting this parameter will only output the number of iterations required for optimization as they finish, so they may only give you a hint.  The second and more interesting thing I found is this library : [scitime](https://github.com/scitime/scitime)  It does exactly what you need, trying to provide an estimate for your system. More info at the docs they provide :) __eou__	User

TP	I need to write a custom random_selection(for random selection of feature i.e "max_feature" and subset of train data i.e. "subsample") module in scikit-learn to be used with sklearn.ensemble.RandomForestClassifier and GradientBoostingClassifier. Can someone point to some example/documentation/discussion etc.? __eou__	User

TP	What is the best score for the Iris Dataset? __eou__	User

TP	@Dgomzi you probably need to look at the source code for that. __eou__	User

TP	Hey! Is there a way of computing the dimensionality of a given dataset in scikit-learn? I have some high-dimensional data, which I believe reside on a lower-dimension manifold, the dimension of which is unknown. What I am looking for is some way of measuring the number of dimension of the manifold. __eou__	User

TP	Hey everyone! I wanted to know why sci-kit learn provides the quantile loss for GBRT but not for the SGDRegressor? I am aware that the quantile loss is non-smooth, however GBRT seems to calculate the subgradient at zero for that reason. Wouldn't this be applicable for SGD? __eou__	User

TP	@arturgvieira_twitter  hi sir __eou__	User

TP	@cphyc the PCA estimator has a `mle` option to automatically choose the number of components to project on. That's not exactly what you want but I think it's related __eou__	User

TP	hello, would adding the `return_std` argument to the `predict` method of the lightgbm regressor class be enough to plug it into scikit optimize? Also what kind of stdev is it referencing to by `std(Y | X)` ? I tried to look how it is added to the estimators provided by the skopt but it appears it is done in different ways for all of them so it is not clear to me.. __eou__	User

TP	Hi Everyone. I would like to contribute to this open-source project. May I know the process, please. __eou__	User

TP	@Akram1234 hi sir __eou__	User

TP	Hello @anjumuaf123_twitter  sir __eou__	User

TP	I  am  beginner   for python I want to  go for time series analysis   @Akram1234  I have my own data @Akram1234 Would you like to help me? Please __eou__	User

TP	sure  @anjumuaf123_twitter __eou__	User

TP	Hi all, what's the reason why one shouldn't set other attributes during construction in sklearn estimators apart from those in the `__init__` signature (see estimator check: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/utils/estimator_checks.py#L2390)? If I understand it correctly, `clone` calls the constructor and so all other attributes will also be re-set. __eou__	User

TP	`clone` doesn't call the constructor with the expected parameters. The parameters are set in `set_params`. That means if setting some other attributes depends on some given attribute to `__init__`, and that logic is not duplicated in `set_params`, the constructor would be in an invalid state. Putting those in `fit` makes them all consistent. But there are ways where you can handle it with setting other parameters in `__init__`, and the estimator not going into an invalid state, if you know what you're doing, then you can ignore that test. __eou__	User

TP	Thanks for the quick reply, yes, so, as long as there is no logic or interaction of attributes in `__init__`, I can initialise other attributes in `__init__`. We are developing a toolbox that extends sklearn to time series and currently have an `is_fitted` attribute in our base estimator which is initialised to `False`, any reason why this may be a bad idea? __eou__	User

TP	That's just a bad idea cause we have a `check_is_fitted` utility function which should be used ;) __eou__	User

TP	I know, I find having an `is_fitted` state cleaner than following the trailing-underscore convention :)  thanks for the help __eou__	User

TP	Hey all, currently doing some dev loop on the pyx in the neighbors package any way to get a faster dev loop than `pip install --editable .` on ubuntu 16.04 __eou__	User

TP	You probably wanna pass `-no-build-isolation` as well @mhamilton723 __eou__	User

TP	@mhamilton723  `make inplace` will only recompile the files that you changed Also if you do lots of back and forth, https://ccache.dev/ might help __eou__	User

TP	Hi everyone! __eou__	User

TP	I've got a few questions considering lin regression. These are mostly focussed on using scikit learn, but I guess the questions are also applicable to all kinds of ML tools :) [More general formulation of the questions: standardization, onehotencoding/dummy coding with mixed var. types for poly. reg](https://stats.stackexchange.com/questions/463894/feature-standardization-for-polynomial-regression-with-categorical-data) [and here the python/sklearn specific formulation of the problem with a focus on onehotencoding](https://stats.stackexchange.com/questions/463690/multiple-regression-with-mixed-continuous-categorical-variables-dummy-coding-s) any help is welcome and appreciated. :) Since I didn't find any information on this topic anywhere in the user guide, I could also help with improving the user guide once I have more information :) __eou__	User

TP	Thank you @adrinjalali  and @NicolasHug :) __eou__	User

TP	hi guys, I wrote some codes  to get Latitude  and longitude of some districts  but error , Please help me __eou__	User

TP	Hi everyone I have a quick question about the math behind the last step of Linear Discriminant Analysis (LDA) for dimensionality reduction. So I understand for the algorithm to calculate for k projection vector(s) you need to determine the eigenvector(s) that corresponds to the top k eigenvalue(s). But does anyone know what you do with those eigenvectors after you have calculated them? My guess is to multiply all of the eigenvectors (projection vectors) together and then multiply that with each point, x, in the original dataset to produce a new point y. Does this seem right? Thank you in advance __eou__	User

TP	@nadimk1 hi sir @nadimk1 I am new  in python . I want to ask that Lubuntu and window 10  can be run in  one machine ? __eou__	User

TP	@anjumuaf123_twitter don't quote me on this, but I think you could probably run it in a virtual machine on your computer using virtualbox __eou__	User

TP	okay super basic question, but does anyone know how to determine if a KNN model (Sklearn-KNN) is overfitting the data __eou__	User

TP	I am trying to run it on the MNIST digits dataset, but the accuracy seems way too high __eou__	User

TP	Hi, anyone with Cython knowledge who would have an idea what is going on in here: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L817  To me, it seems like a node is being subtracted by an array of nodes, which is given as a pointer index. Also, it looks like the index arithmetic is done on the smaller value, causing supposedly a negative index. Is it just the index being subtracted here, or is there something else happening here? __eou__	User

TP	@toldjuuso from what I understand `self.nodes` is both the array of nodes *and* the address of the root node (In C and thus in Cython, an array is just a constant pointer on the first value of that array) so `node - self.nodes` will be positive and it correspond to the offset between the final node `node` and the root i.e. it corresponds to its index in the `self.nodes` array __eou__	User

TP	@NicolasHug Ooh now I see, makes much more sense. I don't program in C, so this has been quite cryptic. I appreciate the help, really! __eou__	User

TP	Hi, I'm trying to use Pipeline for combining feature extraction and clustering. I see that there are ways in sklearn for transforming a single column into one or more columns. What I would like to do is specify a new feature as a function of a number of features, inside a pipeline.  I looked at ColumnTransformer, FeatureUnion, and Pipeline, but neither of these enable what I am looking for. Do I have to create these new *features* before I put them in the pipeline? Or am I missing something int he documentation? __eou__	User

TP	@Bri9k_gitlab maybe the FunctionTransformer is what you're looking for? __eou__	User

TP	@NicolasHug Thanks! I think that will work. __eou__	User

TP	does someone remember what the "s" in pandas stands for? __eou__	User

TP	@amueller  I would have guessed it comes from "Shit, `panda` is already taken" but according to PyPI `pandas` predates `panda` so IDK __eou__	User
TP	lol__eou__	Agent

TP	Hi, I am trying to create anisotropic exponential and anisotropic gaussian kernel function for Sklearns' Gaussian Process Regressor. Any idea how I can do this with sklearns' inbuilt kernels? See attached image. ![alt](https://i.stack.imgur.com/DRKAE.png) I am new to this and I have been looking for help since a long time. Any help will very appreciated. Thanks. __eou__	User

TP	At the bottom of the scikit-learn website it says " <unconvertable> 2007 - 2019, scikit-learn developers (BSD License)". I assume it should be 2020 instead of 2019, right? If yes, I'm happy to make an issue and fix it. __eou__	User

TP	Thanks for noticing @marenwestermann , this is already fixed in the dev version https://scikit-learn.org/dev/user_guide.html __eou__	User

TP	Thanks for pointing it out! I'll have a look there first next time. __eou__	User

TP	Hi, do you have any useful resources for thinking about legal issues in open source (e.g. governance, sponsoring, contributor license agreements)? Would be much appreciated, thanks! :) __eou__	User

TP	@mloning I think this book https://producingoss.com/ covers some of these topics __eou__	User

TP	want a project partner for kaggleing __eou__	User

TP	Thanks @NicolasHug __eou__	User

TP	FYI in case anyone is interested here's a quick intro video to contributing to sklearn: https://www.youtube.com/watch?v=5OL8XoMMOfA&feature=youtu.be I probably forgot a couple of things, this one is made with the upcoming data umbrella sprint in mind __eou__	User

TP	Hey guys, I'm trying to use Agglomerative Clustering. When applying the distance threshold, does it have a specific range? E.g. 0-1? or 0-10? or 0-1000? When I tried 1000, it gave me all the same cluster. When trying 500 it gave me clusters 0 1 2 3. I wanted to do something like: "All vectors above 70% similarity should merge" but I'm not sure how to implement that with distance threshold. __eou__	User

TP	what does 70% similarity mean? the range is applied on the output of your distance metric if you want .7 to mean 70% similarity, then you should give a metric which results in 0.7 when they're 70% similar, with whatever definition you have but the distances are not bounded, and therefore usually you need to sample from your distances, look at the distribution, and decide on what the threshold should be __eou__	User

TP	Yes I do mean something like: if I tried to do cosine distance with 2 vectors and then subtract that by 1 and get the absolute value, I get the similarity. Sorry I'm not familiar with what distance metric refers to Ah okay I'm currently trying to test with cosine, will report back. Thank you! The confusing part for me is the linkage choice. I'm not sure which would give the desired effect. I'll try to search for tutorials online for that though. Okay doing this: AgglomerativeClustering(distance_threshold=1,n_clusters=None, affinity='cosine', linkage='complete').fit(sentence_embeddings) Gives me 147 clusters Hey @anjumuaf123_twitter __eou__	User

TP	When you say the distances are not bounded, do you mean that, for every dataset I apply this to, the clustering behavior would change? Basically my situation is I have a bunch of sentence embeddings and I want to cluster the sentences. I found that with my sentence embedding model, sentences with a similarity above 0.7 tend to be actually similar. __eou__	User

TP	"sentences with a similarity above 0.7 tend to be actually similar. " is not welldefined unless you define you metric, if it's cosine, then your statement may be correct, in which case you should set the `affinity` parameter to cosine the default is euclidean https://scikit-learn.org/dev/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering __eou__	User

TP	@youssefabdelm_twitter Hi Sir, __eou__	User

TP	sir,  are you familiar with python GIS? __eou__	User
TP	I'm not @anjumuaf123_twitter__eou__	Agent
TP	@youssefabdelm_twitter RIGHT SIR__eou__	User
TP	@youssefabdelm_twitter tHANK YOU VERY MUCH__eou__	User

TP	I expected something like 1 cluster for that last line: AgglomerativeClustering(distance_threshold=1,n_clusters=None, affinity='cosine', linkage='complete').fit(sentence_embeddings)" __eou__	User

TP	Just realized where I might've gone wrong in my thinking there. My dataset probably has a bunch of sentences which are in fact that similar. __eou__	User
TP	you not similar, more like orthogonal and dissimilar__eou__	Agent

TP	Ah yes. 1 would refer to more dissimilarity per cluster and 0 more similarity per cluster I think. __eou__	User
TP	yes__eou__	Agent

TP	Can anyone help with the sprint event? I already filled the form. __eou__	User

TP	@reshama should know better about the participants and RSVPs __eou__	User

TP	Hey guys, is the agglomerative clustering in scikit learn soft or hard? Or is there a setting somewhere we could use to set that? Also, while using agglomerative clustering on 100K data points, my computer crashed. I assume this is because I didn't use the 'memory' parameter for caching? Or did I misunderstand something there? __eou__	User

TP	the memory parameter will not help you here. What do you mean by soft or hard? it might be useful to precomputed the distance matrix in a chunked way how much RAM do you have? and which linkage mode are you using? __eou__	User

TP	Hey @amueller ! By soft or hard I mean to refer to soft clustering or hard clustering - which if I understand correctly, soft clustering refers to when datapoints can co-exist in different clusters whereas with hard clustering they can only exist in one cluster. I have 16 GB of RAM Using complete linkage How could I go about precomputing the distance matrix in a chunked way? I don't know if I got this right so far: @rth Do you mean using this in place of agglomerative clustering or for chunking / precomputing the distance matrix? Oops! Now I see where I was wrong And why those 100K data points only exported 1 cluster In the toy example, I did this:  chunked = pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1)  for chunk in chunked: clustering = AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(chunk) I set the parameter metric to 'cosine' in pairwise distances and I set affinity to 'precomputed' in AgglomerativeClustering When I then tried to do this with the 100K datapoints I forgot to change 'cosine' to 'precomputed' in AgglomerativeClustering __eou__	User

TP	from sklearn.metrics import pairwise_distances_chunked from sklearn.cluster import AgglomerativeClustering  chunked = pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1)  for chunk in chunked:        clustering = AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(chunk) print(clustering.labels_) __eou__	User

TP	@youssefabdelm_twitter You could try https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer __eou__	User

TP	@youssefabdelm_twitter you should call pairwise_distances directly I think as it does chunking internally. Though I now realized agglomerativeclustering might already be doing. Also agglomerative clustering is hard clustering @rth I was thinking about that, but shouldn't agglomerativeclustering be already chunked using the working_memory parameter? cc @jnothman __eou__	User

TP	Interestingly I actually tried the code I shared above on a small example and it worked, but on the 100K embeddings it just exported 1 cluster, and the amount of embeddings in that was around 750 which really confused me. I still have no idea what happened. By calling it directly, do you mean something like this:  "for chunk in pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1):" Or this: AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1)) I hope I'm making some other stupid mistake rather than this being a consequence of a large input So far I don't see any difference between the toy example and the code I'm using for the 100K embeddings I might try the 'meta-clustering' approach I talked about where I just compare centroids (or try to apply complete linkage) of clusters from different chunks and then merge, and sort of testing that against what agglomerative clustering would normally do and see how the results vary. Do you think this would yield expected results (All points above 70% similarity should be grouped) or should I instead go with HDBSCAN? My one need is not specifying the number of clusters, but instead a distance threshold of some kind. __eou__	User

TP	@amueller  I tried both, I assume you mean the first as that one works. The second gives me an error. "ValueError: Expected 2D array, got scalar array instead: array=<generator object pairwise_distances_chunked at 0x11009fc78>. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample." __eou__	User

TP	Now I'm getting this new error: "ValueError: Distance matrix should be square, Got matrix of shape {X.shape}" After making that change __eou__	User

TP	This is what I'm using for the 100K embeddings: chunked_distances = pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1, working_memory=3072) for chunk in tqdm(chunked_distances, total=22):    with io.capture_output() as captured:     clustering = AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(chunk) @rth Very useful, thank you! __eou__	User

TP	@youssefabdelm_twitter I don't think that using AgglomerativeClustering on chunks of the distance matrix would give you anything meaningful. I meant using `KNeighborsRegressor`to precompute a sparse distance matrix, but then I'm not sure if `AgglomerativeClustering` actually supports sparse distance matrices. You won't be able to compute a dense as for 100k samples that would be ~80GB.  Generally AgglomerativeClustering doesn't scale well with default options (https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html) so I would suggest starting with a smaller dataset and progressively increase the number of samples to see how it scales. You may run into performance issues before memory ones. @amueller I haven't looked at the code in detail, but internally it never uses `pairwise_distances` on the full dataset. __eou__	User

TP	hm. I was wondering if we need to add an example of doing some chunked clustering __eou__	User

TP	@rth Does this mean that besides a sparse distance matrix (I'll give it a shot soon), there's really no way to do this all at the same time without a more powerful computer? So far what I've been doing is doing the clustering in chunks, meaning slicing the data (38K data points at a time) and then creating clusters from those. However, to me this is undesirable because of course (in my case) I get sentences which are in separate clusters which should more preferably be in one. I thought one idea to mitigate this is to calculate the centroid of each cluster, and then after all the 38K chunks are done, to compare centroids and then merge if they're below a certain distance threshold. Of course this loses the benefit of having the leaves & children info though. I ask because eventually, I want to do this with around 1 million vectors Also curious why you say that using AgglomerativeClustering on chunks of the distance matrix wouldn't give any meaningful results. Is it for the same reason I said above on getting different clusters which should really be one cluster? __eou__	User

TP	@youssefabdelm_twitter Looks like single linkage should scale better for agglomeration clustering https://github.com/scikit-learn/scikit-learn/pull/11514#issuecomment-557349961  Well I mean it's just awkward to work with N separate clustering, but you can if you are comfortable with it. Also I think think Birch or DBSCAN might scale better if want a hierarchy of clusters. If you have 1M samples that certainly going to put constraints on the algorithms you can use (cf above linked HDBSCAN docs for a comparison) unless you can accept working on a subsampled dataset. __eou__	User

TP	Awesome! Helpful charts. Wanted to ask what you meant by "N separate clustering" I assume you mean what I described above with splitting the data? __eou__	User

TP	yes, I mean that if you do `AgglomerativeClustering().fit(X_chunk)` in a loop, as mentionned in your above code, at each iteration you are going to get a new clustering. All previous information is erased when you call fit, it's not a `partial_fit`. So that would be just equivalent to running the this clustering on N subsets of the full dataset I think. __eou__	User
TP	Ah makes sense, thank you__eou__	Agent

TP	Sorry never mind, I didn't know about epsilon! I gotta do more research on HDBSCAN as I'm not that familiar with it. __eou__	User

TP	@adrinjalali https://github.com/scikit-learn/scikit-learn/issues/16951 __eou__	User

TP	I think the "go wanted" tag was removed from the issue cause we already have at least a half solution to the issue. Not sure if we want to work on it during the sprint @Mariam-ke *"help wanted" Is it on the issue list for the sprint @Mariam-ke ? __eou__	User

TP	I have a small concern.  I am wondering if matplotlib should also be installed in setup.py along with NumPy and scipy. Is there is a reason it is not? __eou__	User

TP	@Sahanave there is no reason to. It's a soft dependency and many people run scikit-learn in a server setup where there's not even a screen attached __eou__	User

TP	That makes sense. Thansk @amueller __eou__	User

TP	Hello ! I have a question about the use of GutHub. I have started working on a pull request with Joseph Lucas (https://github.com/JosephTLucas) during the Data Umbrella sprint this week-end (https://github.com/scikit-learn/scikit-learn/pull/17504). I am trying to add a commit to this pull request (while I have not created it, Joseph has). In order to do so, I want to send a pull request to Joseph Lucas on his branch. However, I do not understand why I the github interface does not propose his repo as base when I create the pull request (while he has his own scikit-learn repo https://github.com/JosephTLucas/scikit-learn). What do you recommend I do ? I would prefer not to open a new pull request. __eou__	User

TP	I think when you open a PR there's a checkbox which says "show all forks" or something, and then it'd show you Joseph's fork as well. __eou__	User

TP	Github displays the following :  Open a pull request Create a new pull request by comparing changes across two branches. If you need to, you can also compare across forks. When I click on "compare across forks", it displays a list of forks (base repository), but Joseph's fork does not appear. (the list seems too short to contain all forks of scikit-learn). Github does not manage to find his fork even when I enter the repo name :  JosephTLucas/scikit-learn. Thanks ! That is exactly what I wanted : ) __eou__	User

TP	Yeah this seems be a github interface issue (not 100% sure more of a wild guess). __eou__	User
TP	interesting, I'm not sure then__eou__	Agent

TP	I think if you go to a URL like this: https://github.com/JosephTLucas/scikit-learn/compare/master...ab-anssi:master You should be able to create a PR to JosephTLucas repo There is this weird thing that if you use the "natural way" (at least to me) of creating a PR to JosephTLucas fork it insists on creating a PR to the main repo (scikit-learn/scikit-learn) If you go there: https://github.com/JosephTLucas/scikit-learn/pulls and click "New Pull Request" it creates the PR against scikit-learn/scikit-learn from JosephTLucas fork and as you noticed I was not able to change that to use JosephTLucas fork as the target (**edit** it turns out clicking on the blue arrow between base and head fork is another work-around)... __eou__	User

TP	I have sent the pull request to Joseph. Thanks for help. It is the first time I send a pull request to a contributor to update his pull request to the main repo. If he accepts my pull request (to the branch of the pull request), will it automatically update the pull request (to the main repo) with my commit ? __eou__	User

TP	Short answer: yes. Longer answer: his PR is tied to his branch so as soon he merges your PR on his fork, his branch will be updated and your changes will appear on the PR to scikit-learn/scikit-learn. Side-comment: if you collaborate from time to time with JosephTLucas a reasonable way to make that easier is that JosephTLucas gives you write access to his fork. This way you can push directly to his branch without doing a PR on his fork. __eou__	User

TP	@lesteve Thanks for the "side-comment". I will ask him if he agrees to do so. It would be much easier : ) __eou__	User

TP	Sounds like a plan! __eou__	User

TP	Hello, i hope this is the right place for my question __eou__	User

TP	i have a problem with a 1D-Classification. I simplified my problem to the following: There are 2 classes +1 and -1 and i have a trainvalue for each of the classes 0.0293294646367189  (class -1) and  0.025545042466768184 (class 1) Now that i trained a LinearSVC with those values i throw some random values to the classifier to predict i expect the decision limit to be in the center of the two example values but even 0.025545042466768184, the train data for class 1 is predicted as -1 i even tried to move this to a 2D Problem adding a 2nd feature to the values [0.0293294646367189, 0] and [0.025545042466768184, 0] but this didn't worked either __eou__	User

TP	I have some data I want to so linear regression on. When I use LinearRegression().fit(X_scaled, y[policy]) I get a score of over 0.96 when I use LassoLarsCV(cv=5).fit(X_scaled, y[policy]) I get a score of 0 what am I doing wrong? __eou__	User

TP	what's the complete code? Training set score or test set score? __eou__	User

TP	Hi, I have a large dataset, 600K rows and 2 columns of target. Multioutput xgboost works well, but Random Forest is so slow. How Can I perform it ? Its a regression problem __eou__	User

TP	i am writing a neural network without any external libraries for MNIST with only 4 labels.In the train _labels.csv  file i have one hot encoded data(1000) for all samples, my doubt is how to call the data directly to the function in the code. iam using softmax as my activation function in output layer __eou__	User

TP	Hi @FranciscoPalomares Random Forest is a collection of models which can be trained independently and can be parallelised: I guess for the classification you are using the RandomForestClassifier, you can scale the training through the n_jobs parameter when you instantiate it, something like  ``` RandomForestClassifier(n_estimators=100, n_jobs=-1) ``` __eou__	User

TP	does the default FeatureAgglomeration just leave you with 2 features? __eou__	User

TP	as stated in the [docs](https://scikit-learn.org/dev/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration), `n_clusters=2` is the default value. __eou__	User
TP	does that mean you end up with 2 features?__eou__	Agent
TP	I am not clear what a cluster means here__eou__	Agent
TP	@adrinjalali  (thank you)__eou__	Agent
TP	yes, the examples and the user guide will give you a better idea on how it works and what it does: https://scikit-learn.org/dev/modules/clustering.html#hierarchical-clustering__eou__	User
TP	thank you__eou__	Agent
TP	I am surprised it works so well for my regression problem!__eou__	Agent

TP	I think we could add it to the docstring as well. I think it's clear from the user guide but why not add it to the docstring? __eou__	User

TP	I agree that its docstring can be substantially improved :D the examples can also see some love __eou__	User

TP	Hullo! I took on #9602 as a first issue for the MLH Fellowship. Looking into it further, it seems like the scope of the issue is much larger than clarifying a single docstring. I'm unsure of whether to start poking away at it, or whether to start a larger discussion on the direction of multiclass/multilabel learning in sklearn. Would love to hear thoughts. :) __eou__	User

TP	hm honestly I think we're pretty consistent with the terms in the glossary >  In some places, multiclass/multilabel functionality is explicit and indicated by the name of the function. can you give an example of that? that's for meta-estimators, I guess? __eou__	User

TP	Ah, I guess I went a bit overboard with my comment, seeing an issue where there was none. I'll focus on clarifying the OVR docstring, then. __eou__	User

TP	Does anyone have any tips for making an API proposal go smoothly? (e.g. the structure of a good proposal, good examples of existing proposals)  Context: I was hoping to start working on a proposal for adding Gibbs sampling to LatentDirichletAllocation, going off of Thomas's recommendations. Ah, just found the SLEP template. Ah, thanks for the clarification, @adrinjalali! __eou__	User

TP	I think adding features like that is usually discussed in the issues @joshuacwnewton SLEPs have been a bit more on the general API rather than specific features it can be just an issue like this one, for example https://github.com/scikit-learn/scikit-learn/issues/15346 __eou__	User

TP	Hello, the question I meet is the socres returned from the function 'cross_val_score'  seem to be very different from the result of actual fitting process. This is the last training process where most of the valid loss are about 0.1. However the score I get from the 'cross_val_score' is about 0.49. The score I use is the 'neg mse' which is the similar to the loss function of the network  'mse'. I want to know why it happens and how to fix it. Thanks a lot. [![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/rNGu/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/rNGu/image.png) __eou__	User

TP	hola __eou__	User

TP	Just submitted a new PR for the cross-validation documentation #17781 __eou__	User

TP	Hello, guys. How can I use a custom Distance Function for OPTICS clustering algorithm? __eou__	User

TP	@GF-Huang you can use the metric parameter __eou__	User

TP	Upvote vscode issue to allow colorfull output when fitting with Sklearn. https://github.com/microsoft/vscode-python/issues/12615 __eou__	User

TP	Can I request I/we/someone works on getting this merged https://github.com/scikit-learn/scikit-learn/pull/15007 as part of the SciPy sprint __eou__	User

TP	@raybellwaves That PR is now merged. __eou__	User

TP	I came across this on Twitter today and it made me think that scikit-learn also uses the word "dummy": https://twitter.com/wimlds/status/1279635475825754112 It would probably be good to move away from using this word (Google is trying to do the same btw: https://developers.google.com/style/word-list#dummy-variable), however it would be a big undertaking. A search for this word in scikit-learn gives me 479 results. I just wanted to point this out and ask what you think about it. __eou__	User

TP	I guess we could call the `Dummy{Classifier/Regressor}` a `Trivial{Classifier/Regressor}`? __eou__	User

TP	in some testing files like `sklearn/cluster/tests/test_hierarchical.py` and `sklearn/gaussian_process/tests/test_gpr.py` the comments refer to creating dummy data and dummy optimizers but don't use the dummy classifier or variables. Maybe a good first step would be to change the wording in these comments as it doesn't conflict with the code? __eou__	User

TP	is dummy offensive? __eou__	User

TP	I looked up why it's offensive, and it seems most people think it is or may be offensive because of the way it's use as a derogatory term to call somebody stupid or an idiot. Now if we think of the people whom unfortunately are called often dumb or stupid by their friends or colleagues while many of them being very talented, we realize why the term may be a trigger for them. I personally don't find it offensive in the context which is used in our library, but general rule of thumb is that if there are many reasonable people out there who thing something is offensive, then it's offensive to them and I should avoid using the term, if that makes sense. __eou__	User

TP	Could be wrong but my impression is that we mostly use the word dummy to denote "something designed to resemble and serve as a substitute for the real or usual thing; a counterfeit or sham" (taken from google definition). That might not be true in the case of DummyClassifier/Regressor, where the name might come from the fact that these estimators are over simplistic. Though the previous definition could also hold for them. IDK. Changing their names will force lots of users to change their code though,  so it's not just going to be an internal change.  As a side note we also have a bunch of "sanity checks", deemed non-inclusive by twitter, and I'll admit I'm quite puzzled by this one __eou__	User

TP	thank you __eou__	User

TP	I have a question regarding the usage of the FeatureUnion weights.  I recently had a typo in a key of the weights dictionary, which does not lead to a warning or an error. Suppose:  ```python features = FeatureUnion([   ('f0', my_fancy_extractor_0),   ('f1', my_fanyc_extractor_1), ], transformer_weights={   'f1': 1.5,   'f2': 1.0, }) ```  I understand that tranformers without any weights (e.g., `f0`) will implicitly have assigned a weight of 1.0 (i.e., it is not multiplied with anything), which seems intuitive, but trying to assign weights to a transformer that is not part of the FeatureUnion is simply ignored.  I would argue that it would be beneficial if a warning would be shown instead, as this is a very tedious problem to debug in my opinion. What do you think? __eou__	User

TP	I think this deserves an issue on the issue tracker with a small reproducible bit of code using the latest release. Input feature name validations are improving on our side, but there's still work to do. __eou__	User

TP	Thank you @adrinjalali , i have submitted an issue: #17863 __eou__	User

TP	I dump with joblib a model with all cpus (njobs = -1), when I load the model and predict, not use all cpus __eou__	User

TP	hi sir  how to  ignore negative floating values in a data set ? __eou__	User

TP	@FranciscoPalomares Use `estimator.set_params(n_jobs=1)` after unpickling __eou__	User

TP	Hello! I've got one approval on an open PR -- if anyone has time, may I have a second reviewer? #17662  No rush, of course. I understand how busy things are. Thanks much! :) __eou__	User

TP	My above request has been addressed by @NicolasHug. Thanks! :D __eou__	User

TP	Good morning :) __eou__	User

TP	Hello!  Is there anyone who can assist me in using scikit's normalized mutual information for real, continuous climate data? __eou__	User

TP	@bfiranski as far as I know, it's only implemented for discrete data. computing mutual information for continuous data requires making distributional assumptions We actually have a non-parametric mutual information for feature selection (between a continuous and a discrete distribution). Are both your distributions discrete? __eou__	User

TP	I was wondering about that because I was getting non-nonsensical results many thanks! no, i just kept on getting near unity results no matter what i tried - related data, unrelated data, two noise arrays i tried that one as well, it seemed to behave a bit better, but as you know, is not normalized to a metric.  my problem is that, as you also likely know, estimates of entropy to normalize it are highly dependent on resolution __eou__	User

TP	both are real i.e. temperature and pollution concentration __eou__	User
TP	hm you didn't get a warning? You should have gotten a warning, I think :-/__eou__	Agent

TP	can you please open an issue on the issue tracker for that? I think we should provide an error or at least a warning __eou__	User
TP	okay__eou__	Agent
TP	if both of your variables are univariate, you might want to look at  https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression__eou__	User
TP	however, that's just one particular estimate of the mutual information.__eou__	User
TP	exactly, which is why you need some distributional model of the entropy, which is why we don't really do that__eou__	User
TP	I think most people would use a regression analysis or something like that to answer the question you want to answer, unless you are relatively certain about how to model the distributions__eou__	User
TP	btw, there's a cool non-parameteric estimate using euclidean minimum spanning trees__eou__	User

TP	it's not an easy thing to do it seems, thanks for your input! __eou__	User

TP	hahaha!  you are talking to an atmospheric scientist who has, sadly, no stats and is way in over their head :) __eou__	User
TP	then I'd suggest a regression model, I think__eou__	Agent

TP	there are some papers on using k-nearest neighbour for entropy which supposedly works well for continuous data, but i wanted to confirm that scikit wasn't appropriate before going down that rabbit hole __eou__	User
TP	yes that's the default, and that's what the mutual_info_regression uses__eou__	Agent
TP	and you can probably use that as a starting point and only have to modify it slightly__eou__	Agent

TP	that is good news!  many thanks for providing a direction. for now i am going to report the non-warning issue you requested __eou__	User
TP	thanks!__eou__	Agent

TP	Just to be sure we are on the same page, here is what my results are: ```noise=np.arange(500) wavelength=np.linspace(0.01,1,500)*1e-6  # testing the normalized MI score between wavelength and noise - should return near zero for totally unrelated data sets constant_normalized_mi=normalized_mutual_info_score(wavelength,noise) ``` output: 1.0 __eou__	User

TP	yeah it should just error on floats, I think __eou__	User

TP	hey, I'm new to sklearn opensource community can anyone please help me for my first PR. __eou__	User

TP	Hey @chaitanyamogal , have you already picked an issue to work on? __eou__	User

TP	No, but I am looking for a good first issue. __eou__	User

TP	I have question/curiosity: generally, for new algorithms, are there any reasons that scikit-learn would prefer a Python/NumPy implementation over a Cython implementation?   The only thing I can think of is if the efficiency gain for the Cython version so minimal that it's not worth the added complexity. But, are there any other reasons? __eou__	User

TP	@joshuacwnewton yes, mostly readability and maintainability __eou__	User

TP	there is another potential reason in the future: using the __array_function__ protocol and possibly NEP 37, pure numpy algorithm could directly be ported to GPU or distributed dask datastructures, while that's not possible for Cython implementations. (this doesn't say anything about how efficient that would be though) There is even a (very hypothetical) future even where we might add both Cython and Numpy for some algorithms so we have a fallback in case the array is not a numpy array (this sounds weird; we need a numpy implementation if the array is not native numpy, but that's the way it would be lol) __eou__	User

TP	Goodness, thank you. This is very valuable! I'm writing a uni report for my co-op term comparing Numpy to Cython (specifically in the context of #9661), and it felt a little too one-sided of me to say "Cython faster, Cython better" (lol) I figured there was more nuance there, so thank you. I've got a tab open with Numba too, ehe. Too many tabs, really! If I may pick your brain a little more, what about Cython do you feel makes it more difficult to maintain? __eou__	User

TP	the main thing is really maintainability. In general, there's also the fact that you have to compile Cython, so distribution becomes much harder. But in sklearn we already made that investment so the additional burden is relatively small. Also, new contributors might not be familiar with Cython. A comparison with Numba might also be interesting if you want to go all out ;)    @joshuacwnewton no he means writing code that is closer to C than Python you can use pointers etc in Cython in which case you have to do manual memory allocation, which Python programmers might not be familiar with https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_criterion.pyx#L260 __eou__	User

TP	maintainability has a lot to do with code readability and people who are comfortable with that language. Cython is both less readable (especially when you do C instead of Python in it) and far fewer people know it than Python. It's more of a people issue than a language itself issue. If you go to Shogun's community, they're very comfortable with C++ and in that community C++ is very maintainable. __eou__	User

TP	another point is that while numpy is reasonably straightforward to use and understand, Cython can act in magical ways that are not necessarily intuitive even with some experience (this is even more true for Numba BTW). https://github.com/scikit-learn/scikit-learn/issues/17299 is an example __eou__	User

TP	@amueller, @adrinjalali, and @NicolasHug: I appreciate that you've taken the time to share your thoughts! I now have many great starting points for further reading, so thank you. :) > another point is that while numpy is reasonably straightforward to use and understand, Cython can act in magical ways that are not necessarily intuitive even with some experience  I guess this might be a natural consequence of Numpy being designed for array operations and used as the backbone for Python DS/ML, while Cython can be applied more generally. So, I would imagine that Numpy has stress-tested common use-cases, but that someone might have to recreate themselves using Cython... > (especially when you do C instead of Python in it)  @adrinjalali Just to clarify, is this referring to wrapping external C code? i.e. https://cython.readthedocs.io/en/latest/src/userguide/external_C_code.html Ah! Good to know. Thanks. :) __eou__	User

TP	Ah! This actually came up in the extension I was writing... I had been allocating space using `np.empty` and was curious about how I might do that without Python. Malloc came up in search results but I left it at the time.   Thanks for linking that line, @amueller. :D __eou__	User
TP	well np.empty will create a python object which will be memory managed__eou__	Agent
TP	This conversation probably further emphasizes the above point of cython being less obvious ;)__eou__	Agent

TP	Hehehe exactly. I'm having a "don't know what I don't know" moment here, where I realize there's much more to Cython than I had thought. __eou__	User

TP	you can look at the history of the tree cython files and you'll probably find some fun bugs we fixed over the years https://github.com/scikit-learn/scikit-learn/pull/8002/files __eou__	User

TP	So, if I'm understanding correctly, the "Python version" (so to speak) of Cython memory management would be to create a memview into a Python object that already has its memory managed automatically. But, that Cython LOC you linked up there doesn't seem like it uses memviews at all. Just, dealing directly with a C array?  Although, IIRC from reading, memviews are relatively new to Cython, and it seems like you can use them with C arrays too. > https://github.com/scikit-learn/scikit-learn/pull/8002/files  Oh ho ho, interesting. Thanks for finding that link. :D __eou__	User

TP	yes and we use both ways, usually for historical reasons. I think there used to be a speed difference between raw pointers and memory views but I don't think there is any more __eou__	User
TP	"For historical reasons" -> another point for maintainability, then, hehe :p__eou__	Agent
TP	I hope the benchmarks for the enhancement I'm working on go well, then. I'd love to be able to open a PR and get a code review for what I've written... I imagine I'm making lots of newbie Cython missteps at this stage, heh. :D__eou__	Agent

TP	So, I guess one last question I have is, it seems like for contributors, it might be worth exploring Numpy optimizations just as much as it is Cython optimizations? e.g. replacing for loops in an algorithm with vectorized operations when possible to try and push the pure Python implementation closer to Cython Although, I guess that sometimes leads to less readable Python code, e.g. with fancy np indexing So many tradeoffs... this is all very interesting, hehe. __eou__	User

TP	We do prefer numpy if it's possible, and we prefer like numpy, but if we have to pick, we pick fast numpy over readable numpy, I think __eou__	User

TP	That's good to know... I'll have to revisit the slower python/numpy implementation that I had converted to cython, then, before I can really put this to rest. :) __eou__	User

TP	In the past it happened to me to spend time on a Cython implementation, then realize that it could be as fast or faster with just numpy with a better algorithmic approach. So spending time on numpy optimization as a first step is useful in any case. And of course profiling code (with e.g. snakeviz) is very helpful to make sure that the optimized code is actually the performance bottleneck.. __eou__	User

TP	Ah, I had never heard of snakeviz! I was just using cProfile + pStats. Thank you, @rth. :) __eou__	User

TP	I appreciate the advice, too. I did jump right into Cython, so some planning ahead of time might have saved some trouble! __eou__	User

TP	checkout computer vision pre-trained model   https://github.com/balavenkatesh3322/CV-pretrained-model __eou__	User

TP	Hi! Does anyone have any advice for tackling errors in the azure pipeline? We're failing for linux and windows and I can't find a descriptive error message as to why it's failing other than "Bash exited with code '1'." I'm on a mac. https://github.com/scikit-learn/scikit-learn/pull/17877/checks __eou__	User

TP	It is bit tricky but you need to follow the links to get to this page: https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=20118&view=logs&j=13650d7c-49e8-54f3-0598-c3480d1c1e4f&t=97f2162d-a0c4-54dc-9fce-2fceef7172e3 So to do so: click on "details" on checks frame in the PR page. It will lead you to a page where you have only the "Bash exited with code'1'" most of the time. In this page you can go on the bottom and click on "View more details on Azure Pipelines ". It will redirect you on the azure where you can see the terminal output and there you will get which test is failing and what are the reason usually you can reproduce those locally afterwards __eou__	User

TP	Hello, I understand this is a long shot, but didn't know where else to ask. I am running scikit learn in an online judge environment (DMOJ). Such environment disallows syscalls (for security reasons). The sklearn is installed in conda environment. If I use LogisticRegression().fit(X,y) everything works fine.  However, if I use LogisticRegression(multi_class='multinomial').fit(X,y), the judge stops the process as the LR was apperently trying to make some disallowed syscall. What is the low-level difference between LR and multinomial LR? Is there a workaround for this? __eou__	User

TP	could th eproblem be that the multinomial version tries to spawn another processes even with n_jobs=1?? __eou__	User

TP	could someone help me understand how the regression tree works in sklearn, is it created in the cart, using the standard deviation to create the leaves? I did not find any manual implementation of the cart for regression. __eou__	User

TP	@oplatek it's probably due to some multithreading, probably in BLAS, and I think nowadays you can control that through `threadpoolctl`. @ogrisel or @jeremiedbb know probably much better __eou__	User

TP	@adrinjalali  thats other Ondrej :]. thank you, I already tried   os.environ['MKL_NUM_THREADS'] = '1' os.environ['NUMEXPR_NUM_THREADS'] = '1' os.environ['OMP_NUM_THREADS'] = '1' os.environ['OPENBLAS_NUM_THREADS'] = '1'  or eporting these variables, but with no luck so far __eou__	User

TP	the syscall is sched_setaffinity. is there any way hot to prevent it? __eou__	User

TP	Hi. If I want to do multi-label classifications based on keywords that come from a large-ish vocabulary (~5.000), how do I need to encode the keywords so that an arbitrary amount of keywords can be used as predictors? If I one-hot encode them, how can I pass an arbitrary amount of one-hot encoded vectors to a classifier? Or, is it possible to use the outputs of a multilabelbinarizer as inputs to a classifier? __eou__	User

TP	Hi all, I'm developing a wrapper that wraps Keras models with the Scikit-Learn API. I'd like some input from Scikit-Learn developers on what direction I should take the interface, I'd like to make it as aligned as possible with Scikit-Learn. The package is here: https://github.com/adriangb/scikeras Would any of you able to chat and give some input? __eou__	User

TP	@ldorigo You could  probably`' '.join(keywords)` and then use `CountVectorizer`? @adriangb The design choices in https://github.com/skorch-dev/skorch might be helpful, if you have specific question don't hesitate to ask. __eou__	User

TP	Thanks @rth. I saw how Skorch does things, it's somewhat similar to how SciKeras is doing things now.  An alternative I thought of that I wanted to get feedback on is to have the user define (1) `__init__` and parameters for this model and (2) a `build_keras_model_` function that is expected to return an instance of a Keras `Model`. So a user defined model would look something like: ```python3 class MyModel(KerasClassifier):     def __init__(         self,         random_state=None,     ):         self.random_state = random_state      def build_keras_model(self, X, n_classes_):         model = keras.models.Model         ...         return model ``` Where `build_keras_model` would have access to all of the estimator attributes in addition to variables like `n_classes_` and `X/y` which `KerasClassifier.fit` passes if they exist in the signature of `build_keras_model`. __eou__	User

TP	Hi, everybody. Does this channel have a rule of conduct that I should read before commenting? __eou__	User

TP	The code of conduct (https://www.python.org/psf/conduct/) applies here, and we don't love spam or ads or promotions. Otherwise all good. Happy to answer any specific questions :) __eou__	User

TP	Hello! For the last 2 weeks of the MLH program, I was thinking of helping to address sklearn's backlog of PRs (e.g. by doing code reviews). Does anyone have any recommendations for how best to approach that? (searching through the backlog, triaging PRs, leaving some types of PRs for core maintainers, etc.) __eou__	User

TP	@joshuacwnewton reviewing is always welcome and it's a great way to learn! The things I personally look for during a review are: - backward compatibility and consistency with the current library / ecosystem. - test coverage. Ideally we want tests to be fast as well. - docs: every new feature should be documented at least in the UG. Big ones ideally come with a new example - code clarity and comments: you want your future self to be able to understand why something was done the way it was done  In terms of where to find PRs to review, we have a "waiting for reviewer" tag. Also you can try filtering by PRs which already have one approval: these might be easier to review. Feel free to review old PRs but note that there's a chance these will never be addressed. __eou__	User

TP	By seeing the sgd and sag code in https://github.com/scikit-learn/scikit-learn/sklearn/linear_model I wonder why the solvers do not use the mini-batch implementation? __eou__	User

TP	Hey, everyone, I am new to Scikit-Learn and after reading this [Security & maintainability limitations](https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations) I want to know is their other safer ways to save and load sklearn models. __eou__	User

TP	@kaelgabriel  SGD processes each sample individually so by definition it cannot be a mini-batch approach __eou__	User

TP	@NicolasHug yes, you are right, I was wondering why people chosen SGD over mini-batch SGD (the one that is more common in DL). Just trying to understand. __eou__	User

TP	Hello, everybody. I hope you all's been fine. Has anyone already created a custom classifier with `sample_weight` that can share with me? I'm building my own estimator and i need `sample_weight` to make it work with AdaboostClassifier, tough i have no clue what it is __eou__	User

TP	Hello, everyone. Can anyone clarify to me how `OneVsRestClassifier` performs the classification to decide the output of each input? I am coding a RBF network for classification and the output layer is trained with `n_classes` linear regressors, then the final output is given by applying a softmax function. But `OneVsRestClassifier` could be useful, i just can't confirm if it uses a similar approach. __eou__	User

TP	@rth I have addressed your review comments in #18124 __eou__	User

TP	is it possible to directly assign: VotingClassifier.fit_transform.__doc__ = "Return class labels or probabilities for X for each estimator."  #18150 __eou__	User

TP	hey, someone please review my PR #18185 __eou__	User

TP	The tests are still not passing. You should solve this. __eou__	User

TP	Hello, anyone can explain what the format of the patches in https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html is? https://www.reddit.com/r/scikit_learn/comments/iinxo7/whats_the_format_of_the_patches_returned_by/ are the returned values corner points or what? Why are there duplicates? __eou__	User

TP	The documentation said: `array of shape (n_patches, patch_height, patch_width) or (n_patches, patch_height, patch_width, n_channels)` n_patches is the number of patches so `patches[0]` will be the small patch on the image 2d if this a gray image 3d if this is a color image __eou__	User

TP	That doesn't explain what the contents of the arrays are. __eou__	User

TP	The pixel values __eou__	User

TP	RGB colors? Why are there two of the same? __eou__	User
TP	RGB colors -> n_channels__eou__	Agent
TP	> Why are there two of the same?__eou__	Agent
TP	what do you mean?__eou__	Agent
TP	which things are the same?__eou__	Agent
TP	so why are there two times RGB_1?__eou__	User
TP	it depens of your input__eou__	Agent
TP	if you are passing a numpy array with 3 dimension__eou__	Agent
TP	with shape `(height, width, 3)` (because it is RGB__eou__	Agent
TP	you will get an array of `(n_patches, patch_height, patch_width, 3)`__eou__	Agent
TP	so the arrays returned are "2x2"__eou__	User
TP	because that's patch size__eou__	User
TP	but I don't understand what RGB_1 and RGB_2 refer to?__eou__	User

TP	each patch is a [ [[RGB_1] [RGB_1]] [[RGB_2] [RGB_2]]] __eou__	User

TP	if one takes a 2x2 sample from the image then it should have 4 pixels? so 4 values __eou__	User
TP	but there is no RGB1 in the documentation?__eou__	Agent
TP	> if one takes a 2x2 sample from the image then it should have 4 pixels?__eou__	Agent
TP	I refer to the values in the example https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html__eou__	User
TP	it will be an array `(n_patches, 2, 2, 3)`__eou__	Agent
TP	if this is an RGB image__eou__	Agent
TP	or `(n_patches, 2, 2)` if this is gray-scale__eou__	Agent
TP	so it takes a 2x2 sample from the image and describes its pixel colors?__eou__	User
TP	this is what I assumed__eou__	User
TP	it is just extracting a sub-image__eou__	Agent
TP	with the same number of dimension__eou__	Agent
TP	but what are the 2d subarrays?__eou__	User
TP	[[RGB_1] [RGB_1]] [[RGB_2] [RGB_2]]__eou__	User
TP	> [[RGB_1] [RGB_1]] [[RGB_2] [RGB_2]]__eou__	Agent
TP	do they correspond to columns or rows perhaps?__eou__	User
TP	in 2x2 matrix sense__eou__	User
TP	I am sorry but this not specify anywhere in the doc__eou__	Agent
TP	how to know then what RGB_1 then refers to__eou__	User
TP	it's some pixel color, but which pixel is it?__eou__	User

TP	`[[RGB_1] [RGB_1]] [[RGB_2] [RGB_2]]` This is a (2, 2) numpy array and this is is not RGB they are only gray scale (because you have only one channel) (2, 2, 3) would be 3-channel > but I don't understand why (0,0)=(0,1) and (1,0)=(1,1) it does not it is a coincidence for your specific image __eou__	User

TP	if one uses the python list index convention, then one finds (0,0), (0,1), (1,0), (1,1) __eou__	User
TP	`img[:, :, 0]` will be the red channel__eou__	Agent
TP	but I don't understand why (0,0)=(0,1) and (1,0)=(1,1)__eou__	User
TP	or is it just coincidence in this sample picture__eou__	User
TP	I see__eou__	User
TP	that's a bad example I find then__eou__	User
TP	given in the doc__eou__	User
TP	I don't agree__eou__	Agent
TP	I think a good example would have very distinct values__eou__	User
TP	as in order to not suggest that they're somehow "ordered"__eou__	User
TP	This is just a real life example__eou__	Agent

TP	because those symmetries got me thinking about whether I'm even looking at a 2x2 patch in the actual image or some sklearn abstraction about it __eou__	User
TP	The docstring give a real example__eou__	Agent
TP	and the user guide__eou__	Agent
TP	https://scikit-learn.org/stable/modules/feature_extraction.html#patch-extraction__eou__	Agent
TP	is giving a synthetic example with fake RGB data__eou__	Agent
TP	is there an equivalent function that would spit out "rectangles"?__eou__	User
TP	like the geometries of the pixels__eou__	User
TP	What do you mean rectangle?__eou__	Agent
TP	here you can specify height and width__eou__	Agent
TP	so you can specify the rectangle shape__eou__	Agent
TP	Yes it's possible with known pixel size to infer it__eou__	User
TP	but I was asking whether there exists such function already__eou__	User
TP	I've found this kind of application when one wants to crop pixel areas__eou__	User
TP	then one has to specify the area__eou__	User
TP	I don't understand what you want sorry__eou__	Agent

TP	extract_2d_patches that returns [[[corner1 corner2 length] ... so one knows what area the pixel covers __eou__	User

TP	You will not find in scikit-learn you might want to look at scikit-image thought __eou__	User

TP	Hi, does anyone know more about this efficiency warning? scikit-learn/sklearn/neighbors/_base.py:167: EfficiencyWarning: Precomputed sparse input was not sorted by data.   warnings.warn('Precomputed sparse input was not sorted by data.' so im passing in a csr_matrix, but the csr_matrix is not sorted by data  (it's sorted by indices). i looked at  the code, and it seems this warning is thrown whenever each row in the csr_matrix is not sorted, low->high however,  i can't figure out how to sort the matrix. no matter how i initialize it, the matrix sorts it by index, even if i pass  it in sorted by data does anyone know how to pass in the right data to get rid of this efficiency warning? __eou__	User

TP	Would be nice to have scikit at @pyconindia sprints this year! Link to submit https://t.co/LZ0Hz0fa2I?amp=1 __eou__	User

TP	Suppose,i want to run a grid search  using KNN as te estimator with 5 to 6 k values and i want to see "accuracy" for each K values,is there any way? __eou__	User

TP	Anyone? __eou__	User

TP	`algo_max` Mohammad Masudul Alam (Gitter): have you take results = GridSearchCV, results has a cv_results_ variable.. it should contain what you're looking for. __eou__	User

TP	Hello, can anyone explain why my custom estimator fails when used with `cross_val_score`? The error message is not clear: `ValueError: output_type='binary', but y.shape = (30, 3)` My estimator has its own `score()` function which computes the accuracy __eou__	User

TP	@henrique-voni can you try running check_estimator on your estimator? __eou__	User

TP	Hi all, I was wondering, does scikit support GPU accelerating? __eou__	User

TP	no https://scikit-learn.org/stable/faq.html#will-you-add-gpu-support __eou__	User

TP	Hi all, Ive been wanting to make use of the varimax rotation argument for sklearn.decomposition.FactorAnalysis (version 0.24dev0). So I created a conda environment with this version of sklearn (I have checked conda list, so definitely have the correct version installed), alongside hyperspy.  However when calling FactorAnalysis as follows: skl.decomposition.FactorAnalysis(rotation='varimax') I receive the following error: TypeError: __init__() got an unexpected keyword argument 'rotation'  The documentation for version 0.24 states:  <unconvertable> decomposition.FactorAnalysis now supports the optional argument rotation, which can take the value None, 'varimax' or 'quartimax'. <unconvertable>  Any ideas for what the problem might be? __eou__	User

TP	what does `sklearn.__version__` say? (It works on `master` so you likely don't have the very latest version) __eou__	User

TP	Oh that's odd, it says my current version is 0.23.2  although conda says version is 0.24dev0 __eou__	User

TP	Hi, everybody.  I wouild like to make a system to recognize coins. I have made database with coins text information and a couple images for every coin. My hypothesis is to use dectloscopic approach to recognize coins. May be someone know componen or library that I can use?  Thanks. __eou__	User

TP	`algo_max` Will coins be always nicely centered and rotated in the same direction? Or scattered around the whole image and randomly rotated? Also one coin per image or multiple? `algo_max` Found it.. there was a template matching example for coins in skimage. Is this of any help? https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_template.html __eou__	User

TP	Hello, I would like to talk to the maintainers of scikit-learn about their infrastructure needs: https://scikit-learn.org/dev/about.html#infrastructure-support are you currently all set? Or are you looking for a cloud service provider to offer cloud infrastructures? Ok. Thank you very much :) __eou__	User

TP	We are currently using Microsoft Azure/ CircleCI / Travis for our continuous integration and I think that we are pretty happy with the current services. __eou__	User

TP	we might actually improve the testing for ARM but I think that some work has been done using Travis recently __eou__	User

TP	@remyleone I wouldn't mind having ssh access to ARM nodes from time to time to be able to interactively debug stuff. I the moment we use https://scikit-learn.org/dev/developers/tips.html#building-and-testing-for-the-arm64-platform-on-a-x86-64-machine which is kind of slow. hum I just read: https://www.theregister.com/2020/04/21/scaleway_arm64_cloud_end_of_life/ so probably not interested in this :) __eou__	User

TP	Hi Guys __eou__	User

TP	@ogrisel but Amazon did a huge investment in arm, rihgt? __eou__	User

TP	I just joined open source community and wanted to contribute in some open source project please suggest me some I have hands on experience on python and Machine learning __eou__	User

TP	@anantgna here is the place to start: https://scikit-learn.org/dev/developers/contributing.html . This doc presents how to browse the issue tracker to find issues to tackle. Ideally, it's best to start with a small issue or improvement to an estimator class or function that you use yourself on a regular basis. @amueller  yes but Remy does not work at Amazon :) I remembered that scaleway had an ARM-based cloud and it could have been useful to better support scikit-learn on ARM but it's no longer the case apparently. __eou__	User

TP	@ogrisel thank you __eou__	User

TP	Hello everyone, is there a known dependency issue with Numpy? I am trying to install Scikit-learn in editable mode following the guide on the `contributing to scikit-learn` page but I get a `ModuleNotFoundError` for Numpy during the  build process. The call that triggers this error is `from numpy.distutils.compiler import new_compiler`. I have gotten this on multiple versions of Python 3.8.x and 3.9 virtual environments on a machine running Ubuntu 20.04. Any help trouble-shooting this will be much appreciated. __eou__	User

TP	Hello, everyone! I aim to implement an experimental Artificial Neuron-Glia Network (ANGN) for a  research project. I am not sure why (despite being 13 years in research community) I was not able to find any code for it. Any insights would be highly appreciated. __eou__	User

TP	For those of you unfamiliar with ANGN. Following image is the proposed architecture of a single unit of the network. [![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/geth/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/geth/image.png) __eou__	User

TP	@kushaangupta have you tried with Tensorflow or PyTorch? You should be more able to implement customised topologies for network even at level of single neuron. __eou__	User

TP	HI Team,  I am trying to make a recommendation engine for a food delivery app. I was going through different things like Market Basket Analyisis etc.. Company have huge amount of data for orders from different customers. Could you please help me on where to start? __eou__	User

TP	hello, I am trying to use sklearn linear regression to predict bonuses from enron dataset, and when chaching the score of prediction, I am getting a negative answer, can anyone please tell me the meaning of this __eou__	User

TP	Hello __eou__	User

TP	Hi everyone, does anyone know any good references that discuss the importance of framework toolboxes like scikit-learn for reproducible data science? __eou__	User

TP	Hi! Ive built an interactive git cli - igit. Check it out: https://github.com/kobibarhanin/igit to install: pip install igit __eou__	User

TP	Hello everyone, does anyone know how to solve nonlinear equations of a trained sklearn model?? I mean I have a trained sklearn model `f(x)` and I want to solve equations like `f(x)=c` where c is a constant? __eou__	User

TP	I am trying to use sklearn.decomposition.FastICA on the  audio files from https://cnl.salk.edu/~tewon/Blind/blind_audio.html  by Te-Won Lee. These have  been referenced in Prof. Andrew Ng's old lectures  on Independent component analysis. The unmixing does not seem to work with FastICA. All the code examples that I could find mix simple waveforms like sine, sawtooth with a simple mixing matrix and try to recover it back.   Does any one  have any  insights on this  ? __eou__	User

TP	Hi scikit-learn team! I'll start a new job on 16 November and have some free time until then. I just wanted to let you know that I'll try to add all missing documentation for attributes by then (see #14312). There are only a few undocumented attributes left. So if anyone has time to review my PRs you (hopefully) could have all attributes documented by the end of next week. :) Nicolas and Adrin have already kindly started reviewing the PRs I sent in for this in the last few days. __eou__	User

TP	Thanks @marenwestermann right in time for 0.24! __eou__	User

TP	Hi everyone, I need some help in implementing sklearn.decomposition.SparseCoder in c++. I've been searching it for a while but couldn't find good resource/library so far. Is there any library out there which can solve sparse decomposition in c++? __eou__	User

TP	Have you checked Shogun? They might have something __eou__	User

TP	Not yet, will check __eou__	User

TP	@KartikShrivastava , there is MLPack, https://mlpack.org/doc/mlpack-3.1.0/doxygen/cftutorial.html, I haven't read the complete article, but it does have the phrase "Sparse Matrix" in it. __eou__	User

TP	Yup, I've tried the sparse coding module of that but couldn't get the right results from as, not as good as sklearn, so I thought that it might not be the way to go __eou__	User

TP	Ah, okay! What other solutions have you tried? Would like to know. __eou__	User

TP	I'm also trying nnls solver of eigen library. I've used it in a similar application __eou__	User
TP	That's cool!__eou__	Agent

TP	Took help of mlpack people and it worked using the sparse coder interface :) Just to mention it requires initializing dictionary and a call to encode. I realized that it's very similar to sklearn sparse coder __eou__	User

TP	Hi! I am new __eou__	User

TP	hey __eou__	User

TP	Hi! I want to ask here before bothering people on GitHub: Is there a reason for the submoduled import structure of sklearn? I often find myself importing only to have to search for the submodule name of a thing I already know. For example, I might need "PCA", but I won't remember that it is under "decomposition". Is it worth having each collection of functions and classes under their own module? Granted, there are many submodules, each containing many more things to import, but surely maintaining a list of imports at the top level of sklearn wouldn't be too big of a hurdle, so that `from sklearn import PCA` would be possible. It would be a huge usability boost in my humble opinion. But I might also be dumb. Am I missing something obvious here? Cheers! __eou__	User

TP	I'm not a big fan of heavy import statements, and having all those classes and functions as a top level import would make "import sklearn" very very heavy (since it would load every user-facing API immediately). Also, sometimes there may be conflicts between modules. Other than that, I find the code more readable and understandable when  imports are from the sub-modules instead of a top level import. __eou__	User

TP	Oh that's true! I don't really have a grasp on the full scope of sklearn, so I can imagine the import could be huge. I'm quite satisfied with this being the main explanation. Thanks a bunch! __eou__	User

TP	Hi, guys. Just a quick question. __eou__	User

TP	I'm seriously considering investing my time to solving issues on Scikit, so as to work my way towards becoming a maintainer of the project itself. I really think I would learn a lot in the process,  and something I really like doing - contributing. For now, I think the best way for me is to send PRs and try to solve issues as much as I can. should I keep something specific in mind while doing so, is what I wanted to ask. thanks. :D __eou__	User

TP	> For now, I think the best way for me is to send PRs and try to solve issues as much as I can. yes. Just be sure to read the contributing guideline to be sure to not miss anything :) __eou__	User

TP	I've gone through it 5-6 times now. xD Even went ahead with the asv thing, but still have to really understand how to really use it. __eou__	User

TP	Hi, we're trying to decide how to handle a Keras / sklearn API integration issue over at https://github.com/adriangb/scikeras/issues/131, I wanted to see if anyone in this group can chime in. The jist of it is how to support passing validation data to the estimators. Skorch solves this by having a an `__init__` parameter, but as is pointed out in that issue that means that calling with a different `X, y` would give you different results (since the validation losses would be totally off). __eou__	User

TP	I think that right now we cannot do that. I vaguely recall a discussion where we discuss something linked with something close to the callback mechanism __eou__	User

TP	Yeah I could not find any sklearn estimator that accepts extra data as a fit kwarg Or as an `__init__` param for that matter __eou__	User
TP	yep. I see that we have a recent feature request https://github.com/scikit-learn/scikit-learn/issues/18748__eou__	Agent
TP	I will try to find the discussion back__eou__	Agent
TP	We also put it in the road map: https://scikit-learn.org/dev/roadmap.html__eou__	Agent
TP	The issue will be to find the right API.__eou__	Agent
TP	Yeah I do see they have `eval_set`: `(list or None, optional (default=None)) <unconvertable> A list of (X, y) tuple pairs to use as validation sets`__eou__	User

TP	Yep. If I had to pick between the options that I currently see available, my gut feeling would be to make it a fit kwarg, but that's just a knee jerk reaction which is why I'm asking here :laughing: __eou__	User

TP	We don't have that in scikit-learn and our validation data is always a subset of X-y as passed to fit. IIRC LightGBM adds new arguments to `fit` __eou__	User

TP	hey, I need help with plotting rbf kernel's output in svm.SVC. Where do I go for help? __eou__	User

TP	[![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/Y9W4/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/Y9W4/image.png)  Any material on rbf kernel will help. I am looking for the above function. __eou__	User

TP	@salih.four_gitlab are you looking for this, https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html? __eou__	User

TP	Actually, I tried that module yesterday. For a 100x1 array X as input, the RBF.__call__(X) is returning a 100x100 array. I was looking for a 100x1 array that can be used to create a new axis to the existing 1d data resulting in 2d data. But, by reading the docs I could not make sense of the returned 100x100 array. That is why I came here. > @salih.four_gitlab are you looking for this, https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html? @Rubix982 Do well :thumbsup: . __eou__	User

TP	Oh, okay, got your problem.  May or may not be able to check this out. Busy with University right now. Exams on the way next week. @salih.four_gitlab __eou__	User

TP	Thank you. __eou__	User

TP	@salih.four_gitlab for the RBF(X) you will always get a square matrix because it's the formula above between all rows in X. If you have a single vector, it should be 1x100. If you want to have the kernels with an existing set, it should be RBF(X, Y) though I'm not sure what you're trying to achieve. That is indeed for the GP. we also have a rbf_kernel function (which does the same thing) if you just want to compute the kernel values __eou__	User

TP	Hi. I am looking at the [Classification example](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html). Do we only support `Dense` layer for neural networks for now? I was hoping to try some `AutoML` features of `scikit-learn` to create some convolution networks. __eou__	User

TP	convolution neural networks are indeed not supported and out of the scope of scikit-learn. Please use a library dedicated to deep learning for this. If you need scikit-learn API compatibility, you can try: https://github.com/skorch-dev/skorch or https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn . __eou__	User

TP	Thanks. I think I will stick with tensorflow for now. __eou__	User

TP	hi guys i'm  new to the contributors community and i want to start contributing to scikit-learn, but really i don't know how to start, and to be more specific when i check an issue and feel like i want to start working on it i don't know how to communicate with the developers there, like what should i say or shouldn't and what are the best practices or the ethics i should know before start working, any advices please ?. thank you __eou__	User

TP	@ichisadashioko if you want to use Keras + sklearn check out SciKeras: https://github.com/adriangb/scikeras __eou__	User

TP	@adriangb I was wondering if you could mention what is the benefit of scikeras compared to the KerasClassifier and KerasRegressor? __eou__	User

TP	It's a pretty long list. Some Scikit-Learn specific ones are: __eou__	User

TP	(was on mobile) * Allows pickling of estimators, allow ensembles and gridsearch to work properly * Allows you to use multi-input/output models to sklearn (especially important since some pre-trained models have "extra" outputs) * Working random_state (random state in Keras is pretty complicated) * Adds `partial_fit` and `predict_proba` * Your models can dynamically adjust the Keras input/output shape to the data (previously, that had to be hardcoded) * Support for `class_weights` in the same format as other sklearn estimators (including `class_weights="auto"`). * A lot of compatibility improvements, stuff like implementing `n_features_in_` and `_estimator_type` * Allows you to grid-search parameters for optimizers and losses via [routed params](https://scikeras.readthedocs.io/en/latest/advanced.html#routed-parameters). __eou__	User

TP	Some not Sklearn specific advantages: * Is actively maintained (the TF team may [deprecate](https://github.com/tensorflow/tensorflow/pull/37201#pullrequestreview-391650001) the wrappers in TF) * Has actual [documentation](https://scikeras.readthedocs.io/en/latest/index.html) I should write this stuff down somewhere in a docs page :sweat_smile: __eou__	User

TP	OK so this us super useful :). Thanks for maintaining such wrapper Basically, it would be great to have the info (regarding the deprecation and full-compatibility) in the landing page.  When reading it I was kinda currious about it :) __eou__	User

TP	I hope it's useful! And yep totally agreed, I opened myself an issue to add it to our docs / README.md __eou__	User

TP	we could also add this to our "related projects" __eou__	User

TP	It would be great if you did! Probably could also update `keras Deep Learning library capable of running on top of either TensorFlow or Theano` -> `Keras, the official high-level Deep Learning API for TensorFlow` __eou__	User

TP	hello, I'm wondering if anyone has ever used ElasticNet with both a `sample_weight` argument and a precomputed gram matrix before. I'm trying a simple experiment to run a fit with and without the gram matrix passed in and they end up with different coefficients. I'm using the default setting of 'cyclic' for the coordinate descent so there shouldn't be any randomness.  without gram: ``` en = ElasticNet(alpha=0.001) en.fit(X, y, sample_weight=swgt) ```  with gram: ``` # pre-center data to avoid warning about gram matrix being tossed away # when data is centered in _pre_fit X_cent = X - np.average(X, axis=0, weights=swgt) gram_mat = X_cent.T @ X_cent  en_gram = ElasticNet(alpha=0.001, precompute=gram_mat) en_gram.fit(X_cent, y, sample_weight=swgt) ```  am I doing something dumb here? __eou__	User

TP	maybe this is related to data normalization but I am not sure. Could you please open and issue with a full minimal reproducer (e.g. using data generated on the fly with `numpy.random` for instance? __eou__	User

TP	Will do. __eou__	User

TP	Please help test scikit-learn 0.24.0rc1 and help us spread the news: https://twitter.com/scikit_learn/status/1334562221498753026 __eou__	User

TP	regarding my earlier question, i managed to figure it out, a bit tricky but makes sense after closely reading through some of the preprocessing code in _base.py. See example below, perhaps it could be adapted in to a unit test?  ``` from sklearn.linear_model import ElasticNet from sklearn.datasets import make_regression from numpy.testing import assert_almost_equal import numpy as np  X, y = make_regression(n_samples=int(1e5), noise=0.5)  # random lognormal weight vector. weights = np.random.lognormal(size=y.shape)  en = ElasticNet(alpha=0.01, fit_intercept=True, normalize=False, precompute=False) en.fit(X, y, sample_weight=weights)  X_c = (X - np.average(X, axis=0, weights=weights)) # row wise multiply X_r = X_c * np.sqrt(weights)[:, np.newaxis]  en_precompute = ElasticNet(alpha=0.01, fit_intercept=True, normalize=False, precompute=X_r.T@X_r) en_precompute.fit(X_c, y, sample_weight=weights)  assert_almost_equal(en.coef_, en_precompute.coef_) ``` __eou__	User

TP	Thanks for the follow-up, indeed is not very intuitive. Maybe it's normal, maybe the documentation of the precompute argument could explain better how to deal with the sample_weight. Maybe this could be part of a new tests / doc improvements related to https://github.com/scikit-learn/scikit-learn/pull/17785. __eou__	User

TP	@ogrisel I'm happy to try to make the change myself - do you think it could be done as a PR to the docs for ElasticNet (and the other linear models that take precompute as an argument to their constructors and accept a sample_weight arg to `fit`) __eou__	User

TP	@amidvidy maybe open an issue that states that the documentation is not clear enough on how to use `sample_weight` an `precompute` together. Maybe we could add a new dedicated section to the user guide and reference that section from the docstring for the precompute parameters of those models. And we probably also need a new test to check that this will never be unintentionally broken by other changes in the code base. __eou__	User

TP	sounds good - will do! __eou__	User

TP	Byo.ai an intelligent assistant to make people carbon neutral/positive. Anyone with experience with one or more general purpose programming languages including but not limited to: Python, Java, C/C++ (also Pytorch,) feel free to send your CV to work@byo.ai (passion for the environment, clean technologies and artificial intelligence is a plus!) __eou__	User

TP	hey @ogrisel , I put up a PR for the issue we discussed. thanks for the help. __eou__	User

TP	Hi all, I implemented a new feature for gaussian mixture models and I'm wondering whether it would be useful to have it integrated in scikit-learn. Should I open an issue to discuss this or can it be discussed here? In brief, I implemented the mixture entropy estimators (lower and upper bound) introduced in this paper https://arxiv.org/pdf/1706.02419.pdf __eou__	User

TP	hi @giuliolovisotto , please have a look at our inclusion criteria: https://scikit-learn.org/dev/faq.html?highlight=inclusion%20criteria#what-are-the-inclusion-criteria-for-new-algorithms __eou__	User

TP	Hi Adrin, thanks for that, so if I check, I get:     * [x] 3 years since publication: (2017)   * [ ] 200+ citations: no, 57 at the moment.   * [?] wide use and usefulness: this is a bit arbitrary.   * [?] clear cut improvement: for some settings (in particular with larger no. of features and no. of GMM components) using those bounds gives more efficient and more accurate entropy estimation than using a Monte Carlo sampling approach (which can be done with the GMM.score method).  By seeing this would you say I should open an issue to discuss this on github or just leave it? __eou__	User

TP	Colleagues: looking for best practice tips to get logging output from sklearn, in particular from KNN clustering where Im having trouble figuring out how to use `verbose=1` (parameter doesnt seem to exist on `sklearn.neighbors.KNeighborsRegressor` nor on the `.fit()` method)  Have reviewed lots of issues on the subject but unclear current status of logging context managers or similar. @glemaitre thank you for the prompt response. I dont see a way to get *any* output from KNeighborsRegressor.  Am I missing something? __eou__	User
TP	There is no such parameter for a `KNeighborsRegressor`__eou__	Agent
TP	Indeed the regressor does not anything during `fit` apart of storing the dataset__eou__	Agent

TP	@ijstokes We don't have any logging. The verbose is only printing on the stdout. We are currently looking at improving this part with a real logging system __eou__	User

TP	@giuliolovisotto I'd probably open an issue to discuss it. My gut feeling is that it doesn't pass the inclusion criteria, but it'd be nice to also here what other maintainers think on the issue tracker __eou__	User

TP	Hey folks <unconvertable> Want to talk with you about pipelines <unconvertable> What is the use case they were created to cover? In most of the examples, people groups processing "branches" by feature types (num/cat) Is this the main use case pipelines were designed for? __eou__	User

TP	What I was trying to do, but got frustrated is convert my Pandas-based data preprocessing into sklearn pipelines: __eou__	User

TP	``` # just copied some parts of the notebook to illustrate   # 1. data cleaning like this for feature in (     'PoolQC',      'FireplaceQu',      'Alley',      'Fence',      'MiscFeature',      'BsmtQual',      'BsmtCond',      'BsmtExposure',      'BsmtFinType1',      'BsmtFinType2',     'GarageType',      'GarageFinish',      'GarageQual',      'GarageCond',     'BsmtQual',      'BsmtCond',      'BsmtExposure',      'BsmtFinType1',      'BsmtFinType2',     'MasVnrType', ):     train_df[feature] = train_df[feature].fillna('None')     test_df[feature] = test_df[feature].fillna('None')     full_df[feature] = full_df[feature].fillna('None')  for dataframe in [train_df, test_df, full_df]:     dataframe['MSZoning'] = dataframe.groupby(['Neighborhood'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))     dataframe['MSSubClass'] = dataframe.groupby(['HouseStyle'])['MSSubClass'].transform(lambda x: x.fillna(x.mode()[0]))     dataframe['LotFrontage'] = dataframe.groupby(['Neighborhood', 'MSSubClass'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))     dataframe['Functional'] = dataframe['Functional'].fillna('Typ')  # 2. Some ordinal encoding   ordinal_feature_mapping = {     'ExterQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},      'ExterCond': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},     'BsmtQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'BsmtCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},     'BsmtFinType2': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},     'HeatingQC': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},     'KitchenQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},     'FireplaceQu': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},     'GarageQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'GarageCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'PoolQC': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'Fence': {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4},     'PavedDrive': {'N': 0, 'P': 1, 'Y': 2},     'CentralAir': {'N': 0, 'Y': 1},     'Alley': {'None': 0, 'Pave': 1, 'Grvl': 2},     'Street': {'Pave': 0, 'Grvl': 1},     'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},     'Functional': {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7} }  non_ordinal_cat_features = list(set(cat_features) - set(ordinal_feature_mapping.keys()))  for cat_feature in non_ordinal_cat_features:     train_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(train_df[cat_feature])     test_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(test_df[cat_feature])     full_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(full_df[cat_feature])  for ordinal_feature, feature_mapping in ordinal_feature_mapping.items():     train_df[ordinal_feature + 'Enc'] = train_df[ordinal_feature].map(feature_mapping)     test_df[ordinal_feature + 'Enc'] = test_df[ordinal_feature].map(feature_mapping)     full_df[ordinal_feature + 'Enc'] = full_df[ordinal_feature].map(feature_mapping)  # 3. Excessive feature engineering  for dataframe in [train_df, test_df, full_df]:     dataframe['HasFireplace'] = dataframe['Fireplaces'].apply(lambda x: int(x > 0))     dataframe['HouseAge'] = dataframe['YrSold'].astype('int') - dataframe['YearBuilt'].astype('int')      dataframe['TotalBathrooms'] = (dataframe['FullBath'] + (0.5 * dataframe['HalfBath']) +                                 dataframe['BsmtFullBath'] + (0.5 * dataframe['BsmtHalfBath']))      dataframe['OverallHouseQC'] = dataframe['OverallQual'] + dataframe['OverallCond']     dataframe['IsPavedDrive'] = (dataframe['PavedDrive'] == 'Y') * 1      dataframe['IsNeighborhoodElite'] = (dataframe['Neighborhood'].isin(['NridgHt', 'CollgeCr', 'Crawfor', 'StoreBr', 'Timber'])) * 1    # bunch of other features ``` These three stages are the main in my data processing flow. So ideally I would like to process data in the same order in the pipeline as well. Then it would be converted naturally to pipeline definition So I would imagine a pipeline definition like this: ``` Pipeline([     ('missing_value_imputing', ColumnTransformer([...])),     ('feature_engineering', FeatureUnion([...])),     ('feature_transforming', ColumnTransformer([...])), ]) ``` __eou__	User

TP	However, this doesn't work, because missing_value_imputing step would return me numpy array which is hard to work with on the following stages __eou__	User

TP	In the same time, my data processing has  constrains (feature_engineering step requires all values in place (missing_value_imputing) and feature_transforming requires all set of features (feature_engineering)). There are also operations I could apply on a multiple columns (and would love to do) like "None" constant imputing or ordinal encoding  and single column specific actions like MSZoning imputing Please let me know if all of this makes any sense __eou__	User

TP	With that being said, I'm wondering what is the cleanest way to define sklearn pipeline for this task? __eou__	User

TP	Any thoughts on this? Just added an MCVE example to an old Q on SO. https://stackoverflow.com/a/65366293/6046019 [![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/Peus/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/Peus/image.png) __eou__	User

TP	scikit-learn 0.24.0 is out! https://twitter.com/scikit_learn/status/1341429250696630276 __eou__	User

TP	Are there any plans to allow estimators to customize their expected precision for your estimator checks? The checks are great, but I've found that estimators I have fail because of precision issues (like a single element of a 60 element array being off by 5e-8). It would be nice to be able to at least have some rough measure of precision as an estimator attribute that the tests could then use to adapt their behavior. __eou__	User

TP	Hey people Is this place active? Is there a real time communication platform for scikit learn? Thanks!! __eou__	User

TP	@AdityaPujara23 ask here, somebody will reply you if possible. __eou__	User

TP	Cool thanks! __eou__	User

TP	Hi there! I'm developing an online ensemble learning thingy, and I just wanted to praise you guys for the quality of your code. It's always a pleasure to read! (Even though I always weep looking at `BaseEstimator` and its cool `get/set_params`, wondering whether I should rewrite something similar or re-use your code -- and *wanting* to do neither :P ) __eou__	User

TP	Thanks :) __eou__	User

TP	do any classifiers in scikit-learn handle categorical features directly? I feel there were some PRs about this a long time ago __eou__	User

TP	You're in luck @lesshaste https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_24_0.html#native-support-for-categorical-features-in-histgradientboosting-estimators __eou__	User

TP	@NicolasHug  that's great. I will try it in a few minutes __eou__	User

TP	Hi guys! I wrote a document which is "Hello Kaggle". I hope to get some feedback about the document such as a typo, grammar error, wrong information, etc. https://github.com/stevekwon211/Hello-Kaggle thank you :) __eou__	User

TP	@NicolasHug  it works which is great. hmm. except I can't get it to work with categorical features __eou__	User

TP	what am I doing wrong? https://bpa.st/IXRQ why is it trying to convert a string to a float? __eou__	User

TP	@lesshaste categorical features are supported but the estimators themselves only understand integer values in [0, 255]. You'll need to encode hte categorical features with an OrdinalEncoder before passing them to the predictor. You can take a look at https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_categorical.html#gradient-boosting-estimator-with-native-categorical-support for an example In your case since it seems that you only have categorical features, you can bypass the ColumnTransformer and just  do `clf  = make_pipeline(OrdinalEncoder(), HistGradientBoostingClassifier())` __eou__	User

TP	@NicolasHug  ah ok. And then I list which features are categorical using categorical_features=cat_features? As in clf = make_pipeline(OrdinalEncoder(), HistGradientBoostingClassifier(categorical_features=cat_features)) ? @NicolasHug  I should say that catboost has some very clever tricks for categorical features that would be awesome if included in scikit-learn the default for HistGradientBoostingClassifier seems to  be no categorical features unless I misunderstood it @NicolasHug  thanks.. they actually have two tricks https://arxiv.org/pdf/1706.09516.pdf  explains it better than I could but essentially the first and more most important trick is Ordered boosting   (section 4.2) and the second is Feature combinations __eou__	User

TP	> @NicolasHug  ah ok. And then I list which features are categorical using categorical_features=cat_features? As in clf = make_pipeline(OrdinalEncoder(), HistGradientBoostingClassifier(categorical_features=cat_features)) ?  Yes, or you can use `clf.set_params(histgradientboostingclassifier__categorical_features=...)` after `clf` is defined. And yes, by default all features are treated as continuous, as is the case for the overwhelming majority of estimators (only a few tranformers like OneHotEncoder expect categorical features by default). Regarding CatBoost:  I think  they do target encoding for high-cardinality categorical features. This is in the works https://github.com/scikit-learn/scikit-learn/pull/17323 __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/issues/19092 Hey, __eou__	User

TP	Can someone help me understand if I can start working on this? __eou__	User

TP	how exactly does prediction work for gradient boosted trees? In a random forest you just get a prob from every tree and average them I think. Is this different for gradient boosted trees? __eou__	User

TP	@lesshaste in gradient boosting, one directly optimizes a loss function. For binary classification in sickit-learn this is the log loss, much like logistic regression. What the trees predict is the log-odds ratio (decision_function()) and it's passed into a sigmoid to get a probability (predict_proba()) this is a self plug but this might help: http://nicolas-hug.com/blog/around_gradient_boosting __eou__	User

TP	@NicolasHug  thank you. In low level terms, what happens at prediction time? Is something computed for every tree separately and then averaged? __eou__	User

TP	decision_function is the sum of all the tree values, not the average see https://nbviewer.jupyter.org/github/NicolasHug/nicolashug.github.io/blob/master/assets/gradient_boosting_descent/GradientBoosting.ipynb __eou__	User

TP	I've cross-compiled scikit-learn for an armv7h system and I'm receiving the following error when testing, does anyone have any suggestions for where the issue may lie? ``` File "sklearn/cluster/_dbscan_inner.pyx", line 40, in sklearn.cluster._dbscan_inner.dbscan_inner ValueError: Buffer dtype mismatch, expected 'npy_intp' but got 'long long'  ``` It looks like it could be something 32bit vs 64bit related but from what I can see the `setup.py` build system just imports from `numpy` which I would expect to handle this all properly so I'm wondering if this isn't a cross compile issue and is maybe just an assumption made in the code that the argument will be 64bit? __eou__	User

TP	@NicolasHug thanks __eou__	User

TP	@jackmitch this looks like a bug indeed. I don't have a quick fix from the tip of my head but could you please open an issue on github to avoid forgetting about it? It's probably a temporary buffer that is assigned with an `np.int` instead of `np.intp` in our Cython code or wrapping Python code. Here is a similar bug we had in the past (a long time ago): https://github.com/scikit-learn/scikit-learn/commit/627c564faec36c783788b7488b4cf19a2535916c Ideally a PR would be appreciated because none of us will be able to reproduce it because we do not have access to such hardware. __eou__	User

TP	Also if you want to submit a PR for our doc that documents how you cross-compile scikit-learn for armv7h, that could be useful knowledge to share. For instance it could be a new section at the end of this page: https://scikit-learn.org/dev/developers/advanced_installation.html __eou__	User

TP	How to train neural network and what i the best design of it if I have discrete/continual input and continual output? __eou__	User

TP	Hi, I'm wondering why this test is checking for float64 specifically (as opposed `dtype.kind == "f"`)? https://github.com/scikit-learn/scikit-learn/blob/2218ec46227c92301ac6837c4a8ae9b8dc5d3960/sklearn/utils/estimator_checks.py#L1735 __eou__	User

TP	@adriangb no real reason. This is typically the kind of check that's probably a bit too strict for non-internal estimators. We're working on making the `check_estimator` suite less restrictive (https://github.com/scikit-learn/scikit-learn/issues/13969). BTW @glemaitre , any progress on that? (I don't mean to rush you by any means, just wondering  :) ) __eou__	User

TP	Great, I left a comment on that PR for the record. Would there be any chance that tolerances might be relaxed as well? There are several checks that check rtol/atol to <1e-7, which makes it very flaky when the estimator internally uses <=32 bit precision. __eou__	User

TP	@urosn do you mean like this: ```python estimator.fit(X, [1, 2, 3, 4.5]) estimator.predict(X)  # [1.1, 2.3, 2.7, 4.4] ``` ? __eou__	User

TP	Hi. I want to learn scikit-learn from scratch. Do we have an official book or guide? If not, I will be making notes. Can I contribute one? Thank you __eou__	User

TP	Hi @D3V4N5H, there is plenty on resources in the docs. You can start with the ["Getting Started" pages](https://scikit-learn.org/stable/getting_started.html). __eou__	User

TP	@NicolasHug I was a focus on the release recently but I think that we are going to branch today so it would be one of the task pretty soon. __eou__	User

TP	I am building a classifier and have 10,000 examples of the positive class. I can generate any amount from the negative class. Is there a standard way to take advantage of the unlimited amount of data from the negative class? __eou__	User

TP	@byo-ai  pay? @lesteve  :) __eou__	User

TP	a really simple question. Currently I do clf.score(Xordinal_test, y_test). If I want to use balanced accuracy instead, is there a similar one line solution? __eou__	User

TP	`balanced_accuracy_score(clf.predict(X_ordinal_test), y_test)` This is a one liner solution if you omit the import :) We don't allow to switch the default score in `clf.score` to be more explicit so you need to get the prediction and call the score function __eou__	User

TP	You also have the possibility to use the scorer API but this is not a one liner __eou__	User
TP	thanks!__eou__	Agent
TP	``` scorer = get_scorer("balanced_accuracy") scorer(clf, X, y) ```__eou__	User

TP	Actually you could `get_scorer("balanced_acccuracy")(clf, X, y)` but I think that we don't head toward readable code :) __eou__	User

TP	@glemaitre I like it. Thank you I have a different more general question. I am doing binary classification. I would like to maximize the number of items in the positive class that get a probability higher than any probability from the negative class. Does this correspond to a known loss function? let me edit it to get rid of the word score... __eou__	User

TP	https://github.com/scikit-learn/scikit-learn/pull/16525 You might want this things maybe __eou__	User

TP	Basically, this is tuning the threshold of the argmax when doing the predict from the predict_proba Otherwise, `sample_weigth` or `class_weigth` will allow you to play on the inner loss while training __eou__	User

TP	@glemaitre thank you. I haven't fully understood how to use your suggestions for my problem but I will have a think maybe it could go on scikit-learn discussions as well :) __eou__	User
TP	I think so__eou__	Agent
TP	I might have misunderstood the use-case (a small example with specific number might help :))__eou__	Agent

TP	I can give one in about 90 minutes __eou__	User

TP	> maybe it could go on scikit-learn discussions as well :)  +1. As mentioned in https://github.com/scikit-learn/scikit-learn/discussions/19220#discussioncomment-298015 my feeling (and probably others feeling) is that gitter is not the best place for Q&A. I guess a reasonable approach is to create a discussion and then ping on gitter if you feel you have not received an answer after some time __eou__	User

TP	@lesteve thanks. I do like the interactive nature of gitter to a) improve the question and/or b) realise I shouldn't have asked it in the first place :) __eou__	User

TP	Yeah I agree the threshold about "what is OK to ask on gitter" is not very clear. I would favour an approach as I mention above discussion + ping on gitter after some time. It is not as much interactive but it is a better investment of answerer time since the question + answer will be findable by googling (contrary to gitter) __eou__	User
TP	makes sense__eou__	Agent

TP	Gitter should come with a feature that you cannot scroll-up in your discussion feed because this is a bit what happens in reality :) __eou__	User
TP	:)__eou__	Agent

TP	This isn't a full example but hopefully it will help clarify. Say my positive class items get 0.1, 0.3, 0.7, 0.9 from predict_proba and my negative class items get 0.01, 0.2, 0.2, 0.5. Then two of the positive class items get a prob (0.7, 0.9) larger than the largest prob (0.5) from the negative class. @glemaitre does that make it any clearer? __eou__	User

TP	So the cutoff classifier intend to change the probability from 0.5 to another threshold such that you can for instance the maximum number of predictions of the positive label __eou__	User

TP	@glemaitre  yes. But the cutoff is a function of the probs that the negative class items are given my example of 0.5 above wasn't a great choice :) argh... I hate how easy it is to be confusing. @lesteve do you think my post is clear now? __eou__	User

TP	Oh you want to reinforce your learning step I see __eou__	User

TP	In some way, I could think about a boosting strategy as AdaBoost, but instead of learning new learner favoring misclassified samples, you want to favor specific samples from the positive class. __eou__	User

TP	I don't know if there is something in active learning allowing such stuff But I am not knowing so much in this area __eou__	User

TP	thanks. I was going to post on discussions but I can't think of a suitable title :) __eou__	User

TP	"Reinforce sample weight for online learning" __eou__	User

TP	posted __eou__	User

TP	with the link there is even more chances that someone answers :wink: https://github.com/scikit-learn/scikit-learn/discussions/19239 __eou__	User
TP	@lesteve thanks :)__eou__	Agent

TP	I guess it's equivalent to maximizing true positives when you have 0 false positives...? now I am tempted to try one of the options mentioned in the discussions. Now really sure which one though __eou__	User

TP	*Not __eou__	User

TP	Can any of the classifiers in scikit learn directly optimize auc as the loss function? __eou__	User

TP	Hello all! I'm not a library developer, I'm a student developer and user of scikit-learn. Is there any work going on for scikit-learn to use Apple's ML Compute frameworks so that ML calculations can be accelerated by the 16-core neural engine in the recent Apple Silicon macs? __eou__	User

TP	Hi All. Had a question about a possible discrepancy between user guide and autogenerated docs for LASSO Linear model. Auto-gen docs (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) says that:  ``` (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 ```  Is minimised.  However, the user guide seems to to imply that it's `||Xw - y||` rather than `||y-Xw||` (https://scikit-learn.org/stable/modules/linear_model.html#lasso).  `y-Xw` makes more sense to me. Am I reading something incorrectly, or is the user guide wrong? __eou__	User

TP	@JosephRedfern both are the same. ||-x|| = ||x|| __eou__	User

TP	I think the common way would be `y - y_hat = y - Xw` but they lead to the same as @jeremiedbb just mentioned :) __eou__	User

TP	oh boy, what a brain fart! apologies, I should have thought before posting. __eou__	User

TP	I am trying HistGradientBoostingClassifier for multiclass classification. It seems to stop too early. I.e. I get [22/200] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 4.36169, val loss: 2.27454, in 0.114s [23/200] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 5.42246, val loss: 2.60417, in 0.113s [24/200] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 5.19449, val loss: 3.30153, in 0.113s Fit 624 trees in 2.951 s, (19300 total leaves) you can get a much smaller loss using catboost for example but it is much faster than catboost for multiclass classification __eou__	User

TP	hmm.. is multiclass classification meant to work yet with HistGradientBoostingClassifier ? __eou__	User

TP	I'm trying to modify the sklearn transformer interface for transformers that need to transform `X`, `y` and `sample_weight` together, i.e. the entire dataset. This is the signature I came up with that allows chaining these transformers with a `Pipeline`, I'm wondering if anyone has any better ideas? I really don't like having a `dummy` parameter.  ```python3     class DatasetTransformer(BaseEstimator, TransformerMixin):         def fit(self, data, dummy=None) -> "DatasetTransformer":             X, y, sample_weight = data             ...             return self          def transform(self, data):             X, y, sample_weight = data             ...             return (X, y, sample_weight) ``` One alternative I see is to not have the `dummy` parameter, and instead specify `"passthrough"` as the last estimator in pipelines. I think this may be better? `dummy` is only there because it would be confusing to have a `y` parameter when `y` is part of `data`. __eou__	User

TP	Byo.ai an intelligent assistant to make people carbon neutral/positive. Anyone with experience with one or more general purpose programming languages including but not limited to: Python, Java, C/C++ (also Pytorch,) feel free to send your CV to work@byo.ai (equity only) - passion for the environment, clean technologies and artificial intelligence is a plus! __eou__	User

TP	Hi, is there any way to make it so that sklearn doesn't normalize the columns in PCA? __eou__	User

TP	I don't think that we have a parameter to do that. __eou__	User

TP	It would be weird to not normalize the columns since it is an assumption of PCA if I am not wrong __eou__	User

TP	Not centering will make that you will get an intercept while transforming __eou__	User

TP	Uhm now that I think about it, there is the TruncatedSVD https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html This should do the job __eou__	User

TP	Hi! I have a quick question about the capabilities of Scikit-Learn's Pipeline (also asked in the other channel): Currently my data looks like the following 0 20 1 23 2 25 3 29 4 24 ... Where the index is a step in time, and the value is the value associated with that timestep. I am transforming it into 0 [20, 23, 25] [29, 24] 1 [29, 24, 24] [23, 22] 2 [23, 22, 26] [23, 25] ... By running a sort of window function over it. The index then is sample number, the first array is the input features, and the second array is the target.  If I input my data values as X into a pipeline, how to address the following: my custom transformer generates y from the input X, but the pipeline requires a y to begin with my custom transformer changes the length of X, which I think the pipeline also complains about.  Especially the latter is really important to me, as I want to be able to change the amount of features in X. Other options for example would be: 0 [20, 23] [25, 29] 1 [25, 29] [24, 29] ... or even 0 [20, 23, 25, 29] [24, 29] 1 [24, 29, 24, 24] [23, 22] etc.  Is there a standard solution for this, or should I write a wrapper for this? Thanks in advance! __eou__	User

TP	Our pipelines are unfortunately no meant to change the number of samples or the target. So I guess this kind of preprocessing will have to happen outside of the pipeline it-self... If the goal of the pipeline is to do parameter tuning for size of the time-window of the feature extraction preprocessor in a Grid/RandomizedSearchCV (for instance), then I think it's better to switch to a parameter tuner with a more flexible / less opinionated API such as https://optuna.org/ for instance. > Is there a specific reason why that is prevented from being changed?  Our transformers where never meant to change y. This is an early design decision that is really hard to change now (without breaking users code). And changing the number of samples without changing y is meaningless. __eou__	User

TP	Is there a specific reason why that is prevented from being changed? Thank you! __eou__	User

TP	That's a shame, because I can imagine resampling / different sampling strategies / generating additional samples with differing noise levels would be a very useful thing to put in a pipeline so it can be gridsearched. I'll have a look at optuna, thanks for your answers! __eou__	User

TP	But's really trick to get right, especially with the metrics. For instance when dealing with imbalanced classification problems, you want to resample at training time but not at test  validation / prediction / score time.  Imbalanced Learn has a custom pipeline and a custom API for resampling transformers with the fit_resample method: https://imbalanced-learn.org/  However this is not what you want for your use case: you want to perform the same transformation both at fit and predict time. It's hard to express all those use cases in a simple and intuitive unified API that would not led to users to shoot themselves in the foot. And there the backward compat constraints to take into account which makes it really hard to make our API evolve in that regard. @KylevdLangemheen would you mind reposting this question to github discussions: https://github.com/scikit-learn/scikit-learn/discussions   I think this would make it more googleable and linkable for others that have related issues. __eou__	User

TP	Very understandable! I'm just sad I couldn't get an easy way out :P  I'll repost it there when I can, likely later today. __eou__	User

TP	Hello all I am new to this chatroom  i have a question regarding dataset imbalance, can someone help me out ? __eou__	User

TP	@SyedMuhamadYasir  You can freely ask your question and you might get an answer __eou__	User

TP	@glemaitre  thank you, i am going to ask right now __eou__	User

TP	I have a dataset which is for binary classification ( or at least we are approaching it from a binary classification perspective )  There are a total of 2.5 million rows, with label 0 belonging to around 220000 (2.2 million) rows and label 1 belonging to around 321000 (0.3 million) rows , there are around 45 features.  The imbalance approaches a ratio of around 1 : 7  My problem is very straightforward, even WITHOUT any data preprocessing if i try to classify the data  the classification algorithms, no matter what parameters are set, give around 99% in ALL performance metrics ( accuracy, precision, recall, f1 score etc )  This would probably suggest a bad case of overfitting but i am not sure, feel free to explain and add your opinion regarding what could be the reason  I tried to visualize the graph using TSNE and saw that the entire data is shaped like an ellipse and there is heavy overlap between both the labels. This means that (1) data is badly imbalanced (2) data is badly overlapped , i highly doubt i can use anomaly detection there as all the 'anomalies' (label 1) are sitting close with the 'normal' (label 0) data  any suggestions on how i should proceed ? __eou__	User

TP	I am not sure that I would give to much weight with what you observe with TSNE. While accuracy will be boost for sure, the precision and recall will be good measure with imbalance problem. You can use the `balanced_accuracy_score` instead of `accuracy_score` as a baseline. I would say that one potential error would be to forget to set `pos_label` in precision/recall if it is not `1` (that does not seem to be the case). You can probably look at the entire confusion matrix to be sure that the stats seem correct Then if you still get good results by properly cross-validated your experiment, everything should be fine. __eou__	User

TP	thank you, i see some really good points in your answer than i can experiment with however, it is important to tell you the context of the problem so you can understand what i am trying to do exactly I am trying to do a Feature Selection / Feature Reduction task __eou__	User

TP	since the original dataset, without any preprocessing and with ALL features, gives near perfect results, it will annul the validity of using any type of Feature Reduction techniques i will try out what you said in your answer, but i thought that it would be better to let you know the entire context and the actual reason for why exactly we need 'bad' results before applying any feature reduction > It might still allow you to fit and predict faster and this probably what feature selection is best at.  that .. is actually a very good point, thanks ! @glemaitre  Je vous remercie  :) __eou__	User

TP	It might still allow you to fit and predict faster and this probably what feature selection is best at. __eou__	User

TP	hi, quick question, is there a preprocessor in sci-kit that allows me to split a feature into two? __eou__	User

TP	ok, there's column transformer, I can use that __eou__	User

TP	has anyone started a PR on dirichlet calibration of probabilities? I couldn't find it if they have as in this paper https://arxiv.org/abs/1910.12656 __eou__	User

TP	That's rather a new paper, and I'm not sure if it passes our inclusion criteria. __eou__	User

TP	I am getting some unrelated errors (azure pipeline stack trace) for a PR I just pushed. Anyone who can check and let me know how I can fix? https://github.com/scikit-learn/scikit-learn/pull/19387 __eou__	User

TP	@adrinjalali  yes. I was just wondering if anyone thought it was interesting. __eou__	User

TP	Why column transformer convert datatype to objects after calling fit_transform? __eou__	User

TP	I have a general question: If for my dataset a kneighbor classifier works well (compared to e.g. SVC and Random Forest),  are there other classifiers that might also work equally well? __eou__	User

TP	@benny Stuff based on distances then __eou__	User

TP	can you give some examples? __eou__	User

TP	Hi everyone, I am not certain if this is the right place to ask. I am a first-time contributor. I love the library and it has helped me immensely in my studies so far. I was hoping to work on this issue as my first issue: https://github.com/scikit-learn/scikit-learn/issues/18338  As far as I can understand, this issue requires that the documentation be updated, does that indicate the docstring within the function definition only, or is that referring to another piece of documentation?  One of the commentators on the issue also mentions ensuring there are tests that break if this documentation doesn't exist, how do I go about doing that effectively? __eou__	User

TP	> I have a general question: If for my dataset a kneighbor classifier works well (compared to e.g. SVC and Random Forest),  are there other classifiers that might also work equally well?  I think it will depend on the data set.  It also depends on how you are pre-processing your data. So kinda hard to say without knowing more. __eou__	User

TP	when I do OrdinalEncoder on my matrix X how can I make the mapping the same for each column? currently it is different if there is one new value in a column that doesn't occur in another column __eou__	User

TP	Hello Happy scikit-learners ! I need some help please. I want to serve an onnx model.  Input = 144 columns ( medical records, some categoricals, some  not ).  Output = classification.  Pipeline = StandardScaler + LabelEncoder + LightGBM.  I am stuck with the LabelEncoder. Any example of such configuration somewhere ? Google was not my friend. I was able to produce an onnx model when bypassing the LabelEncoder... but I need it and want to avoid 1HE because LightGBM performs much better without 1HE.  Anyone ? __eou__	User

TP	 @citron You probably want OneHotEncoder not the LabelEncoder Also tree based models it's better to use OrdinalEncoder instead for categorical features __eou__	User

TP	> Also tree based models it's better to use OrdinalEncoder instead for categorical features  I'm not sure that's true, using OE will make the trees treat categories as ordered values, but they're not. Native categorical support (as in LightGBM) properly treats categories as un-ordered and can yield the same splits with less tree depth __eou__	User

TP	Yes, you are right. I guess I'm too used to scikit-learn tree based models not having native categorical support ) __eou__	User

TP	I agree with @NicolasHug in theory, but in practice the difference with `OrdinalEncoder` (with tuned hyperparams) is typically negligible ;) __eou__	User

TP	@citron Using `OrdinalEncoder` is probably the pragmatic solution. `OneHotEncoder` is only efficient if you use sparse output which are currently not supported by ONNX as far as I know. __eou__	User

TP	@citron: what's the issue with LabelEncoder and ONNX? (I'm the main author of sklearn-onnx). __eou__	User

TP	@citron also you said "Pipeline = StandardScaler + LabelEncoder + LightGBM." but I assume you use  a column transformer to separate to only scale the numerical features and encode the categorical feature separately: https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data  BTW, StandardScaling the numerical features if often useless for tree-based models in general, and even more so for implementations such as LightGBM than bin the features. __eou__	User

TP	Bonjour @xadupre , @ogrisel, @rthy:matrix.org, @NicolasHug . Yes I do use a ColumnTransformer. Maybe I should better express my needs.  The training set is made of 300000 rows.  Colums types are either floating points, integers ( and sadly Pandas does not provide the R Dataframe handling of N/A ), booleans, categories or list of categories. For instance, some category columns may have 2 or 10 numerical categories, some only have "string" categories, some have a list of medicaments or a list of pathologies.  I have tried plenty of frameworks and among them, lightGBM was the best. Now, as I need to export the model and the pipeline in ONNX/ONNX-ML format, I need to wrap lightGBM in something to keep the pipeline around. @ogrisel Yes, no problem with pure int columns. __eou__	User

TP	pandas 1.0 and later has support for explicit missing values in integer columns: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html scikit-learn however will convert this to a float anyway (but no big deal). __eou__	User

TP	For the categorical columns, try to use OrdinalEncoder. In 0.24+ we have better support for unknown categories at test time:  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html  Although I am not sure that sklearn-onnx has replicated that feature yet. __eou__	User
TP	@ogrisel maybe @xadupre knows ?__eou__	Agent

TP	If you have specific problems exporting a pipeline with OrdinalEncoder to onnx, better report the exact error message with a simple reproduction case to https://github.com/onnx/sklearn-onnx __eou__	User

TP	I wrote this example about converting a pipeline including a lightgbm model in a scikit-learn pipeline: http://onnx.ai/sklearn-onnx/auto_tutorial/plot_gexternal_lightgbm.html. __eou__	User

TP	@xadupre Thanks! The binder link at the end of the page has a problem. In fact, that's the example I started with. Works fine without labelEncoder. I forgot to tell an important thing : I do use FLAML to select the best hyperparameters and thus the best model. __eou__	User

TP	I'll investigate the issue with LabelEncoder then. What is the error you get? __eou__	User

TP	I think it would be a good idea to encourage creating a Github Discussion (rather than gitter) for anything else than simple questions/answers: https://github.com/scikit-learn/scikit-learn/discussions/new. gitter is not properly indexed by search engines so it is not a great use of time for people who answer questions. I agree that "simple qestion/answer" does not have a very-well defined boundary but in the case of @citron's questions I think we have crossed this boundary a long time ago ... __eou__	User

TP	@lesteve I understand and agree. __eou__	User

TP	@citron then if you find the time maybe create a Github Discussion and post the link in the gitter so that the discussion can continue in the Github Discussion? __eou__	User

TP	Hello guys! Me and my friends are looking to tackle some open issues on scikit-learn soon. We're very new so I would love a high level overview of the architecture. Can anyone help or point to some resources? __eou__	User

TP	Have a look at https://scikit-learn.org/stable/developers/contributing.html for a getting starting guide. __eou__	User

TP	Hello ( I am back and I will try not to flood the your screen ) __eou__	User

TP	Using Scikit-learn 0.24.1 and sklearn-onnx 1.7.0, I try to export a pipeline embedding an HistGradientBoostingClassifier. The data contains only StandardScaled floating point features.  convert_sklearn prompts an error : 'numpy.bool' object has no attribute 'encode'StringTensorTypeStringTensorType Any idea please ? __eou__	User

TP	Hello,  I'm trygin to use `SimpleImputer(strategy='most_frequent')` in Pipeline on dataframe with ~ 1.5 M samples but I take a lot of time  Is it normal ? If so, are there some alternatives to solve this issue ?  ``` def vectorizer_df(input_data, categorical_cols, numerical_cols):   categorical_pipe = Pipeline([      ('imputer', SimpleImputer(strategy='most_frequent'))  ])   numerical_pipe = Pipeline([      ('imputer', SimpleImputer(strategy='median')),      ('bucketizer', KBinsDiscretizer(n_bins=10, strategy='uniform', encode='ordinal'))  # ordinal  ])   preprocessing = ColumnTransformer(      [('cat', categorical_pipe, categorical_cols),       ('num', numerical_pipe, numerical_cols)       ])   vectorizer_pipeline = Pipeline([      ('vectorize', preprocessing)  ])   return vectorizer_pipeline.fit_transform(input_data) ```  Thanks __eou__	User

TP	@razou which version of scikit-learn are you using? We merged the following improvement in 0.24 -> https://github.com/scikit-learn/scikit-learn/pull/18987 that make it efficient to work with string while it was not really possible before because it was too slow __eou__	User

TP	Thanks @glemaitre  I'm using ``` scikit-learn==0.22.2.post1 sklearn-crfsuite==0.3.6 ``` __eou__	User

TP	yep so this should be the reason. You can update to 0.24 via conda-forge or PyPI and it should work better __eou__	User

TP	Thanks @glemaitre  for your answers (y) __eou__	User

TP	Could anyone help me with this error? https://www.reddit.com/r/learnpython/comments/ibk04a/linalgerror_svd_did_not_converge_in_linear_least/ __eou__	User

TP	Hello, can you tell me why GridSearchCV .best_score_ is worse than when I evaluate the same dataset with .score() ? shouldn't those be the same? __eou__	User

TP	or is it because .best_score_ is only evaluated on the test-cross validation split? __eou__	User

TP	Hello, when I run pytest -Werror::RuntimeWarning  sklearn/ensemble/tests/test_iforest.py I get an 'ImportError while loading confest' error.  How can I bypass this? __eou__	User

TP	Could you provide the traceback @icky254 __eou__	User

TP	@benny:michael-enders.com By checking the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), `grid_search.best_score_` is mean cross-validated score of the best_estimator. So the `best_estimator.score` is called on each fold (supposing that you do `KFold`) and averaged. __eou__	User

TP	> when I evaluate the same dataset with .score()  It depends what you mean. You might do something wrong here. Calling a single time `score` will not give you an estimate of the variability of your model. If you reuse `best_estimator_` (with best parameter set), you need to evaluate on some left-out data . You might want to look the following example regarding why you need to nest grid-search and cross-validation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py __eou__	User

TP	[![Screenshot from 2021-02-25 19-06-35.png](https://files.gitter.im/541a528c163965c9bc2053e1/3Pzi/thumb/Screenshot-from-2021-02-25-19-06-35.png)](https://files.gitter.im/541a528c163965c9bc2053e1/3Pzi/Screenshot-from-2021-02-25-19-06-35.png) @glemaitre __eou__	User

TP	As the message mentioned, it seems that scikit-learn was not build Be sure to have activated the environment where you installed and built scikit-learn FYI: https://scikit-learn.org/stable/developers/advanced_installation.html#building-from-source basically to install and build: `pip install --verbose --no-build-isolation --editable .` could work if the previous message steps where already done __eou__	User

TP	Apologies for the PSA style post, but in a collaboration with Microsoft, we just published a blog post on using compilation techniques to accelerate SKLearn models for production, it's also another route to run models on GPUs (and potentially other accelerators) as well! Check it out if interested: https://medium.com/octoml/compiling-classical-ml-for-up-to-30x-performance-gains-and-hardware-portability-2aef760af694 __eou__	User

TP	@glemaitre, solved. Thank you. __eou__	User

TP	@binarybana: Interesting, you may want to send it on the mailing list (or in github discussions). I don't think that many people people check gitter.. __eou__	User

TP	@binarybana really nice! __eou__	User

TP	Hey there!  I am trying to develop a new python module for video chatting with a bot. (You may also collaborate)  Repo: https://github.com/avaish1409/VideoChatBot/ Gitter: https://gitter.im/VideoChatBot/community  Downloads: 429 (in first 2 days of launch)  ''' pip install VideoChatBot ''' or ''' pip install https://files.pythonhosted.org/packages/5b/cc/9dbb790525fe3daa8f0822e60eec38dfea8af5e33af0334dc66b4a022ac4/VideoChatBot-0.0.2.tar.gz '''  Do contribute on github, let's build it together!  Plz star the repository if you like it .. you can also contribute on github <unconvertable> __eou__	User

TP	Hello, everyone. I am new here. I have found that the user guide of sklearn 0.24 is well-structured. Is there any way for me to download a pdf version of it? I could only find those of the older versions. Thank you. __eou__	User

TP	@lester1027 I think we stopped generating pdf versions and simply provide the html files, so the docs look like just as on the website. Generating pdfs involved Latex and it was difficult to maintain (random failures every now and then, etc) if you really want pdfs you can try converting the docs to pdf with pandoc __eou__	User

TP	I have one hot encoded feature vectors which I am using for multiclass classification.  If in my training set there is a feature which is always 0, what happens in testing when it comes across one that is a 1 for the feature? __eou__	User

TP	@lesshaste: By default OHE would error. You would need to set `handle_unknown='ignore'` to ignore it. __eou__	User

TP	@rthy:matrix.org thanks.  Do you know if that would be the same for logistic regression for example? can sklearn.metrics.pairwise be made to work for hamming or levenshtein distance? __eou__	User

TP	what are good options for supervised categorical encoding when the target is also categorical?  target encoding looks attractive but doesn't really make sense when the target is categorical __eou__	User

TP	Hi everyone, I am a developer who's trying to test multiple models in different frameworks. I just want to know if this idea makes sense - I want to create a single sklearn pipeline script for testing various models (all mapped to the sklearn-keras interface or using skorch). But from what I understand is that models are not just plain classifiers or regressors. The models are a combination and sklearn pipeline doesn't apparently support it. Is my understanding correct? Is this a futile effort? Can someone please help me out with this issue. Thank you. If there's an alternative, please let me know. I'm talking about object detection models. I really like the pipeline method/interface and would like to extend my models to match the same .fit, .predict interface __eou__	User

TP	HistGradientBoostingClassifier seems to have no n_jobs argument.  Is there any way set the number of threads/cores? __eou__	User

TP	You should use the  OMP_NUM_THREADS env variable https://scikit-learn.org/stable/computing/parallelism.html#openmp-based-parallelism __eou__	User

TP	@NicolasHug  thank you. Is anyone working on adding n_jobs for this classifier? it would make it inline with the other classifiers and can it be done in the script itself? __eou__	User

TP	it's been discussed but we ended up staying with the status quo https://github.com/scikit-learn/scikit-learn/issues/14265 __eou__	User

TP	@NicolasHug  that is surprising. I normally agree with all the decisions of scikit learn devs __eou__	User

TP	I have two questions about HistGradientBoostingClassifier.  a) When using early stopping do you end up with the "best model" according to the validation loss or the most recent one after it stops? b) Is the validation set chosen by  HistGradientBoostingClassifier chosen at random and is it the same set for every iteration of the training? maybe these should be asked on github as an issue? __eou__	User

TP	a) there's no notion of best model. early stopping stops the training process  if the score hasn't improved by more than `tol` in the last  `n_iter_no_change` iterations. The score can be the loss or an arbitrary scorer and it can be computed on the training set or on the validation set b) yes and yes __eou__	User

TP	Thank you.  Maybe best model couid be a good addition? __eou__	User

TP	I'm not sure what you mean by best model. There's no notion of best model, only one model is built. If you mean "model with the lowest training loss" that's basically the model at the last iteration,  under the assumption that the training loss is always supposed to decrease (unless your learning rate becomes too high). If you mean "model with the lowest training loss that doesn't make the validation loss go up", that's what early stopping is supposed to give you (and it's preferable to the former) __eou__	User

TP	@NicolasHug  let's say the latter example you gave. The problem is that with early stopping you wait some number of iteration before deciding to stop. So the final iteration is not the best. That's why catboost for example has a use_best parameter. It is common in early stopping for the final valudation loss to be higher than the loss a few epochs before.  How long you wait to see if the loss will start going down again is sometimes called "patience" . I think that's what pytorch lightning calls it __eou__	User

TP	does using the fit function on a fitted model replace the fitted model, or update it? __eou__	User

TP	I'm trying to use a Lasso in a machine learning context, and I want to keep updating it with each test run I do obviously, I could in theory, take the model, and train it with the results of the particular test run, then merge the coefficents with the last model myself, but it would be better if I could avoid that I will use that in the project my team is doing. Thanks for helping __eou__	User

TP	`fit` make a full training from scratch `partial_fit` is doing an update __eou__	User

TP	@glemaitre thank you.  what kinds of models is partial_fit available for? hrm... it seems like all of the ones with that method only are for classification, not for regressed output. __eou__	User

TP	SGD estimator is one of them __eou__	User
TP	oh perfect__eou__	Agent

TP	I wrote a [short blog post](https://www.bodyworkml.com/posts/scikit-learn-meet-production) that might be of interest to the community - deploying Scikit-Learn models to Kuberentes using [Bodywork](https://github.com/bodywork-ml/bodywork-core) (an open source deployment tool that I have developed). __eou__	User

TP	I would like to ask how to join two pereprocessors I did saved in two separate files. I have one file with a model and another with prerpocessor (doing average and filling NaN boxes)  then another file with a model (same estimator) and forth file is preprocessor for second model? I would like to merge these four files into two (one joint model and one joint estimator). __eou__	User

TP	I have transformer1.file, model1.file, transformer2.file and model2.file (same estimator in model1 and model2). I would like to have tranformer_composite.file and model_composite.file. __eou__	User

TP	Yo __eou__	User

TP	Hi everyone, I'm a graduate student at Cornell and I had a paper (https://dl.acm.org/doi/abs/10.1145/3429445) published a while ago in correcting the bias of feature importance in tree-based methods. Impurity-based feature importances can be misleading for high cardinality features (many unique values), which is already noted in the docstring of feature_importances_ in RandomForest. I just opened a new pull request #20058 to implement a new feature importance measurement based on out-of-bag samples, which is guaranteed to remove this bias.  I think this feature is going to be useful for scikit-learn users. Any comments or suggestions will be helpful! __eou__	User

TP	Hi! What week scheduled for release scikit-learn 1.0? __eou__	User

TP	There is no specific week scheduled @PetrovKP , but we try to release every 6 months  and the previous one was released in december __eou__	User

TP	So perfectly June but I think that for 1.0 we want a couple of feature to be inside the release so we might be delayed. __eou__	User

TP	Hi, my name is Zoe Prieto. I am currently working on neutron and photon transport problems. I have some questions and maybe one of you can help me. __eou__	User

TP	Sure, don't hesitate to write them here. __eou__	User

TP	Thanks! I have a list of particles with their caracteristics (position, direction, energy and stadistic weight). This variables are correlated. I want to fit that curves and later sample new particles. I want to know how scikit-learn keep the correlation. I'm sorry if it is a beginner question. And thanks again. __eou__	User

TP	Well you need to define what are your feature variables and the target variable. So for instance you could try to predict the position from all the other variables. Correlations would be taken into account depending on the model. So for instance if your model is linear the target would be a linear combination of the features. If you do have a known analytical relation between your variables it might be easier and more reliable to use scipy.optimize or scipy.odr to find the coefficients you would like to learn though. __eou__	User

TP	Thanks for your answer, I forgot to mention that I fit my data with KDE and I sample new particles from this model. Is this model keeping the correlation between the different variables? Or it assumes the variables are independent from each other? Thanks again! __eou__	User

TP	Hi everyone, I'm trying to create a streamlit app but face the following message when I run `streamlit run <file.py>`: `Make this Notebook Trusted to load map: File -> Trust Notebook`. I Googled this issue, and even after making Chrome my default browser, nothing changes. Please help. __eou__	User

TP	Hello! in `sklearn.decomposition.PCA`, how do I tell it which column represents the label?  For example, I have a dataframe with the following columns:  `feature_0 feature_1 feature_2 label`  How do I tell PCA that `label` is the dependent variable? __eou__	User

TP	@nyanpasu:matrix.org you don't, PCA is unsupervised and doesn't take the labels as input, only the features. __eou__	User

TP	Hello Is the `accuracy_at_k` (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html#sklearn.metrics.top_k_accuracy_score) an implementation of the `hit ratio at k`(https://www.researchgate.net/publication/344486356_Hit_ratio_An_Evaluation_Metric_for_Hashtag_Recommendation) Thanks __eou__	User

TP	Hi folks, I am a master's student in CS and I have a question for you.  I am working on a multi-class text classification problem, and I am using scikit-learn to implement my solution. I want to predict for a paragraph x if x belongs to one out of seven categories of information. I already implemented my solution using your library, but I am not confident if the steps I am following are correct or not, or if I am missing something. Could you please take a look at the image below and give your opinion? If this is not the right place for this kind of question, please let me know. Thank you in advance for your contribution!  ![Image](https://i.imgur.com/ZSr7jAT.png) __eou__	User

TP	Hi, I want to start working on the Sci-kit learn bug fixes. Anyone who is already working can I team up with you? __eou__	User

TP	Hi all! We're working on a generic implementation of a discrete time survival model for random forests. Similar to [this](https://link.springer.com/article/10.1007/s10618-020-00682-z) and [this](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0863-0). Basically, the idea is to split on hazard curves which are a bit like the class probabilities of regular classification random forests but then stratified per duration since inception of an observation. We want to use scikit-learn for a base. Is anyone here familiar with the random forest code? Also tips for a good PR are very welcome. __eou__	User

TP	hi __eou__	User

TP	I have only one question, please!!! __eou__	User

TP	What would people recommend for clustering strings (e.g. english words) of the same length? __eou__	User

TP	or is this better off at github discuss? __eou__	User

TP	It really depends on the kind of data that you have. If you have a corpus of documents LDA would be one way to get cluster/topics https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html You could also try pre-trained embeddings like word2vec and the likes  why do they have to be of the same length? __eou__	User

TP	Hello ... Greetings to all..!!! I will participate in the Sprint on Saturday June 26..!!! __eou__	User

TP	Welcome @asnramos ! __eou__	User

TP	Hello all . I'm also participating in the sprint next saturday,i'm  excited to be able to help checking and fixing an issue! __eou__	User

TP	Hello, how can I join the sprint? __eou__	User

TP	@um_duaa123_twitter sure ask __eou__	User

TP	Thanks __eou__	User

TP	why isn't the website working? __eou__	User

TP	works for me __eou__	User

TP	Now it also works for me. Had tried with two different networks yesterday... didn't work that time.. anyway, I wanted to ask what version of LAPACK (libblas.so) does sklearn use (assuming it uses it. If not, what blas library is used)? __eou__	User

TP	If I have a neural network classifier I can easily simulate data from the probability distribution implied by the classifier. Can this be done for any of the classifiers in scikit learn? __eou__	User

TP	scikit-learn custom compilation: Is it possible to pass custom gcc flags during scikit-learn build as described here  https://scikit-learn.org/stable/developers/advanced_installation.html the documentation uses pip install. But since the pyx files get compiled in a C file first before finally compiled into a SO file, I wondered if it was possible to pass custom gcc flags in the intermediate stage __eou__	User

TP	Is there any option to build scikit-learn with DEBUG symbols? __eou__	User

TP	Yes, you can pass `CFLAGS` env variable https://stackoverflow.com/a/10867041/1791279 __eou__	User

TP	thanks @rthy:matrix.org __eou__	User

TP	Hello, I am wondering why this PR (https://github.com/scikit-learn/scikit-learn/pull/18758) doesn't show up at the top here: https://github.com/scikit-learn/scikit-learn/pulls  Is it because I had submitted it a long time ago, but my recent changes are considered updates? __eou__	User

TP	I think that PRs are ordered by PR numbers by default in github __eou__	User
TP	Oh, wow, that's interesting.  Not what I would have expected.__eou__	Agent

TP	However, you have the option of "Sort by recently updated". This would actually be a good default while reviewing :) __eou__	User

TP	Yes, that's what I was looking for (and expecting as a default). Thanks! __eou__	User

TP	The "Refined GitHub" browser extension makes it the default among other improvements. __eou__	User

TP	I see that the binary_tree.pxi passes the value num_samples in the _recursive_build() procedure as an argument. The num_samples is calculated using self.data_arr.shape[0] whereas _recursive_build() expects an ITYPE_t argument. ITYPE_t is defined as np.intp_t which I assume is only 32 bit signed integer. So how is the binary tree built in cases when n_samples is greater than this value - let's say 10million data points? In default python, this would've been handled by increasing the data size to long long implicitly. Does cython take care of it? I don't see any methods to take care of such scenario in the scikit-learn implementation code. @rthy:matrix.org  thanks for pointing it out. A silly mistake on my part. __eou__	User

TP	Except for that idx_end - idx_start < 2 will be true in this case (due to signed integer overflow?) and the node 0 will be made a leaf node. But this is an unexpected behaviour, right? __eou__	User

TP	@HarshVardhanKumar: maximum value for int32 is ~2e9 not 2e6. So probably no one has tried using it with more than 2 billion samples.  Not sure it's really an issue for the near future. +1 to check for that overflow though. __eou__	User

TP	Hello, I ran `pytest sklearn` and see the following.  Is this ok, or is there something wrong with my build: ``` SKIPPED [16] sklearn/utils/tests/test_validation.py:1374: could not import 'pandas': No module named 'pandas' ==== 355 failed, 19625 passed, 1443 skipped, 117 xfailed, 37 xpassed, 3371 warnings in 2380.84s (0:39:40) ==== (sklearn-dev)  ``` OK, it works now.  Thanks Thomas Fan. __eou__	User

TP	Hello, I was working on this issue https://github.com/scikit-learn/scikit-learn/issues/20435 However I was not able to find the file to contribute the documentation into, can someone please help me with that __eou__	User

TP	@yashasvimisra2798 the documentation is generated from the docstrings in the `.py` files where those classes are implemented. You should look for classes which inherit that class, and find the relevant part of the docstring there. __eou__	User

TP	Thanks, @adrinjalali I will look into it. __eou__	User

TP	Does Scikit support regression parameters with multidimensional data structure, e.g., 3-dimensional point data? I would like to perform a regression to predict the position of a 3-dimensional point (x,y,z) using other known 3-dimensional points while weighting by inverse-distance. I have a small sample of dependents Y comprised (xi, yi, zi) and a complete set of independents X1, X2, X3, ... each comprised of (xi, yi, zi). I would like to test a simple model Y = X1 + X2 + ... <unconvertable> appreciated __eou__	User

TP	you probably want to look at the following: https://scikit-learn.org/stable/modules/multiclass.html#multioutput-regression __eou__	User

TP	Can I use a confusion  matrix to see the accuracy of SVR(support vector regression) ? or is it only for classification ? __eou__	User

TP	confusion matrix and derived metrics are only for classification look at the regression metrics instead __eou__	User

TP	Hi all, how do you manage your changelog? Any useful tools or files that you could point me too? __eou__	User

TP	@mloning most PRs come with a changelog entry, e.g. https://github.com/scikit-learn/scikit-learn/pull/20727/files Then we have scripts to process / check it which are described in https://scikit-learn.org/stable/developers/maintainer.html (we call the change log a "What's new") __eou__	User

TP	@NicolasHug thanks - that's very helpful! __eou__	User

TP	A very good evening everyone!! I am trying to setup scikit-learn project into my development env but encountered with an error when I try to run "pip install --verbose --no-build-isolation --editable ." Can someone help me to fix this error? __eou__	User

TP	@RAVANv2 can only help if you tell us what the issue is :) __eou__	User

TP	Hi @adrinjalali, Actually, I solve it on my own :) __eou__	User

TP	Hi all, I made one PR on issue #20754 but it failing the linting job of black and showing the error "##[error]Bash exited with code '1'". Can anyone tell me where I am doing wrong? I am totally a newbie in open-source but I m really enjoying :) __eou__	User

TP	You can see how to fix that here: https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist You need to run black on your code __eou__	User

TP	Hello Everyone, I am a newbie to open source with basic knowledge about scikit-learn and want to contribute to this repository. Can anyone tell me how should I start? I have basic knowledge about generating my first PR and solving good first issues. Now I want to fix bugs and do some code contributions but I am unable to understand the issues. Can anyone guide me? __eou__	User

TP	Hi @KiranHipparagi , thanks for wanting to contribute! Have you read the contributor's guide? I would suggest you look for issues tagged "good first issue". Many of those are multi-part issues where you can pick just a part to work on see https://scikit-learn.org/dev/developers/contributing.html and https://github.com/scikit-learn/scikit-learn/labels/good%20first%20issue for good first issues this one might be good to get started: https://github.com/scikit-learn/scikit-learn/issues/20308 __eou__	User

TP	Hello i am starting out this one project and some of the parts are really complicated . And i am happy to learn during the complicated parts. But i feel i need more people. If anyone is interested in working together and learning together feel free to dm me my discord is Aura#5549 __eou__	User

TP	Deleted this message from the /dev/ channel.  Copying and pasting here: >I am Bhavya Bhardwaj (https://github.com/Bhavya1705). I am a student of Electronics and Communication at Amrita Vishwa Vidyapeetham, India. My thanks to you and the team for sklearn. I have been try to make some contributions to the scikit-learn library - scikit-learn/scikit-learn#5516. I have made the code, and the necessary changes to the init file and test files, in addition to the _classification file. This is the links to my commits - scikit-learn/scikit-learn#20861, as you will see, there are many mistakes, that I have made, Any help that you can render to me would be much appreciated and would be a wonderful learning experience. Thank You __eou__	User

TP	@reshamas Thank You, I have managed to solve the issue. __eou__	User

TP	Hi. I am trying to develop my own Estimator based on TransformerMixin and BaseEstimator. To make sure I am doing things right I have added a test to my project : ``` import MyEstimator from sklearn.utils.estimator_checks import check_estimator def test () :      me = MyEstimator(**params)     check_estimator(me) ``` If I run the test, I get the following error message :  ``` AssertionError: The error message should contain one of the following patterns:                0 feature\(s\) \(shape=\(\d*, 0\)\) while a minimum of \d* is required. ```  I don't understand how I am supposed to take care of that. I am even more surprise because my fit_transorm method uses self._validate_data at the beginning. I would expect that function to take care of case like these. Could someone help me with that issue ? __eou__	User

TP	@adriente: Could you please open a Github issue with full traceback and tag me (@rth) in? __eou__	User

TP	Hello! I opened this feature request a week ago. Just bumping it here in case it got lost: https://github.com/scikit-learn/scikit-learn/issues/20890 __eou__	User

TP	@freddyaboulton I can assure you it is not lost :) I saw it but I did not look at it yet because we are kind of working on releasing 1.0. Once the release done, you might get some attention from core-devs __eou__	User

TP	Morning all __eou__	User

TP	I was kind of curious . Ive been dragging my feet using torch and I was wondering does this lib offer anything over torch ? Maybe this is better suited for the low level scientist trying to learn theory ? Or is it just an alternative ? Omg Im trapped __eou__	User

TP	@makingglitches pytorch and scikit-learn operate at different levels of abstractions but simply put in terms of scope, pytorch is for deep-learning while scikit-learn is for the rest of ML that's *not* deep learning. So one might be better suited than the other, depending on the theory that you're interested in. __eou__	User

TP	[FEATURE REQUEST] Add GitHub Organisation README profile  Just found out this new GitHub feature on GitHub org.  Like this: https://twitter.com/vinzvinci/status/1438033675313025024 __eou__	User

TP	When a PR generates html doc, where to see it? __eou__	User

TP	@lobpcg do you have a PR number? __eou__	User

TP	 @adrinjalali #21148 __eou__	User

TP	When the documentation CIs are finished there will be a "ci/circleci: doc artifact <unconvertable> Link to 0/doc/_changed.html " line You can clicked on "Details" It will redirect to an HTML page where you will have hyperlinks to each documentation page that has been generated in PRs we only generate documentation pages where there is a modification __eou__	User

TP	@glemaitre great, found it - thanks! __eou__	User

TP	Morning all I want to know if scikit-learn 1.1 will be released in late 2021? __eou__	User

TP	There will be minor releases this year (1.0.1 for instance), but the next major release will be next year. __eou__	User

TP	 Hello everyone, we are having a live community office hour on discord. Feel free to join to discuss your PRs! https://discord.gg/YBdN45kD __eou__	User

TP	:boom: :+1: __eou__	User

TP	has anyone tried to implement GAM's via sklearn pipeline's before? __eou__	User

TP	Hello I'm using scikit-learn `0.22.2.post1` and getting the following error  ` AttributeError: '_CalibratedClassifier' object has no attribute 'classes_' `when I try to use `predict_proba`on calibrated classifier  Do you you know if this issue is related to the scikit-learn's version ? Thanks __eou__	User

TP	Hi, can I post a call for participants in an interview study on open source projects here? If any mod wants more details via DM first, then I'm happy to oblige :) __eou__	User

TP	Is cohen kappa score and balanced accuracy score supposed to work w/ multiclass labels?  I have a 3-class classification and I'm trying to use `cross_validate`, but it returns nans for all my scores. I tested the problem by running `cross_val_score` on all scores individually and isolated it to those 2 metrics.  X = (100, 5) y = (100, 3) clf is a Random Forest Classifier ``` from sklearn.model_selection import cross_val_score  cross_val_score(clf, X, y, cv=5, scoring='balanced_accuracy') ``` __eou__	User

TP	@razou could you please paste a fully reproducible piece of code? __eou__	User

TP	@NoahWoehler_twitter we get quite a bit of these requests these days (which is a good thing, shows people are looking into issues). But it would help people decide if they want to spend time on it, if you give a tiny bit of intro on what it is. Also, feel free to send an email to the mailing list with that information if you want to reach more people. __eou__	User

TP	Sure, I wasn't sure whether this falls under advertising. We are looking for open source contributors who are willing to talk to us about how security and trust are handled within their projects' communities. This is the landing page with more info: https://research.teamusec.de/2021-interviews-oss/ __eou__	User
TP	ah interesting. I don't think we do much of that in this project, but others may think differently.__eou__	Agent

TP	> @razou could you please paste a fully reproducible piece of code?  ``` from sklearn.calibration import CalibratedClassifierCV from sklearn.multioutput import ClassifierChain from lightgbm import LGBMClassifier  base_estimator = LGBMClassifier() calibrator = CalibratedClassifierCV(base_estimator=base_estimator) clf = ClassifierChain(base_estimator=calibrator, order='random', random_state=20) clf.fit(X=train_x, Y=train_y)  y_pred_proba = clf.predict_proba(validation_x) ``` The aim was to perform multi-label classifier and retourn probability scores for each (label, profile) pair. NB: y was encoded wit h MultiLabelBinarizer __eou__	User

TP	@razou please provide a minimal reproducible piece of code, that is a piece a piece of code that we can just copy and paste in a python shell or python script and run to trigger the problem. Here the code you provide does not include the definition of `train_x` and `train_y` which is probably the core of the problem. Using minimal random data from np.random.normal(size=(n_samples, n_features) or np.random.randint(low=0, high=10, size=n_samples) and also add the necessary code to preprocess train_y and the code that computes the cross validation with the score you want. Minimal stands for removing anything that is not necessary. For instance are CalibratedClassifierCV ClassifierChain necessary to reproduce the problem? Or can you just reproduce the problem by cross validating the base estatimtor directly? If so simplify the code snippet. https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports __eou__	User

TP	Thanks you guys for your answers  1. libraries ``` pip install lightgbm==3.2.1 pip install scikit-learn==0.22.2.post1 ```  2. Code snipet  ``` from sklearn.datasets import make_multilabel_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import MultiLabelBinarizer from sklearn.calibration import CalibratedClassifierCV from sklearn.multioutput import ClassifierChain  from lightgbm import LGBMClassifier  X, y = make_multilabel_classification(n_samples=2000, n_classes=10, n_labels=2, allow_unlabeled=True) train_x, validation_x, train_y, validation_y = train_test_split(X, y, test_size=0.25)  mlb = MultiLabelBinarizer() train_y_encoded = mlb.fit_transform(train_y) validation_y_encoded = mlb.transform(validation_y)  base_estimator = LGBMClassifier() calibrator = CalibratedClassifierCV(base_estimator=base_estimator) clf = ClassifierChain(base_estimator=calibrator, order='random', random_state=20) clf.fit(X=train_x, Y=train_y_encoded)  y_pred_proba = clf.predict_proba(validation_x) print(y_pred_proba[:3]) ``` __eou__	User

TP	I don't understand why you are using `MultiLabelBinarizer` here because `y` is already a binary representation of the target variable since in this snippet you used `make_multilabel_classification`. Please provide a snippet that causes the same error message as the problem you observe with cross-validation cohen kappa score.  Anyways by reading the scikit-learn documentation https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa I don't see how this would work for binary encoded multilabeled data. The scikit-learn error message is actually quite explicit:  ```python >>> from sklearn.metrics import cohen_kappa_score >>> cohen_kappa_score([[0, 1], [1, 1]], [[0, 0], [1, 0]]) Traceback (most recent call last):   File "<ipython-input-19-2a87559cbf88>", line 1, in <module>     cohen_kappa_score([[0, 1], [1, 1]], [[0, 0], [1, 0]])   File "/Users/ogrisel/code/scikit-learn/sklearn/metrics/_classification.py", line 639, in cohen_kappa_score     confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)   File "/Users/ogrisel/code/scikit-learn/sklearn/metrics/_classification.py", line 304, in confusion_matrix     raise ValueError("%s is not supported" % y_type) ValueError: multilabel-indicator is not supported ``` __eou__	User

TP	Where Kappa metric cames from ? I did not used it ... __eou__	User

TP	@razou sorry I mixed 2 conversations. Ignore the bit on Cohen's Kappa then. @razou your code snippet works with `clf.fit(X=train_x, Y=train_y)` instead of `clf.fit(X=train_x, Y=train_y_encoded)`. __eou__	User

TP	Thanks @ogrisel  for answers (y) __eou__	User

TP	how to get precision and recall from function 'precision_recall_curve' for class 0. I posted a question with a code on this topic [here](https://ai.stackexchange.com/questions/32127/how-to-get-precision-and-recall-from-function-precision-recall-curve-for-class) __eou__	User

TP	You should use either https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html#sklearn.metrics.PrecisionRecallDisplay.from_estimator https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html#sklearn.metrics.PrecisionRecallDisplay.from_predictions with `pos_label=0` __eou__	User

TP	@glemaitre Solved the problem by turning class 0 into 1.                                                              But it's still not clear what kind of data I get by setting label=0. Updated the code and added two videos with label=0 and label=1. I put the code and videos [here](https://ai.stackexchange.com/questions/32127/how-to-get-precision-and-recall-from-function-precision-recall-curve-for-class/)  It is quite possible that I am difficult to understand, since English is not my native language. There is no opportunity to practice in English.  __eou__	User

TP	turning class 0 to 1 is equivalent to change `pos_label=0` without changing the label. __eou__	User

TP	Thanks! __eou__	User

TP	If we run this command after setup `pytest maint_tools/test_docstrings.py -k sklearn.utils.extmath.cartesian`, we got  platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 rootdir: /workspaces/scikit-learn, configfile: setup.cfg plugins: cov-3.0.0 collected 0 items / 2 skipped                                                                                                   =================================================== short test summary info =================================================== SKIPPED [2] maint_tools/test_docstrings.py:12: could not import 'numpydoc.validate': No module named 'numpydoc' ===================================================== 2 skipped in 0.47s ====================================================== __eou__	User

TP	you need to install numpydoc via pip or conda __eou__	User
TP	Okay__eou__	Agent
TP	otherwise the test is skipped__eou__	User
TP	we dont impose it because this is an optional dependency__eou__	User

TP	Working on #21350 issue __eou__	User

TP	===================================================== test session starts ===================================================== platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 rootdir: /workspaces/scikit-learn, configfile: setup.cfg plugins: cov-3.0.0 collected 2110 items / 2109 deselected / 1 selected                                                                             maint_tools/test_docstrings.py .                                                                                        [100%]  ============================================= 1 passed, 2109 deselected in 0.98s ============================================== now, we got passed, then we make a PR for it ? __eou__	User

TP	yes __eou__	User

TP	> yes  Done! __eou__	User

TP	hey, i'm unclear on whether this channel is for user- or developer-related questions (or both)? can someone clarify? i don't want to bother others with off-topic questions :-) > hey, i'm unclear on whether this channel is for user- or developer-related questions (or both)? can someone clarify? i don't want to bother others with off-topic questions :-)  oh, i guess the topic answers that (thanks) so, for the actual Q: __eou__	User

TP	i'm trying to use sklearn.clustering.DBSCAN on a large document corpus (coming from gensim, converted to numpy sparse matrix) the matrix is this: <6748785x4974743 sparse matrix of type '<class 'numpy.float64'>'         with 677079990 stored elements in Compressed Sparse Column format> that's ~5M documents with ~7M features (TFIDF-weighted words) i'm trying to cluster this using DBSCAN(n_jobs=64).fit(corpus_csc) how do I know if it will ever terminate? there seems to be no way of having a progress indication (RAM doesn't seem to be an issue for now, it's running on a machine with 1 TiB RAM, but it's sitting at ~70 GiB for now) it's been going on for ~3 days now, any guess on whether it has any chances of terminating in reasonable time? (or how to enable progress logging, if that's possible) __eou__	User

TP	my guess is most of the time is spent on ball_tree or kd_tree in nearestneighbors. If I were to investigate, I'd add a few logging info in those areas to see what's happening, but probably most of them are in cython space. We don't really have logging in those areas, if you want to figure it out, you should add it to your copy of scikit-learn and see where the time is spent. __eou__	User

TP	@adrinjalali: thanks for your feedback. Aside from that (which I'm gonna try) any reference number about the performances of sklearn's DBSCAN implementation on large datasets? it'd be very useful to have an idea if I'm "almost there" or, like, only 1e-6 % done __eou__	User

TP	I personally haven't worked with very large datasets and DBSCAN, so I can't really help you there unfortunately. __eou__	User

TP	Does logistic regression support pandas dataframe as input for the X matrix?  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit  Does this then allow us to make use of the new attribute name: `feature_names_in`? __eou__	User

TP	you can pass a daframe currently `feature_names_in`would be useless in classifier and regressor because we will convert into a numpy array to make the optimisation the idea for the moment is to propagate feature names in the preprocessing steps such that you for instance know the name of the columns of the numpy array that will be passed to the logistic regression (and potentially build the dataframe) The work that could be done for classifier and regressor is to make that the fitted attribute could use the feature names to decorate themselves but it can be still be easily done by getting the `feature_names_in` from the last preprocessing stage and create for instance a pandas series using the coefficients but for the moment we try that we can move the feature names in the preprocessing __eou__	User

TP	Hi folks! I have a very simple fix for https://github.com/scikit-learn/scikit-learn/issues/12052 that only addresses this issue for CalibratedClassifierCV. I know SLEP006 seeks to fix this for the general case, but it appears to still be a WIP. Is it worth me throwing together a PR to simply add the groups parameter to CalibratedClassifierCV with checks to ensure the cv supports it? I've tested this out on my own by subclassing CalibratedClassifierCV and writing a modified fit method. It's essentially a 2 line code change. I'm happy to submit a PR if it seems likely this will be accepted? __eou__	User

TP	Hi everyone. I was wondering if anyone could get older versions of scikit-learn that are below 1.0.0 installed in a Python 3.9 virtual environment. I think there is no wheel for scikit-learn 0.21.0 or 0.21.1 that works with Python 3.9, so it has to be built from source, which doesn't work very well on a lot of systems. I know installing with Anaconda works, but I was curious if there are some solutions to get it to work with Python 3.9's typical pip install command. __eou__	User

TP	Hello! __eou__	User

TP	Hello, Scikit devs and contributors. I have a question regarding one of the examples left in PR #21958. `/examples/linear_model/plot_sparse_logistic_regression_mnist.py`. I guess this one is left because is not as straightforward to accelerate. This example involves a logistic regression using the saga algorithm and l1 penalty. In my attempt to optimize it, I found out that most of the running time is spent fetching the data instead of running the regression itself. The most I could do was 5% faster, which translates to 2 seconds difference. Unfortunately, by reducing `train_samples` the running time is not meaningfully decreased.  Another option I considered was to run LogisticRegression with `tol=0.9` and `n_jobs=2`, and `max_iter=40`. Unfortunately, can't get more than 5%.  One of the tweaks I managed to do was in the plot itself, by running the reshape outside the plot and using a list comprehension itself. Overall it makes the plot faster.  Checking other PRs, it seems that the acceleration obtained in this example is quite low. I am not sure even if this is an example that can be further improved considering the most expensive operation here is `fetch_openml`. For the same reason, not sure submitting a PR with such a low improvement is even a good idea.  I appreciate your thoughts on it. Maybe I'm missing something relevant. Thanks! __eou__	User

TP	Hi guys, I want to install the scikit-learn package in an environment that previously TensorFlow, NumPy, pandas, scipy, and matplotlib were installed. I installed the scikit-learn package in Windows 7 64-bit. When, in activated enviroment I write: conda install scikit-learn; it shows: ERROR conda.core.link:_execute(699): An error occurred while installing  package 'defaults::scikit-learn-1.0.1-py38hf11a4ad_0'. Rolling back  transaction: done  LinkError: post-link script failed for package  defaults::scikit-learn-1.0.1-py38 hf11a4ad_0 location of failed  script: G:\programfile\anaconda3\envs\tf\Scripts\.scikit-lear n-post-link.bat  ==> script messages <==  <None> ==> script output <==  stdout:  stderr:  return code: 1   () __eou__	User

TP	@ojeda-e thanks for investigating those ones. Please leave the same comment on the issue, and I'll mark them as "won't change" __eou__	User

TP	Hi __eou__	User

TP	hello guys i am getting this error can anybody please tell ImportError while loading conftest 'E:\scikit\scikit-learn\sklearn\conftest.py'. sklearn\__init__.py:81: in <module>     from . import __check_build  # noqa: F401 sklearn\__check_build\__init__.py:50: in <module>     raise_build_error(e) sklearn\__check_build\__init__.py:31: in raise_build_error     raise ImportError( E   ImportError: No module named 'sklearn.__check_build._check_build' E   ___________________________________________________________________________ E   Contents of E:\scikit\scikit-learn\sklearn\__check_build: E   setup.py                  _check_build.pyx          __init__.py E   __pycache__ E   ___________________________________________________________________________ E   It seems that scikit-learn has not been built correctly. E E   If you have installed scikit-learn from source, please do not forget E   to build the package before using it: run `python setup.py install` or E   `make` in the source directory. E E   If you have used an installer, please check that it is suited for your E   Python version, your operating system and your platform. __eou__	User

TP	i have tried puthon setup.py install too __eou__	User

TP	which error? Please copy the command you used and the error message. __eou__	User

TP	Hello guys, i am having a lot of trouble creating a dummy classifier with scikit-learn. I asked a question with all the details in stackoverflow, can you please check what am i doing wrong? Link: https://www.stackoverflow.com/questions/70866945/assertionerror-not-equal-to-tolerance __eou__	User

TP	@henrique-voni I've answered on SO __eou__	User

TP	Hello guys while testing  the file my test are getting skipped how to fix it  '''  ==================================================================== test session starts ==================================================================== platform win32 -- Python 3.9.10, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 rootdir: D:\OpenSource\sci-kit\scikit-learn, configfile: setup.cfg plugins: cov-3.0.0 collected 0 items / 2 skipped '''' __eou__	User

TP	what is the command you are using ? __eou__	User

TP	pytest sklearn/tests/test_docstrings.py -k sklearn.datasets._california_housing.fetch_california_housing __eou__	User

TP	I think you forgot to install documentation dependencies https://scikit-learn.org/stable/developers/contributing.html#building-the-documentation __eou__	User

TP	@PurnaChandraMansingh  thanks ,Now its working __eou__	User

TP	I get an error trying to use a callable for metric with KNN. What is the correct way to use it?  ``` clf = KNeighborsClassifier(metric=dm.get_metric("minkowski")) clf.fit(X,y)  ValueError: Metric '<sklearn.metrics._dist_metrics.EuclideanDistance object at 0x000001ECEC658DD0>' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['brute']) to get valid options. Metric can also be a callable function. ``` good point :-) __eou__	User

TP	https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html __eou__	User

TP	@amy12xx it's always easier for people to give you a meaningful answer if you paste a minimally reproducible code, which can in its entirety be copy/pasted to produce your issue. __eou__	User

TP	``` from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import DistanceMetric as dm from sklearn.datasets import load_iris iris = load_iris() X = iris.data y = iris.target clf = KNeighborsClassifier(metric="euclidean") clf.fit(X,y) clf = KNeighborsClassifier(metric=dm.get_metric("euclidean")) clf.fit(X,y)  Traceback (most recent call last):   File "<stdin>", line 1, in <module>   File "C:\Users\Amanda\Miniconda3\envs\mlenv\lib\site-packages\sklearn\neighbors\_classification.py", line 198, in fit     return self._fit(X, y)   File "C:\Users\Amanda\Miniconda3\envs\mlenv\lib\site-packages\sklearn\neighbors\_base.py", line 437, in _fit     self._check_algorithm_metric()   File "C:\Users\Amanda\Miniconda3\envs\mlenv\lib\site-packages\sklearn\neighbors\_base.py", line 374, in _check_algorithm_metric     raise ValueError( ValueError: Metric '<sklearn.metrics._dist_metrics.EuclideanDistance object at 0x0000018099BB9780>' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['brute']) to get valid options. Metric can also be a callable function.  ``` __eou__	User

TP	That's indeed curious @amy12xx , I've opened and issue, and you can follow the discussion there: https://github.com/scikit-learn/scikit-learn/issues/22348 __eou__	User

TP	Hi, I am using TransformedTargetRegressor with KNeighborsRegressor for precomputed metric, however when I do cross validation with GridSearchCV, an error is raised saying that the dimension of the metric is not correct. The code is like this: ``` from sklearn.preprocessing import MinMaxScaler target_scaler = MinMaxScaler() estimator = Pipeline([         ('scaler', MinMaxScaler()),         ('model', TransformedTargetRegressor(           KNeighborsRegressor(metric='precomputed'),           transformer=target_scaler         ))])  clf = GridSearchCV(estimator, param_grid=grid_params,         scoring=scoring,         cv=cv, return_train_score=True, refit=True,         error_score='raise') clf.fit(D_app, y_app) ... ``` May I ask what may be the problem? In case it is not supported, is there other ways to corrected scale targets in GridSearchCV (as well as HalvingGridSearchCV, etc.). Thank you very much! __eou__	User

TP	I am getting this error-:       Could not find conda environment: sklearn-env. You can list all discoverable environments with `conda info --envs`. __eou__	User

TP	 E   ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject __eou__	User

TP	@Shubham1450: can you please open an issue with this error message and the full output of `conda list`? __eou__	User

TP	Hey smart people. I am trying to figure out/understand the warning. Solution I found just tell you to disable to warning. Maybe someone give a hint why I am seen the following warning is this super simple Multiple Regression example? ``` data_file = pd.read_csv("FuelConsumption.csv")  data_frame = data_file[     [         'ENGINESIZE',         'CYLINDERS',         'FUELCONSUMPTION_CITY',         'FUELCONSUMPTION_HWY',         'FUELCONSUMPTION_COMB',         'CO2EMISSIONS'     ] ]  data_set_x = ['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB'] data_set_y = ['CO2EMISSIONS']  mask = np.random.rand(len(data_frame)) < 0.8 train = data_frame[mask] test = data_frame[~mask]  lr_regression = linear_model.LinearRegression() train_x = np.asanyarray(train[data_set_x]) train_y = np.asanyarray(train[data_set_y]) lr_regression.fit(train_x, train_y)  y_hat = lr_regression.predict(test[data_set_x]) test_x = np.asanyarray(test[data_set_x]) test_y = np.asanyarray(test[data_set_y]) ```  The line 26: ``` y_hat = lr_regression.predict(test[data_set_x]) ```  Produces this warning: ``` sklearn/base.py:443: UserWarning: X has feature names, but LinearRegression was fitted without feature names   warnings.warn( ``` __eou__	User

TP	In your example, `lr_regression.fit` was called with an ndarray, while `lr_regression.predict` was called with a DataFrame. To prevent the warning, you can `fit` with the DataFrame directly:  ```python lr_regression.fit(train[data_set_x], train[data_set_y]) ```  without casting to a ndarray. __eou__	User

TP	Interesting! Thanks. I tried that, and it still has a warning though. I think I figured it out. I use the `np array` on all of them now. ``` lr_regression = linear_model.LinearRegression() train_x = np.asanyarray(train[data_set_x]) train_y = np.asanyarray(train[data_set_y]) lr_regression.fit(train_x, train_y)  test_x = np.asanyarray(test[data_set_x]) test_y = np.asanyarray(test[data_set_y]) y_hat = lr_regression.predict(test_x) ```  This works. I am also wondering, (yet to look into it) why `np arrays` are used if just the data frame can be used, as you have suggested?e __eou__	User

TP	I am a question with sklearn.PCA. Whether data needs to be standardized? eg.  the variance may be used before create the pca object? [![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/rsRh/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/rsRh/image.png) Thank you for answering __eou__	User

TP	Hi scikit-learn team! I've got a new computer (MacBookPro, chip: Apple M1 Pro) on which I installed the development version of scikit-learn. When I ran pytest I encountered the following:  ``` (sklearn-dev) <unconvertable>  scikit-learn git:(main) pytest =========================================================================================== test session starts ============================================================================================ platform darwin -- Python 3.9.10, pytest-7.0.1, pluggy-1.0.0 rootdir: /Users/maren/Documents/scikit-learn, configfile: setup.cfg, testpaths: sklearn plugins: xdist-2.5.0, forked-1.4.0, cov-3.0.0 collecting ... [1]    54294 killed     pytest ```` __eou__	User

TP	this does not look good :) __eou__	User

TP	I then tried to check what's going on and found the following. Do you have an idea of what I need to do?  ``` (sklearn-dev) <unconvertable>  scikit-learn git:(main) python -vvv -c "import sklearn" import _frozen_importlib # frozen import _imp # builtin import '_thread' # <class '_frozen_importlib.BuiltinImporter'> import '_warnings' # <class '_frozen_importlib.BuiltinImporter'> import '_weakref' # <class '_frozen_importlib.BuiltinImporter'> import '_io' # <class '_frozen_importlib.BuiltinImporter'> import 'marshal' # <class '_frozen_importlib.BuiltinImporter'> import 'posix' # <class '_frozen_importlib.BuiltinImporter'> import '_frozen_importlib_external' # <class '_frozen_importlib.FrozenImporter'> # installing zipimport hook import 'time' # <class '_frozen_importlib.BuiltinImporter'> import 'zipimport' # <class '_frozen_importlib.FrozenImporter'> # installed zipimport hook # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/__init__.cpython-39.pyc matches /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__init__.py # code object from '/Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/__init__.cpython-39.pyc' # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.cpython-39-darwin.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.abi3.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.py # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/__pycache__/codecs.cpython-39.pyc matches /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.py # code object from '/Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/__pycache__/codecs.cpython-39.pyc' import '_codecs' # <class '_frozen_importlib.BuiltinImporter'> import 'codecs' # <_frozen_importlib_external.SourceFileLoader object at 0x101613be0> # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.cpython-39-darwin.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.abi3.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.py # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/aliases.cpython-39.pyc matches /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.py # code object from '/Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/aliases.cpython-39.pyc' import 'encodings.aliases' # <_frozen_importlib_external.SourceFileLoader object at 0x101643190> import 'encodings' # <_frozen_importlib_external.SourceFileLoader object at 0x1016139d0> # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.cpython-39-darwin.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.abi3.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.py # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/utf_8.cpython-39.pyc matches /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.py # code object from '/Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/utf_8.cpython-39.pyc' import 'encodings.utf_8' # <_frozen_importlib_external.SourceFileLoader object at 0x1016138b0> import '_signal' # <class '_frozen_importlib.BuiltinImporter'> # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/latin_1.cpython-39-darwin.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/latin_1.abi3.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/latin_1.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/latin_1.py # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/latin_1.cpython-39.pyc matches /Users/maren/mambaforg ``` __eou__	User
TP	which compilers are you using when installing the dev version__eou__	Agent

TP	It looks like I can't post the full error message because it's too long. I followed the installation here: https://scikit-learn.org/stable/developers/advanced_installation.html#macos-compilers-from-conda-forge So I installed `compilers` and `llvm-openmp`. And I used the `Miniforge3-MacOSX-arm64` download from here: https://github.com/conda-forge/miniforge#miniforge __eou__	User

TP	let me try with the last version of comilers on my M1 machine I assume that you forced installing python 3.9 and not 3.10? __eou__	User

TP	you can use https://gist.github.com to post the full error log and give a link here hopefully you will be able to find a workaround if it proves too complex to fix __eou__	User

TP	Thank you! Here is the link: https://gist.github.com/marenwestermann/9ffddb7a2f0ef6798d350f3595997ed1 __eou__	User
TP	I can reproduce__eou__	Agent

TP	clang and llvm have been updated __eou__	User

TP	so temporary I think that installing `compilers=1.3` should fix the problem. I will give it a try. Then we need to understand why the new compilers are failing. But I can see that clang and llvm have been updated uhm it is not the compilers :( __eou__	User

TP	I just tried using `compilers=1.3` but it didn't solve the problem __eou__	User
TP	@marenwestermann: can you please open an issue?__eou__	Agent
TP	Yes, will do__eou__	User

TP	I'm about to head off to a PyLadies Berlin open source hack night that I'm hosting, so will do it then. This is actually a good example case that I can show. :) __eou__	User
TP	nice :)__eou__	Agent

TP	Hello all. I'm new to contributing here. Can anybody guide me how should I start to contribute in sklearn! __eou__	User

TP	I am trying to rejuvenate scikit-learn/scikit-learn#14636 which would then close scikit-learn/scikit-learn#8834 and scikit-learn/scikit-learn#8842. That requires merging https://github.com/scipy/scipy/pull/15391 Could someone please help by reviewing? Even though it's SciPy PR, it is a must to merge for sklearn spectral embedding and clustering that relies on SciPy to construct the graph Laplacian. __eou__	User

TP	@adrinjalali Hi, thanks for your advice. Here is my code: ``` import numpy as np from sklearn.pipeline import Pipeline from sklearn.preprocessing import MinMaxScaler from sklearn.compose import TransformedTargetRegressor from sklearn.neighbors import KNeighborsRegressor from sklearn.model_selection import GridSearchCV  target_scaler = MinMaxScaler() estimator = Pipeline([         ('scaler', MinMaxScaler()),         ('model', TransformedTargetRegressor(           KNeighborsRegressor(metric='precomputed'),           transformer=target_scaler         ))])  grid_params = {'model__regressor__n_neighbors': [3, 5, 7]} scoring = 'accuracy' clf = GridSearchCV(estimator, param_grid=grid_params,                        scoring=scoring,                        cv=5, return_train_score=True, refit=True,                        error_score='raise')  D_app = np.random.rand(10, 10) y_app = np.random.rand(10) clf.fit(D_app, y_app) ``` The following error is raised: ``` File "/media/ljia/DATA/research-repo/projects/202110 Redox/codes/Redox/issues/target_scaling.py", line 34, in <module>     clf.fit(D_app, y_app)    File "/home/ljia/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py", line 891, in fit     self._run_search(evaluate_candidates)    File "/home/ljia/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py", line 1392, in _run_search     evaluate_candidates(ParameterGrid(self.param_grid))    File "/home/ljia/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py", line 838, in evaluate_candidates     out = parallel(    File "/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py", line 1043, in __call__     if self.dispatch_one_batch(iterator):    File "/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py", line 861, in dispatch_one_batch     self._dispatch(tasks)    File "/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py", line 779, in _dispatch     job = self._backend.apply_async(batch, callback=cb)    File "/home/ljia/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py", line 208, in apply_async     result = ImmediateResult(func)    File "/home/ljia/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py", line 572, in __init__     self.results = batch()    File "/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py", line 262, in __call__     return [func(*args, **kwargs)    File "/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py", line 262, in <listcomp>     return [func(*args, **kwargs)    File "/home/ljia/.local/lib/python3.8/site-packages/sklearn/utils/fixes.py", line 211, in __call__     return self.function(*args, **kwargs)    File "/home/ljia/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py", line 681, in _fit_and_score     estimator.fit(X_train, y_train, **fit_params)    File "/home/ljia/.local/lib/python3.8/site-packages/sklearn/pipeline.py", line 394, in fit     self._final_estimator.fit(Xt, y, **fit_params_last_step)    File "/home/ljia/.local/lib/python3.8/site-packages/sklearn/compose/_target.py", line 246, in fit     self.regressor_.fit(X, y_trans, **fit_params)    File "/home/ljia/.local/lib/python3.8/site-packages/sklearn/neighbors/_regression.py", line 213, in fit     return self._fit(X, y)    File "/home/ljia/.local/lib/python3.8/site-packages/sklearn/neighbors/_base.py", line 489, in _fit     raise ValueError(  ValueError: Precomputed matrix must be square. Input is a 8x10 matrix. ``` May I ask what may be the problem? In case it is not supported, are there other ways to correctly scale targets in GridSearchCV (as well as HalvingGridSearchCV, etc.)? Thank you very much! __eou__	User

TP	Hello! I hope everyone is well. Just wondering, when is the next sklearn release happening, `1.0.3`? __eou__	User

TP	Hello Everyone, I have a very specific use case designed around scikit-learn and wanted to see if it was possible to code it up. The overall idea is to train a 1D embedding using a pretrained GLM. For this, I take a pretrained Poisson Regressor model trained using scikit-learn(this is GIVEN and persay has been trained on 50 features), I want to add a new feature to it i.e a random variable X ~ N(0, 1) and retrain the model to get a 51 parameter model. Once this is done, I treat the input X as a parameter, freeze the model and get optimal value for X_i using gradient based approaches. Finally, upon having the optimal set of X_i, I want to retrain end to end using all 51 features. This is the gist of the algorithm that is designed. Any leads towards APIs or whether this is achievable or not would be really helpful. Thanks! __eou__	User

TP	in sklearn.inspection.permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0 what is the default for scoring? A " baseline metric, defined by scoring, is evaluated on a (potentially different) dataset defined by the x" so it has to be something __eou__	User

TP	I think it's using `estimator.score(X_test, y_test)` by default. So accuracy for classifier and R2 for regressors. __eou__	User

TP	@ogrisel:matrix.org  thank you I am really confused though... __eou__	User

TP	model = xgbr.fit(X_train, y_train) print(model.score(X_test, y_test)) r = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=0) for i in r.importances_mean.argsort()[::-1]:     print(f"{r.importances_mean[i]:.3f}" f" +/- {r.importances_std[i]:.3f}") __eou__	User

TP	that simple code that give feature importance gives me a value over 1.20 for the top one. But how can you have a permutation feature importance higher than 1? __eou__	User

TP	At least a partial answer is that r2 can be arbitrarily negative __eou__	User

TP	Hi! I promoted the scikit-learn office hours on PyLadies slack and WiMLDS slack because many people don't seem to be aware of them. I also did a tweet via PyLadies Berlin (https://twitter.com/PyLadiesBer/status/1519981569343164417). I think that the biweekly office hours is a great initiative that is especially helpful for folks belonging to groups which are underrepresented in tech and open source. I hope that the office hours and also spreading the word about them in these communities will help with contributor retention. __eou__	User

TP	Thanks Maren. Indeed, I assume that it could be motivating to have a closer follow-up on some PR. __eou__	User

TP	Hi everyone, i'm new in this community and i want to apologize in advance for any errors i might make in asking the following question. So, i have to implement a classification task using scikit-multiflow for a big dataset (84 feature x 2,5 milion of exemples), processed like a stream. After many and many attempts my code finally run without warnings or errors but there is a problem: i am using the class Evaluate Prequential and its methods for the classification and, by setting adquate metrics to evaluate the goodness of this classification, i obtain very high values for each metric used. This is "strange" considering the dataset i am working on, reason why i want to generate the confusion matrix in order to understand on wich classes my classification algorithm works better and on wich classes it makes more misclassification. Generating confusion matrix is very easy using scikit-learn, but this method needs to have as input parameter true labels and predicted labels and here is the problem: i cannot isolate from Evaluate Prequential, in particular from the method "evaluate", predicted labels, consequently i have no way to generate the confusion matrix because i have not predicted labels to make a comparison with true labels. For sure there is trick to get around this problem but all of my attempts since two days failed and i have no more ideas on how i could do it. Please, do you have an idea on how to solve this problem? Thank you a lot. __eou__	User

TP	ok I have a set of market returns. But I want to explain the returns using a set of features, then find the hierarchy and find an equal weighted portfolio that minimizes variance. How can this be done? ``ok I have a set of market returns. But I want to explain the returns using a set of features, then find the hierarchy`` this much Idk how to do, I can figure out the rest. __eou__	User

TP	Hey folks, I have maybe a silly question. But is there any fundamental difference between `model_selection.cross_validate` and doing it manually?  For instance, take a look at the example below, I can't figure out why the roc scores in red are different while the logloss scores in green are the same?  I expect that the roc scores should match, unless I'm doing sth stupid and haven't noticed yet? [![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/s4Ci/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/s4Ci/image.png) __eou__	User

TP	I seems that `cross_validate` uses silently a different splitting mechanism depending on label `y`. For int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, Fold is used.  If that's true then this is an undesired behaviour for me as a user. I want things to be explicit and not implicit like this spending hours searching to find what's going on?  On an additional note I don't understand the phrase `In all other cases, Fold is used.`, so in the case of my example instead of using `StratifiedGroupKFold` is `cross_validate` implicitly using just `Fold`? __eou__	User

TP	> If that's true then this is an undesired behaviour for me as a user. I want things to be explicit and not implicit like this spending hours searching to find what's going on?  Stratification will always be the best but this is not possible with regression. So we stated in the `cv` argument doc and the user guide as well:  > When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.  One should improve the documentation to make it explicit for the default `None` and add `cross_validate` together with `cross_val_score`. > On an additional note I don't understand the phrase In all other cases, Fold is used., so in the case of my example instead of using StratifiedGroupKFold is cross_validate implicitly using just Fold?  Oh actually this is already documented properly. `Fold` is a typo, it should be replace with `KFold` and link to the cv splitter. __eou__	User

TP	Any suggestions on how to get my PR reviewed? __eou__	User

TP	If your PR is ready, you can add [MRG] to the title of PR. You can check the PR checklist: https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist. And I found the wiki have some reviewers you can ping: https://github.com/scikit-learn/scikit-learn/wiki/Available-reviewers. __eou__	User

TP	Thanks! I didn't know about that checklist __eou__	User

TP	Hello, I see `ElasticNet` does not have an `n_jobs` parameter. Does that mean it uses all available resources on the machine to train? https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html __eou__	User

