[17:47] <5824457cd73408ce4f34ebbd> Hello everyone, I have a quick question regarding normalization (min-max-scaling) of output values. Usually you are supposed to use normalization only on the training data set and then apply those stats to the validation and test set. But for instance, my prediction variable is a single percentage value ranging [0, 100%]. And I know for sure that in the "real world" regarding my problem statement, that I will get samples ranging form 60 - 100%. But my training set is to small and does not contain enough data points including all possible output values. So here comes my question: Should I stay with my initial statement (normalization only on training data) or should I apply the maximum possible value of 100% for my output value to max()-value of the normalization step?  
[17:51] <57007657187bb6f0eadd96e8> Are you doing regression? Typically minn max scaling is used on the feature vector values that are inputs into the classifier. Not necessarily on the outputs of the classifier
[17:51] <5824457cd73408ce4f34ebbd> yes, regression
[17:54] <5824457cd73408ce4f34ebbd> Another silly idea was to squash my output value into the range [0,1] by applying target/100. Does that make sense? 
[17:54] <57007657187bb6f0eadd96e8> thats just chaning how you look at it, not really changing the properties of it
[17:55] <57007657187bb6f0eadd96e8> just checking I understand your problem though. You are saying in your training set, you have min max output of  something like  .2 - .6 and so you are scaling that to 0-1. 
[17:56] <57007657187bb6f0eadd96e8> It depends on the problem you are trying to solve. You'll have to decide if it is appropriate for your problem or not. 
[17:58] <57007657187bb6f0eadd96e8> what min max scaling does in that case is stretch the predictions between .2 and .6 to a higher resolution, and then make evertying outside .2 or .6 equal to each other
[17:59] <57007657187bb6f0eadd96e8> if that makes sense to do will be application specific
[18:00] <5824457cd73408ce4f34ebbd> Maybe I'm just dump. So, my target value ranges from something like 60% - 100% and I'm trying to predict it by using MLP/CNN. My understanding is that I have to scale my output value that it matches my output activation function (ReLU). But I got cought up with normalization in general and the correct usage on training,validation and test data.
[18:00] <5824457cd73408ce4f34ebbd> That's how I got confused
[18:01] <57007657187bb6f0eadd96e8> ok, yeah so any scaling that you do on your training data. You will want to apply those same normalization to the validation and test data 
[18:04] <57007657187bb6f0eadd96e8> you can also set the normalization of the training to be what you expect from your real data 
[18:09] <5824457cd73408ce4f34ebbd> Exactly, but what if my training set doesn't include all possible outcomes for my output value due to the small sample size. Usually that would just mean, I would have to include more samples. But that is not possible at this point. So furthermore this leads to the situation that my test sample will have output values which are not present in the training set.  One solution would be to give the model the information in advance that the max possible range of the output value will be 100%. 
[18:09] <5824457cd73408ce4f34ebbd> But this idea contradicts the Literature
