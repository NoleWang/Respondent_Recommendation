[00:04] <5ecf797dd73408ce4fe523d7> Hey @amueller ! By soft or hard I mean to refer to soft clustering or hard clustering - which if I understand correctly, soft clustering refers to when datapoints can co-exist in different clusters whereas with hard clustering they can only exist in one cluster.
[00:04] <5ecf797dd73408ce4fe523d7> I have 16 GB of RAM
[00:04] <5ecf797dd73408ce4fe523d7> Using complete linkage
[00:05] <5ecf797dd73408ce4fe523d7> How could I go about precomputing the distance matrix in a chunked way?
[00:40] <5ecf797dd73408ce4fe523d7> I don't know if I got this right so far:
[16:23] <5ecf797dd73408ce4fe523d7> @rth Do you mean using this in place of agglomerative clustering or for chunking / precomputing the distance matrix?
[16:39] <5ecf797dd73408ce4fe523d7> Oops! Now I see where I was wrong
[16:39] <5ecf797dd73408ce4fe523d7> And why those 100K data points only exported 1 cluster
[16:40] <5ecf797dd73408ce4fe523d7> In the toy example, I did this:  chunked = pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1)  for chunk in chunked: clustering = AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(chunk)
[16:40] <5ecf797dd73408ce4fe523d7> I set the parameter metric to 'cosine' in pairwise distances
[16:40] <5ecf797dd73408ce4fe523d7> and I set affinity to 'precomputed' in AgglomerativeClustering
[16:40] <5ecf797dd73408ce4fe523d7> When I then tried to do this with the 100K datapoints
[16:41] <5ecf797dd73408ce4fe523d7> I forgot to change 'cosine' to 'precomputed' in AgglomerativeClustering
--------------------------------------------------------------------------------------------------
[00:41] <5ecf797dd73408ce4fe523d7> from sklearn.metrics import pairwise_distances_chunked from sklearn.cluster import AgglomerativeClustering  chunked = pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1)  for chunk in chunked:        clustering = AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(chunk)
[00:41] <5ecf797dd73408ce4fe523d7> print(clustering.labels_)
--------------------------------------------------------------------------------------------------
[13:41] <589b9e0fd73408ce4f490ba4> @youssefabdelm_twitter You could try https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer
--------------------------------------------------------------------------------------------------
[14:14] <54d4a1d6db8155e6700f853b> @youssefabdelm_twitter you should call pairwise_distances directly I think as it does chunking internally. Though I now realized agglomerativeclustering might already be doing. Also agglomerative clustering is hard clustering @rth I was thinking about that, but shouldn't agglomerativeclustering be already chunked using the working_memory parameter? cc @jnothman
--------------------------------------------------------------------------------------------------
[16:15] <5ecf797dd73408ce4fe523d7> Interestingly I actually tried the code I shared above on a small example and it worked, but on the 100K embeddings it just exported 1 cluster, and the amount of embeddings in that was around 750 which really confused me. I still have no idea what happened.
[16:16] <5ecf797dd73408ce4fe523d7> By calling it directly, do you mean something like this:  "for chunk in pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1):"
[16:16] <5ecf797dd73408ce4fe523d7> Or this: AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1))
[17:20] <5ecf797dd73408ce4fe523d7> I hope I'm making some other stupid mistake rather than this being a consequence of a large input
[17:29] <5ecf797dd73408ce4fe523d7> So far I don't see any difference between the toy example and the code I'm using for the 100K embeddings
[21:30] <5ecf797dd73408ce4fe523d7> I might try the 'meta-clustering' approach I talked about where I just compare centroids (or try to apply complete linkage) of clusters from different chunks and then merge, and sort of testing that against what agglomerative clustering would normally do and see how the results vary. Do you think this would yield expected results (All points above 70% similarity should be grouped) or should I instead go with HDBSCAN? My one need is not specifying the number of clusters, but instead a distance threshold of some kind.
--------------------------------------------------------------------------------------------------
[16:28] <5ecf797dd73408ce4fe523d7> @amueller  I tried both, I assume you mean the first as that one works. The second gives me an error. "ValueError: Expected 2D array, got scalar array instead: array=<generator object pairwise_distances_chunked at 0x11009fc78>. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
--------------------------------------------------------------------------------------------------
[17:07] <5ecf797dd73408ce4fe523d7> Now I'm getting this new error: "ValueError: Distance matrix should be square, Got matrix of shape {X.shape}"
[17:07] <5ecf797dd73408ce4fe523d7> After making that change
--------------------------------------------------------------------------------------------------
[17:39] <5ecf797dd73408ce4fe523d7> This is what I'm using for the 100K embeddings:
[17:39] <5ecf797dd73408ce4fe523d7> chunked_distances = pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1, working_memory=3072) for chunk in tqdm(chunked_distances, total=22):    with io.capture_output() as captured:     clustering = AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(chunk)
[21:17] <5ecf797dd73408ce4fe523d7> @rth Very useful, thank you!
--------------------------------------------------------------------------------------------------
[20:49] <589b9e0fd73408ce4f490ba4> @youssefabdelm_twitter I don't think that using AgglomerativeClustering on chunks of the distance matrix would give you anything meaningful. I meant using `KNeighborsRegressor`to precompute a sparse distance matrix, but then I'm not sure if `AgglomerativeClustering` actually supports sparse distance matrices. You won't be able to compute a dense as for 100k samples that would be ~80GB.  Generally AgglomerativeClustering doesn't scale well with default options (https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html) so I would suggest starting with a smaller dataset and progressively increase the number of samples to see how it scales. You may run into performance issues before memory ones. @amueller I haven't looked at the code in detail, but internally it never uses `pairwise_distances` on the full dataset.
--------------------------------------------------------------------------------------------------
[20:49] <54d4a1d6db8155e6700f853b> hm. I was wondering if we need to add an example of doing some chunked clustering
--------------------------------------------------------------------------------------------------
[21:09] <5ecf797dd73408ce4fe523d7> @rth Does this mean that besides a sparse distance matrix (I'll give it a shot soon), there's really no way to do this all at the same time without a more powerful computer? So far what I've been doing is doing the clustering in chunks, meaning slicing the data (38K data points at a time) and then creating clusters from those. However, to me this is undesirable because of course (in my case) I get sentences which are in separate clusters which should more preferably be in one. I thought one idea to mitigate this is to calculate the centroid of each cluster, and then after all the 38K chunks are done, to compare centroids and then merge if they're below a certain distance threshold. Of course this loses the benefit of having the leaves & children info though.
[21:11] <5ecf797dd73408ce4fe523d7> I ask because eventually, I want to do this with around 1 million vectors
[21:12] <5ecf797dd73408ce4fe523d7> Also curious why you say that using AgglomerativeClustering on chunks of the distance matrix wouldn't give any meaningful results. Is it for the same reason I said above on getting different clusters which should really be one cluster?
--------------------------------------------------------------------------------------------------
[21:14] <589b9e0fd73408ce4f490ba4> @youssefabdelm_twitter Looks like single linkage should scale better for agglomeration clustering https://github.com/scikit-learn/scikit-learn/pull/11514#issuecomment-557349961  Well I mean it's just awkward to work with N separate clustering, but you can if you are comfortable with it.
[21:17] <589b9e0fd73408ce4f490ba4> Also I think think Birch or DBSCAN might scale better if want a hierarchy of clusters. If you have 1M samples that certainly going to put constraints on the algorithms you can use (cf above linked HDBSCAN docs for a comparison) unless you can accept working on a subsampled dataset.
--------------------------------------------------------------------------------------------------
[21:20] <5ecf797dd73408ce4fe523d7> Awesome! Helpful charts. Wanted to ask what you meant by "N separate clustering"
[21:22] <5ecf797dd73408ce4fe523d7> I assume you mean what I described above with splitting the data?
--------------------------------------------------------------------------------------------------
[21:24] <589b9e0fd73408ce4f490ba4> yes, I mean that if you do `AgglomerativeClustering().fit(X_chunk)` in a loop, as mentionned in your above code, at each iteration you are going to get a new clustering. All previous information is erased when you call fit, it's not a `partial_fit`. So that would be just equivalent to running the this clustering on N subsets of the full dataset I think.
[21:25] <5ecf797dd73408ce4fe523d7> Ah makes sense, thank you
--------------------------------------------------------------------------------------------------
[21:55] <5ecf797dd73408ce4fe523d7> Sorry never mind, I didn't know about epsilon! I gotta do more research on HDBSCAN as I'm not that familiar with it.
--------------------------------------------------------------------------------------------------
