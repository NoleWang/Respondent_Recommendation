[04:52] <561d08d0d33f749381a937bf> Why does predict_proba give better accuracy than the predict function? Whats the difference?
[04:53] <561d08d0d33f749381a937bf> What are the best parameters for param_grid when performing GridSearchCV on  svm?
[05:04] <561d08d0d33f749381a937bf> @ogrisel wohoo
--------------------------------------------------------------------------------------------------
[05:11] <56c4f19ae610378809c1f8ae> I'll try to answer your question in a few minutes when I return to a computer.
--------------------------------------------------------------------------------------------------
[05:22] <56c4f19ae610378809c1f8ae> @Fredilly as for your second question, it really depends on your dataset. Could you describe it?
[05:24] <56c4f19ae610378809c1f8ae> for predict_proba vs predict, theyre two completely different methods. predict_proba predicts the probability of your input being a certain class. Predict returns what class the model predicts your input to be (e.g. by taking the class with the highest probability from predict_proba)
[05:24] <56c4f19ae610378809c1f8ae> does that make sense?
[05:28] <56c4f19ae610378809c1f8ae> (if anyone else wants to clarify / correct / validate my explanation, please do)
--------------------------------------------------------------------------------------------------
[09:18] <561d08d0d33f749381a937bf> @nelson-liu That makes sense. From my understanding, predict_proba does the same thing as predict but it requires a threshhold value to activate in case of binary outputs. It still doesnt explain why it gets better results
[09:19] <56c4f19ae610378809c1f8ae> Im not sure what you mean by <unconvertable> get better results. are you doing binary classification?
--------------------------------------------------------------------------------------------------
[09:21] <561d08d0d33f749381a937bf> Yes...I get .55 score with predict and .88 score with predict_proba
--------------------------------------------------------------------------------------------------
[09:23] <56c4f19ae610378809c1f8ae> how are you getting a score with predict_proba? shouldnt it output an array with 2 elements?
[09:25] <561d08d0d33f749381a937bf> It does, but I slice the output and use the one that gives a true value.
[09:26] <56c4f19ae610378809c1f8ae> so to verify, you slice the output, find out which has the highest probability, and use that as the predicted, correct? what class are you using for classification?
[09:29] <56c4f19ae610378809c1f8ae> hmm how are you using two?
[09:31] <56c4f19ae610378809c1f8ae> well generally the way predict works it that it takes the likelihoods generated by predict_proba, and then chooses the most likely one
[09:32] <56c4f19ae610378809c1f8ae> so they should be the same.
[09:32] <56c4f19ae610378809c1f8ae> i feel like that shouldnt be happening haha but I have no empirical evidence to back up my claims. would you mind letting me see your code to ensure there are no random bugs?
--------------------------------------------------------------------------------------------------
[09:27] <561d08d0d33f749381a937bf> Logistic Regression and Gradient Boosting Classifier
[09:30] <561d08d0d33f749381a937bf> I tried both separately just to see which produced a better result and noticed that predict_proba gave a much higher accuracy in both cases.
[09:31] <561d08d0d33f749381a937bf> Should that always be the case? Im always discovering new techniques that completely invalidate everything else Ive learned.
--------------------------------------------------------------------------------------------------
[09:39] <561d08d0d33f749381a937bf> ``` encoder = preprocessing.OneHotEncoder() encoder.fit(np.vstack((X, X_test))) X = encoder.transform(X) X_test = encoder.transform(X_test)  # LogisticRegression logreg = LogisticRegression(C=3) logreg.fit(X, y) y_pred = logreg.predict_proba(X_test)[:, 1] df = pd.DataFrame({'id': test.id, 'Action': y_pred}) df.tail() df.to_csv('kagglesubmission.csv') #scores about .88  ```
--------------------------------------------------------------------------------------------------
[09:44] <56c4f19ae610378809c1f8ae> why do you slice [:, 1]?
[09:46] <56c4f19ae610378809c1f8ae> Maybe theres some pandas magic going on that Im not familiar with, but if you slice [:,1] arent you always getting the probabilities of the second element of model.classes_? Is there some way that you check whether this is greater than or less than the probability of the other class, and then put the correctly labeled prediction into the df?
[09:47] <561d08d0d33f749381a937bf> That way I select the column with a probability of 1.
[09:47] <561d08d0d33f749381a937bf> This is more magic than science to me.
[09:49] <56c4f19ae610378809c1f8ae> its possible that you might not be controlling for randomness between grid search and if you do a train test split or something. You should set a consistent random seed to ensure replicability.
[09:52] <56c4f19ae610378809c1f8ae> (in response to your previous comment about selecting the column with probability 1) hmm thats not how it works. lets say you have a model that takes in inputs x, y and you want to predict whether these inputs belong in A or B from it (binary classification). if you call predict_proba([x,y]), then it would output an array with the probabilities that the input [x,y] is each class, e.g. np.array([[ 0.4,0.6]]).
[09:53] <56c4f19ae610378809c1f8ae> lets say the output of `model.classes_` is `[A,B]`. This would indicate that the input vector you fed in (`[x,y]`) has a 40% chance of being something of class A, and a 60% chance of being something in class B.
--------------------------------------------------------------------------------------------------
[09:49] <561d08d0d33f749381a937bf> One quick question: Is it possible that gridsearchcv give worse score accuracy? I get .946 off the bat on an svm implementation but I get about .92 with the best parameters from gridsearchcv.
--------------------------------------------------------------------------------------------------
[09:55] <561d08d0d33f749381a937bf> I see what you mean but choosing the column with the highest true positives means you can immediately compare with null accuracy and decide whether to improve sensitivity of specificity.
--------------------------------------------------------------------------------------------------
[09:56] <56c4f19ae610378809c1f8ae> what do you mean highest true positives?
--------------------------------------------------------------------------------------------------
[09:58] <561d08d0d33f749381a937bf> Predicting a correct value of true. Eg predicting >0.5 when the real value is 1
[09:59] <561d08d0d33f749381a937bf> In my classification problem, if I just predicted 1 everytime, I would be correct 94% of the time.
[10:00] <561d08d0d33f749381a937bf> One of the two columns, [X, y] would have more 1s. When I slice, I use that column for predictions.
[10:02] <561d08d0d33f749381a937bf> They represent whether a user is granted access permission or not
--------------------------------------------------------------------------------------------------
[10:00] <56c4f19ae610378809c1f8ae> Oh. is that just an innate feature of the dataset? So basically you want to predict based on some sort of confidence ratio? e.g. you know its most likely 1, so predict 1 every time unless theres a very high confidence for 0?
[10:01] <56c4f19ae610378809c1f8ae> what do the two columns [X,y] represent?
[10:03] <56c4f19ae610378809c1f8ae> no sorry i mean is [x,y] the input, the result of predict_proba(), etc?
[10:04] <561d08d0d33f749381a937bf> its the result of the predict_proba
[10:06] <56c4f19ae610378809c1f8ae> when you have the return result of predict_proba(), the confidence of 1 is in one column and the confidence of 0 is the other. you cant really pick out <unconvertable> which column has more ones?
[10:07] <56c4f19ae610378809c1f8ae> (on that note, the reason why predict() and predict_proba() are different is because of the way youre using predict_proba() is not the same as how predict() would generate labels for your input vectors)
[10:08] <561d08d0d33f749381a937bf> You certainly can. Simply print out the first X elements of both columns and youll see a pattern immediately.
[10:08] <561d08d0d33f749381a937bf> I see what you mean.
[10:09] <56c4f19ae610378809c1f8ae> when you print out the two columns, do you get 0s and 1s..?
[10:09] <56c4f19ae610378809c1f8ae> the 0s and 1s are categorical and are assigned to one column each
[10:10] <56c4f19ae610378809c1f8ae> ok, that seems fairly reasonable. I feel like the reason you see a pattern is simply because 95% of your dataset is labeled one haha.
[10:11] <561d08d0d33f749381a937bf> Exactly.....going through the trouble of squeezing that extra 1% is insane.
--------------------------------------------------------------------------------------------------
[10:10] <561d08d0d33f749381a937bf> I get 0.10~0.99
--------------------------------------------------------------------------------------------------
[10:12] <56c4f19ae610378809c1f8ae> then why even bother using ml if you could just always guess `1` and be right 95% of the time?
[10:12] <561d08d0d33f749381a937bf> Tell that to Kaggle
[10:12] <56c4f19ae610378809c1f8ae> oh its a kaggle competition? would you mind sending me the link. that may help me explain haha
[10:13] <561d08d0d33f749381a937bf> https://www.kaggle.com/c/amazon-employee-access-challenge
[10:13] <561d08d0d33f749381a937bf> Getting my feet wet.....my head is spinning....most of this is just trial and error.
--------------------------------------------------------------------------------------------------
[10:15] <56c4f19ae610378809c1f8ae> hmm if you want to get your feet wet, id suggest titanic on kaggle. This is a pretty good tutorial https://github.com/savarin/pyconuk-introtutorial
[10:16] <56c4f19ae610378809c1f8ae> but regardless, predict_proba() doesnt work the way you think it does. Taking one column and encoding everything as that only happens to work because a large portion of your dataset is one label.
[10:18] <56c4f19ae610378809c1f8ae> did you use predict_proba() in titanic?
[10:19] <56c4f19ae610378809c1f8ae> its probably best to just stick to using predict() and further tuning your model / performing feature selection
[10:19] <56c4f19ae610378809c1f8ae> Its just a coincidence in this case that predict_proba() seems to work so much better than predict()
[10:27] <56c4f19ae610378809c1f8ae> Beyond that, feature engineering in terms of normalization and other transformations on the data can also be quite useful to do things like remove outliers, etc.
--------------------------------------------------------------------------------------------------
[10:17] <561d08d0d33f749381a937bf> I did that one....was pretty helpful. The employee challenge and vowpal rabbit challenge prepare you more thoroughly for real world ML problems
--------------------------------------------------------------------------------------------------
[10:20] <561d08d0d33f749381a937bf> From what Ive gathered so far, tuning features is tedious and not usually worth the hassle unless you have like a 100 features.
[10:20] <56c4f19ae610378809c1f8ae> thats not always true haha. feature engineering is quite important in the real world to distinguish signal from noise in data.
[10:22] <56c4f19ae610378809c1f8ae> e.g. if you wanted to predict whether an employee has access and you were given information about their salary, working hours, and favorite place to eat lunch on amazons campus. this is only 3 features, but its quite evident that where they eat lunch on amazons campus likely has no correlation / is not related to whether theyd have access.
[10:23] <56c4f19ae610378809c1f8ae> actually thats not necessarily true, maybe there are more high scale places reserved for executives or something but do you see my point?
[10:26] <56c4f19ae610378809c1f8ae> Yup.
--------------------------------------------------------------------------------------------------
[10:24] <561d08d0d33f749381a937bf> Youre probably right. Thats where <unconvertable> domain expertise <unconvertable> or just plain old common sense comes into play.
--------------------------------------------------------------------------------------------------
[10:29] <561d08d0d33f749381a937bf> Youre absolutely right. Have you tried gbm? Its awesome....I wanna learn more about XGboost. They produce high accuracies right off the bat.
--------------------------------------------------------------------------------------------------
[10:50] <564789be16b6c7089cbab8b7> In https://github.com/scikit-learn/scikit-learn/pull/5491 I am confused by which classifier is having its hyper parameters optimized in the examples with graphs. Does anyone know?
[11:30] <564789be16b6c7089cbab8b7> @MechCoder If you happen to be about I think this question is aimed at you :)
--------------------------------------------------------------------------------------------------
[14:33] <541a528b163965c9bc2053de> if your possible target classes are consecutive integers like `[0, 1, 2]` the `predict(X_test)` should return the same as `predict_proba(X_test).argmax(axis=1)`. `predict_proba` is just a way to ask for the confidence levels of the model when it's making a prediction. The final classification decision should be exactly the same.
--------------------------------------------------------------------------------------------------
