[10:07] <564789be16b6c7089cbab8b7> when I do OrdinalEncoder on my matrix X how can I make the mapping the same for each column?
[10:10] <564789be16b6c7089cbab8b7> currently it is different if there is one new value in a column that doesn't occur in another column
--------------------------------------------------------------------------------------------------
[11:03] <5479972adb8155e6700d9370> Hello Happy scikit-learners ! I need some help please. I want to serve an onnx model.  Input = 144 columns ( medical records, some categoricals, some  not ).  Output = classification.  Pipeline = StandardScaler + LabelEncoder + LightGBM.  I am stuck with the LabelEncoder. Any example of such configuration somewhere ? Google was not my friend. I was able to produce an onnx model when bypassing the LabelEncoder... but I need it and want to avoid 1HE because LightGBM performs much better without 1HE.  Anyone ?
--------------------------------------------------------------------------------------------------
[13:22] <matrix-rthy:matrix.org>  @citron You probably want OneHotEncoder not the LabelEncoder
[13:23] <matrix-rthy:matrix.org> Also tree based models it's better to use OrdinalEncoder instead for categorical features
--------------------------------------------------------------------------------------------------
[15:14] <5baf7d9ad73408ce4fa9c9b2> > Also tree based models it's better to use OrdinalEncoder instead for categorical features  I'm not sure that's true, using OE will make the trees treat categories as ordered values, but they're not. Native categorical support (as in LightGBM) properly treats categories as un-ordered and can yield the same splits with less tree depth
--------------------------------------------------------------------------------------------------
[15:28] <matrix-rthy:matrix.org> Yes, you are right. I guess I'm too used to scikit-learn tree based models not having native categorical support )
--------------------------------------------------------------------------------------------------
[16:38] <541a528b163965c9bc2053de> I agree with @NicolasHug in theory, but in practice the difference with `OrdinalEncoder` (with tuned hyperparams) is typically negligible ;)
--------------------------------------------------------------------------------------------------
[16:39] <541a528b163965c9bc2053de> @citron Using `OrdinalEncoder` is probably the pragmatic solution. `OneHotEncoder` is only efficient if you use sparse output which are currently not supported by ONNX as far as I know.
--------------------------------------------------------------------------------------------------
[16:46] <5de8bd5ad73408ce4fd32399> @citron: what's the issue with LabelEncoder and ONNX? (I'm the main author of sklearn-onnx).
--------------------------------------------------------------------------------------------------
[16:46] <541a528b163965c9bc2053de> @citron also you said "Pipeline = StandardScaler + LabelEncoder + LightGBM." but I assume you use  a column transformer to separate to only scale the numerical features and encode the categorical feature separately: https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data  BTW, StandardScaling the numerical features if often useless for tree-based models in general, and even more so for implementations such as LightGBM than bin the features.
--------------------------------------------------------------------------------------------------
[17:02] <5479972adb8155e6700d9370> Bonjour @xadupre , @ogrisel, @rthy:matrix.org, @NicolasHug . Yes I do use a ColumnTransformer. Maybe I should better express my needs.  The training set is made of 300000 rows.  Colums types are either floating points, integers ( and sadly Pandas does not provide the R Dataframe handling of N/A ), booleans, categories or list of categories. For instance, some category columns may have 2 or 10 numerical categories, some only have "string" categories, some have a list of medicaments or a list of pathologies.  I have tried plenty of frameworks and among them, lightGBM was the best. Now, as I need to export the model and the pipeline in ONNX/ONNX-ML format, I need to wrap lightGBM in something to keep the pipeline around.
[17:06] <5479972adb8155e6700d9370> @ogrisel Yes, no problem with pure int columns.
--------------------------------------------------------------------------------------------------
[17:05] <541a528b163965c9bc2053de> pandas 1.0 and later has support for explicit missing values in integer columns: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html
[17:06] <541a528b163965c9bc2053de> scikit-learn however will convert this to a float anyway (but no big deal).
--------------------------------------------------------------------------------------------------
[17:08] <541a528b163965c9bc2053de> For the categorical columns, try to use OrdinalEncoder. In 0.24+ we have better support for unknown categories at test time:  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html  Although I am not sure that sklearn-onnx has replicated that feature yet.
[17:08] <5479972adb8155e6700d9370> @ogrisel maybe @xadupre knows ?
--------------------------------------------------------------------------------------------------
[17:09] <541a528b163965c9bc2053de> If you have specific problems exporting a pipeline with OrdinalEncoder to onnx, better report the exact error message with a simple reproduction case to https://github.com/onnx/sklearn-onnx
--------------------------------------------------------------------------------------------------
[17:22] <5de8bd5ad73408ce4fd32399> I wrote this example about converting a pipeline including a lightgbm model in a scikit-learn pipeline: http://onnx.ai/sklearn-onnx/auto_tutorial/plot_gexternal_lightgbm.html.
--------------------------------------------------------------------------------------------------
[17:36] <5479972adb8155e6700d9370> @xadupre Thanks! The binder link at the end of the page has a problem. In fact, that's the example I started with. Works fine without labelEncoder.
[17:59] <5479972adb8155e6700d9370> I forgot to tell an important thing : I do use FLAML to select the best hyperparameters and thus the best model.
--------------------------------------------------------------------------------------------------
[18:11] <5de8bd5ad73408ce4fd32399> I'll investigate the issue with LabelEncoder then. What is the error you get?
--------------------------------------------------------------------------------------------------
