[14:06] <579618a040f3a6eec05c5e42> Hey folks, I have maybe a silly question. But is there any fundamental difference between `model_selection.cross_validate` and doing it manually?  For instance, take a look at the example below, I can't figure out why the roc scores in red are different while the logloss scores in green are the same?  I expect that the roc scores should match, unless I'm doing sth stupid and haven't noticed yet?
[14:06] <579618a040f3a6eec05c5e42> [![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/s4Ci/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/s4Ci/image.png)
--------------------------------------------------------------------------------------------------
[15:08] <579618a040f3a6eec05c5e42> I seems that `cross_validate` uses silently a different splitting mechanism depending on label `y`. For int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, Fold is used.  If that's true then this is an undesired behaviour for me as a user. I want things to be explicit and not implicit like this spending hours searching to find what's going on?  On an additional note I don't understand the phrase `In all other cases, Fold is used.`, so in the case of my example instead of using `StratifiedGroupKFold` is `cross_validate` implicitly using just `Fold`?
--------------------------------------------------------------------------------------------------
