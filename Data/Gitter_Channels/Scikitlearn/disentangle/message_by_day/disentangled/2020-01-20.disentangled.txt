[07:04] <5a2c58c8d73408ce4f8294ba> Try TF-IDF
--------------------------------------------------------------------------------------------------
[13:15] <5dce8ae9d73408ce4fd11e31> Hello. How can I make the program better predict? When you enter the numbers 771, 322, 344, 632, 10, the program predicts 234168, but I need it to be 200000-210000. In linear regression, more than 1000 examples are already embedded.
--------------------------------------------------------------------------------------------------
[15:18] <5a7dae0ad73408ce4f8c6d2e> Thanks @NicolasHug . I'm trying to figure out why sklearn's GradientBoostingClassifier gives different estimates from R's GBM. I had thought it might be the criterion for splitting, but maybe not. Any suggestions?
--------------------------------------------------------------------------------------------------
[19:25] <5baf7d9ad73408ce4fa9c9b2> @jacobcvt12 I'm not familiar with R's gbm. The splitting criterion will definitely be a major factor. I'd suggest checking the parameters of each implementation and try to find equivalent settings. In a vanilla implementation of gradient boosting (ignoring the sub-estimator which is a tree in our case), the only parameters are the learning rate / shrinkage, the loss, and the number of iterations.
--------------------------------------------------------------------------------------------------
