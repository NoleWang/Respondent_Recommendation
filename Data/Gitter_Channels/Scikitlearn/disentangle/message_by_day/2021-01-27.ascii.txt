[14:23] <564789be16b6c7089cbab8b7> I am trying HistGradientBoostingClassifier for multiclass classification. It seems to stop too early. I.e. I get [22/200] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 4.36169, val loss: 2.27454, in 0.114s [23/200] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 5.42246, val loss: 2.60417, in 0.113s [24/200] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 5.19449, val loss: 3.30153, in 0.113s Fit 624 trees in 2.951 s, (19300 total leaves)
[14:23] <564789be16b6c7089cbab8b7> you can get a much smaller loss using catboost for example
[14:23] <564789be16b6c7089cbab8b7> but it is much faster than catboost for multiclass classification
[14:32] <564789be16b6c7089cbab8b7> hmm.. is multiclass classification meant to work yet with HistGradientBoostingClassifier ?
[22:24] <5e3f3d7cd73408ce4fd915a4> I'm trying to modify the sklearn transformer interface for transformers that need to transform `X`, `y` and `sample_weight` together, i.e. the entire dataset. This is the signature I came up with that allows chaining these transformers with a `Pipeline`, I'm wondering if anyone has any better ideas? I really don't like having a `dummy` parameter.  ```python3     class DatasetTransformer(BaseEstimator, TransformerMixin):         def fit(self, data, dummy=None) -> "DatasetTransformer":             X, y, sample_weight = data             ...             return self          def transform(self, data):             X, y, sample_weight = data             ...             return (X, y, sample_weight) ```
[23:44] <5e3f3d7cd73408ce4fd915a4> One alternative I see is to not have the `dummy` parameter, and instead specify `"passthrough"` as the last estimator in pipelines. I think this may be better? `dummy` is only there because it would be confusing to have a `y` parameter when `y` is part of `data`.
