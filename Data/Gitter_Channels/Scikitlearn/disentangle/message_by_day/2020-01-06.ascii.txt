[16:12] <54d4a1d6db8155e6700f853b> ugh they credit me as the creator of sklearn
[16:16] <567f5d7716b6c7089cc043a8> haha, yeah I saw :D THE creator :P to be fair, you're the sole maintainer contact on pypi (IIRC)
[16:16] <54d4a1d6db8155e6700f853b> that means nothing lol
[17:40] <59605bcbd73408ce4f6c2b60> @amueller Thanks for the feedback haha! I'll edit the post soon to correct what you just pointed out. I sincerely thought you were the main creator of sklearn, as you are the top contributor, and also that you are very very involved. I'd love to know if there is anything I could do to help, or if you have any idea of things you'd like to see in Neuraxle to help with making sklearn more integrated in Deep Learning projects.   For instance, I think the following code snippet is really talkative as a way to do Deep Learning pipelines using the pipe and filter design pattern: https://www.neuraxle.org/stable/Neuraxle/README.html#deep-learning-pipelines  Would you have any ideas to share, or things you'd like to point out for me to work on next with Neuraxle? 
[17:44] <59605bcbd73408ce4f6c2b60> @glemaitre "Limitations and Caveats ..." sounds cool! I could rename the article. I wanted it to catch the eye, seems like it worked hehe. I love sklearn tho :)   On my side, I've already fixed 95% of the issues I listed, in Neuraxle (as per the links to the Neuraxle website documentation for each problem listed). 
[17:51] <55d21ee30fc9f982beadabb8> Regarding deep learning pipeline, I think that we want to be conservative: https://scikit-learn.org/stable/faq.html#why-is-there-no-support-for-deep-or-reinforcement-learning-will-there-be-support-for-deep-or-reinforcement-learning-in-scikit-learn
[17:53] <55d21ee30fc9f982beadabb8> Issues regarding serialization and hyperparameter search could be discussed, however.
[17:53] <55d21ee30fc9f982beadabb8> I think that onnx-sklearn provide a nice way to deployed scikit-learn model in production
[17:55] <567f5d7716b6c7089cc043a8> yeah, that's the goal (onnx-sklearn), but it still needs a bit of work. I'm all in favor of focusing a bit on partial_fit (mini batches) though.
[17:55] <55d21ee30fc9f982beadabb8> but this is rather challenging to retrain models and update models across versions. This might not be in the scope of scikit-learn but having a third-library to manage those could be nice
[17:57] <55d21ee30fc9f982beadabb8> @adrinjalali Incremental learning, early stopping, and callbacks are things which would be nice
[17:57] <55d21ee30fc9f982beadabb8> they are in the roadmap I think
[17:58] <567f5d7716b6c7089cc043a8> yeah they are, they're just hard :P
[17:58] <55d21ee30fc9f982beadabb8> :) yes indeed
[17:59] <59605bcbd73408ce4f6c2b60> Nice to confirm that you scope scikit-learn like that. I feared I'd play a bit too much in your backyard but it seems fine, I'm glad you have this opinion. I'm totally down to make Neuraxle a way to handle all those callbacks and things required for doing deep learning, + serialization. I don't know about Onyx, but there could be a way that I adapt to that to save every neural net usign that instead of building custom savers. For now I'm doing 2 other libraries already: Neuraxle-TensorFlow and Neuraxle-PyTorch to provide default neural net savers to allow serialization and checkpointing and have those models have their special callbacks. Might also do Neuraxle-Keras and so forth. 
[18:00] <55d21ee30fc9f982beadabb8> you also have keras-onnx
[18:00] <55d21ee30fc9f982beadabb8> and pytorch-onnx
[18:01] <55d21ee30fc9f982beadabb8> which manage the same way than sklearn-onnx
[18:01] <55d21ee30fc9f982beadabb8> but this is for prediction only
[18:01] <54d4a1d6db8155e6700f853b> They are for serialization and deployment, though. I think @gulliaume-chevalier wants training as well
[18:01] <54d4a1d6db8155e6700f853b> lol ok you beat me to it ;)
[18:01] <59605bcbd73408ce4f6c2b60> I'll need to look into that. For now, with Neuraxle, someone could do this using 3 tf functions that builds tf graphs:  ``` model = TensorflowV2ModelStep(     create_model, create_loss, create_optimizer,     has_expected_outputs=False ).set_hyperparams(hp).set_hyperparams_space(hps) ``` And I have savers that allows for saving and reloading and continue a fit (already!) 
[18:04] <59605bcbd73408ce4f6c2b60> Same API would work for TF v1 using a TensorflowV1ModelStep instead, also PyTorch (using some `nn.Module`s), and eventually Keras in some ways
[18:05] <59605bcbd73408ce4f6c2b60> I also have a `ParallelTransform` class which uses the savers for parallelizing instead of using joblib. So all the pytorch, tf, and keras code is parallelizeable. I also am building right now a `ClusteringWrapper` which acts like the ParallelTransform using savers, but sends the saved wrapped pipeline over a worker that has a REST API. So the Clustering Wrapper can split a batch of data to N workers, by first sending the model, and then sending the data it splitted in parallel. 
[18:09] <59605bcbd73408ce4f6c2b60> The same concept applies to a new `StreamingPipeline` class I'm creating right now :D it has the ability to have some steps (e.g.: sub-pipelines) run in different threads, and to have queues between each thread like a consumer-producer design pattern. I also already have a [MiniBatchSequentialPipeline](https://www.neuraxle.org/stable/api/neuraxle.pipeline.html#neuraxle.pipeline.MiniBatchSequentialPipeline) that just like a single-threaded [Pipeline](https://www.neuraxle.org/stable/api/neuraxle.pipeline.html#neuraxle.pipeline.Pipeline), but that already uses mini-batches, meaning that it splits the batches into mini batches, and it's just like having a normal Pipeline but calling `.fit` many times in a row (sorry, I didn't name it `partial_fit`, my `fit` is already thought of as potentially always a partial one. 
[18:15] <59605bcbd73408ce4f6c2b60> @glemaitre You said:  > @adrinjalali Incremental learning, early stopping, and callbacks are things which would be nice  If you look closely [here](https://www.neuraxle.org/stable/Neuraxle/README.html#deep-learning-pipelines), I already have incremental learning (e.g.: if you CTRL+F for the `MiniBatchSequentialPipeline `). I'd love to add early stopping and other callbacks soon, good idea. I opened an issue [here](https://github.com/Neuraxio/Neuraxle/issues/228) for such things, I'd add callbacks to it! 
[18:16] <59605bcbd73408ce4f6c2b60> So in the issue [#228](https://github.com/Neuraxio/Neuraxle/issues/228) I just linked to, there is some example API code, but it might not be enough. I'd like to really discover the good design patterns for that, although I at least found something that seems like it would work properly. 
