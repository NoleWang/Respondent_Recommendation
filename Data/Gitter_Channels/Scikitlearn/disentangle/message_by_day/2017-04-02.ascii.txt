[06:43] <588f320bd73408ce4f46ee3d> The overlap of the shaded curve hulls around 150 an 175 training samples would bring doubt to me, if wether this presentation is useful to judge the parameters impact - but I am alien to machine learning ;-)
[09:19] <55476cb515522ed4b3dfe7eb> Hy guys, I'm trying to learn scikit clustering, but can not get into final step before giving data into clustering algorithms, hope someone will be able to point me out direction of next step to get two dimmensional array from dataframe so it can be used by algorithms like MeanShift or may be DBSCAN or something else  ``` import pandas as pd df = pd.read_csv('http://sandbox.mac-blog.org.ua/sample.csv') # C1..C5 - categorical, D1..D10 - dates, B1..B28 - binary, 100K rows df = df.drop(['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10'], 1) # do not understand how to deal with this df = df.fillna(False) # looking around, all categorical data has values, treating NaN for all binaries as false  # going to convert all binaries into 0..1 ints for c in df.columns:     if c.startswith('B'):         df[c] = df[c].astype('int')  # totally not sure should such things be done print('Before:', len(df)) # 100000 df = df.drop_duplicates() print('After:', len(df)) # 35944  # not sure is it good idea at all # but after that I have reduced number of columns from 33 to 12 from sklearn.feature_selection import VarianceThreshold sel = VarianceThreshold(threshold=(.8 * (1 - .8))) sel.fit(df) labels = [df.columns[x] for x in sel.get_support(indices=True)] print('Before:', len(df.columns)) # 33 df = pd.DataFrame(sel.fit_transform(df), columns=labels) print('After:', len(df.columns)) # 12  # not sure do I need something like StandardScaler or LabelEncoder?  # every example of clustering algorithms like https://www.youtube.com/watch?v=EQZaSuK-PHs expect 2 dimensional array - kind of stuck here  df.head() ```
[12:11] <55476cb515522ed4b3dfe7eb> Seems that have found one way:  step 1 looking around on data   ``` from sklearn.decomposition import PCA      pca = PCA(n_components=2) pca.fit(df) existing_2d = pca.transform(df) plt.scatter(existing_2d.T[0], existing_2d.T[1], c='b') ```  step 2 clustering  in my case i definitely see 4 clusters so using kmeans  ``` from sklearn.cluster import KMeans km = KMeans(n_clusters=4) km.fit(df) df['Cluster'] = pd.Series(clusters, index=df.index) # append cluster column to dataframe ```  step 3 get usefull data  ``` desired = [] for col in df.columns:     if col != 'Cluster':         vals = [] # will contain top 1 value from each cluster         for cluster in list(set(km.labels_)):             vals.append(df[df['Cluster']==cluster][col].value_counts().head(h).reset_index().rename(columns={'index': col, col: 'Count'}).iloc[0][col])         if len(np.unique(vals)) > 1:             desired.append(col) # we are looking only for columns that are changing between clusters  xx = [] for cluster in list(set(km.labels_)):     x = {'Cluster': cluster}     for col in desired:         h=1         z = df[df['Cluster']==cluster][col].value_counts().head(h).reset_index().rename(columns={'index': col, col: 'Count'})         x[col] = z.iloc[0][col]     xx.append(x)      pd.DataFrame(xx) ```  not sure if this is a right way but got answer dataframe with 6 columns (5 categorical and 1 binary) describing top1 from each cluster (4 rows)  hope that may be helpful
