[13:23] <5fdb56f7d73408ce4ff6c2f5> Hi all, I implemented a new feature for gaussian mixture models and I'm wondering whether it would be useful to have it integrated in scikit-learn. Should I open an issue to discuss this or can it be discussed here? In brief, I implemented the mixture entropy estimators (lower and upper bound) introduced in this paper https://arxiv.org/pdf/1706.02419.pdf
[13:58] <567f5d7716b6c7089cc043a8> hi @giuliolovisotto , please have a look at our inclusion criteria: https://scikit-learn.org/dev/faq.html?highlight=inclusion%20criteria#what-are-the-inclusion-criteria-for-new-algorithms
[14:14] <5fdb56f7d73408ce4ff6c2f5> Hi Adrin, thanks for that, so if I check, I get:     * [x] 3 years since publication: (2017)   * [ ] 200+ citations: no, 57 at the moment.   * [?] wide use and usefulness: this is a bit arbitrary.   * [?] clear cut improvement: for some settings (in particular with larger no. of features and no. of GMM components) using those bounds gives more efficient and more accurate entropy estimation than using a Monte Carlo sampling approach (which can be done with the GMM.score method).  By seeing this would you say I should open an issue to discuss this on github or just leave it?   
[15:44] <5547dd8a15522ed4b3dfed5a> Colleagues: looking for best practice tips to get logging output from sklearn, in particular from KNN clustering where Im having trouble figuring out how to use `verbose=1` (parameter doesnt seem to exist on `sklearn.neighbors.KNeighborsRegressor` nor on the `.fit()` method)  Have reviewed lots of issues on the subject but unclear current status of logging context managers or similar.
[16:36] <55d21ee30fc9f982beadabb8> @ijstokes We don't have any logging. The verbose is only printing on the stdout.
[16:37] <55d21ee30fc9f982beadabb8> We are currently looking at improving this part with a real logging system
[16:50] <5547dd8a15522ed4b3dfed5a> @glemaitre thank you for the prompt response. I dont see a way to get *any* output from KNeighborsRegressor.  Am I missing something?
[16:51] <55d21ee30fc9f982beadabb8> There is no such parameter for a `KNeighborsRegressor`
[16:51] <55d21ee30fc9f982beadabb8> Indeed the regressor does not anything during `fit` apart of storing the dataset
[17:02] <567f5d7716b6c7089cc043a8> @giuliolovisotto I'd probably open an issue to discuss it. My gut feeling is that it doesn't pass the inclusion criteria, but it'd be nice to also here what other maintainers think on the issue tracker
[19:09] <5fdbaa88d73408ce4ff6cb87> Hey folks <unconvertable> Want to talk with you about pipelines <unconvertable>
[19:09] <5fdbaa88d73408ce4ff6cb87> What is the use case they were created to cover? 
[19:10] <5fdbaa88d73408ce4ff6cb87> In most of the examples, people groups processing "branches" by feature types (num/cat)
[19:11] <5fdbaa88d73408ce4ff6cb87> Is this the main use case pipelines were designed for?
[19:12] <5fdbaa88d73408ce4ff6cb87> What I was trying to do, but got frustrated is convert my Pandas-based data preprocessing into sklearn pipelines: 
[19:19] <5fdbaa88d73408ce4ff6cb87> ``` # just copied some parts of the notebook to illustrate   # 1. data cleaning like this for feature in (     'PoolQC',      'FireplaceQu',      'Alley',      'Fence',      'MiscFeature',      'BsmtQual',      'BsmtCond',      'BsmtExposure',      'BsmtFinType1',      'BsmtFinType2',     'GarageType',      'GarageFinish',      'GarageQual',      'GarageCond',     'BsmtQual',      'BsmtCond',      'BsmtExposure',      'BsmtFinType1',      'BsmtFinType2',     'MasVnrType', ):     train_df[feature] = train_df[feature].fillna('None')     test_df[feature] = test_df[feature].fillna('None')     full_df[feature] = full_df[feature].fillna('None')  for dataframe in [train_df, test_df, full_df]:     dataframe['MSZoning'] = dataframe.groupby(['Neighborhood'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))     dataframe['MSSubClass'] = dataframe.groupby(['HouseStyle'])['MSSubClass'].transform(lambda x: x.fillna(x.mode()[0]))     dataframe['LotFrontage'] = dataframe.groupby(['Neighborhood', 'MSSubClass'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))     dataframe['Functional'] = dataframe['Functional'].fillna('Typ')  # 2. Some ordinal encoding   ordinal_feature_mapping = {     'ExterQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},      'ExterCond': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},     'BsmtQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'BsmtCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},     'BsmtFinType2': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},     'HeatingQC': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},     'KitchenQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},     'FireplaceQu': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},     'GarageQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'GarageCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'PoolQC': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'Fence': {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4},     'PavedDrive': {'N': 0, 'P': 1, 'Y': 2},     'CentralAir': {'N': 0, 'Y': 1},     'Alley': {'None': 0, 'Pave': 1, 'Grvl': 2},     'Street': {'Pave': 0, 'Grvl': 1},     'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},     'Functional': {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7} }  non_ordinal_cat_features = list(set(cat_features) - set(ordinal_feature_mapping.keys()))  for cat_feature in non_ordinal_cat_features:     train_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(train_df[cat_feature])     test_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(test_df[cat_feature])     full_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(full_df[cat_feature])  for ordinal_feature, feature_mapping in ordinal_feature_mapping.items():     train_df[ordinal_feature + 'Enc'] = train_df[ordinal_feature].map(feature_mapping)     test_df[ordinal_feature + 'Enc'] = test_df[ordinal_feature].map(feature_mapping)     full_df[ordinal_feature + 'Enc'] = full_df[ordinal_feature].map(feature_mapping)  # 3. Excessive feature engineering  for dataframe in [train_df, test_df, full_df]:     dataframe['HasFireplace'] = dataframe['Fireplaces'].apply(lambda x: int(x > 0))     dataframe['HouseAge'] = dataframe['YrSold'].astype('int') - dataframe['YearBuilt'].astype('int')      dataframe['TotalBathrooms'] = (dataframe['FullBath'] + (0.5 * dataframe['HalfBath']) +                                 dataframe['BsmtFullBath'] + (0.5 * dataframe['BsmtHalfBath']))      dataframe['OverallHouseQC'] = dataframe['OverallQual'] + dataframe['OverallCond']     dataframe['IsPavedDrive'] = (dataframe['PavedDrive'] == 'Y') * 1      dataframe['IsNeighborhoodElite'] = (dataframe['Neighborhood'].isin(['NridgHt', 'CollgeCr', 'Crawfor', 'StoreBr', 'Timber'])) * 1    # bunch of other features ```
[19:20] <5fdbaa88d73408ce4ff6cb87> These three stages are the main in my data processing flow. 
[19:22] <5fdbaa88d73408ce4ff6cb87> So ideally I would like to process data in the same order in the pipeline as well. Then it would be converted naturally to pipeline definition 
[19:25] <5fdbaa88d73408ce4ff6cb87> So I would imagine a pipeline definition like this: ``` Pipeline([     ('missing_value_imputing', ColumnTransformer([...])),     ('feature_engineering', FeatureUnion([...])),     ('feature_transforming', ColumnTransformer([...])), ]) ```
[19:28] <5fdbaa88d73408ce4ff6cb87> However, this doesn't work, because missing_value_imputing step would return me numpy array which is hard to work with on the following stages 
[19:33] <5fdbaa88d73408ce4ff6cb87> In the same time, my data processing has  constrains (feature_engineering step requires all values in place (missing_value_imputing) and feature_transforming requires all set of features (feature_engineering)). There are also operations I could apply on a multiple columns (and would love to do) like "None" constant imputing or ordinal encoding  and single column specific actions like MSZoning imputing 
[19:35] <5fdbaa88d73408ce4ff6cb87> Please let me know if all of this makes any sense 
[19:36] <5fdbaa88d73408ce4ff6cb87> With that being said, I'm wondering what is the cleanest way to define sklearn pipeline for this task?
