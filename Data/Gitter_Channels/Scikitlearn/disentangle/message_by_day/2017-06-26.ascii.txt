[09:43] <55502d0c15522ed4b3e03330> Hi all, I want to revisit the question regarding how to interpret the output of `decision_function`into something meaningful. With the output from `decision_function`, how can I determine if a prediction is trust worthy or not, given some threshold `alpha`?
[09:45] <541a528b163965c9bc2053de> There is no generic method, it depends on your estimator and the kind of multiclass reduction used by the underlying model (e.g. One vs One for SVC, One vs All for most other models such as LinearSVC).
[09:48] <541a528b163965c9bc2053de> You might also have heteroschedastic prediction errors: some regions of your feature space might lead to a higher error rate and this heteroschedasticity might not be part of the model assumptions (or maybe cannot be handled properly by the model capacity).
[09:51] <541a528b163965c9bc2053de> Which model family are you interested in? One interesting thing to consider is using a calibrated classifier (especially for binary classification): http://scikit-learn.org/stable/modules/calibration.html This way you can better interpret the output of `clf.predict_proba`. This won't solve any heteroschedasticity issues though.
[09:53] <541a528b163965c9bc2053de> What you can also do is plot the precision / recall curve, select an admissible precision level (e.g. 0.8) according to business considerations and select the model and threshold that maximizes the recall at that precision level. We currently don't have a high level API to do this but the http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html function of scikit-learn returns the thresholds for each point along the PR curve.
[09:57] <541a528b163965c9bc2053de> Once you have fitted your best model, you can compute its precision and recall on a held out test set to check that this is still fine, and further analyse that you don't have strong heteroschedasticity in your precision / recall metrics. For instance if you use your classifier to build a recommender systems for users, you can check that you get approximately similar performance for various ways to split your user base, e.g.: male vs female users, age groups, geography, very engaged users vs casual users...
[12:23] <55502d0c15522ed4b3e03330> @ogrisel thanks Olivier :smile: I'll give it a try
[12:23] <55502d0c15522ed4b3e03330> I am doing facial recognition, so it's multiclass
[12:25] <541a528b163965c9bc2053de> you should definitely try to label your test dataset with additional info such as facial hair, glasses, long hair vs short air, gender, maybe ethnicity so as to compare the accuracy of your model for different groups of people.
