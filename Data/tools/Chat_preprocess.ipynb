{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7fe54e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wangsimin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/wangsimin/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as py\n",
    "from preprocess_helper import PorterStemmer\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1630d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Preprocess:\n",
    "    def __init__(self):\n",
    "        self.stopwords_list = None\n",
    "        self.tag_list = None\n",
    "        self.reservedkeywords = None\n",
    "    \n",
    "    def create_reservedkeywords(self, path, num_words = 100):\n",
    "        df = pd.read_csv(path)\n",
    "        df.sort_values(by=['Count'], ascending=False, inplace=True, ignore_index=True)\n",
    "        self.tag_list = list(df[\"Tag\"].values)\n",
    "        reservedkeywords_extra = [\"c#\",\"f#\",\"c++\",\"node.js\",\"nodejs\",\".json\",\".js\",\".net\",\"objective-c\",\n",
    "                                  \"asp.net\",\"ruby-on-rails\",\"angular.js\"]\n",
    "        self.reservedkeywords = list(df[\"Tag\"].values)[:num_words]\n",
    "        self.reservedkeywords.extend(reservedkeywords_extra)\n",
    "        self.reservedkeywords = set(self.reservedkeywords)\n",
    "        #print(self.reservedkeywords)\n",
    "    \n",
    "    def create_stopwords(self):\n",
    "        self.stopwords_list = stopwords.words('english')\n",
    "        stop_words_extra = [\"i'd\",\"sometime\",\"sometimes\",\"something\",\"someone\",\"somebody\",\"anything\",\"anyone\",\"anybody\",\n",
    "                            \"everytime\",\"everything\",\"everyone\",\"everybody\",\"e.g.\",\"e.g\",\"e.g.,\",\"i.e.\",\"i.e\",\"i.e.,\",\"love\",\n",
    "                            \"know\",\"'s\",\"wonder\"]\n",
    "        self.stopwords_list.extend(stop_words_extra)\n",
    "        stopwords_unsure_list = set(self.stopwords_list).intersection(set(self.tag_list))\n",
    "        self.stopwords_list = set(self.stopwords_list).difference(stopwords_unsure_list)\n",
    "        #print(len(self.stopwords_list),\"stopwords\",self.stopwords_list)\n",
    "    \n",
    "    def remove_non_ascii(self,sentence):\n",
    "        return ''.join(char for char in sentence if ord(char) < 128)\n",
    "    \n",
    "    def html_Filter(self, sentence):\n",
    "        sentence = BeautifulSoup(sentence, \"lxml\").text\n",
    "        #print(\"after html_Filter\",sentence)\n",
    "    \n",
    "        return sentence\n",
    "    \n",
    "    def keywords_transform(self, sentence):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.replace(\"node js\",\"node.js\")\n",
    "        sentence = sentence.replace(\"objective c\",\"objective-c\")\n",
    "        sentence = sentence.replace(\"ruby on rails\",\"ruby-on-rails\")\n",
    "        sentence = sentence.replace(\"angular js\",\"angular-js\")\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def POSTag_generation(self, sentence):\n",
    "        sentence_clean = []\n",
    "        \n",
    "        #split text into indiviual sentences for better pos tagging\n",
    "        sentence_list = sent_tokenize(sentence)\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            text = word_tokenize(sentence)\n",
    "            text_new = []\n",
    "            skip_num = False\n",
    "            \n",
    "            for i,word in enumerate(text):\n",
    "                #keep \"c#\" and \"F#\"\n",
    "                \n",
    "                if skip_num:\n",
    "                    skip_num = False\n",
    "                    continue\n",
    "                \n",
    "                if word == \"c\" or word == \"f\":\n",
    "                    if i+1 <len(text) and text[i+1] == \"#\":\n",
    "                        text_new.append(word+\"#\")\n",
    "                        skip_num = True\n",
    "                    else:\n",
    "                        text_new.append(word)\n",
    "                else:\n",
    "                    text_new.append(word)\n",
    "            \n",
    "            print(text_new)\n",
    "            pos_words = nltk.pos_tag(text_new)\n",
    "         \n",
    "            for i,pos_word in enumerate(pos_words):\n",
    "                #keep \"c#\" and \"F#\"\n",
    "                word = pos_word[0]\n",
    "                if word == \"c\" or word == \"f\":\n",
    "                    if i+1 <len(pos_words) and pos_words[i+1][0] == \"#\":\n",
    "                        sentence_clean.append(word+\"#\")\n",
    "                                 \n",
    "                sentence_clean.append(word)\n",
    "         \n",
    "        #return re.sub(r'\\W+', ' ', ' '.join(sentence_clean))\n",
    "        \n",
    "        #print(\"after POSTag_Removal\",' '.join(sentence_clean))\n",
    "        return ' '.join(sentence_clean)\n",
    "    \n",
    "    def remove_specialchar(self, sentence, char_to_keep = {'#','+','.','-'}):\n",
    "        punct_set = set(punctuation).difference(char_to_keep)\n",
    "\n",
    "        for i in punct_set:\n",
    "            # Replace the special character with an empty string\n",
    "            sentence=sentence.replace(i,\" \")\n",
    "        \n",
    "        return sentence\n",
    "        \n",
    "    def sentence_stem(self, sentence):\n",
    "        p = PorterStemmer()\n",
    "        output = \"\"\n",
    "\n",
    "        for token in sentence.split(' '):\n",
    "            if token.isalnum():\n",
    "                output += p.stem(token, 0,len(token)-1)+' '\n",
    "            elif token in self.reservedkeywords:\n",
    "                output += token+' '\n",
    "        \n",
    "        #print(\"after sentence_stem\", output.strip())\n",
    "        return output.strip()\n",
    "    \n",
    "    def load_dataset(self, path, data_selectedId=None):\n",
    "        # creating cleaned input, output pairs\n",
    "        print(\"Start loading data...\")\n",
    "        allfiles = os.listdir(path)\n",
    "        \n",
    "        if data_selectedId == None:\n",
    "            data_selectedId = [i for i in range(len(allfiles))]\n",
    "        \n",
    "        questions = []\n",
    "        \n",
    "        for filename in data_selectedId:\n",
    "            text = \"\"\n",
    "            #retreive qa_text\n",
    "            try:\n",
    "                with open(path+str(filename+1), 'r', encoding=\"utf-8\") as f:\n",
    "                    for line in f.readlines():\n",
    "                        line = line.replace(\"\\n\",\"\")\n",
    "                        line = line.strip()\n",
    "                        if line == \"\":\n",
    "                            continue\n",
    "\n",
    "                        text += line+\" \"\n",
    "    \n",
    "                questions.append(text.strip())\n",
    "            except:\n",
    "                print(str(filename+1),\"is missing!\")\n",
    "            \n",
    "            print(\"Loaded\",len(questions),)\n",
    "        return questions\n",
    "\n",
    "    def call(self, dataset = None, keywords = None):\n",
    "        data_clean = []\n",
    "        tag_path = \"../StackExchange/final_data/tag_dict.csv\"\n",
    "        \n",
    "        if dataset == None:\n",
    "            file_path = \"../StackExchange/final_data/rawdata/\"\n",
    "            with open(\"../StackExchange/final_data/selected_id.txt\", 'r') as f:\n",
    "                all_ids = f.readlines()\n",
    "\n",
    "            data_selectedId = sorted([int(tid.replace(\"\\n\",\"\")) for tid in all_ids])\n",
    "            \n",
    "            dataset = load_dataset(file_path, data_selectedId)\n",
    "        \n",
    "        #initialize tag_list\n",
    "        if keywords == None:\n",
    "            self.create_reservedkeywords(tag_path)\n",
    "        else:\n",
    "            df = pd.read_csv(tag_path)\n",
    "            self.tag_list = list(df[\"Tag\"].values)\n",
    "            self.reservedkeywords = keywords\n",
    "        \n",
    "        #initialize stopwords\n",
    "        self.create_stopwords()\n",
    "        \n",
    "        for sentence in dataset:\n",
    "            sentence = self.html_Filter(sentence)\n",
    "            sentence = self.remove_non_ascii(sentence)\n",
    "            sentence = self.keywords_transform(sentence)\n",
    "            sentence = self.POSTag_generation(sentence)\n",
    "            #sentence = self.remove_specialchar(sentence)\n",
    "            #sentence = self.sentence_stem(sentence)\n",
    "            \n",
    "            data_clean.append(sentence.strip())\n",
    "        \n",
    "        return data_clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
