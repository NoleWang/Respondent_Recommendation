{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel = \"Scikitlearn\"\n",
    "channel2 = \"Angular\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('../Gitter_Channels/Scikitlearn/Scikitearn_user.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4318 entries, 0 to 4317\n",
      "Data columns (total 3 columns):\n",
      "displayName    4318 non-null object\n",
      "id             4318 non-null object\n",
      "username       4318 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 134.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>displayName</th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jitesh Khandelwal</td>\n",
       "      <td>529c6a18ed5ab0b3bf04d26a</td>\n",
       "      <td>jiteshk23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rahul Kulhari</td>\n",
       "      <td>529c6bdded5ab0b3bf04d758</td>\n",
       "      <td>RahulKulhari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jaepil Jeong</td>\n",
       "      <td>529c6c0bed5ab0b3bf04d7db</td>\n",
       "      <td>jaepil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Edwin Khoo</td>\n",
       "      <td>529c6c8eed5ab0b3bf04d94a</td>\n",
       "      <td>edwinksl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanh Nguyen</td>\n",
       "      <td>529c6c9bed5ab0b3bf04d96b</td>\n",
       "      <td>tienthanh8490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         displayName                        id       username\n",
       "0  Jitesh Khandelwal  529c6a18ed5ab0b3bf04d26a      jiteshk23\n",
       "1      Rahul Kulhari  529c6bdded5ab0b3bf04d758   RahulKulhari\n",
       "2       Jaepil Jeong  529c6c0bed5ab0b3bf04d7db         jaepil\n",
       "3         Edwin Khoo  529c6c8eed5ab0b3bf04d94a       edwinksl\n",
       "4       Thanh Nguyen  529c6c9bed5ab0b3bf04d96b  tienthanh8490"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read chat logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = \"../../Analysis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_logs(utterance_list):\n",
    "    prev_user = \"\"\n",
    "    dialog_id = 0\n",
    "    results = []\n",
    "    profile = {}\n",
    "    isHead =False\n",
    "    create_time = \"\"\n",
    "    \n",
    "    for utterance in utterance_list:     \n",
    "        if len(utterance) == 0:\n",
    "            continue\n",
    "        if '-------------------------------------' in utterance:\n",
    "            if dialog_id != 0:\n",
    "                issue[\"Id\"] = dialog_id\n",
    "                print(issue[\"Id\"])\n",
    "                issue[\"asker\"] = prev_user\n",
    "                print(issue[\"asker\"])\n",
    "                issue[\"time\"] = create_time\n",
    "                print(issue[\"time\"])\n",
    "                issue[\"content\"] = content\n",
    "                print(issue[\"content\"])\n",
    "                issue[\"lines\"] = line\n",
    "                print(issue[\"lines\"])\n",
    "                results.append(issue)\n",
    "            dialog_id += 1\n",
    "            line = 0\n",
    "            issue = {}\n",
    "            content = \"\"\n",
    "            isHead =True\n",
    "        else:\n",
    "            line += 1\n",
    "            start_index = 0\n",
    "            end_index = 0\n",
    "            #retreive time\n",
    "            while utterance[start_index] != \"[\":\n",
    "                start_index += 1\n",
    "        \n",
    "            start_index += 1\n",
    "            end_index = start_index\n",
    "            while utterance[end_index] != \"]\":\n",
    "                end_index += 1\n",
    "        \n",
    "            time = utterance[start_index:end_index]\n",
    "        \n",
    "            #retreive user\n",
    "            start_index = end_index\n",
    "            while utterance[start_index] != \"<\":\n",
    "                start_index += 1\n",
    "        \n",
    "            start_index += 1\n",
    "            end_index = start_index\n",
    "            while utterance[end_index] != \">\":\n",
    "                end_index += 1\n",
    "            \n",
    "            curr_user= utterance[start_index:end_index]\n",
    "        \n",
    "            #retreive message\n",
    "            message = utterance[(end_index+2):]\n",
    "            #print(\"message:\",message)\n",
    "\n",
    "            \n",
    "            if line == 1:\n",
    "                prev_user = curr_user\n",
    "                create_time = time\n",
    "            \n",
    "            if isHead and prev_user == curr_user:\n",
    "                content += message\n",
    "            else:\n",
    "                isHead =False\n",
    "                if prev_user == curr_user:\n",
    "                    continue\n",
    "            \n",
    "                if len(profile) > 0 and curr_user in profile.keys():\n",
    "                    if dialog_id in profile[curr_user].keys():\n",
    "                        profile[curr_user][dialog_id][\"Answer\"] += message\n",
    "                    else:\n",
    "                        profile[curr_user][dialog_id] = {}\n",
    "                        profile[curr_user][dialog_id][\"Time\"] = time\n",
    "                        profile[curr_user][dialog_id][\"Answer\"] = message\n",
    "    \n",
    "                else:\n",
    "                    profile[curr_user] = {}\n",
    "                    profile[curr_user][dialog_id] = {}\n",
    "                    profile[curr_user][dialog_id][\"Time\"] = time\n",
    "                    profile[curr_user][dialog_id][\"Answer\"] = message\n",
    "    \n",
    "    return results, profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Gitter_Channels/\"+channel+\"/disentangle/\"+channel+\"_result.txt\", \"r\") as f:\n",
    "        utterance_list = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "541a528b163965c9bc2053de\n",
      "2014-09-18 03:36\n",
      "Hi all!\n",
      "\n",
      "1\n",
      "2\n",
      "544906e2db8155e6700cdd16\n",
      "2014-10-23 19:59\n",
      "I don't know if this is the right place but I need some advice on how to train a classifier using a data set with multiple feature types: text, integers, floats, dates\n",
      "any idea?\n",
      "\n",
      "2\n",
      "3\n",
      "544906e2db8155e6700cdd16\n",
      "2014-10-23 20:01\n",
      "My first approach was to convert each feature to binary, example: feature 1 -> \"the word w_i is in column j?\" , feature 2 -> \"the value in column j is greater than 10?\", feature 3 -> \"the month in the date of column j was abril?\", etc.\n",
      "by the way, Oliver Grisel: Hi! :D\n",
      "\n",
      "2\n",
      "4\n",
      "53a5cf04a9176b500d1ced1a\n",
      "2015-01-23 15:38\n",
      "This message is without content. I wonder if anyone hangs out here.\n",
      "\n",
      "1\n",
      "5\n",
      "541a528b163965c9bc2053de\n",
      "2015-01-23 16:49\n",
      "@mac2bua, I just saw your message. It's probably better to ask specific question with running code snippets on toy example data on stackoverflow. You might be interested in having a look at sklearn_pandas for feature extraction from heterogeneously typed columns. FeaturesUnion might also be helpful.\n",
      "\n",
      "1\n",
      "6\n",
      "544906e2db8155e6700cdd16\n",
      "2015-01-23 17:48\n",
      "@ogrisel Thanks a lot for the advice! Fortunately I found this very useful blog: http://bit.ly/1mPEEhH and I decided to start using feature union and pipelines. The result was very good! :smile:  I know that you are a member of the core team of scikit learn so I'd like to thank you, sklearn is so awesome!.  Just one more question: Is there any easy way to get the name of the features?  So often I needed to print out the names in order to understand the relationship between them and the different classes. I wrote some code that gather the information of the different vocabularies in the case of the textual or categorical features and the name of the column in the case of the numerical ones.  Sorry for my bad english!\n",
      "\n",
      "1\n",
      "7\n",
      "541a528b163965c9bc2053de\n",
      "2015-01-23 17:56\n",
      "No, not in general unfortunately: most of the time the data will get internally converted to numpy arrays for efficiency and code simplicity reasons  and even if the column were named in the original representation (e.g. a DataFrame) that information is lost along the way.\n",
      "\n",
      "1\n",
      "8\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-06 11:13\n",
      "hey\n",
      "\n",
      "1\n",
      "9\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-06 11:13\n",
      "hi @amueller\n",
      "\n",
      "1\n",
      "10\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-06 11:14\n",
      "Thanks for starting on the reviews :)\n",
      "\n",
      "1\n",
      "11\n",
      "54a99c64db8155e6700e5ac4\n",
      "2015-02-06 11:14\n",
      "Hi\n",
      "\n",
      "1\n",
      "12\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-06 11:14\n",
      "I want to do the `check_array` empty data first\n",
      "\n",
      "9\n",
      "13\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-06 11:18\n",
      "Most of the bug-fixes are isolated to one estimator. So I think they are important for the release, but shouldn't interact with other issues so much. The ones that are more API-ish are the dtype=object one (https://github.com/scikit-learn/scikit-learn/pull/4057), the clustering / pipeline one https://github.com/scikit-learn/scikit-learn/pull/4064 and the input validation one https://github.com/scikit-learn/scikit-learn/pull/4136\n",
      "\n",
      "9\n",
      "14\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-06 11:23\n",
      "it did? I didn't see that. haha I know the open tabs issue. Well the ``slinear`` broke some cases of ``fit`` and ``fit_transform`` not doing the same thing. Maybe it broke other things, too.\n",
      "Bugs that are not regressions are that fit and fit_transform were not consistent before in the case of ties, and that having sample_weight=0 in multiple places can lead to infinite loops in the isotonic regression code\n",
      "\n",
      "8\n",
      "15\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-06 11:35\n",
      "I want to implement the `concurrent.futures` API. That includes porting cloudpickle.py to Python 3 or implemeting something similar with dill.\n",
      "The goal is to be able to use SGE / SLURM clusters easily, without having to write bash or boilerplate python scripts.\n",
      "There is also cloudpi.pe to watch in the same space.\n",
      "going grab some lunch, see you later\n",
      "I agree\n",
      "...\n",
      "I don't understand what you mean by finite targets\n",
      "integers & categorical labels?\n",
      "\n",
      "8\n",
      "16\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-06 12:05\n",
      "Args. I am slightly confused by the current API requirements in pipeline. If ``fit_transform`` accepts ``y=None`` we don't require ``transform`` to accept a ``y=None``. That is somewhat inconsistent and weird, I think...\n",
      "Huh ok transform is never passed ``y`` at all... never mind...\n",
      "\n",
      "2\n",
      "17\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-06 13:32\n",
      "@amueller I am reviewing #4057\n",
      "In another PR I suggested in a comment to add support for the `dtype=[np.float64, np.float32]` idiom.\n",
      "but I don't remember which PR ;)\n",
      "I think the dtype='numeric' is a fine, loose default but I would like to check with the other PR if we also need the `dtype=<list of accepted dtypes idiom>` for specific cases as well.\n",
      "do you remember which PR it was?\n",
      "\n",
      "5\n",
      "18\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-06 13:41\n",
      "Ok found it: it was collapsed in: https://github.com/scikit-learn/scikit-learn/pull/4136#discussion_r24176562\n",
      "\n",
      "1\n",
      "19\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-06 14:04\n",
      "sorry didn't watch the chat\n",
      "yeah so a dtype list for check_array would be nice, and then we could get rid of ``as_float_array``\n",
      "\n",
      "2\n",
      "20\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-06 14:08\n",
      "\"unfriend all multi-output estimators on facebook.\"\n",
      ":)\n",
      "\n",
      "2\n",
      "21\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-06 14:10\n",
      "There was only slight frustration on my part. I'm not sure if the ``y_numeric`` is a good workaround, because it somehow stipulates that by default \"y\" are arbitrary objects aka classification labels\n",
      "btw, there are not really that many tests on ``y``.  Many estimators just used ``np.asarray(y)``...\n",
      "we should probably test for finite targets for regression\n",
      "\n",
      "3\n",
      "22\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-07 07:24\n",
      "I meant for regression, where the target is a float\n",
      "\n",
      "1\n",
      "23\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-08 10:15\n",
      "Hey all! :) @ogrisel You might want to set up gitter to get live activity feeds on the right! Choose github and travis under integrations ( under settings )\n",
      "\n",
      "1\n",
      "24\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-08 20:57\n",
      "@ragv done! thanks for the tip\n",
      "\n",
      "1\n",
      "25\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-08 22:33\n",
      "It seems that I broke travis but I cannot reach the report page. Running the tests locally.\n",
      "\n",
      "1\n",
      "26\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-08 22:50\n",
      "Ok fixing the broken tests ATM\n",
      "\n",
      "1\n",
      "27\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-09 18:01\n",
      "Scikit learn has crossed 5000 stars on github :beer:\n",
      "\n",
      "1\n",
      "28\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-11 18:21\n",
      "hey hey\n",
      "ogrisel, are you around?\n",
      "\n",
      "2\n",
      "29\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-12 09:45\n",
      "Hi @amueller sorry I was offline for the past couple of days as I had to attend local events. Today I will be working on clusterlib with a colleague. tomorrow I will be available to review PRs on sklearn\n",
      "\n",
      "1\n",
      "30\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-12 13:36\n",
      "Alright. Next week I'll be at strata, though, and won't have much time. We should settle on a timeline for release.\n",
      "Sounds good :)\n",
      "\n",
      "2\n",
      "31\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-12 13:50\n",
      "What about setting the objective of cutting the first beta on Friday Feb 27.\n",
      "Then one beta every two weeks.\n",
      "And release end of March?\n",
      "\n",
      "3\n",
      "32\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-15 08:40\n",
      "@ogrisel Would you be interested in setting up http://landscape.io and landscape bot? #3888\n",
      "\n",
      "1\n",
      "33\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-15 08:46\n",
      "Refer [this](https://landscape.io/github/ragv/scikit-learn) for live preview of our repo and [this](https://github.com/hugovk/word-tools/pull/7#issuecomment-67533284) for an example comment by the bot.\n",
      "A general suggestion... I feel it would be more helpful to debug test results if we replaced docstrings for tests by comment...  Output when a docstring is present for a test ``` Tests all estimators which support partial_fit ... ok Tests all estimators which support partial_fit ... ok Tests all estimators which support partial_fit ... ok ``` Output when no docstring is present for a test ``` test_common.test_partial_fit('Perceptron', <class 'sklearn.linear_model.perceptron.Perceptron'>) ... ok test_common.test_partial_fit('Perceptron', <class 'sklearn.linear_model.perceptron.Perceptron'>) ... ok test_common.test_partial_fit('Perceptron', <class 'sklearn.linear_model.perceptron.Perceptron'>) ... ok test_common.test_partial_fit('SGDClassifier', <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>) ... ok ```\n",
      "\n",
      "2\n",
      "34\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-15 11:47\n",
      "Trying out gitter from irc\n",
      "\n",
      "1\n",
      "35\n",
      "54e07def15522ed4b3dc0864\n",
      "2015-02-15 12:02\n",
      "@ragv thanks for the invite\n",
      "\n",
      "1\n",
      "36\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-15 15:56\n",
      "I agree about the test docstring issue.\n",
      "\n",
      "1\n",
      "37\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-15 16:32\n",
      "@ogrisel would it be worth to have a pr for that? Or should I just create an issue and refer that when any related code is changed?\n",
      "Ah! thanks ;) that was helpful! Sorry should have googled a bit before asking...\n",
      "\n",
      "2\n",
      "38\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-15 16:33\n",
      "Explaining the pbm in an issue is a good idea. Then a bunch of small PRs related to that issue if people have no objection.\n",
      "\n",
      "2\n",
      "39\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-15 16:35\n",
      "Alternatively we could use this on travis: https://pypi.python.org/pypi/nose-ignore-docstring\n",
      "\n",
      "1\n",
      "40\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-15 17:04\n",
      "and in the makefile, I'd say\n",
      "don't we even have a nose config file in the repo?\n",
      ":) oh we do need to install a package, I thought it was a build-in option\n",
      "\n",
      "3\n",
      "41\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-15 17:05\n",
      "Im about to send a PR for that ;)\n",
      "and yea `setup.cfg`\n",
      "yea It will go in `continuous-integration/install.sh` after `pip install nose`\n",
      "I'll take care of the setup...\n",
      "\n",
      "4\n",
      "42\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-15 18:15\n",
      "@amueller How do you feel about https://github.com/finnp/gitter-irc-bot ?\n",
      "It look really very simple and could cleanly sync irc and gitter...\n",
      "\n",
      "2\n",
      "43\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-18 00:53\n",
      "sounds ok but I don't have too many cycles to look into it. you can do it and see if anyone complains about noise (which is unlikely)\n",
      "\n",
      "1\n",
      "44\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-18 14:28\n",
      "okay! thanks! :)\n",
      "\n",
      "1\n",
      "45\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-20 14:43\n",
      "+1\n",
      "\n",
      "1\n",
      "46\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-23 00:00\n",
      "hey hey. I'm kinda back, though somehow in Dallas.\n",
      "And I only have ~400 unread github notifications\n",
      "\n",
      "2\n",
      "47\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-23 08:05\n",
      "...\n",
      "@amueller how did you enjoy strata?\n",
      "\n",
      "2\n",
      "48\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-23 15:43\n",
      "It was pretty awesome :) met a lot of interesting people. Are you bug-crunching this week? I have to do some organisational stuff for GSOC but the rest of the week should be free for release things.\n",
      "\n",
      "1\n",
      "49\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-23 18:02\n",
      "I was having a look at the FDR PR and trying to understand why the actual FDR is always lower than alpha. I am working on something else tonight. Will resume tomorrow. I think I have an idea.\n",
      "\n",
      "1\n",
      "50\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-23 20:40\n",
      "> And I only have ~400 unread github notifications  haha :p\n",
      "\n",
      "1\n",
      "51\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-24 13:23\n",
      "How do I make sure the cython sources are compiled when I rebuild for testing? ( related to #4288 ) ?\n",
      "\n",
      "1\n",
      "52\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-24 14:06\n",
      "cython path/to/file.pyx\n",
      "\n",
      "2\n",
      "53\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-24 22:25\n",
      "@amueller here I mean\n",
      "\n",
      "1\n",
      "54\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-24 22:28\n",
      "so much sense\n",
      "sorry about that\n",
      "btw what time is it in Paris? Are you in Paris?\n",
      "\n",
      "3\n",
      "55\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-24 22:30\n",
      "yes I am in paris\n",
      "it's almost midnight\n",
      "I am rebasing #3945 then going to bed\n",
      "\n",
      "3\n",
      "56\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-24 22:34\n",
      "sweet, thanks. tomorrow I will be in the office earlier. Thanks for working late :)\n",
      "\n",
      "1\n",
      "57\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-24 22:35\n",
      "For some reason I tagged the old SVC.sparse_decision_function PR with 0.16, but not my new one. That doesn't make sense. Do you think I should tag my PR or untag the old?\n",
      "\n",
      "1\n",
      "58\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-24 22:41\n",
      "Actually I was primarily working on dl-machine this evening :)\n",
      "> For some reason I tagged the old SVC.sparse_decision_function PR with 0.16, but not my new one. That doesn't make sense. Do you think I should tag my PR or untag the old?  I am not sure which is which\n",
      "https://github.com/deeplearningparis/dl-machine\n",
      "\n",
      "3\n",
      "59\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-24 22:42\n",
      "dl-machine?\n",
      "\n",
      "1\n",
      "60\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-24 22:42\n",
      "rephrasing my question: should the sparse decision function be tagged for 0.16 or not (i.e. is it a priority to go into the release)\n",
      "ah, I see. Are you working with torch or theano primarily?\n",
      "\n",
      "2\n",
      "61\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-24 22:46\n",
      "setting up tutorial material for a workshop on theano, but adding support for torch was easy so I added it.\n",
      "+1 for including the sparse decision function fix\n",
      "\n",
      "5\n",
      "62\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-24 22:47\n",
      "that means 23 PRs left for the release lol\n",
      "\n",
      "2\n",
      "63\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-24 22:48\n",
      "have a good night. Hope we make some more progress on the PRs this week\n",
      "\n",
      "3\n",
      "64\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-24 23:00\n",
      "good night :)\n",
      "\n",
      "1\n",
      "65\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-25 18:40\n",
      "anything in particular that you would like me to review?\n",
      "\n",
      "1\n",
      "66\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-25 18:44\n",
      "Did you have a look at this one yet: https://github.com/scikit-learn/scikit-learn/pull/4192 I haven't really followed what is happening with the pos_label etc in the metrics\n",
      "\n",
      "1\n",
      "67\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-25 18:46\n",
      "Hey :) sorry to interrupt you... do you feel we could do the data indep. cv part first and later do the renaming?\n",
      "\n",
      "1\n",
      "68\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-25 18:51\n",
      "There are two roads we could go down: either doing both together, but using the fact to have a deprecation path for it. That would mean copying a lot of code for a while. OR doing them separately. If we do them separately, I would do the CV objects first. Also, I wouldn't do either for the 0.16 release, I think.\n",
      "Actually, I think the code duplication thing is not necessary, so we should just focus on the cv objects\n",
      "@ogrisel green button on https://github.com/scikit-learn/scikit-learn/pull/4249 (sign flip) ?\n",
      "\n",
      "3\n",
      "69\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-25 18:59\n",
      "Okay! so I'll salvage #4294 to do the CV related fixes alone?\n",
      "\n",
      "5\n",
      "70\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-25 19:04\n",
      "Just one more minor thing... what is the expected range of scores ( w.r.t #4295 ) ( could it by any chance be >= 40? )\n",
      "\n",
      "1\n",
      "71\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-25 19:07\n",
      "the sigmoid should be between 0 and 1 as you plotted, and the votes shouldn't be larger than the number of classes, right?\n",
      "\n",
      "1\n",
      "72\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-25 19:09\n",
      "if the values for `sum_of_confidences` ( previously `scores` ) exceed 34 we get 1 which is not desirable... i.e `expit(34)` is 1... with 0.5 it extends upto 73...\n",
      "or am I too paranoid? perhaps `scores` do not extend  upto 34?\n",
      "sorry its at 37.... and yea in my system `expit(37) == 0`... would you mind confirming the same pl?\n",
      "\n",
      "5\n",
      "73\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-25 19:11\n",
      "sorry, I don't follow. scores can be arbitrary high. You say that it actually reaches 1? that seems a bit odd\n",
      "I see... maybe then we need to do 0.9 * expit(scores) ? which would be just another magic value....\n",
      "I commented on the issue and pinged @mblondel\n",
      "np. you are right, that is an issue\n",
      "\n",
      "4\n",
      "74\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-25 19:14\n",
      "In [9]: expit(37) Out[9]: array(0.9999999999999999)  In [10]: expit(38) Out[10]: array(1.0)\n",
      "\n",
      "1\n",
      "75\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-25 19:17\n",
      "Perhaps we could scale to `[0, 1]` before mapping it using sigmoid? ( it would be a tad slower )\n",
      "\n",
      "2\n",
      "76\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-25 19:53\n",
      "took me only 3 days to catch up with 7 days of sklearn notifications....\n",
      "now for some actual work\n",
      "\n",
      "2\n",
      "77\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-25 20:06\n",
      "I have to do some pystruct stuff now because I commited to some research projects. Other than that, I'm working on getting as many PRs as possible that are tagged with 0.16 in, and writing as many bug-fixes for 0.16 tagged issues as possible\n",
      "mostly reviewing currently\n",
      "\n",
      "2\n",
      "78\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-25 20:30\n",
      "@ogrisel this one should also be simple: https://github.com/scikit-learn/scikit-learn/pull/4082\n",
      "\n",
      "1\n",
      "79\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-26 15:12\n",
      "morning\n",
      "\n",
      "1\n",
      "80\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-26 15:13\n",
      "morning :)\n",
      "\n",
      "1\n",
      "81\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-26 15:13\n",
      "or afternoon ;)_\n",
      "\n",
      "1\n",
      "82\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-26 15:18\n",
      "are you working on the isotonic stuff right now? Anything I should review?\n",
      "ah, you are doing https://github.com/scikit-learn/scikit-learn/pull/4082, cool :)\n",
      "\n",
      "2\n",
      "83\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-26 15:26\n",
      "yes #4082, I am resolving conflicts (both syntactic and logic), almost done. running the full test suite\n",
      "\n",
      "11\n",
      "84\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-26 15:31\n",
      "I kind of understand the infinite loop. there is a while x != y and if both are NAN then it never finishes. The easiest way to solve the problem is to remove data points with zero sample weight. But if you do that, you can not implement fit_predict the way it currently is.\n",
      "The other way would be to take care of zero sample weight in the Cython code, but I don't understand that part 100%\n",
      "fixed https://github.com/scikit-learn/scikit-learn/pull/4189\n",
      "ok. I'll try to look into it more today. Now my tests are failing the same way yours are doing. But I had different results on my different laptop\n",
      "this one is on 0.13.3\n",
      "bye\n",
      "which ones in particular? For the release only the tagged issues are important imho\n",
      "\n",
      "7\n",
      "85\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-26 15:35\n",
      "ok, but this seem completely independent from the bug found in transform by mjbommard in #4185\n",
      "\n",
      "1\n",
      "86\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-26 15:36\n",
      "when I last checked, I had the impression that some of the inconsistencies come from the way that fit_transform is implemented. If it is indeed only an error in transform, then they are unrelated.\n",
      "I'll double check.\n",
      "it does look like fit_transform works correctly, never mind then\n",
      "it seems different versions of scipy give different results\n",
      "\n",
      "4\n",
      "87\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-26 15:40\n",
      "which version of scipy are you running and how the test fail in your case ?\n",
      "I need to run catch my shuttle. not sure I will be able to work on that further today but will catch up tomorrow\n",
      "bye\n",
      "\n",
      "3\n",
      "88\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-26 15:45\n",
      "what is the difference between \"predict\" and \"transform\" in isotonic regression?\n",
      "ah, there is none. Isn't that slightly confusing?\n",
      "\n",
      "2\n",
      "89\n",
      "541a528b163965c9bc2053de\n",
      "2015-02-26 19:47\n",
      "It is, mayve we could deprecate transform in favor of predict.\n",
      "\n",
      "3\n",
      "90\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-26 19:53\n",
      "@amueller can I take up the newly tagged easy issues, assuming this will help u release the 0.16 beta cut or should I leave those for a new contributor instead?\n",
      "\n",
      "1\n",
      "91\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-26 20:02\n",
      "I had meant #4298, #4296, #4292... but I just realized I had one unfinished 0.16 tagged PR - #4225... I'll work on it instead :)\n",
      "\n",
      "1\n",
      "92\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-02-26 20:04\n",
      "you can leave the three issues for new people, or you can go for them. But they are really low priority. Finishing up the other PRs you already have open might be better for us at the moment.\n",
      "\n",
      "1\n",
      "93\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-02-26 20:09\n",
      "yea :) thanks!\n",
      "\n",
      "1\n",
      "94\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 14:19\n",
      "@ogrisel what is your plan for the day? I didn't have much time on the weekend unfortunately :-/\n",
      "\n",
      "1\n",
      "95\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 14:44\n",
      "no pbm. I just pushed #4317 to tackle the remaining docstring and test issues w.r.t. radius_neighbors. Please feel free to have a look.\n",
      "I will now catch up on your work on isotonic regression. I think it looks good from a first look at it. Will test it a bit more. That might make it possible to remove the random tie breaking in the calibration code.\n",
      "\n",
      "2\n",
      "96\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 14:46\n",
      "The behavior is different from what it was before, but I think this is the only way to make fit().transform() and fit_transform() be the same.\n",
      "I'll head t othe office now and look at #4317 once I'm ther\n",
      "\n",
      "2\n",
      "97\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 14:52\n",
      "Alright. I will be offline for 1h (commute back home from saclay) but should get back online afterwards.\n",
      "\n",
      "2\n",
      "98\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 14:56\n",
      "It's also because I have to commute early in the morning and in the afternoon to take a shuttle bus that avoids most of the Paris rush hour traffic :) I would not wake up at 6:30am naturally otherwise...\n",
      "I a good paper to read on hyperparam search on the way back home :)\n",
      "it seems only useful when you can use a big compute cluster for a week though. Still it looks interesting.\n",
      "It means that we might be able to use the future MLP model for hyperparam search instead of trying to fix the GP to make them efficient ;)\n",
      "\n",
      "4\n",
      "99\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 14:59\n",
      "the one kyle sent around? Jasper Snoek will give a talk at NYU on Friday :)\n",
      "\n",
      "2\n",
      "100\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 15:04\n",
      "well spearmint uses the sklearn random forests for hyperparameter search l)\n",
      "so we could also use that ;)\n",
      "\n",
      "2\n",
      "101\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 15:10\n",
      "yes but apparently it does not reach the quality of the solution of bayes optimization with GPs or NNs.\n",
      "Nor does TPEs, at least in the benchmarks run in this paper.\n",
      "anyway let's focus back on the release :)\n",
      "\n",
      "3\n",
      "102\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 15:41\n",
      "btw, for https://github.com/scikit-learn/scikit-learn/issues/2274 currently the random projections are pretty loud\n",
      "\n",
      "1\n",
      "103\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 16:26\n",
      "#4318\n",
      "\n",
      "1\n",
      "104\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 17:34\n",
      "wait, which is the paper you read on the train? not this one, right? http://arxiv.org/pdf/1502.03492v2.pdf\n",
      "I guess this one: http://arxiv.org/pdf/1502.05700.pdf\n",
      "sweet thanks :)\n",
      "\n",
      "3\n",
      "105\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 18:27\n",
      "do we have a link to the pdf docs on the website? I think we don't :-/ where should it go?\n",
      "\n",
      "1\n",
      "106\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 19:06\n",
      "I read the one by Snoek, but also read the other 2 weeks ago I think.\n",
      "> do we have a link to the pdf docs on the website? I think we don't :pensive: where should it go? no we don't, we don't build it on a regular basis but we could. It should be possible to upload it as part of the build process.\n",
      "I made a mistake:\n",
      "train_idx, validation_idx = train_test_split(np.arange(n_samples), test_size=0.2, random_state=0)\n",
      "missing the np.arange\n",
      "I thought about k-means where predict is possible but this is not always the case, for instance when clustering is done on a precompute distance or similarity matrix\n",
      "I am testing it now\n",
      "the isotonic\n",
      "\n",
      "8\n",
      "107\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 19:21\n",
      "after this the latex build should be relatively clean: https://github.com/scikit-learn/scikit-learn/pull/4320\n",
      "anything you want me to review / work on?\n",
      "\n",
      "2\n",
      "108\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-02 20:36\n",
      "Is there a way to make `GridSearchCV` act without the CV part ( do parameters `cv=None` or `refit` help in this context )?\n",
      "\n",
      "1\n",
      "109\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 20:38\n",
      "You can precompute a single test train split: train_idx, validation_idx = train_test_split(n_samples, test_size=0.2, random_state=0) cv = [(train_idx, validation_idx)] GridSearchCV(model, cv=cv).fit(X, y)\n",
      "\n",
      "2\n",
      "110\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-02 20:43\n",
      "I am using your code in a comment at #4301...\n",
      "\n",
      "1\n",
      "111\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 21:05\n",
      "it might be interesting to have the ability to not split into training and test data for unsupervised algorithms.\n",
      ":)\n",
      "\n",
      "2\n",
      "112\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-02 21:09\n",
      "what does setting cv to None do currently?\n",
      "Perhaps we could set cv to -1 for that behaviour?\n",
      "\n",
      "2\n",
      "113\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 21:20\n",
      "@amueller unsupervised scores can still overfit\n",
      "\n",
      "1\n",
      "114\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 21:21\n",
      "it depends on the context. For clustering, where you can not even evaluate on non-training data, it totally makes sense.\n",
      "I'm looking into the sphinx thing but I'm not entirely clear on how to achieve this. I don't know where to hook it in. Did you have time to look at the isotonic stuff?\n",
      "\n",
      "3\n",
      "115\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-02 21:53\n",
      "@amueller for #4301, do you think the metrics might be useful?\n",
      "\n",
      "4\n",
      "116\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 22:06\n",
      "+1 for not working on not introducing any new feature that has not already received a significant amount of reviews.\n",
      "Rephrasing: I meant not working on PR introducing new features unless it has already been reviewed extensively.\n",
      "\n",
      "6\n",
      "117\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 22:36\n",
      "https://github.com/scikit-learn/scikit-learn/pull/4313\n",
      "is part of the kneighbors_graph fix where you can press the green button. I agree that this should be in the beta\n",
      "\n",
      "2\n",
      "118\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 22:39\n",
      "haha I merged it before reading your message here ;)\n",
      "\n",
      "4\n",
      "119\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 22:45\n",
      "so the plan is uploading a non-default pipy package and writing a mail? and wheels?\n",
      "Maybe then I should focus on the very verbose tests ^^\n",
      "As I said above, if you have anything on your wishlist, let me know\n",
      "\n",
      "4\n",
      "120\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-02 22:56\n",
      "it's midnight here, I think I will call it a day on my side\n",
      "see you tomorrow. Tomorrow morning I will work to prepare a team meeting on probability calibration and meeting in the afternoon. Should be online after that.\n",
      "\n",
      "2\n",
      "121\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-02 22:58\n",
      "well but for those we need another reviewer, right?\n",
      "have a good night and see you tomorrow\n",
      "\n",
      "4\n",
      "122\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-03 15:05\n",
      "looks like you got a lot of reviewing done already, sweet :)\n",
      "care to weight in on the class_weight discussion?\n",
      "\n",
      "2\n",
      "123\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-03 16:07\n",
      "we really need someone else for reviews :-/\n",
      "\n",
      "1\n",
      "124\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-03 19:13\n",
      "@agramfort are you around?\n",
      "\n",
      "1\n",
      "125\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-03 19:48\n",
      "there is another bugfix here @ogrisel  https://github.com/scikit-learn/scikit-learn/pull/4326\n",
      "I merged the RBM fix and docs after Kyle +1'ed\n",
      "thanks\n",
      "woah getting late again on your side ;)\n",
      "10PRs left I think\n",
      "oh, wrong filter. 14PRs\n",
      "\n",
      "6\n",
      "126\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-03 21:21\n",
      "Done.\n",
      "15 left.\n",
      "15 PRs left.\n",
      "\n",
      "3\n",
      "127\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-03 22:14\n",
      "the agglomerative clustering one I am not sure about. I don't know what the canonical meaning of alpha is. https://github.com/scikit-learn/scikit-learn/pull/3758\n",
      "\n",
      "1\n",
      "128\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-04 00:47\n",
      "@amueller for the partial_fit tests PR do you think there is still a lot of testing code that could be reduced/removed?\n",
      "\n",
      "1\n",
      "129\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-04 15:11\n",
      "I'm not really sure\n",
      "\n",
      "1\n",
      "130\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-04 17:12\n",
      "@ogrisel are you there and do you have time to talk about an outline for the webcast?\n",
      "\n",
      "1\n",
      "131\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-05 08:21\n",
      "@amueller  Sorry I was feeling sick yesterday night and I did not go online.\n",
      "\n",
      "1\n",
      "132\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-05 08:26\n",
      "I started thinking about it, we can do: - general intro to ML in Python with scikit-learn - couple of words on the project organization and the team of contributors - highlight of some new stuff in 0.16: improvement on speed or scalability of some methods such as:    - IncrementalPCA    - clustering (I am preparing a notebook comparing DBSCAN, MiniBatchKMeans, KMeans and Birch on 100k blobish 2D points to simulate someone working to extract Point of Interests from raw geo location data (e.g. GSM phone records) - new Approximate Nearest Neighbors search with LSHForest Then for the future we could talk about TSNE and algorithmic improvements under review in the pipeline\n",
      "I forgot, I also wanted to say a couple of words about probability calibration\n",
      "sounds good to me\n",
      "I am half feeling better, really weird\n",
      "some hours I feel sick and then completely fine for a while\n",
      "that's weird\n",
      "I played a bit with clustering on 1e5 points in 2D\n",
      "http://nbviewer.ipython.org/github/ogrisel/notebooks/blob/master/sklearn_demos/Large%20Scale%202D%20Clustering.ipynb\n",
      "DBSCAN is 3x faster than 0.15.2\n",
      "MB KMeans is still the fastest although it does not deal with outliers as DBSCAN does\n",
      "Birch core set extraction seems to works great but the final affinity clustering pass is not as scalable as the core set extraction\n",
      "If it's ready soon, +1 for a backport to the 0.16.X branch.\n",
      "\n",
      "16\n",
      "133\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 14:47\n",
      "Right, calibration is not on my list.  Is it in whatsnew?\n",
      "Oh, and a title\n",
      "Hope you are feeling better today\n",
      "\n",
      "3\n",
      "134\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 14:53\n",
      "News from scikit-learn 0.16 and soon-to-be gems for the next release. ??\n",
      "\n",
      "1\n",
      "135\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 15:02\n",
      "cool :) and there are still faster DBSCAN variants in the PRs\n",
      "there is a pretty new PR on that I think\n",
      "\n",
      "2\n",
      "136\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 15:43\n",
      "what's on todays agenda for the release?\n",
      "or what was for you ;)\n",
      "no progress on isotonic I see\n",
      "would you mind having a look at https://github.com/scikit-learn/scikit-learn/issues/4324\n",
      "go for it. It shouldn't interfere with anything.\n",
      "\n",
      "11\n",
      "137\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 16:04\n",
      "I guess I start reviewing the non-release PRs now. Or is there anything to review left?\n",
      "Delaying the beta even more seems a bit silly to me, but the only other option is to merge the remaining urgent fixes with fewer reviews\n",
      "Alright. I don't think I'll make it this year.\n",
      "you are not coming to scipy are you?\n",
      "\n",
      "4\n",
      "138\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-05 20:39\n",
      "I am having a look at the DBSCAN. Let's release the beta tomorrow. Whatever the review state of the remaining PRs. we will do backports for the fixes afterwards for the second beta.\n",
      "\n",
      "1\n",
      "139\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 20:40\n",
      "you want a second beta? Alright....\n",
      "\n",
      "5\n",
      "140\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-05 20:53\n",
      "I was planning to attend ICML as a free-styler because is just 1h by train\n",
      "\n",
      "1\n",
      "141\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-05 21:07\n",
      "Hey :) could you help me with #4228 ? I don't understand a few things... When testing you said I could use multilabel data right? but which classifier do I test against? (or if you might have time could you take over?)\n",
      "\n",
      "6\n",
      "142\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-05 21:11\n",
      "okay I'll push in sometime... pl see if I am in the right direction or kindly take over... sorry for having kept you waiting over that :)\n",
      "A small doubt do you use multiple monitors? ;)\n",
      "\n",
      "2\n",
      "143\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-05 21:24\n",
      "no why?\n",
      "\n",
      "1\n",
      "144\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-05 21:27\n",
      "Just curious how @amueller could be online and commenting at the same time :O I am online sometimes while I work since I use vim in a transparent guake terminal over the gitter chat window... :)\n",
      "\n",
      "4\n",
      "145\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 22:03\n",
      "are you working on https://github.com/scikit-learn/scikit-learn/pull/4228 now or should I go for it? I only have like 2 hours of work left today, though ;)\n",
      "\n",
      "1\n",
      "146\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-05 22:13\n",
      "Please go for it :) :P Most ests don't seem to support sparse y... :)\n",
      "\n",
      "1\n",
      "147\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-05 22:13\n",
      "\n",
      "0\n",
      "148\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 22:17\n",
      "maybe... but with dense y it works?\n",
      "\n",
      "1\n",
      "149\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-05 22:22\n",
      "Even after changing to `_num_samples(y)` the error seems to persist :|\n",
      "\n",
      "3\n",
      "150\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-05 22:29\n",
      "I am working on the polishing of the no-shuffle DBSCAN\n",
      "running the tests and should merge soon.\n",
      "I have not read the reference. Do you?\n",
      "I meant \"have you?\" not \"do you?\"\n",
      "Great!\n",
      "Alright, I will call it a day. See you tomorrow!\n",
      "++\n",
      "\n",
      "7\n",
      "151\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 22:34\n",
      "I'm thinking about the class_weight heuristic\n",
      "\n",
      "1\n",
      "152\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-05 22:39\n",
      "skimmed. But it makes more sense. I'll post a regression test in a second.\n",
      "\n",
      "1\n",
      "153\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-05 23:10\n",
      "@amueller , just so you're aware, when I was messing with how to structure the forests' class_weight implementations, I noticed float-point numerical differences in some little toy test datasets between a couple of different implementations that differed only in where or how i was multiplying, say sample weight and bootstrap weights.\n",
      "I'd expect that the change in #4347 will definitely change the way small floating point differences are evaluated. This obviously has a big effect in trees as magnitude doesn't matter, just gini improvement... So the outputs using class_weight between implementations will likely be different for forests, probably trees. Not that this really matters as they are both entering @ 0.16\n",
      "Long story short, I think SVMs and Linear Models could potentially change a little bit with #4347, though not as bad as trees I'm guessing. I'm not sure what \"promise\" is made to users about reproducibility between scikit-learn versions...\n",
      "\n",
      "3\n",
      "154\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-06 09:36\n",
      "Unfortunately we cannot promise much for those cases. It would be interesting to check the actual impact of the correct heuristic on some datasets though.\n",
      "\n",
      "1\n",
      "155\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-06 10:58\n",
      "@amueller @ogrisel could we use `0.5 - np.finfo(float).resolution` for #4295 (With the assumption that, it wouldn't be considered a magic no. since 0.5 is a standard symmetric scaling factor?) to scale it to `(-0.5, 0.5)`?\n",
      "\n",
      "1\n",
      "156\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-06 15:23\n",
      "@ogrisel fair enough. I'll try to find some time this weekend to run some experiments. I agree that the new implementation is more intuitive, just throwing it out there.\n",
      "\n",
      "1\n",
      "157\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-06 15:25\n",
      "Thanks\n",
      "\n",
      "1\n",
      "158\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-06 16:02\n",
      "@amueller shall we try to cut the 0.16.X branch?\n",
      "\n",
      "1\n",
      "159\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 16:25\n",
      "@ogrisel feel free. do we want to backport the isotonic stuff then? Or take @mjbommar's +1?\n",
      "@mjbommar's fix is not in a PR...\n",
      "\n",
      "2\n",
      "160\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-06 16:25\n",
      "Let's do the isotonic merge now\n",
      "I think it's tested enought both by the isotonic tests and the probability calibration tests to be correct.\n",
      "\n",
      "5\n",
      "161\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 16:31\n",
      "ok. and merge the other fix on top? It has my +1 and yours\n",
      "\n",
      "10\n",
      "162\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 16:40\n",
      "I did a rebase thing\n",
      "https://github.com/scikit-learn/scikit-learn/pull/4352\n",
      "\n",
      "2\n",
      "163\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 16:46\n",
      "should we wait for feedback on https://github.com/scikit-learn/scikit-learn/pull/4322 and then backport it?\n",
      "warnings in the beta seem not that bad\n",
      "\n",
      "2\n",
      "164\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-06 16:47\n",
      "yes +1 for waiting for this one\n",
      "I have to go now, sorry for leaving you alone.\n",
      "\n",
      "2\n",
      "165\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-06 17:40\n",
      "@amueller unfortunately I won't have time to cut the branch tonight. I have to go now. Feel free to do it, push a commit with the 0.16b1 version number (that follows PEP-440) and push a tag with pointing to it. That should get the CI workers to build the wheels. If everything goes well we should be able to push the release to PyPI (after testing on https://testpypi.python.org/pypi first) using wheelhouse-uploader.\n",
      "I will be busy tomorrow (we organize a deep learning workshop in Paris) and on sunday I should be mostly offline.\n",
      "I can push the release on PyPI on Monday if appveyor works well.\n",
      "travis is slow today...\n",
      "I would like to wait for appveyor and MacPython : https://github.com/MacPython/wiki/wiki/Spinning-wheels\n",
      "wrong copy and paste\n",
      "I meant: https://github.com/MacPython/scikit-learn-wheels\n",
      "I changed the master to follow PEP-440, so it's 0.16.dev0 at the moment\n",
      "It should move to 0.17.dev0 once the 0.16.X branch has been cut.\n",
      "I granted you push rights to https://github.com/MacPython/scikit-learn-wheels\n",
      "\n",
      "10\n",
      "166\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 17:41\n",
      "Ok, then I'll do it this afternoon. Mail to mailing list after pypi push? Or wait for appveyor?\n",
      "Are there instructions on the wheel builts in the docs?\n",
      "Ok, wasn't sure if we do that after the beta or the release, but makes sense if we branch.\n",
      "thanks\n",
      "\n",
      "4\n",
      "167\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 17:43\n",
      "what version number do you leave on master after doing the beta? Still 0.16-dev?\n",
      "\n",
      "1\n",
      "168\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-06 17:46\n",
      "once the scikit-learn 0.16b1 tag has been pushed to our github repo, we have to update the git submodules of https://github.com/MacPython/scikit-learn-wheels to point to it to get the travis bots of that repo to build all the Mac OSX wheels for that release.\n",
      "\n",
      "2\n",
      "169\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 17:52\n",
      "merge this on green then branching? https://github.com/scikit-learn/scikit-learn/pull/4352\n",
      "\n",
      "2\n",
      "170\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-06 18:17\n",
      "Andy could you quickly take a look at #4228 to see if the test to be satisfied is correct. I'm using that as an objective for that PR! (I'll iterate different fixes till that test is satisfied...)\n",
      "\n",
      "1\n",
      "171\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-06 18:26\n",
      "I've reopened #4228 as #4354 since #4228 won't reopen (more precisely, github won't track branches against which closed PRs were raised... So #4228 remains at 0+0- even after pushing the new commit to that branch and hence won't reopen as it brings about no change) Sorry for wasting an issue no.! :/\n",
      "\n",
      "1\n",
      "172\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 18:42\n",
      "Sorry, as I'll have to take care of the beta now, so I won't have time to look into the issue\n",
      "\n",
      "2\n",
      "173\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 22:49\n",
      "@ogrisel so if we don't build /upload the docs on beta, what should the links in the navigation of the \"documentation\" page be? I guess we have to fix them later....\n",
      "because now there will not be any 0.16 documentation online\n",
      "\n",
      "2\n",
      "174\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-06 23:06\n",
      "pushed the tag, updated the MacPython wheels.\n",
      "\n",
      "1\n",
      "175\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-07 09:32\n",
      "No there won't be any 0.16 website, before the 0.16 final release but I think this is fine. dev points to 0.17.dev0 and stable points to 0.15.2\n",
      "next time we upload the website we should remove the version for the link to the development documentation number from the documentation menu.\n",
      "\n",
      "2\n",
      "176\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-07 12:11\n",
      "Is there a nice cython guide that I could use? (Related to #4354 for `_tree.pyx` to make it support sparse y)\n",
      "\n",
      "1\n",
      "177\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-07 12:40\n",
      "@amueller I tried to install the macwheels in a new venv and it works:\n",
      "``` (sklearn-0.16b1)0 [~]$ pip install -f http://wheels.scipy.org/ --pre numpy scipy scikit-learn Downloading/unpacking numpy   http://wheels.scipy.org/ uses an insecure transport scheme (http). Consider using https if wheels.scipy.org has it available   Downloading numpy-1.9.2-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.7MB): 3.7MB downloaded   Storing download in cache at ./.pip/cache/https%3A%2F%2Fpypi.python.org%2Fpackages%2Fcp34%2Fn%2Fnumpy%2Fnumpy-1.9.2-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl Downloading/unpacking scipy   http://wheels.scipy.org/ uses an insecure transport scheme (http). Consider using https if wheels.scipy.org has it available   Downloading scipy-0.15.1-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (19.0MB): 19.0MB downloaded   Storing download in cache at ./.pip/cache/https%3A%2F%2Fpypi.python.org%2Fpackages%2F3.4%2Fs%2Fscipy%2Fscipy-0.15.1-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl Downloading/unpacking scikit-learn   http://wheels.scipy.org/ uses an insecure transport scheme (http). Consider using https if wheels.scipy.org has it available   Downloading scikit_learn-0.16_git-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.8MB): 4.8MB downloaded   Storing download in cache at ./.pip/cache/http%3A%2F%2Fwheels.scipy.org%2Fscikit_learn-0.16_git-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl Installing collected packages: numpy, scipy, scikit-learn Successfully installed numpy scipy scikit-learn Cleaning up... (sklearn-0.16b1)0 [~]$ python -c \"import sklearn; print(sklearn.__version__)\" 0.16-git ```\n",
      "However the version number is not good\n",
      "Actually it's my bad, it picked up the wrong wheel (because of the old version  number 0.16-git that is considered more recent than 0.16.b1)\n",
      "We won't have the problem when we upload on pypi as we won't upload that 0.16-git wheel\n",
      "If I fix the version of scikit-learn, it works: ``` (sklearn-0.16b1)0 [~]$ python -c \"import sklearn; print(sklearn.__version__)\" 0.16-git (sklearn-0.16b1)0 [~]$ pip uninstall -y scikit-learn Uninstalling scikit-learn:   Successfully uninstalled scikit-learn (sklearn-0.16b1)0 [~]$ pip install -f http://wheels.scipy.org scikit-learn==0.16b1 Downloading/unpacking scikit-learn==0.16b1   http://wheels.scipy.org uses an insecure transport scheme (http). Consider using https if wheels.scipy.org has it available   Downloading scikit_learn-0.16b1-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (5.0MB): 5.0MB downloaded   Storing download in cache at ./.pip/cache/http%3A%2F%2Fwheels.scipy.org%2Fscikit_learn-0.16b1-cp34-cp34m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl Installing collected packages: scikit-learn Successfully installed scikit-learn Cleaning up... (sklearn-0.16b1)0 [~]$ python -c \"import sklearn; print(sklearn.__version__)\" 0.16b1 ```\n",
      "I am launching the tests now\n",
      "All tests pass with the 0.16b1 wheel on my OSX laptop\n",
      "On windows on the other hand, the wheels were not uploaded because the tests are broken: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/history\n",
      "They have been broken for a while and I had not noticed\n",
      "yes I see it thanks!\n",
      "pushed\n",
      "let's wait for appveyor to build the wheels\n",
      "\n",
      "13\n",
      "178\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-07 12:40\n",
      "\n",
      "0\n",
      "179\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-07 12:55\n",
      "I created this to track it: #4358\n",
      "\n",
      "1\n",
      "180\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-07 13:01\n",
      "I pushed a fix for master (hopefully)\n",
      "Apparently you did not push the 0.16.X branch to the repo. I cannot cherry-pick the fix (c23d4f31a3fcd7b24e46d46df6f2d274a839b201) to get it into the 0.16.X branch.\n",
      "\n",
      "2\n",
      "181\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-07 17:21\n",
      "huh, pretty sure I pushed the 0.16.X branch.\n",
      "the tag is in the branch\n",
      "hum ok guess I didn't push it. I pushed with --tags, apparently that only pushes the taggs\n",
      "\n",
      "3\n",
      "182\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-07 17:24\n",
      "so which branch is this commit in? 09dc09a1e9d9088c2cb783c818980f5509d77a11\n",
      "fixed it, I think\n",
      "\n",
      "2\n",
      "183\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-07 17:37\n",
      "@amueller I fixed #4358\n",
      "it was a missing `random_state`, do you want me to do the backport?\n",
      "\n",
      "3\n",
      "184\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-07 17:51\n",
      "btw, where is the continuous integration on appveyor? I'm sure there is a badge for that so we can see when it breaks\n",
      "\n",
      "3\n",
      "185\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-07 17:53\n",
      "I have to go offline now. I might get back online tomorrow evening Paris time. If we are both online at that point we can push the release to PyPI together. Otherwise on monday. If the windows wheels are all generated correctly and you feel confident to use the upload-all command of the wheelhouse-uploader tool, feel free to do it before that.\n",
      "ok have a nice WE then :)\n",
      "\n",
      "3\n",
      "186\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-07 18:14\n",
      "I think I would need to login to get the badge url\n",
      "\n",
      "1\n",
      "187\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-07 18:20\n",
      "well we can use shields: https://img.shields.io/appveyor/ci/sklearn-ci/scikit-learn/master.svg\n",
      "\n",
      "1\n",
      "188\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-08 13:34\n",
      "spectral embedding doesn't accept lists... Should that be considered an issue?\n",
      "\n",
      "1\n",
      "189\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-08 21:21\n",
      "the function or the class?\n",
      "\n",
      "1\n",
      "190\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-08 21:38\n",
      "the `fit` method I had meant\n",
      "\n",
      "1\n",
      "191\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-08 21:40\n",
      "are you sure? That should be a common test. It might be that the common test doesn't cover clustering yet. Then this should be fixed.\n",
      "\n",
      "2\n",
      "192\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-10 18:10\n",
      "Now that we have appveyor tests, there is an appveyor integration for gitter that we could configure!\n",
      "Oh! I didn't know that :)\n",
      "\n",
      "2\n",
      "193\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-10 18:10\n",
      "we've had appveyor tests for a long time ;)\n",
      "we just didn't look at them\n",
      "\n",
      "2\n",
      "194\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-10 18:37\n",
      "what needs to be done for integration?\n",
      "\n",
      "1\n",
      "195\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-10 19:00\n",
      "Try Settings --> Integrations --> appveyor. Use this webhook https://webhooks.gitter.im/e/0dc8e57cd38105aeb1b4 :)\n",
      "@ragv wonders who added the webhook :O\n",
      "`/me` messages look too large in gitter :/\n",
      "@ogrisel Seems to have added that from the git blame... I think he would have integrated it already (why aren't we getting notifications then?) Hmm...\n",
      "\n",
      "4\n",
      "196\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-10 19:15\n",
      "BTW there seems to be a chrome app for gitter for linux users - https://chrome.google.com/webstore/detail/gitter/ldhcdmnhbafhckhidlhdbeekpifobpdc\n",
      "\n",
      "2\n",
      "197\n",
      "52f10c205e986b0712ef525f\n",
      "2015-03-10 20:38\n",
      "hello! yes, seems there are still several kinks to work out in the PR comparison logic\n",
      "working on it!\n",
      "\n",
      "2\n",
      "198\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-10 20:46\n",
      "haha didn't know you were here. Well your help is much appreciated!\n",
      ":) comment whereever you like\n",
      "\n",
      "2\n",
      "199\n",
      "52f10c205e986b0712ef525f\n",
      "2015-03-10 20:46\n",
      "@ragv just sent me an invite to the chatroom, probably to stop me cluttering up PR comments :)\n",
      "\n",
      "1\n",
      "200\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-10 20:48\n",
      "@carlio Not at all :) I added you so we could ping and trouble you whenever we hit a snag with landscape :p :P Thanks for the nice work!\n",
      "\n",
      "1\n",
      "201\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-10 20:50\n",
      "@carlio so does a push to a PR trigger a restart? I realize there would be some delay, though on the master branch landscape.io was pretty fast. Coveralls takes like a day to visit a PR if ever\n",
      "\n",
      "1\n",
      "202\n",
      "52f10c205e986b0712ef525f\n",
      "2015-03-10 20:57\n",
      "@amueller Yes, Landscape gets a GitHub push hook to trigger the check, so it should start within a few seconds. It's a bit wobbly right now. It also polls every 5 minutes or so (legacy behaviour for before I set up the push hooks to trigger on PRs)\n",
      "\n",
      "1\n",
      "203\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-10 21:01\n",
      "Does this (push to trigger) work for the old PRs too?\n",
      "\n",
      "5\n",
      "204\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-10 21:05\n",
      "This one didn't get a report for half an hour: https://github.com/scikit-learn/scikit-learn/pull/4381 still pending or something up there?\n",
      "slow travis today :-/\n",
      "maybe we should ask around if someone wants to pay 250 USD / month for more travis power ^^\n",
      "\n",
      "3\n",
      "205\n",
      "52f10c205e986b0712ef525f\n",
      "2015-03-10 21:08\n",
      "I sort of broke it a bit but it's running now\n",
      "\n",
      "1\n",
      "206\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-10 21:16\n",
      "\n",
      "1\n",
      "207\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-10 21:16\n",
      "@amueller I think you can manually restart it... In the travis build page there is an option to log in, once logged in you should get the option for restarting it...\n",
      "incase you didn't know that already...\n",
      "\n",
      "2\n",
      "208\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-10 21:20\n",
      "@ragv I know I can restart travis. The problem with travis is not that it is not restarting, the problem is that is is queueing.\n",
      "\n",
      "2\n",
      "209\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-10 21:48\n",
      "closing issues as non-issues is fun. anyhow, I gotta run. ttyl\n",
      "btw, which time zone are you on @ragv?\n",
      "\n",
      "2\n",
      "210\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-10 21:48\n",
      "See ya! I am in India GMT+05:30 :D\n",
      "\n",
      "1\n",
      "211\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-10 22:00\n",
      "Just throwing in a minor suggestion! We could perhaps have another CI build and test in parallel... like codeship / wercker... Let me know your opinion about that when you get online again!! both seem to be very easy to setup!\n",
      "\n",
      "3\n",
      "212\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-11 14:26\n",
      "@ogrisel why b2?\n",
      "any major issues?\n",
      "\n",
      "2\n",
      "213\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-11 14:29\n",
      "no, it's just that if we keep on backporting stuff to the 0.16.X branch, I don't want the artifacts generated by the CI bots to be named 0.16b1 to avoid confusion.\n",
      "\n",
      "2\n",
      "214\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-11 14:31\n",
      "but the artifacts are only generated for tags, right?\n",
      "\n",
      "1\n",
      "215\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-11 14:35\n",
      "Our current configuration on appveyor generates them continuously for everything and they get uploaded on to http://windows-wheels.scikit-learn.org . But we never documented that URL publicly until now. It's just useful to help us automate the releases.\n",
      "What I would like to have is all tags get uploaded to http://wheels.scipy.org (both OSX and windows wheels) while development wheels (without tags) get uploaded to a separate container (e.g. http://wheels-dev.scikit-learn.og or something).\n",
      "but never found the time to do so so far.\n",
      "\n",
      "3\n",
      "216\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-11 14:42\n",
      "BTW, I fixed the wheelhouse-uploader bug under Python 2. If you want to try again: ``` pip install -U wheelhouse-uploader cd /path/to/scikit-learn python setup.py fetch_artifacts ```\n",
      "\n",
      "1\n",
      "217\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-11 17:07\n",
      "today is my meeting day, don't expect much ;)\n",
      "\n",
      "1\n",
      "218\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-13 14:39\n",
      "Hi @amueller, I fixed a bug in @larsmans PR #4157 to optimize DBSCAN. It works great in my experiment. If we merge it quickly, I would like to get it in 0.16.\n",
      "\n",
      "1\n",
      "219\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-13 22:14\n",
      "Fine with me :)\n",
      "\n",
      "1\n",
      "220\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-14 07:36\n",
      "since `svd_flip` is public, do you also want `_deterministic_vector_sign_flip` to be made public too, or I'll leave it as such?\n",
      "\n",
      "1\n",
      "221\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-14 12:51\n",
      "keep it private\n",
      "svd_flip should probably not have been private in the first place.\n",
      "BTW it's probably better to have this kind of specific technical discussions in the comment threads of the issue itself rather than on gitter\n",
      "\n",
      "3\n",
      "222\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-14 12:52\n",
      "Thanks! and yes sure!\n",
      "\n",
      "1\n",
      "223\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-16 16:04\n",
      "we screwed up with the GSOC registration. I didn't remember you had to fill in melange and the form :-/\n",
      "\n",
      "1\n",
      "224\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 14:48\n",
      "Can anyone explain to me how to implement Affinity Propogation in Map/Reduce? http://www.chinacloud.cn/upload/2015-01/15011111364805.pdf\n",
      "I don't really understand the paper\n",
      "and the English is pretty much well... chinese :P\n",
      "\n",
      "3\n",
      "225\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 14:54\n",
      "scikit-learn is not a community of mapreduce users so I doubt you will get good feedback on here. You might rather ask on a discussion forum with spark or mahout users. I am not sure that using the hadoop mareduce API is a good thing for iterative machine learning anyway. It's probably better to build upon higher level parallel construct like allreduce or the spark API in general.\n",
      "\n",
      "1\n",
      "226\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 14:55\n",
      "The thing is that I have a huge dataset and I don't want to use K-Means since that means I have to guess the optimal number of clusters each time\n",
      "\n",
      "5\n",
      "227\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 14:56\n",
      "Use minibatch kmeans on a subset with init_size=int(1e4) and batch_size=int(1e3)\n",
      "\n",
      "1\n",
      "228\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 14:59\n",
      "if you are trying to learn, start with smaller datasets (e.g. a random subset) where algo run fast. To learn stuff you will have to fail many times to learn from your mistakes. If each failures take days of cluster programming and execution you will learn slowly :) By working on a subset that fits in memory on your laptop you will learn much faster.\n",
      "\n",
      "15\n",
      "229\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:08\n",
      "I'm working with geolocation data (x, y)\n",
      "I think that eucelidian distances is a good choice\n",
      "\n",
      "6\n",
      "230\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 15:09\n",
      "yes +1 for euclidean distance for geo data.\n",
      "How many samples?\n",
      "\n",
      "2\n",
      "231\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 15:10\n",
      "If you use scikit-learn master you might want to try DBSCAN on a 1M subset with eps the distance in meters of two points that are close enough to be considered part of a common cluster (assuming x and y are meters as well)\n",
      "the implementation of DBSCAN in sklearn 0.15.2 will be much to slow.\n",
      "> x & y are coordinates  I understand, but which unit? meters, km, miles, GPS degrees?\n",
      "they decide how much GPS degrees should be considered close points. Start with lower values to generate smaller clusters.\n",
      "Birch is probably a good candidate as well if you want to compress your 200M points into a smaller summary dataset.\n",
      "\n",
      "5\n",
      "232\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 15:11\n",
      "affinity propagation is not a great method imho. people rarely use it in practice I think, in particular not on such large data\n",
      "dbscan or birch or any other method that selects the number of clusters are much likelier candidates\n",
      "otherwise we keep duplicating fixes\n",
      "\n",
      "3\n",
      "233\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 15:14\n",
      "btw @ogrisel if you have the time, it would be awesome if you could work through some of the MRG + 1 PRs. There is a ton of them\n",
      "\n",
      "3\n",
      "234\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:15\n",
      "It's 200M unique points\n",
      "That's why I was looking at map/reduce\n",
      "\n",
      "2\n",
      "235\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 15:22\n",
      "``` >>> 200e6 * 2 * 8 / 1e9 3.2 ```  3.2GB of double precision floats => it fits in memory\n",
      "minibatch kmeans and birch can eat it\n",
      "yes AF is not scalable\n",
      "\n",
      "3\n",
      "236\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:22\n",
      "AF raises MemoryError\n",
      "\n",
      "1\n",
      "237\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:22\n",
      "Unless you're able to run it on multiple nodes which makes it scalable\n",
      "\n",
      "1\n",
      "238\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 15:23\n",
      "I am pretty sure that minibatch kmeans will converge in less than an hour\n",
      "\n",
      "7\n",
      "239\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 15:24\n",
      "it is a lot less automatic than any of the other methods\n",
      "it is really hard to tune\n",
      "\n",
      "3\n",
      "240\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:24\n",
      "And I really want to learn to apply ML on M/R\n",
      "\n",
      "2\n",
      "241\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 15:24\n",
      "all clustering algorithms have parameters that influence the number of clusters. some are explicit, like k-means, some are implicit, as in mean-shift, dbscan and birch.\n",
      "\n",
      "4\n",
      "242\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:25\n",
      "That's why the presented AF M/R is hierarchical\n",
      "\n",
      "1\n",
      "243\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 15:26\n",
      "the good number depends on what kind of application you want to use the result of the clustering for\n",
      "\n",
      "7\n",
      "244\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:26\n",
      "AF also has a hierarchical version but not in sklearn\n",
      "just saying yeh?\n",
      "Because I have a paper on it that explains (somewhat) how to implement it in M/R which is something that I want to be able to understand and implement\n",
      "The number will increase and we have other jobs executing with even more data\n",
      "\n",
      "4\n",
      "245\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 15:27\n",
      "but is probably not scalable to 200MB unless you impose neighbors constraints which will be similar to DBSCAN or Birch\n",
      "\n",
      "3\n",
      "246\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 15:30\n",
      "do not focus on MR, they are better ways to distribute machine learning on a cluster: spark and the H20 runtime\n",
      "http://mahout.apache.org/ : even mahout is moving away from mapreduce: 25 April 2014 - Goodbye MapReduce  The Mahout community decided to move its codebase onto modern data processing systems that offer a richer programming model and more efficient execution than Hadoop MapReduce. Mahout will therefore reject new MapReduce algorithm implementations from now on. We will however keep our widely used MapReduce algorithms in the codebase and maintain them.  We are building our future implementations on top of a DSL for linear algebraic operations which has been developed over the last months. Programs written in this DSL are automatically optimized and executed in parallel on Apache Spark.  Furthermore, there is an experimental contribution undergoing which aims to integrate the h20 platform into Mahout.\n",
      "\n",
      "2\n",
      "247\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:33\n",
      "We're using Druid over here\n",
      "And the jobs are written in python. Not java\n",
      "\n",
      "2\n",
      "248\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 15:39\n",
      "@amueller I merged @lesteve's PR with doc fixes. I have to catch my bus. Will probably reconnect later tonight.\n",
      "spark and h20 are JVM stuff as well. But it's a detail in practice. What matters is how the memory is used, how the algo scales and can it be efficiently be distributed on a cluster or not.\n",
      "\n",
      "4\n",
      "249\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:41\n",
      "We got disco and druid over here\n",
      "\n",
      "4\n",
      "250\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:41\n",
      "I'm trying AgglomerativeClustering on 10k points to see what happens in comparance to K-MEANS\n",
      "\n",
      "1\n",
      "251\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:46\n",
      "@ogrisel Do you know a Python library that is similar to mahout?\n",
      "\n",
      "3\n",
      "252\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:48\n",
      "I already have Druid (druid.io) and pydruid\n",
      "I'm talking about a batched implementation of ML algos in Python that integrates with tools like spark or druid\n",
      "I can adapt spark to druid\n",
      "\n",
      "3\n",
      "253\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-18 15:52\n",
      "@ogrisel I tried AgglomerativeClustering and I can't find where to fetch the cluster centers from\n",
      "\n",
      "6\n",
      "254\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 18:10\n",
      "`ac.labels_` gives you the cluster membeship of each sample (row) in the matrix `X` with shape `(n_samples, n_features)`, in your case `n_features==2`\n",
      "`AgglomerativeClustering` has no notion of center. The clusters can have non-convex (e.g. folded) shapes.\n",
      "Same for `DBSCAN` (and `Birch` to some extent).\n",
      "(Minibatch) k-means makes the convex clusters assumption, hence the cluster centers are meaningful and computed by default.\n",
      "I agree.\n",
      "\n",
      "5\n",
      "255\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 18:15\n",
      "Although whatever the clustering algorithm you can always compute the center of mass of any cluster a posteriori with basic numpy operations: ```python center_of_cluster_42 = X[ac.labels_ == 42].mean(axis=0) ``` assuming `X` is a numpy array.\n",
      "\n",
      "1\n",
      "256\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 18:22\n",
      "Btw, AF also has no notion of centers ;)\n",
      "yeah definitely\n",
      "\n",
      "2\n",
      "257\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 19:13\n",
      "hurray, thanks for merging the OMP fix @ogrisel. I'm quite confident that it is the right fix, but only time will actually tell ^^\n",
      "\n",
      "1\n",
      "258\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 19:26\n",
      "It's no big deal if it fails though. We can still re-add the travis skip as a temporary fix.\n",
      "\n",
      "1\n",
      "259\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 19:39\n",
      "I was confused by all the commits showing up twice in my notifications, but just realized they are for the two branches ^^\n",
      "Merging this one soon will probably also help: #4370 not sure if it needs two reviews, it is just removing deprecated stuff\n",
      "Just don't backport it ;)\n",
      "btw, did we set a date for 0.16?\n",
      "I guess 0.16.0\n",
      "\n",
      "5\n",
      "260\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-18 19:43\n",
      "Hey which do you feel is better? `if n is not None` or `if n`?\n",
      "\n",
      "1\n",
      "261\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 19:46\n",
      "they are different. If you want to ask \"if n is not None\" you should do that.\n",
      "the second one will also be false if n is 0\n",
      "\n",
      "2\n",
      "262\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-18 19:47\n",
      "Ah! explicit is always better as usual :) thanks!!\n",
      "\n",
      "1\n",
      "263\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-18 19:56\n",
      "@ogrisel @amueller could you kindly take a look at this comment - https://github.com/scikit-learn/scikit-learn/pull/4294#issuecomment-83123205 - This stands in the way of completion of the rest of the PR...\n",
      "\n",
      "1\n",
      "264\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 19:56\n",
      "No official date for 0.16.0 but maybe we could target next thursday?\n",
      "\n",
      "1\n",
      "265\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 19:57\n",
      "fine with me\n",
      "@ragv currently the PR doesn't yet use the new method in the GridSearchCV etc, right? Isn't that the big thing to do for that PR?\n",
      "I agree. lets do next thursday (the 26th)\n",
      "have you fixed cross_val_score?\n",
      "and make sure to keep them backward compatible with custom CV objects people might have written\n",
      "I also commented on you question in the issue\n",
      "\n",
      "6\n",
      "266\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 19:58\n",
      "from the 30th of March to April 1st we have a team retreat at work so I won't be able to work on the release. Then there is the oreilly webcast & PyData Paris and it would be great to have it released at that time.\n",
      "\n",
      "1\n",
      "267\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-18 19:59\n",
      "@amueller Oh! I haven't touched `grid_search.py` :/ Thanks! I'll work on that for now :)\n",
      "\n",
      "1\n",
      "268\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-18 20:02\n",
      "Not yet! Thanks a lot!! :) and okay sure! ( Sorry for pestering :p )\n",
      "\n",
      "1\n",
      "269\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-18 20:11\n",
      "@amueller off to the movie theater to watch citizenfour. Will resume PR reviews tomorrow morning a bit, otherwise Friday. See you!\n",
      "\n",
      "3\n",
      "270\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-18 20:16\n",
      "@amueller I was also chanting while standing on one leg, and waving burning sage around during the test passes, not sure if that's relevant.\n",
      "\n",
      "2\n",
      "271\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 20:21\n",
      "thanks for your extensive investigation, this one is .... interesting\n",
      "/play bezos\n",
      "damn, would have been to good if that worked\n",
      "\n",
      "3\n",
      "272\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 20:25\n",
      "the selection of sounds supported by campfire is great: http://www.emoji-cheat-sheet.com/\n",
      "\n",
      "1\n",
      "273\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-18 20:28\n",
      "haha. yeah no problem. i can investigate more, but having two systems that build without failing is nice too\n",
      "\n",
      "4\n",
      "274\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 20:33\n",
      "just if you have nothing better to do ;) I'm not sure about how to move forward. It seems strange that it works with the much older version on travis and not with the newer versions. What you could do is create a virtualenv with the travis 2.6 version of python and numpy and see if that also fails for you\n",
      "\n",
      "3\n",
      "275\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-18 20:48\n",
      "btw @ogrisel is there a reason not to enable OS X with travis?\n",
      "\n",
      "1\n",
      "276\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-19 09:22\n",
      "to enable OSX on travis we would need to adapt our CI script, but it should be doable.\n",
      "\n",
      "1\n",
      "277\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-19 10:14\n",
      "how do I plot AgglomerativeClustering correctly?\n",
      "\n",
      "1\n",
      "278\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-19 13:10\n",
      "@omerzimp are you looking for dendograms? https://github.com/scikit-learn/scikit-learn/pull/3464 or what do you want to plot?\n",
      "\n",
      "1\n",
      "279\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-19 13:11\n",
      "I have a scatter plot\n",
      "I want to show where the clusters are and color them according to the number of data points in a cluster\n",
      "yes\n",
      "\n",
      "3\n",
      "280\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-19 13:12\n",
      "what do you mean by where the clusters are? A common way to plot a clustering is to plot he points and color by cluster membership\n",
      "\n",
      "8\n",
      "281\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-19 13:18\n",
      "http://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.html\n",
      "\n",
      "1\n",
      "282\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-19 13:21\n",
      "and a label is the cluster number in this case?\n",
      "thanks. It's pretty accurate\n",
      "How come the results are very similar to K-MEANS?\n",
      "Also I still don't understand where the cluster centers are\n",
      "Ok that's good to know\n",
      "How do I check if a value would end up in a cluster? I have predict in K-MEANS but not in agglomerative clustering\n",
      "\n",
      "6\n",
      "283\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-19 14:28\n",
      "there are no cluster centers in agglomerative clustering\n",
      "\n",
      "1\n",
      "284\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-19 14:39\n",
      "@amueller Is there a parallel version of agglomerative clustering? I see that K-MEANS has one that uses joblib\n",
      "\n",
      "1\n",
      "285\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-19 14:53\n",
      "the k-means uses joblib just to parallelize the restarts. there is no parallel version of agglomerative clustering, but it should be pretty fast\n",
      "the way the clustering is constructed there is not really a way to predict in agglomerative clustering, but you could assign clusters to new points using nearest neighbors for example\n",
      "this seems to be somewhat tricky...\n",
      "which version? they talk about the locally oriented version which is described in a different paper\n",
      "yes but they don't say how\n",
      "\n",
      "5\n",
      "286\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-19 15:02\n",
      "I've seen papers about parallelizing them\n",
      "@amueller See http://www.cs.cornell.edu/~kb/publications/irt08.pdf\n",
      "Clause 3.3\n",
      "but doable right?\n",
      "Both are parallelizable according to 3.3\n",
      "You have to look at other papers\n",
      "\n",
      "7\n",
      "287\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-20 03:26\n",
      "@amueller ... you around? should i also update `y_numeric` for https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/kernel_ridge.py#L144 and https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/stochastic_gradient.py#L868 ? Both are explicitly regressors. there's a couple others that use fit between classifiers and regressors too... gbm, adaboost, bagging... Not sure I want to touch them for 0.16 though, would be more involved than a +1/-1 change\n",
      "presumably all of the above get through tests somehow though...\n",
      "\n",
      "2\n",
      "288\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-20 04:14\n",
      "nm. i copied these questions to the PR. you can comment there if you like, but i think those do not call the offending np function\n",
      "\n",
      "1\n",
      "289\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-20 14:41\n",
      "if the tests pass, I don't think we need to do anything\n",
      "\n",
      "1\n",
      "290\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-20 15:45\n",
      "thanks for all the merges @ogrisel :)\n",
      "as a thank you, I pinged you in 4 other issues ^^\n",
      "\n",
      "2\n",
      "291\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-20 15:51\n",
      "K. Well #4422 should be good then. It passes all tests locally on 1.8.0\n",
      "\n",
      "2\n",
      "292\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-20 15:54\n",
      "that will save us some eror reports on the new release!\n",
      "\n",
      "2\n",
      "293\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-20 18:13\n",
      "Hey :) @ogrisel @amueller Could I take up \"Multiple metric support for cross-validation and gridsearches\" as my GSoC project...? Sorry for the delay in my proposal... Final semester takes up a lot of my time :| I chose that since so far no one has proposed for the same and also I am working on a related PR...\n",
      "\n",
      "1\n",
      "294\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-20 18:16\n",
      "yes, that would be good and could also tie into finishing up the cross-validation stuff you are working on\n",
      "btw does anyone remember a PR adding more git rebase docs to the developer documentation?\n",
      "I though someone did it but I can't find it any more\n",
      "docs for the contributors on how to do git magic, like rebasing, not merging in master, and squashing\n",
      "we were talking about that somewhere (ML? issues?) and I though someone made PR. but maybe that was just wishful thinking\n",
      "apparently today is minor doc fix day. third push to master in a row.\n",
      "trevorstephens: psource should give you the source in ipython\n",
      "\n",
      "7\n",
      "295\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-20 18:17\n",
      "\"git rebase docs\" you mean contributor docs??\n",
      "\n",
      "1\n",
      "296\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-20 18:52\n",
      "@ogrisel in a recent PR you posted \"only 202 PRs to go\". Now it's 227. GSoC I guess ;) And that even with pretty aggressive merging...\n",
      "\n",
      "1\n",
      "297\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-20 18:53\n",
      "(shamefully) Thats me :p Joel had posted an issue asking for a better contributor docs... I am still doing that :| btw I did update wiki with a neat tree outlining the structure of our improved contributors guide... https://github.com/scikit-learn/scikit-learn/wiki/Contributors-Guide\n",
      "\n",
      "1\n",
      "298\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-20 19:26\n",
      "Something I spend way too much time doing when writing new stuff, or figuring out what's happening in an existing method (and perhaps there's already an easy way that this dum dum doesn't know)... But would adding the actual source code file in which a given developer func is located to the sklearn.utils.x functions' docstrings be helpful?\n",
      "(only for non-public functions in utils I think)\n",
      "\n",
      "2\n",
      "299\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-20 20:08\n",
      "that is magical... thx! but doesnt seem to tell the file it's in. :-/\n",
      "\n",
      "1\n",
      "300\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-21 09:09\n",
      "You could also do a `git grep \"def function_name\"` right? Thats the easiest I think! :)\n",
      "\n",
      "1\n",
      "301\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-21 15:44\n",
      "for the file you can use function.__file__\n",
      "that is ``function.__file__``\n",
      "\n",
      "2\n",
      "302\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-21 16:44\n",
      "__file__ didnt work, but func.__code__ seems to get what i want. thanks!\n",
      "that is `func.__code__`\n",
      "\n",
      "2\n",
      "303\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-21 18:56\n",
      "it might be that __file__ only works for modules, not functions\n",
      "psource should give you the same as file\n",
      "err code\n",
      "\n",
      "3\n",
      "304\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-22 14:08\n",
      "Given that there is a huge interest among students in learning about ML, do you think it would be within the scope of/beneficial to skl to have all the exercises and/or concepts, from a good quality book (ESL / PRML / Murphy) or an academic course like NG's CS229 (not the less rigorous coursera version), implemented using sklearn? Or perhaps we could instead enhance our tutorials and examples, to be a self study guide to learn about ML? I was planning to include this in my GSoC proposal... does it seem like an useful idea? (Also is this a correct place to discuss this or the Mailing list might be better for such questions?)\n",
      "Or would it be better if I simply add more examples?\n",
      "\n",
      "2\n",
      "305\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-22 17:52\n",
      "Yes please!\n",
      "I really want to learn ML and it's pretty hard as it is\n",
      "An extensive tutorial would be lovely.\n",
      "\n",
      "3\n",
      "306\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-22 17:55\n",
      "Thanks for the response!! Lets wait for a confirmation from the core devs, since any additional code does come with its own maintenance cost... They would be the best at judging if that additional maintenance is worth the benefit it offers :)\n",
      "\n",
      "1\n",
      "307\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-23 00:16\n",
      "> Also is this a correct place to discuss this or the Mailing list might be better for such questions?  NM! I've copied this to the ML, so as to reach a wider audience :) Please let me know your views there!\n",
      "\n",
      "1\n",
      "308\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-23 08:33\n",
      "@ragv +1 for a GSoC on cross-validation improvements. I can volunteer to be a mentor on that one.\n",
      "\n",
      "1\n",
      "309\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-23 08:34\n",
      "Thanks!! :D would you be able to take a look at my proposal at https://github.com/scikit-learn/scikit-learn/wiki/GSoC-2015-Proposal:-Multiple-metric-support-for-CV-and-grid_search-and-other-general-improvements and let me know your views?\n",
      "\n",
      "3\n",
      "310\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-23 12:58\n",
      "@ragv I don't know about NG's course, but for the books, sklearn is useless for the exercises, right? They are mostly mathematical proofs. How would sklearn help with that?\n",
      "\n",
      "1\n",
      "311\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-23 12:59\n",
      "@ragv I think we should focus your GSoC on improving cross-val, writing a generic tutorial to ML is interesting but too long to do in addition to a GSoC\n",
      "for the cross-val, we could have the data-independent refactoring and then for the second part of the GSoC experiment with high level helper API to do out-of-core cross-validation for models that support partial_fit\n",
      "I have personally not started to thing about what such an API would look like but I think this is an important use case\n",
      "multiple metrics is interesting as well\n",
      "\n",
      "4\n",
      "312\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-23 13:01\n",
      "I thought we wanted to do multiple metrics?\n",
      "\n",
      "1\n",
      "313\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-23 13:02\n",
      "btw have you looked at any of the gsoc proposals yet?\n",
      "\n",
      "3\n",
      "314\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-23 13:21\n",
      "I guess I need to review them all...\n",
      "\n",
      "1\n",
      "315\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-23 14:31\n",
      "btw, @ogrisel I thought about doing the split of the ``utils`` module into the private and public modules, that people have been talking about for ever. What do you think about that?\n",
      "\n",
      "1\n",
      "316\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 14:45\n",
      "Hey all, I have noticed [here](http://sourceforge.net/p/scikit-learn/mailman/message/24403926/) and [here](http://mail.scipy.org/pipermail/scipy-dev/2010-January/013709.html) that there was once a genetic algorithm module in scikits.learn, it appears to have been removed mostly due to code rot, maybe API differences too, but does anyone know if there is an underlying general issue with genetic algorithms that are not scikit-learn friendly?\n",
      "For context, I am writing a symbolic regression class that constrains genetic programming's flexibility to the main use cases found in scikit-learn (regression, classification, transformation) and sticks with the existing scikit-learn API style. So just wanted to check in to ensure I'm not going off the deep end as I'm nearing a functional regressor already... Though still a fair ways from a PR :smile:\n",
      "\n",
      "2\n",
      "317\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-23 14:59\n",
      "@ogrisel green button on https://github.com/scikit-learn/scikit-learn/pull/4432/files to avoid merge conflicts?\n",
      "\n",
      "1\n",
      "318\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-23 15:01\n",
      "@trevorstephens genetic algorithms are probably going off the mariana trench. what is your application of symbolic regression?\n",
      "\n",
      "1\n",
      "319\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 15:02\n",
      "haha. what do you mean by application? where would it be used?\n",
      "\n",
      "1\n",
      "320\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 15:04\n",
      "its just a regressor, except that the final result is expressed as a non-linear equation.\n",
      "could also be used for automated feature extraction in a supervised transformer\n",
      "\n",
      "2\n",
      "321\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 15:06\n",
      "basic idea is to take an initial sample of random equations and apply genetic operations such as mutation, reproduction, etc to the fittest individuals in a population. the equations are expressed like LISP trees... well im just using python lists, but similar structure\n",
      "\n",
      "1\n",
      "322\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-23 15:09\n",
      "@amueller about the split of utils I have no strong opinions. I am +1 on introducing new private utils with a `_` prefix.\n",
      "\n",
      "6\n",
      "323\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 15:13\n",
      "ive seen people using symbolic regression in combination with stacking on kaggle (the higgs boson comp was memorable) to extract new features that helped their gbm's latch onto some very interesting segments of the feature space. there is a lot of research into competitive results though i dont know if they have been compared to RFs\n",
      "\"Also, is that what people currently use in symbolic regression?\", do you mean LISP trees? yes, it is very common. basically a flattened tree representation\n",
      "\n",
      "2\n",
      "324\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-23 15:16\n",
      "@amueller about the MLP PR, I have implemented an adaptative schedule for the learning rate here: https://github.com/ogrisel/scikit-learn/commit/3c4cc13b39fdb6a91be44a0977766e16d45ed5dc . It seems to be very useful in practice, although Ilya Sutskever recommends to use a validation set instead of the training score. Using a validation set is complicated from an API point of view though.\n",
      "Nesterov momentum is not that complicated. Have you read:  http://www.cs.toronto.edu/~fritz/absps/momentum.pdf ?\n",
      "it's mostly the ordering of the update.\n",
      "@amueller this experimental notebook might help http://nbviewer.ipython.org/github/ogrisel/notebooks/blob/master/Gradient.ipynb\n",
      "\n",
      "4\n",
      "325\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-23 15:21\n",
      "also I am currently experimenting with a from scratch implementation in theano to have a comparision point (and more NN experience): http://nbviewer.ipython.org/github/ogrisel/notebooks/blob/master/representations/MNIST%20experiments.ipynb\n",
      "about the utils stuff it does not seem to be particularly high priority to me but feel free to work on that if you find it important yourself.\n",
      "but it's utility would have to be proven first. So better implement that in a dedicated python package first.\n",
      "I have to go now, I will review the other sections and give you finer feedback tomorrow\n",
      "\n",
      "4\n",
      "326\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-23 15:26\n",
      "I read the momentum pdf. but I'll have a look at your notebook. I feel we should merge this asap as it has been lying around for so long. adding monitoring etc can always be done afterwards\n",
      "and the use of a validation set would be great but is a whole other can of worms\n",
      "@trevorstephens sorry for being ambiguous. I meant are people using genetic programming for symbolic regression.\n",
      "\n",
      "3\n",
      "327\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-23 15:33\n",
      "I am not sure we should merge that early. The optimizer is clearly suboptimal, the one in my branch seems to be much more robust but changes the meaning of the 'constant' hyperparameter.\n",
      "I think it's very important to have a robust hyperparameter by default.\n",
      "But I want to focus on the 0.16 release first.\n",
      "@trevorstephens encourage you to implement that as a third party project first. If it proves empirically useful then we might consider it for inclusion in the future. But Genetic Programming stuff is culturally very different from the sklearn spirit. Especially we do not want to have a generic Evoluationary Algorithm module as part of sklearn as the API would not be suited for it.\n",
      "A black box Genetic programming solver hidden in a SymbolicRegressor estimator might be interesting though.\n",
      "\n",
      "5\n",
      "328\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-23 15:47\n",
      "@ogrisel is there much to do for 0.16?\n",
      "@ogrisel is there anything to be done for the EML branch? It has no +1 yet.\n",
      "this one would be nice: https://github.com/scikit-learn/scikit-learn/pull/4350\n",
      "\n",
      "3\n",
      "329\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-23 16:46\n",
      "@amueller @ogrisel Thanks for the reviews!! Based on your comments I'll replace that section of my prop. with a more significant work > They are mostly mathematical proofs. How would sklearn help with that?  I was thinking more on the lines of http://www-bcf.usc.edu/~gareth/ISL/code.html.. But like you said for the practical part of it, we do have some nice books! Perhaps out of GSoC, it would  be better if I simply cleaned up existing examples and added a few more perhaps!\n",
      "\n",
      "1\n",
      "330\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 17:07\n",
      "@amueller , yes, it is the 'classic' use of GP.\n",
      "\n",
      "1\n",
      "331\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 17:09\n",
      "@ogrisel . \"A black box Genetic programming solver hidden in a SymbolicRegressor estimator might be interesting though.\" Yep. That's exactly what I'm going after. Though understand if you don't want it in the package yet, makes perfect sense.\n",
      "FWIW, here's the (did I mention super-early) code: https://github.com/trevorstephens/scikit-learn/blob/genetic_programming/sklearn/genetic.py\n",
      "\n",
      "2\n",
      "332\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 17:12\n",
      "running is as simple as `gp = SymbolicRegressor(random_state=415)`  `gp.fit(diabetes.data, diabetes.target)`  `print gp.program_`\n",
      "\n",
      "1\n",
      "333\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-23 17:24\n",
      "@amueller I am working on pinpointing numpy version issues for #3747 at the moment\n",
      "ho you merged it already.\n",
      "Did you do the backport as well ?\n",
      "I am not sure about the 1.9 version comparison in the test\n",
      "I am testing with numpy 1.8.0\n",
      "Ok the tests pass with 1.8 as well :)\n",
      "@trevorstephens would be great to extract that as a new library. You can write some sklearn compat tests with stuff in`sklearn.utils.estimator_checks`.\n",
      "you can make it easy to install by writing a simple setup.py.\n",
      "As you don't need compiled extensions, you don't need the complicated numpy.distutils, you can just use the regular distutils or setuptools.\n",
      "you could name your project symbolicregressor or something\n",
      "symregressor\n",
      "\n",
      "11\n",
      "334\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 17:34\n",
      "billie-gene has a nice ring to it\n",
      ":smile:\n",
      "\n",
      "2\n",
      "335\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-23 17:37\n",
      "@ragv I would move generic tutorial out of the scope of your GSoC but you can expand the documentation item of your GSoC proposal to include a tutorial on how to do proper cross-validation: how to select the CV strategy for a given problem, how to use stratification, how to check that the i.i.d. assumption is not violated...\n",
      "\n",
      "1\n",
      "336\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-23 18:05\n",
      "thanks for the advice as well @ogrisel\n",
      "\n",
      "1\n",
      "337\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-23 18:11\n",
      "@ogrisel Thanks for the suggestion! On it :) Do the other sections look okay...?\n",
      "Okay! bye :)\n",
      "\n",
      "2\n",
      "338\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 14:07\n",
      "@ogrisel the thing about the mlp is, that as long as it is a PR, we will not get feedback about it from anyone. But once it is merged, people will use it, and people will sent PRs. I don't think we need to \"nail it\" before it is merged. The naive end-user will only use it after the release\n",
      "\n",
      "1\n",
      "339\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-24 16:17\n",
      "That seems like a good idea... More after merging to master, it becomes easier for anyone to contribute to it by a normal PR rather than a PR to a PR... (for whatever my comment is worth) +1 from me for that! :)\n",
      "also, if it is not considered convoluted, we could raise a `BetaModule` warning like `DeprecationWarning` to warn users not to use it in production code?\n",
      "\n",
      "2\n",
      "340\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 17:47\n",
      "well, if it is the dev version, people should know\n",
      "\n",
      "1\n",
      "341\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 18:40\n",
      "I have the common tests on OneClassSVM (I think) hanging randomly on my box. no good.\n",
      "\n",
      "1\n",
      "342\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-24 18:52\n",
      "@amueller Should #1674 be closed in favor of #1626, since @mblondel has been positive about the same in his last comment?\n",
      "\n",
      "1\n",
      "343\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 19:12\n",
      "which comment?\n",
      "\n",
      "1\n",
      "344\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-24 19:13\n",
      "https://github.com/scikit-learn/scikit-learn/issues/1674#issuecomment-13982364\n",
      "\n",
      "5\n",
      "345\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-24 19:19\n",
      "Well.. I thought of including it in my proposal ;) I feel this is a tough one and will take easily around 1.5 months? (Is that adequate amount of time that could be allocated for the same?)\n",
      "\n",
      "4\n",
      "346\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 19:22\n",
      "the main question would be how to pass it the validation set so to make it pipeline compatible and GridSearchCV compatible\n",
      "\n",
      "1\n",
      "347\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 20:44\n",
      "any GSOC applicant looking for stuff to do can check: https://github.com/scikit-learn/scikit-learn/issues/4442#issuecomment-85683185\n",
      "(ragv you have enough contribs and don't need to worry about it ;)\n",
      "not that help would not be appreciated, but I think you have enough stuff to do\n",
      "\n",
      "3\n",
      "348\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-24 20:46\n",
      "LOL okay :D I am a bit busy this month due to my academics moreover, thats why I am not able to spend enough time on scikit-learn  :cry:\n",
      "Sure! I haven't yet... I was fine tuning it a bit.. sorry :)\n",
      "\n",
      "5\n",
      "349\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-24 20:48\n",
      "@ragv have you put you proposal on melange?\n",
      "I cannot find it\n",
      "if so please edit the title to add \"scikit-learn\" in it\n",
      "\n",
      "3\n",
      "350\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-24 20:56\n",
      "@ogrisel Done! but the formatting is awefully bad... A little more description is needed too... I'll fix both ASAP :)\n",
      "Is \"path algorithms\" a good name to mean all cv related objects?\n",
      "\n",
      "4\n",
      "351\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 21:01\n",
      "wait, depends on what you mean by cv related objects\n",
      "It applies to LinearModelCV objects\n",
      "\n",
      "2\n",
      "352\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-24 21:01\n",
      "cross_validation / learning_curve / model_selection etc... oh.. sorry then I'll call them CV objects simply :)\n",
      "\n",
      "5\n",
      "353\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 21:03\n",
      "I think you will have a great proposal. I haven't read your most recent version, I just wanted to reiterate that you should try to focus, in particular provide concrete deliverables\n",
      "sound more appropriate than the current title.\n",
      "sorry if my feedback today is a bit short, I am feeling sick and wanted to at least briefly comment on all submission\n",
      "s\n",
      "\n",
      "4\n",
      "354\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-24 21:07\n",
      "Not an issue at all! Feel free to comment whenever you feel like :) And more over I got the basic idea and hence can work out my proposal according to your guidelines :) Also take care :) Sorry to trouble you at the last min...\n",
      "HI5 :p\n",
      "\n",
      "2\n",
      "355\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 21:11\n",
      "haha well we just started to review now so it is our fault ;)\n",
      "\n",
      "1\n",
      "356\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-24 21:21\n",
      "+1 on the new title and focus\n",
      "\n",
      "1\n",
      "357\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-24 21:34\n",
      "I at least gave some cursory comments on all proposals. I think all the mentors should review at least one that they are interested in.\n",
      "@ogrisel do you want to put your name on one or more proposals on the list (more than one meaning one of these will be selected)\n",
      "\n",
      "2\n",
      "358\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-24 21:49\n",
      "@amueller you mean in the google doc or on melange?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5\n",
      "359\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-24 22:03\n",
      "cheers!\n",
      "\n",
      "1\n",
      "360\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-25 00:19\n",
      "ok I did a pass on the GSoC proposals. I need to go to sleep now. Tomorrow will be very busy with meetings and meetups.\n",
      "I will probably be back online on Thursday morning to resume work on GSoC reviews and sklearn 0.16 release.\n",
      "\n",
      "2\n",
      "361\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-25 18:39\n",
      "I might cherry-pick some more things today, I have to check\n",
      "\n",
      "1\n",
      "362\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-25 20:58\n",
      "@amueller do you think we should probably fine tune landscape settings to avoid reporting not-so-useful fixes that are probably ignored... Thing is without it raising red flags only for important issues that a core dev would normally frown upon (like PEP8, imports, the docstring format), I am afraid this will eventually be ignored like the coveralls bot. :/ WDYT?\n",
      "\n",
      "1\n",
      "363\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-25 21:03\n",
      "the main problem is that it reports new problems that are not related to the code at all.\n",
      "\n",
      "2\n",
      "364\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-25 21:22\n",
      "maybe ;)\n",
      "\n",
      "1\n",
      "365\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-25 21:23\n",
      "@ogrisel do you think there is a chance @GaelVaroquaux will review and / or put his name next to some of the proposals? It would be nice to have tentative mentors.\n",
      "\n",
      "1\n",
      "366\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-25 21:42\n",
      "@ogrisel I backported some doc fixes and the faster polynomial features\n",
      "\n",
      "1\n",
      "367\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 01:08\n",
      "Will I be able to edit my prop after 27th? :p\n",
      "\n",
      "1\n",
      "368\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 01:10\n",
      "I don't think so. the mentors might\n",
      "but you should not count on it ;)\n",
      "vim\n",
      ";)\n",
      "I should probably switch at some point\n",
      "\n",
      "5\n",
      "369\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 01:11\n",
      "Okay! Would you be able to take a look at the revised prop and leave your comments? :)\n",
      "\n",
      "3\n",
      "370\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 01:20\n",
      "melange's formatting is very bad :neutral_face:  The wiki pages look neat compared to melage ...\n",
      "\n",
      "1\n",
      "371\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-26 01:20\n",
      "You could copy paste, I think\n",
      "\n",
      "1\n",
      "372\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 01:21\n",
      "still sick, meh\n",
      "\n",
      "1\n",
      "373\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 01:21\n",
      "Thats bad :/ tc!\n",
      "Hey @xuewei4d :)\n",
      "I tried but its still very bad :(\n",
      "This site was useful for me tho :D - http://markable.in/editor/\n",
      "I edited entirely in md and copy pasted the html code into melange :P\n",
      "haha... subl seems to be better in so many ways... hmm I guess time to move away from vim...\n",
      "\n",
      "7\n",
      "374\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-26 01:21\n",
      "GSOC 2016: melange\n",
      "This is what I did. I use sublime to preview markdown, then copy paste html page\n",
      "\n",
      "2\n",
      "375\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 01:28\n",
      "Thanks atleast I got one person saying that :O Every other person I meet recommends subl or pycharm... I love vim but hate them ranting about their shiny editors :p\n",
      "What do you use? :)\n",
      "\n",
      "2\n",
      "376\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-26 03:23\n",
      "\"i've been using vim for three years, but that's only because i cant figure out how to exit\"\n",
      "\n",
      "1\n",
      "377\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 07:47\n",
      "@trevorstephens you released gplearn? nice!\n",
      "\n",
      "1\n",
      "378\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 07:48\n",
      "you should create an `examples/` folder with a sample script on some regression dataset from scikit-learn (e.g. load_boston) and another on a synthetic data\n",
      "where you know the symbolic ground truth\n",
      "\n",
      "2\n",
      "379\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 10:18\n",
      "@amueller I am checking the documentation of the 0.16.X branch. It looks good. Will run trough the release checklist next.\n",
      "let me know if you find any release blocker in the mean time.\n",
      "Thanks\n",
      "ok, thanks\n",
      "about the version number, shall we use 0.16.0 or 0.16?\n",
      "Last time we did 0.15.0. But I used 0.16b1 instead of 0.16.0b1\n",
      "I think we should do 0.16.0 for lexicographical sorting of folders and stuff like that\n",
      "@amueller shall we do the `git log <last_release>.. | git shortlog -s -n` report for this release, or not?\n",
      "as you said earlier it, this vanity contest might encourage contributors not to squash their PR by default (and it does not reflect review work).\n",
      "\n",
      "9\n",
      "380\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-26 10:31\n",
      "I think that everyone in this room should read http://www.amazon.com/Scaling-Machine-Learning-Distributed-Approaches/dp/0521192242\n",
      "This is f***ing awesome!\n",
      "\n",
      "2\n",
      "381\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-26 10:59\n",
      "np\n",
      "\n",
      "1\n",
      "382\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 14:45\n",
      "I didn't know about it. But it seems a bit dated (2011)\n",
      "@ogrisel no blockers that I know of. Anything you want me to work on?\n",
      "scheduling the release for the day before the GSoC deadline was a genious move on our part ^^\n",
      "this needs backporting I think: https://github.com/scikit-learn/scikit-learn/pull/4448\n",
      "\n",
      "4\n",
      "383\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-03-26 14:49\n",
      "@amueller I'm sure there is a new edition somewhere\n",
      "\n",
      "2\n",
      "384\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-26 14:56\n",
      "Hi @ogrisel . Just reserved the name on Pypi and am getting the hang of all the code review tools last few days. Still a long way to go before I'm satisfied enough to release it. Definitely need to add a lot of tests (coverage at 4% haha), documentation, examples, etc etc. But it'll get there\n",
      "I'd plan on some practical examples like boston, and then another like x^3 + x^2 + x + 1 or something, maybe in two dims\n",
      "Maybe I'll try to write some docs for sklearn on the process of setting up your own <plugin>learn or something\n",
      "So long as it remains stable. A lot of thought right now going into how I'll support various versions of sklearn. I may need to grab a copy of a lot of the tests and utils to keep from being harassed by bug reports\n",
      "Since those don't go through deprication cycles so far as I know\n",
      "But yeah, that looks like a good start @ragv\n",
      "\n",
      "6\n",
      "385\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 15:00\n",
      "First step would be to build at a nice testing framework (#3810) I feel :)\n",
      "\n",
      "1\n",
      "386\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 15:12\n",
      "@ogrisel I just did the backport\n",
      "\n",
      "1\n",
      "387\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 15:38\n",
      "I'm -0. We could do alphabetical order?\n",
      "Is 0.16.0 according to the pep thing? I think 0.16.0b1 was not, so we didn't do it. I think 0.16.0 is good\n",
      "The whatsnew already mentioned the contributors next to the changes. Maybe that's enough vanity? That encourages cool things ;)\n",
      "FUCK again broke the SVC trying to rebase on master. That is really finnicky\n",
      "\n",
      "4\n",
      "388\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 15:46\n",
      "@ogrisel can you do me a favor and physically force @GaelVaroquaux to review #4189 so we can merge it? I only pick him because no-one else is in physical proximity ;)\n",
      "why is fabian so high up in the commits?\n",
      "oh I think my git log was wrong\n",
      "\n",
      "3\n",
      "389\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 15:49\n",
      ">  physically force @GaelVaroquaux to review #4189  Haha :laughing:\n",
      "\n",
      "1\n",
      "390\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 15:51\n",
      "@ogrisel any parts of the release you want me to do? update the links in the website? built the website?\n",
      "damn I though #4326 was in\n",
      "\n",
      "2\n",
      "391\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 17:47\n",
      "all 0.16b1 0.16.0b1 0.16 0.16.0 are valid according to pep 440\n",
      "+1 for not including the stats\n",
      "I had to leave the office early (going to a meetup to try and hire graduating students to work on sklearn next year) ...\n",
      "ok for 0.16.0 then\n",
      "Let's merge #4189: it's mostly a fix, no new API, no hyperparams, no documentation or example to update. It should not be controversial.\n",
      "git log 0.15.2..0.16.X | git shortlog -s -n\n",
      "if you want the stats\n",
      "@amueller I let you do the cherry-pick for https://github.com/scikit-learn/scikit-learn/pull/4189/commits into 0.16.X\n",
      "to fix your ranking for 0.16.0 commit number :)\n",
      "argl actually the cherry-pick of #4189 is not trivial. We probably also need to bacport #4326 too but this one is not trivial either...\n",
      "I have to go to the meetup soon, I won't have time to do the #4189 backport today.\n",
      "\n",
      "13\n",
      "392\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-26 17:48\n",
      "+1 0.16.0\n",
      "\n",
      "1\n",
      "393\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 18:01\n",
      "@ogrisel @amueller Any final comments on my revised prop? I made it a bit ambitious but I think it is doable with full time involvement. WDYT?\n",
      "\n",
      "5\n",
      "394\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 18:07\n",
      "Thanks... sorry for hijacking into your 0.16 discussions... :)\n",
      "\n",
      "1\n",
      "395\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 18:08\n",
      "no pbm, it's our fault to have scheduled to do the release on the same week as the GSoC deadline\n",
      "maybe it is in then\n",
      "I had conflicts that looked non trivial when picking #4189 so it did a git reset --hard on my sandbox\n",
      "\n",
      "3\n",
      "396\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 18:11\n",
      "#4326 was before branching even\n",
      "\n",
      "1\n",
      "397\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 18:16\n",
      "@ogrisel it was the docstring stuff by @ragv that caused the cherry-pick error. I backported it\n",
      "pushed to 0.16.X should be fine now\n",
      "\n",
      "2\n",
      "398\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 18:21\n",
      "Yayy I got 50 commits :P\n",
      "\n",
      "5\n",
      "399\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 18:30\n",
      "@ragv I would move the out-of-core cross-val as a low priority optional in case everything else is implemented first.\n",
      "I doubt we will have the time to implement everything else but one never knows :)\n",
      "@amueller would be great to have a highlight section.\n",
      "\n",
      "3\n",
      "400\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 18:31\n",
      "hum, https://github.com/scikit-learn/scikit-learn/pull/4420 would have been nice, but without a review by a PLS person I'm doubtful\n",
      "\n",
      "4\n",
      "401\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 18:31\n",
      "\n",
      "1\n",
      "402\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 18:32\n",
      "other than that, version change to 0.16.0, update the links in the menus for the doc, put the 0.16.0 tag and push it, update the submodule in the MacPython repo to get travis build wheels for OSX, wait for appveyor to do the same for windows. Then let me know, I will manually test install on my mac and a windows VM\n",
      "\n",
      "1\n",
      "403\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-26 18:33\n",
      "@ogrisel Okay! Thanks for the review.... :) Should I perhaps concentrate on gen cv and early stopping for the month of July?\n",
      "Except for the month of July, I believe I have framed a satisfactory (to myself :p) timeline. It is the month of July I am not quite sure, what I will get to work on that could be useful for sk and also be feasible for me...\n",
      "@ogrisel thanks :D\n",
      "\n",
      "4\n",
      "404\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 18:40\n",
      "my presentation will start soon, then there will be beers so I won't be of any use tonight.\n",
      "but if appveyor and macpython / travis are all green, I can do the manual tests tomorrow morning\n",
      "and build the doc to rsync the new version.\n",
      "@ragv don't worry too much, I think there is a lot of testing /  benchmarking / polishing / documentation cleaning to do\n",
      "\n",
      "14\n",
      "405\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 18:42\n",
      "https://github.com/amueller/scikit-learn/commit/dbe3cd33145677cdae9bdbcbef0c3791bf94bca3\n",
      "ok\n",
      "\n",
      "2\n",
      "406\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 18:45\n",
      "Maybe reword as \"Scalable approximate nearest neighbors search with Locality-sensitive hashing forests\"\n",
      "the auto-tuning part could still be improved :)\n",
      "\n",
      "2\n",
      "407\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 18:47\n",
      "input validation was already mostly in 0.14, right?\n",
      "that's what I ment\n",
      "yeah\n",
      "true\n",
      "\n",
      "4\n",
      "408\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 18:48\n",
      "remember the work you did in test_common over the past 6 months :P ?\n",
      "\n",
      "3\n",
      "409\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 18:49\n",
      "what I did when in test_common is mostly a blur ;) I remember touching every single file during the last sprint when removing check_arrays lol\n",
      "\n",
      "2\n",
      "410\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 18:50\n",
      "DBSCAN mentioning jay or nay?\n",
      "maybe also \"Many speed improvements, reduced memory requirements, bug-fixes and better default settings.\"\n",
      "\n",
      "7\n",
      "411\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-26 18:55\n",
      "Don't forget to cherry-pick the paragraph both in master and 0.16.\n",
      "\n",
      "3\n",
      "412\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 19:10\n",
      "@ogrisel have you looked at Vinayak's proposal by any chance?\n",
      "\n",
      "1\n",
      "413\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-26 19:17\n",
      "Hi, I am wondering how many slots does scikit-learn have this year, if it is not a top secret? I remember the number is 4 in 2014.\n",
      "\n",
      "1\n",
      "414\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 19:20\n",
      "https://wiki.python.org/moin/SummerOfCode/FrequentlyAskedQuestions#How_many_slots_does_python_get.3F__How_many_does_project_.24x_get.3F\n",
      "we currently have 5 mentors registered (if no one else registered) which means we would get a max of 5 slots\n",
      "\n",
      "2\n",
      "415\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-26 19:31\n",
      "OK. Thanks! Hope we have 5.\n",
      "\n",
      "1\n",
      "416\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-03-26 19:48\n",
      "Hello everyone! Just got an invitation to join the chat here. Any final bit of advice on my proposal?\n",
      "\n",
      "4\n",
      "417\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 19:49\n",
      "?\n",
      "\n",
      "1\n",
      "418\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 19:50\n",
      "hi @bryandeng I have to look at your proposal again. Sorry, we are a bit busy today, we are also doing a perfectly timed release today\n",
      "The work-load seemed a bit light. The proposal on the idea page was not super fleshed out, unfortunately\n",
      "Maybe you can give a bit more background on the improvements for the existing algorithms. currently it is only half a sentence.\n",
      "Do you have any other ideas in the semi-supervised scope?\n",
      "\n",
      "5\n",
      "419\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-03-26 19:58\n",
      "I think I can implement more than one semi-supervised algorithms, candidates of which can be discussed in the community bonding period.\n",
      "\n",
      "1\n",
      "420\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 20:06\n",
      "I know it is pretty late now, but it would be great if you could propose some. In the other semi-supervised proposal I mentioned transductive SVMs. I'm not entirely sure they are the best way to go, but at least they are a common benchmark. The more your proposal shows you are familiar with the material and already looked into it, the better\n",
      "\n",
      "1\n",
      "421\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-03-26 20:34\n",
      "OK. My original intention to write like this is that I want to serve the needs of the community to the greatest extent. Since what algorithms we most want in sklearn.semi_supervised hasn't been fully discussed in the community, I decided not to show personal preferences. Now I'll pick some algorithms according to my understandings.\n",
      "\n",
      "1\n",
      "422\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-26 20:39\n",
      "I understand your motivation, and it would have been better if we could have had a broader discussion previously on the mailing list. Unfortunately, mostly because of lack of time of the developers, that didn't happen.\n",
      "For your proposal to be strong, you do need to say more than you currently say, though.\n",
      "\n",
      "2\n",
      "423\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-27 14:14\n",
      "@ogrisel right, I forgot to fix the menu. Is there a good way to upload just the html so you don't have to optimize the pngs again?\n",
      "\n",
      "1\n",
      "424\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-27 14:32\n",
      "I was about to find a way to not optimize the png again :)\n",
      "I tested on OSX, now on testing on windows and the sdist from linux in a docker container in //\n",
      "\n",
      "2\n",
      "425\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-27 14:37\n",
      "Well I can rebuild the docs in the folder with the optimized png once I get into the office\n",
      "if you want to wait for an hour\n",
      "\n",
      "2\n",
      "426\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-27 14:42\n",
      "I already fetched them local with rsync\n",
      "actually not locally, to some rackspace VM but this is the same\n",
      "\n",
      "4\n",
      "427\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-27 14:52\n",
      "do you need help with anything? Otherwise I might turn around and sleep on\n",
      "\n",
      "2\n",
      "428\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-27 14:54\n",
      "let me know when you tweet so I can retweet ;)\n",
      "\n",
      "1\n",
      "429\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-27 14:54\n",
      "I have pbm testing under windows as apparently the latest scipy from christoph gohlke under Python 3.4 is broken.\n",
      "will try on python 2.7 instead\n",
      "ok\n",
      "for the GSoC do you think we need to do anything else in the short term? We can still enable the edit mode on the GSoC proposals that students on a case by case basis for 2 weeks after today's deadline if we need to.\n",
      "yes\n",
      "I'll put some feedback on the semi-supervised proposal (directly as comment on melange)\n",
      "thanks\n",
      "can you do it?\n",
      "I don't remember by nickserv passwd\n",
      "so I cannot do op stuff anymore\n",
      "\n",
      "10\n",
      "430\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-27 14:58\n",
      "ok. well it would be good to reread the final versions. I did that only with like two of them. But we can do that later\n",
      "\n",
      "1\n",
      "431\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-03-27 15:05\n",
      "@ogrisel I'm still in the process of adding some stuff. Will be finished in 40 minutes.\n",
      "\n",
      "2\n",
      "432\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-27 15:09\n",
      "My proposal in the google-melange has unnecessary html marks, like underscores. I think it comes from google-melange, since I did not add them at all. I also submitted a dropbox link to a PDF version, just in case.\n",
      "\n",
      "1\n",
      "433\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-27 15:10\n",
      "@xuewei4d Can you try to clean up the formatting?\n",
      "\n",
      "2\n",
      "434\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-27 15:17\n",
      "OK. Now it's clean. Copy/paste the html source code instead of paste into the melange own editor\n",
      "\n",
      "2\n",
      "435\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-27 16:22\n",
      "@amueller let's the tweeting storm start!\n",
      "\n",
      "1\n",
      "436\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-27 16:23\n",
      "Yayy 0.16 :beers:\n",
      "\n",
      "5\n",
      "437\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-27 16:29\n",
      ":beers:\n",
      "\n",
      "1\n",
      "438\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-27 16:31\n",
      "It says I need to be channel op to do that... How do I become one?\n",
      "\n",
      "8\n",
      "439\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-27 16:35\n",
      "@NelleV is around I'll ask her :) (her right? :p)\n",
      "\n",
      "1\n",
      "440\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-27 16:37\n",
      "`scikit-learn 0.16 is out! http://scikit-learn.org/0.16/whats_new.html... Try out the same with \"pip install scikit-learn==0.16\"` Would be good?\n",
      "\n",
      "5\n",
      "441\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-03-27 16:50\n",
      ":beers:\n",
      "\n",
      "1\n",
      "442\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-27 20:23\n",
      "I think I'm the only IRC op ;)\n",
      "\n",
      "1\n",
      "443\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-27 20:31\n",
      "@ogrisel the oreilly people asked for the slides for the webcast on Monday night ^^ we should probably talk about the content\n",
      "\n",
      "3\n",
      "444\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-27 20:36\n",
      "I am generic intro slides in there: https://speakerdeck.com/ogrisel/machine-learning-in-python-with-scikit-learn-1\n",
      "but they overlap a lot with the slides I presented for the 0.15 release.\n",
      "and do not cover the new stuff\n",
      "\n",
      "3\n",
      "445\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-27 20:37\n",
      "I can work on sunday on new slides to present stuff like LSH Forest, DBSCAN and / or Birch\n",
      "we can use google docs to work collaboratively on a new deck\n",
      "those slides are in keynote, but feel free to do as many screen grabs as you want\n",
      "I think we should prepare a slide on t-SNE too\n",
      "Hope you will get better\n",
      "tomorrow is going to get complicated for me. But sunday I should be able to work on it for some reasonable amount of time.\n",
      "\n",
      "6\n",
      "446\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-27 20:40\n",
      "I feel super sick, I hope I'll be well enough tomorrow to work on new slides. But I also have to work on slides for a talk I give on Monday :-/\n",
      "google docs seems like a good idea\n",
      "\n",
      "2\n",
      "447\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-27 20:45\n",
      "I'll try to make some slides on T-SNE, LDA and the GP, ok? We also need slide for Birch, the logistic path algorithm (?) and calibartion\n",
      "do we want to talk about MLPs and do you already have slides for that?\n",
      "\n",
      "7\n",
      "448\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-27 20:52\n",
      "a bit yes\n",
      "ok take care\n",
      "\n",
      "2\n",
      "449\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-03-28 02:13\n",
      "Thanks for all the work on releasing 0.16 guys! It is much appreciated! :beers:\n",
      "\n",
      "1\n",
      "450\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-28 21:13\n",
      "@ogrisel Any reviews for #4362? :)\n",
      "\n",
      "1\n",
      "451\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-28 21:29\n",
      "There is an unrelated [test failure](https://travis-ci.org/scikit-learn/scikit-learn/jobs/56248127) in Python 3.4 `:/ `\n",
      "\n",
      "1\n",
      "452\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-28 21:40\n",
      "[Using code formatted smiley in gitter looks good `:D`]\n",
      "\n",
      "1\n",
      "453\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-28 21:45\n",
      "@xuewei4d You could also use commit headers like `ENH` `FIX` `TST` `COSMIT` `MAINT` etc... to describe your commits `:)` just a minor suggestion... feel free to ignore `:)`\n",
      "\n",
      "1\n",
      "454\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-28 21:49\n",
      "Sure.\n",
      "\n",
      "1\n",
      "455\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-03-28 21:50\n",
      "COSMIT means cosmetic?\n",
      "\n",
      "1\n",
      "456\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-28 21:54\n",
      "Yeah `:)`\n",
      "\n",
      "1\n",
      "457\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-28 22:24\n",
      "I think @ogrisel is busy today and I'm sick and need to make slides :-/\n",
      "I re-triggered travis for you though\n",
      "\n",
      "2\n",
      "458\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-03-28 23:03\n",
      "Thanks! and take care `:)`\n",
      "\n",
      "1\n",
      "459\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-29 15:51\n",
      "@ogrisel are you around?\n",
      "\n",
      "1\n",
      "460\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-29 15:52\n",
      "yes\n",
      "are you feeling better?\n",
      "\n",
      "2\n",
      "461\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-03-29 15:53\n",
      "yeah, somewhat. So do we do only slides, no notebooks?\n",
      "\n",
      "3\n",
      "462\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-29 15:55\n",
      "actually it's possible to have a screen share for live demo but it needs flash and or java\n",
      "I would not count on it too much\n",
      "we can use google doc and export the slides as PDF\n",
      "I think\n",
      "let's try\n",
      "I sent you an invite\n",
      "\n",
      "8\n",
      "463\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-30 23:12\n",
      "@amueller I think I am ok on my side for the slides.\n",
      "\n",
      "1\n",
      "464\n",
      "541a528b163965c9bc2053de\n",
      "2015-03-30 23:35\n",
      "@amueller I let you send the slides to yasmina and ben once you are happy with them\n",
      "\n",
      "1\n",
      "465\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-04-01 12:32\n",
      "@ogrisel around?\n",
      "\n",
      "1\n",
      "466\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-01 12:42\n",
      "@ragv yes\n",
      "\n",
      "1\n",
      "467\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-01 12:46\n",
      "> If you don't think it is horrible, I think I'll leave it as is  @amueller I only see a single slide with bullet bullet points for incremental PCA. Is it this slide you are talking about or do you have a figure for IncrementalPCA?\n",
      "\n",
      "1\n",
      "468\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-01 14:05\n",
      "@ogrisel I don't have a figure for IncrementalPCA. I couldn't produce a good one\n",
      "I could have added a code block showing partial fit, but I felt that added little\n",
      "\n",
      "2\n",
      "469\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-01 14:23\n",
      "no pbm\n",
      "I think it's fine like this\n",
      "there is a lot of material to cover already\n",
      "\n",
      "4\n",
      "470\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-04-01 16:44\n",
      "@ogrisel I saw you online and thought of pestering you to review #3907 `;) :p`\n",
      "\n",
      "1\n",
      "471\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-01 19:27\n",
      "@ragv  I am too tired to do it now, I put that on my todo list for tomorrow morning. Don't hesitate to ping me again if I fail to deliver :)\n",
      "\n",
      "1\n",
      "472\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-04-01 23:01\n",
      "Thanks :D > Don't hesitate to ping me again if I fail to deliver  don't worry about that `:p`\n",
      "\n",
      "1\n",
      "473\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-02 14:46\n",
      "sprinters here?\n",
      "\n",
      "1\n",
      "474\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-02 14:49\n",
      "I feel excluded :-/\n",
      "\n",
      "1\n",
      "475\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-02 15:23\n",
      "hi @amueller sorry I was in the metro to get back home for the webcast\n",
      "there are still people at the sprint but I think nobody is on gitter\n",
      "maybe @NelleV is on IRC\n",
      "\n",
      "3\n",
      "476\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-02 15:28\n",
      "@amueller the call starts in 3 mins right? I wanted to make sure that I am not off by one hour because of the summer time stuff.\n",
      "\n",
      "1\n",
      "477\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-02 15:34\n",
      "@ogrisel you are on time :)\n",
      "at least the same time I think it is\n",
      "\n",
      "2\n",
      "478\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-02 15:35\n",
      "I will call with my phone\n",
      "\n",
      "1\n",
      "479\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-02 17:12\n",
      "good!\n",
      ":)\n",
      "\n",
      "2\n",
      "480\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-04-02 18:07\n",
      "sprinter? Is there any event recently? So many new PRs ...\n",
      "\n",
      "1\n",
      "481\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-02 18:17\n",
      "yes there was a short one day sprint in Paris today (it should be over now). There was ~15 people in the room when I was there earlier this afternoon.\n",
      "\n",
      "1\n",
      "482\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-04-02 18:19\n",
      "Glad to see these new contributors. :)\n",
      "\n",
      "1\n",
      "483\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-04-02 18:54\n",
      "Wow looking forward to  participating in one!!! :D :D\n",
      "\n",
      "1\n",
      "484\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-02 22:12\n",
      "ragv you'll be at the epi-center of sprints soon, I heard :)\n",
      "I home I can come to paris for the next one, will be awesome!\n",
      "\n",
      "2\n",
      "485\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-04-03 16:52\n",
      "Yeaa :D Thats awesome... I'm really looking forward to that :)\n",
      "\n",
      "1\n",
      "486\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-03 17:38\n",
      "this looks pretty bad: https://github.com/scikit-learn/scikit-learn/pull/4507\n",
      "how about we do a bug-fix release when this is done with the fix to isotonic, my CCA fix, and this?\n",
      "\n",
      "2\n",
      "487\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-04 18:12\n",
      "@ogrisel where was your mlp notebook again?\n",
      "\n",
      "1\n",
      "488\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-04 18:22\n",
      "@ogrisel it looks like the current partial_fit goes over the data multiple times. That seems odd.\n",
      "\n",
      "1\n",
      "489\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-06 12:19\n",
      "+1 for a 0.16.1 release with such bad bug fixes.\n",
      "I created the milestone. However I have to work on my talks for PyCon now. Maybe I can help for the 0.16.1 release during the sprints next week.\n",
      "\n",
      "2\n",
      "490\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-06 13:41\n",
      "The k-means fortran layout bug might need another fix and we should check if the bug appears elsewhere, too....\n",
      "\n",
      "1\n",
      "491\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-06 13:49\n",
      "yes we could do a new common test for that, leveraging @ragv's `assert_same_model` utility.\n",
      "\n",
      "1\n",
      "492\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-06 14:00\n",
      "or we could just use ``fit_predict``....\n",
      "Ok, I'll try to use his branch and see if that works...\n",
      "\n",
      "2\n",
      "493\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-06 14:02\n",
      "`fit_predict` or `fit_transform` would do as well.\n",
      "BTW, I had forgotten to upload the 0.16.0 artifacts to sourceforge, this is fixed.\n",
      "\n",
      "6\n",
      "494\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-06 14:35\n",
      "I think Gael is offline till tomorrow.\n",
      "I agree.\n",
      "\n",
      "2\n",
      "495\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-06 15:24\n",
      "ok\n",
      "\n",
      "1\n",
      "496\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-06 15:25\n",
      "ok back to my PyCon preparation\n",
      "\n",
      "1\n",
      "497\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-06 16:00\n",
      "@ogrisel I cherry picked some stuff in the 0.16.X branch, I'll work on the kmeans and then I think this should be good to go.\n",
      "The isotonic is a major concern for me as we introduced something that seems to be pretty broken.\n",
      "fixed\n",
      "should have been #4535\n",
      "I'd like to think that #4535 wasn't as bad before I introduced ``check_array`` and that I broke it and that scikit-learn wasn't always i that state...\n",
      "ok I was just being silly. There are no breakages with fortran ordering, at least not with default settings.\n",
      "\n",
      "6\n",
      "498\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-06 21:37\n",
      "@amueller in your latest comment on #3907, you link to #3907 itself.\n",
      "\n",
      "1\n",
      "499\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-06 23:11\n",
      "the k-means fix in #4531 seems good....\n",
      "\n",
      "1\n",
      "500\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-04-07 14:50\n",
      "I don't quite understand #4507 and #4531. What's the problem there?\n",
      "\n",
      "1\n",
      "501\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-07 16:04\n",
      "The problem is that the results on fortran-ordered data are garbage with precompute_distances=False\n",
      "results should be independent of the memory layout of the data.\n",
      "If you run his code, the cluster-assignments are [0, 0, 1] which is clearly garbage, as the two last points are identical\n",
      "favourite bug of the day: a pandas dataframe with a \"dtype\" column\n",
      "\n",
      "4\n",
      "502\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-04-07 17:29\n",
      "I see. Thanks!\n",
      "\n",
      "1\n",
      "503\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-07 21:35\n",
      "haha\n",
      "\n",
      "1\n",
      "504\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-04-07 22:31\n",
      "@amueller Just saw your comment in Melange.\n",
      "\n",
      "1\n",
      "505\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-04-07 22:38\n",
      "I did some investigation several days ago. Zhu has a nice tutorial giving introduction on how to implement S3VMs:  http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf\n",
      "It introduces 5 implementations, of which SVM^light has corresponding implementation in C: http://svmlight.joachims.org/ . We may write a wrapper for it.\n",
      "\n",
      "2\n",
      "506\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-04-07 23:11\n",
      "I can also consult the author of libSVM, Professor Lin, to get some suggestions.\n",
      "\n",
      "1\n",
      "507\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-08 17:50\n",
      "Don't worry about asking the libSVM author. We can't use SVM^light because of licensing issues, also we don't want to wrap more c code.\n",
      "@bryandeng sorry for the late reply. Don't worry too much about it, I think your proposal is quite good already.\n",
      "\n",
      "2\n",
      "508\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-08 18:02\n",
      "A review on #4541 would be cool, as that might be an additional nice fix for 0.16.1\n",
      "\n",
      "1\n",
      "509\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-08 18:03\n",
      "[not the weird .dtype column part, the part where having dataframes with dtype object inside doesn't get correctly cast]\n",
      "\n",
      "1\n",
      "510\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-04-08 18:03\n",
      "@amueller I'm glad that you like it. And sorry for being inactive on Github these days. I'm writing a term paper (Hausarbeit) and playing with deep neural networks. I'll come back to scikit-learn this week. It's still during semester break so I have pretty much time.\n",
      "\n",
      "1\n",
      "511\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-08 18:17\n",
      "cool :) yeah being a bit more active would be great.\n",
      "You are welcome to help us with the MLP if you like ;)\n",
      "does anyone know how to disable the comments by coverall?\n",
      "\n",
      "3\n",
      "512\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-04-08 18:51\n",
      "[![Untitled.png](https://files.gitter.im/scikit-learn/scikit-learn/hYei/thumb/Untitled.png)](https://files.gitter.im/scikit-learn/scikit-learn/hYei/Untitled.png)\n",
      "should show up at the bottom of https://coveralls.io/r/scikit-learn/scikit-learn for the owner(s)\n",
      "\n",
      "2\n",
      "513\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-04-08 19:04\n",
      "assuming you mean PR bot comments @amueller ... Otherwise check the notifications tab to turn off emails, etc. which i think is personalized per user.. https://coveralls.io/r/scikit-learn/scikit-learn/notifications/email\n",
      "\n",
      "1\n",
      "514\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-08 20:02\n",
      "I don't have that showing up at the bottom, maybe because I'm not an owner. Which is weird since I'm a repo owner\n",
      "yeah I logged in\n",
      "\n",
      "2\n",
      "515\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-04-08 20:34\n",
      "That's strange. Are you logged into coveralls? It uses your GH login but I don't think it automatically logs you in\n",
      "\n",
      "1\n",
      "516\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-04-08 21:47\n",
      "maybe this? https://github.com/lemurheavy/coveralls-public/issues/199\n",
      "\n",
      "1\n",
      "517\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-04-08 21:54\n",
      "ie visit https://coveralls.io/refresh?private=true ...?\n",
      "\n",
      "1\n",
      "518\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-04-08 21:56\n",
      "or this if you dont want coveralls to know about private repos perhaps: https://developer.github.com/v3/orgs/members/#publicize-a-users-membership\n",
      "though im pretty sure you're a public owner of skl. \\\n",
      "\n",
      "2\n",
      "519\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-04-10 20:40\n",
      "Won't this fail with an attribute error if there are no empty clusters? https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cluster/_k_means.pyx#L289\n",
      "\n",
      "2\n",
      "520\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-10 21:55\n",
      "@omerzimp the loops goes over the empty clusters. If there are none, it never gets executed\n",
      "\n",
      "1\n",
      "521\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-04-11 09:13\n",
      "@amueller I noticed it a while after I sent that message\n",
      "Why not just nest it in the if?\n",
      "\n",
      "2\n",
      "522\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-04-11 09:23\n",
      "@amueller Here's a PR that does just that https://github.com/scikit-learn/scikit-learn/pull/4570\n",
      "Also am I missing something in https://github.com/scikit-learn/scikit-learn/pull/4568 ?\n",
      "\n",
      "2\n",
      "523\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-04-11 10:01\n",
      "How good is the test coverage?\n",
      "I'm not sure if I broke something or not\n",
      "\n",
      "2\n",
      "524\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-13 15:40\n",
      "@ogrisel when do the sprints start? Any specific plans?\n",
      "@omerzimp #4568 supposed to be a speed improvement? issparse is just an instance check, so pretty neglible compared to everything else.\n",
      "\n",
      "2\n",
      "525\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-04-13 15:44\n",
      "Yeh but it's something :P\n",
      "I contribute what I can\n",
      "\n",
      "2\n",
      "526\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-13 15:45\n",
      "I would call this premature / overoptimizing. It makes the code a bit longer and a bit harder to read, but doesn't improve anything really.\n",
      "\n",
      "2\n",
      "527\n",
      "5385f2fe048862e761fa2d40\n",
      "2015-04-13 15:50\n",
      "How neglegable is it when you have a lot of data points?\n",
      "\n",
      "1\n",
      "528\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-13 17:06\n",
      "Hi @amueller, sorry I was busy chatting in the real life, I had not checked the gitter room yet\n",
      "So the sprints have just started today, we are 4 people at the sklearn table at the moment.\n",
      "We are digging down easy fix issues for first time contributors\n",
      "\n",
      "3\n",
      "529\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-13 17:13\n",
      "> I contribute what I can  But you have to keep in mind that the goal of the project is to stay maintainable and therefore code simplicity is an asset. We need to find the right trade-offs between simplicity and performance.\n",
      "\n",
      "1\n",
      "530\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-13 19:20\n",
      "@ogrisel  ok I'll keep an eye on the tracker.\n",
      "@omerzimp the check is independent of the dataset size\n",
      "\n",
      "2\n",
      "531\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-13 20:17\n",
      "@ogrisel btw if anyone at the sprint wants a brain-teaser, I recommend this one: https://github.com/scikit-learn/scikit-learn/pull/4435\n",
      "no machine learning required ^^\n",
      "\n",
      "2\n",
      "532\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-14 15:33\n",
      "I think we should do 0.16.1 soon. This is the only one I'd like to get in that hasn't been merged: https://github.com/scikit-learn/scikit-learn/pull/4541\n",
      "ok just got merged. I'll need to do two backports and rewrite whatsnew, then I think we are in good shape.\n",
      "I'm just doing it\n",
      "\n",
      "4\n",
      "533\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 15:46\n",
      "I agree. Do you want me to backport 4541\n",
      "?\n",
      "\n",
      "2\n",
      "534\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-14 15:51\n",
      "should I also backport your astype fix?\n",
      "\n",
      "1\n",
      "535\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-14 16:02\n",
      "@ogrisel astype fix backport yes / no? otherwise tagging now\n",
      "alright :)\n",
      "\n",
      "2\n",
      "536\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 16:02\n",
      "I don't think it fixes any user bug in itself.\n",
      "Let's tag\n",
      "\n",
      "2\n",
      "537\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-14 16:05\n",
      "gah, forgot to add an entry to the \"news\" on the website...\n",
      "\n",
      "2\n",
      "538\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 16:06\n",
      "appveyor will have to do another 1h build :)\n",
      "\n",
      "3\n",
      "539\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-14 16:15\n",
      "macpython should be building\n",
      "\n",
      "1\n",
      "540\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 16:20\n",
      "great\n",
      "\n",
      "1\n",
      "541\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-14 17:00\n",
      "is it possible to unqueue on appveyor? it'll take 8 hours at least to run through the queue\n",
      "\n",
      "9\n",
      "542\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 17:08\n",
      "otherwise you get the next ancient job in state running, and canceling running jobs takes more time than queued jobs\n",
      "BTW, if you are interested in all of this windows build I gave a talk at pycon on that: https://twitter.com/ogrisel/status/587326055171694594\n",
      "off to lunch, see you later! It's great to have this release out this week. Thanks so much!\n",
      "\n",
      "3\n",
      "543\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 18:18\n",
      "@amueller the windows build is up: http://windows-wheels.scikit-learn.org/\n",
      "the OSX wheels are up as well: http://wheels.scipy.org/\n",
      "Do you want me to do the upload to PyPI and sourceforge or do you want to do it?\n",
      "\n",
      "3\n",
      "544\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-14 19:57\n",
      "The docs are up. Can you do pypi upload please?\n",
      "how do you upload the wheels btw?\n",
      "or maybe you do it ;) I still get the \"unknown url type error\"\n",
      "\n",
      "3\n",
      "545\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 19:58\n",
      "you can either use upload_all or the twine command\n",
      "\n",
      "5\n",
      "546\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 20:04\n",
      "it works with python 2.7 on a new virtualenv were I just pip installed wheelhouse-uploader\n",
      "\n",
      "11\n",
      "547\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-14 20:17\n",
      "done\n",
      "ah, twine makes sense ^^\n",
      "\n",
      "2\n",
      "548\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 20:19\n",
      "ok\n",
      "\n",
      "1\n",
      "549\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-14 21:10\n",
      "@amueller is your LGTM still valid in light of the latest changes to #4590?\n",
      "I uploaded the release to sourceforge as well\n",
      "\n",
      "2\n",
      "550\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-15 01:51\n",
      "thanks\n",
      "\n",
      "1\n",
      "551\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-19 17:57\n",
      "@agramfort https://github.com/scikit-learn/scikit-learn/pull/4550\n",
      "@agramfort this one would be cool: https://github.com/scikit-learn/scikit-learn/pull/4534\n",
      "https://github.com/scikit-learn/scikit-learn/pull/4526\n",
      "https://github.com/scikit-learn/scikit-learn/pull/4467\n",
      "https://github.com/scikit-learn/scikit-learn/pull/4365\n",
      "\n",
      "5\n",
      "552\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-19 18:44\n",
      "https://github.com/scikit-learn/scikit-learn/issues/3855\n",
      "that last one is about changing the number of samples\n",
      "\n",
      "2\n",
      "553\n",
      "5537027215522ed4b3df56ab\n",
      "2015-04-22 02:09\n",
      "Hey guys, I know this is slightly off topic, but is there any recommendation regarding structured/sequence learning libraries in Python?\n",
      "I have been using crfsuite but unforunately it's only first order and not very good for named entity recognition\n",
      "The Stanford NER Java lib is crazy huge/unwieldy -_-\n",
      "nm looks like @amueller has ChainCRF in PyStruct! I hope it's fast. Please be fast.\n",
      "\n",
      "4\n",
      "554\n",
      "5537027215522ed4b3df56ab\n",
      "2015-04-22 02:30\n",
      "even has great docs =)\n",
      "\n",
      "1\n",
      "555\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-22 07:04\n",
      "PyStruct and seqlearn are two projects maintained by scikit-learn developers (@amueller and @larsmans respectively).\n",
      "\n",
      "1\n",
      "556\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-22 14:30\n",
      "@ldqc the docs are new and work in progress ;) pystruct implements maximum margin learning and perceptron, seqlearn implements HMMs and perceptron,  I think. So both don't implement maximum likelihood CRFs\n",
      "\n",
      "1\n",
      "557\n",
      "5537027215522ed4b3df56ab\n",
      "2015-04-23 03:28\n",
      "looks like i can't use sparse matrices for yours atm\n",
      "\n",
      "1\n",
      "558\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-23 17:33\n",
      "@lqdc unfortunately yes. hopefully soon(ish)\n",
      "\n",
      "1\n",
      "559\n",
      "544906e2db8155e6700cdd16\n",
      "2015-04-24 01:45\n",
      "Hello guys! I'm having problems with a django application that uses a random forest classifier to classify items. The error that I'm receiving says: [ 'Thread' object has no attribute '_children' ] and googling it leads to http://stackoverflow.com/questions/9749875/strange-error-while-starting-threads-inside-django-application On the other hand, the exact line where the application throws the error is \"clf.predict_proba(items)\" and I'm not using threads at all but I set n_jobs=-1 in the initialization of the classifier so I wonder if this error could be related to joblib (it uses ThreadPool in the method \"  call  \" of the class Parallel). Any idea? Can I modify the n_jobs variable in order avoid parallelism?\n",
      "Sorry! I wasn't able to find the backticks in my spanish keyboard :/\n",
      "\n",
      "2\n",
      "560\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-24 08:52\n",
      "@mac2bua please create a new stackoverflow question with a minimalistic reproduction script that generates its own test data.\n",
      "\n",
      "1\n",
      "561\n",
      "544906e2db8155e6700cdd16\n",
      "2015-04-24 16:27\n",
      "Done! http://stackoverflow.com/questions/29852680/thread-object-has-no-attribute-children-django-scikit-learn\n",
      "\n",
      "1\n",
      "562\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-24 16:52\n",
      "answered. Please include the full traceback to make it possible for us to fix the real cause of the problem.\n",
      "\n",
      "1\n",
      "563\n",
      "544906e2db8155e6700cdd16\n",
      "2015-04-24 17:11\n",
      "Thanks @ogrisel the error seems to be fixed after I set the n_jobs parameter to 1.\n",
      "Here is the complete traceback: ``` File \"/home/cristian/env/local/lib/python2.7/site-packages/django/core/handlers/base.py\" in get_response line 111.                     response = wrapped_callback(request, *callback_args, **callback_kwargs) File \"/home/cristian/env/local/lib/python2.7/site-packages/django/views/decorators/csrf.py\" in wrapped_view line 57.         return view_func(*args, **kwargs) File \"/home/cristian/env/local/lib/python2.7/site-packages/django/views/generic/base.py\" in view line 69.             return self.dispatch(request, *args, **kwargs) File \"/home/cristian/env/local/lib/python2.7/site-packages/rest_framework/views.py\" in dispatch line 452.             response = self.handle_exception(exc) File \"/home/cristian/env/local/lib/python2.7/site-packages/rest_framework/views.py\" in dispatch line 449.             response = handler(request, *args, **kwargs) File \"/home/cristian/env/local/lib/python2.7/site-packages/rest_framework/decorators.py\" in handler line 50.             return func(*args, **kwargs) File \"/home/cristian/filters/classifiers/views.py\" in classify_item line 70. \t\t\ty_pred = clf.predict_proba(pd.DataFrame(item_dict)) File \"/home/cristian/env/local/lib/python2.7/site-packages/sklearn/pipeline.py\" in predict_proba line 159.         return self.steps[-1][-1].predict_proba(Xt) File \"/home/cristian/env/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py\" in predict_proba line 468.             for i in range(n_jobs)) File \"/home/cristian/env/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py\" in __call__ line 568.             self._pool = ThreadPool(n_jobs) File \"/usr/lib/python2.7/multiprocessing/pool.py\" in __init__ line 685.         Pool.__init__(self, processes, initializer, initargs) File \"/usr/lib/python2.7/multiprocessing/pool.py\" in __init__ line 136.         self._repopulate_pool() File \"/usr/lib/python2.7/multiprocessing/pool.py\" in _repopulate_pool line 199.             w.start() File \"/usr/lib/python2.7/multiprocessing/dummy/__init__.py\" in start line 73.         self._parent._children[self] = None  Exception Type: AttributeError at /items/ Exception Value: 'Thread' object has no attribute '_children' ```\n",
      "\n",
      "4\n",
      "564\n",
      "544906e2db8155e6700cdd16\n",
      "2015-04-24 17:47\n",
      "I've just added the traceback to stackoverflow!\n",
      "\n",
      "1\n",
      "565\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-04-25 19:31\n",
      "finalllllly all my exams got over :D been inactive for too long :/ full time scikit-learn from today B)\n",
      "\n",
      "1\n",
      "566\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-25 21:24\n",
      "sweet :)\n",
      "\n",
      "1\n",
      "567\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-27 10:57\n",
      "welcome back @ragv, unfortunately myself I will be busy for the next couple of weeks. (strata in london / then offline for vacation / the moving to a new flat).\n",
      "\n",
      "1\n",
      "568\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-04-27 10:57\n",
      "sure :) no problem :) I'll take care of all the pending works for now :))\n",
      "and I do have a lot of pending stuff :O\n",
      "\n",
      "2\n",
      "569\n",
      "5474d9eadb8155e6700d8178\n",
      "2015-04-27 19:28\n",
      "heyyy I got in GSoC ;) Thanks a lot @amueller @ogrisel @jnothman and @MechCoder   :) Congrats to @xeuwei4d and @barmaley-exe too :)\n",
      "\n",
      "1\n",
      "570\n",
      "553e8e1015522ed4b3df97f7\n",
      "2015-04-27 19:31\n",
      "Congrats @ragv!\n",
      "\n",
      "2\n",
      "571\n",
      "553e8e1015522ed4b3df97f7\n",
      "2015-04-27 19:32\n",
      "and to @xuewei4d and @Barmaley-exe!\n",
      "\n",
      "1\n",
      "572\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-27 19:50\n",
      "hurray :)\n",
      "\n",
      "1\n",
      "573\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-04-27 20:40\n",
      "Thanks! It will be a great summer!\n",
      "\n",
      "1\n",
      "574\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-27 22:19\n",
      "@ogrisel can I get a +1 on #4371?\n",
      "\n",
      "1\n",
      "575\n",
      "551084ad15522ed4b3ddb3b0\n",
      "2015-04-28 04:48\n",
      "Greetings! I've done a pure Python implementation of the k-modes algorithm for clustering categorical data: https://github.com/nicodv/kmodes\n",
      "It's modeled after the k-means code in sklearn. Any interest in this?\n",
      "\n",
      "2\n",
      "576\n",
      "551061f615522ed4b3ddb1c0\n",
      "2015-04-28 12:59\n",
      "Congratulations! @ragv @xuewei4d @Barmaley-exe\n",
      "Though not accepted into GSoC :worried: , I'll continue working on the tasks proposed.\n",
      "\n",
      "2\n",
      "577\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-28 15:57\n",
      "great, thanks :)\n",
      "\n",
      "1\n",
      "578\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-28 15:58\n",
      "I'm sorry you didn't make it, we didn't have enough mentors for all the projects.\n",
      "\n",
      "1\n",
      "579\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-28 22:07\n",
      "@nicodv we could link to your code in related projects, but I think we decided against including kmodes in the past because it was very slow. is that not the case? I forgot the details.\n",
      "\n",
      "1\n",
      "580\n",
      "551084ad15522ed4b3ddb3b0\n",
      "2015-04-28 23:35\n",
      "@amueller In pure Python, it's almost exactly an order of magnitude slower than k-means. But it scales roughly the same as k-means in terms of number of points (but worse in terms of n_clusters and dimensions)\n",
      "\n",
      "1\n",
      "581\n",
      "551084ad15522ed4b3ddb3b0\n",
      "2015-04-28 23:50\n",
      "It's less that an order of magnitude, actually. Let's say 5 to 10 times slower.\n",
      "\n",
      "1\n",
      "582\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-29 00:46\n",
      "wait, I think i confused it with k-mediod. Sorry, never mind. medians is just the l1 variant. that should be pretty easy to add to the current code, I think.\n",
      "\n",
      "1\n",
      "583\n",
      "541a528b163965c9bc2053de\n",
      "2015-04-29 08:07\n",
      "how do you compute the ndim medians in l1 space?\n",
      "\n",
      "1\n",
      "584\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-29 18:38\n",
      "per-coordinate median, right?\n",
      "argh, ok, got double confused, then.\n",
      "\n",
      "2\n",
      "585\n",
      "551084ad15522ed4b3ddb3b0\n",
      "2015-04-29 18:38\n",
      "from k-medians Wikipedia: \"The median is computed in each single dimension in the Manhattan-distance formulation of the k-medians problem\"\n",
      "but k-modes, my code, does not use Euclidean space because it clusters categorical variables\n",
      "so, quite different from k-medians or k-medoids\n",
      "there's k-modes, for which all is assumed categorical; and there's k-prototypes (this combines k-modes and k-means), which receives X as a list of 2 arrays, one for numerical and one for categorical variables\n",
      "\n",
      "4\n",
      "586\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-29 18:41\n",
      "Feel free to sent a PR to include it as related project. We don't really have anything for categorical variables at the moment, and we haven't really figured out the API. How do you denote which variables are categorical and what are the inputs? Or are just all features assumed to be categorical?\n",
      "\n",
      "1\n",
      "587\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-29 18:45\n",
      "ok. So this list would mess with the sklearn api a lot, for cross-validation tries to sample along the first axis.\n",
      "\n",
      "2\n",
      "588\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-29 18:50\n",
      "yeah, that is what I think we would like to do for the forests, but I'm not sure. It is a bit awkward that they are float then, but not a big deal I guess\n",
      "using pandas dataframes would also be an option, maybe not in scikit-learn though.\n",
      "\n",
      "2\n",
      "589\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-30 16:15\n",
      "anyone a quick +1 for #4526 ? should be an easy fix\n",
      "\n",
      "1\n",
      "590\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-04-30 22:08\n",
      "I'm always complaining about the many github notifications, and I just sent 74 for #3306 ^^ I'm glad I don't get them in my inbox\n",
      "\n",
      "1\n",
      "591\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-01 02:41\n",
      "Hey @amueller ... Just a FYI, ran your check_estimator API on my package. It works for the most part. Though I raise a copied version of the NotFittedError and get a fail since it isn't explicitly using the scikit-learn version (I wanted to support 0.15.2 as well so exported a bunch of functions to reside in gplearn.skutils.etc).\n",
      "\n",
      "1\n",
      "592\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-01 16:29\n",
      "Thanks for the feedback. Is there a reason your NotFittedError doesn't inherit from the sklearn one? Well I guess actually we wanted people to be able to provide compatible code without needing to rely on sklearn.... hum...\n",
      "@trevorstephens maybe I'm slow, how is that related to the remark in the parentheses?\n",
      "\n",
      "2\n",
      "593\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-01 16:48\n",
      "NotFittedError was new in 0.16.0 I think, so I'd fail tests on my own Travis builds based on 0.15.2 if I tried to inherit from sklearn I guess\n",
      "And to guard against other changes to the non-public API, I grabbed a few key utils modules from 0.16.0 and stuffed them into a folder in my project\n",
      "I could potentially wrap all the important stuff in try blocks, like the fixes module in sklearn. But that seems like a lot of work given how interconnected some of the utils are\n",
      "\n",
      "3\n",
      "594\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-01 17:14\n",
      "yeah no, don't do that. Actually we should aim at allowing people to pass tests without inheriting from sklearn.\n",
      "\n",
      "1\n",
      "595\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-01 17:29\n",
      "@trevorstephens gael and I think we shouldn't check for NotFittedError, but a public one. I'm not sure if ValueError or AttributeError\n",
      "yeah, probably.\n",
      "\n",
      "2\n",
      "596\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-01 17:32\n",
      "that'd work. mine is a direct copy of the scikitlearn error. probably other's are using older code bases where I think a ValueError was directly raised when not fitted\n",
      "actually maybe not... https://github.com/scikit-learn/scikit-learn/pull/4029/files shows a mix of both were in the code base. perhaps check for either of them. i think checking from both is overkill and makes an unnecessary burden on people who want to work with sklearn, but not require it.\n",
      "\n",
      "2\n",
      "597\n",
      "54df2ad815522ed4b3dc0295\n",
      "2015-05-02 00:35\n",
      "Hi! Any tips regarding GradientBoostingClassifier vs. class imbalance?  Contrary to e.g. LogisticRegression, there is no class_weight parameter.  Without doing anything, I get high precision but tiny recall.\n",
      "I guess I could just pre-process the dataset manually (adding copies of the less represented class) but that feels dirty.  Surely there's a better way?\n",
      "\n",
      "2\n",
      "598\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-02 02:02\n",
      "@pasky , you could use the function `compute_sample_weight` found at https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/class_weight.py#L68-L166 like so:  from sklearn.utils.class_weight import compute_sample_weight # \"auto\" class weights inversely proportional to class frequencies sample_weight = compute_sample_weight(\"auto\", y) # dict to define them yourself with {class_symbol: weight} sample_weight = compute_sample_weight({1: 2, 2: 1}, y) ... then just feed that as sample_weight to the fit method\n",
      "geeze. apparently python comments are taken to mean bold... but you get the idea :smile:\n",
      "it's a private util function, so not really advertised. its used under the hood in 0.16.0's random forest and decision tree classifiers.\n",
      "\n",
      "3\n",
      "599\n",
      "54df2ad815522ed4b3dc0295\n",
      "2015-05-02 02:23\n",
      "Oh, that sounds pretty nice! Thanks, I'll try that out. I completely forgot about the option to also pass the sample weights to fit() method instead of specifying class weights in constructor.\n",
      "\n",
      "1\n",
      "600\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-02 02:40\n",
      "Yep, it's kind of sort of an indirect method to oversample/undersample.\n",
      "\n",
      "1\n",
      "601\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-03 03:19\n",
      "we should probably add class_weights to the gradient boosting classifier\n",
      "\n",
      "1\n",
      "602\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-03 05:17\n",
      "@amueller ... #4215 is just waiting on a #4347 merge so that there isnt more refactoring on your part required.\n",
      "unless you want me to address the comments and add more work to your pile??\n",
      "im good either way :smile:\n",
      "\n",
      "3\n",
      "603\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-03 05:22\n",
      "... it would effect gbm, as well as the  adaboost and bagging meta-ensembles\n",
      "\n",
      "1\n",
      "604\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-03 15:08\n",
      "oh, I forgot that PR.\n",
      "#4215 that is. My PR needs work to deprecate the class_weight in trees as you know. I should do that, but I didn't really get any reviews yet :-/\n",
      "\n",
      "2\n",
      "605\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-03 15:19\n",
      "that's why I figured I'd wait. Boosting has the subsample option too\n",
      "\n",
      "1\n",
      "606\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-04 09:27\n",
      "@trevorstephens  ```python try:     from sklearn.utils.validation import NotFittedError except ImportError:     # backward compat for scikit-learn < 0.16.0     class NotFittedError(Exception):         pass ```  should work.\n",
      "\n",
      "1\n",
      "607\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-04 13:39\n",
      "@ogrisel what do you think of #4661 ?\n",
      "\n",
      "1\n",
      "608\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-04 14:58\n",
      "after three years, I'm apparently still confused by scale_C vs n_samples\n",
      "@ogrisel if you have time, your input on #4597 would be nice.\n",
      "\n",
      "2\n",
      "609\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-05 03:07\n",
      "Thanks @ogrisel , that's what I was thinking of, though I have several other scikit-learn utils that I've included in my package in order to avoid dependency hell-fire. Only a few of them are actually used so it might not be that heavy, I guess, if I was to pick out only the ones that matter. Maybe once I get 0.1.0 released I'd think about making it more sensible on the utils.\n",
      "@amueller .. I think you know the appropriate emoji for dependency hell-fire :smile:\n",
      "\n",
      "2\n",
      "610\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-05 18:31\n",
      "haha\n",
      "\n",
      "1\n",
      "611\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-05 18:32\n",
      "And believe me, I have been there, more than once ;)\n",
      "\n",
      "1\n",
      "612\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-05 18:37\n",
      "@ogrisel the #4597 issue is not really with pandas, it is with Cython, which doesn't support read-only buffers. So my question was more: why does joblib produce a read-only buffer there?\n",
      "ah, I haven't read your comment there, sorry\n",
      "\n",
      "2\n",
      "613\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-05 21:27\n",
      "@amueller indeed I read some more discussions in this thread: https://mail.python.org/pipermail/cython-devel/2013-February/003384.html and then replied the following: https://github.com/pydata/pandas/issues/10043#issuecomment-99227037\n",
      "\n",
      "3\n",
      "614\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-05 21:35\n",
      "Calling `.values` will just force a copy if am not mistaken. The \"real\" fix can only be done in pandas by using the ndarray type instead of a typed memoryview (or even better in Cython by adding support for readonly typed memoryviews).\n",
      "\n",
      "1\n",
      "615\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-05 21:49\n",
      "I don't think values will force a copy always. It is not writeable here, so I don't think a copy was made. Well the real fix is clearly in cython but we have to be backward compatible, so we need to do a workaround now matter what\n",
      "\n",
      "1\n",
      "616\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-05 22:05\n",
      "Indeed `.values` does not copy for single block data frames. but it does for multiple blocks dfs\n",
      "\n",
      "1\n",
      "617\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-05 22:10\n",
      " can you reproduce the error with multiple block dfs?\n",
      "I posted my try at #4597\n",
      "\n",
      "2\n",
      "618\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-05 22:24\n",
      "I cannot reproduce the error without manually assigning a readonly memmap to `df._data.blocks[0].values` or replicating what the joblib.pool.MemmapingPool class does (that is replacing np.ndarray instances by np.memmap instances at pickling time).\n",
      "Let me work on fixing the joblib.load with `mmap_mode='r'` on structures that have numpy arrays with dtype=object inside.\n",
      "in joblib master\n",
      "Ok going to bed now. Tomorrow is second day of strata london. I gave a tutorial today. Tomorrow I will just attend the conf and maybe work on sklearn issues if the talks are boring.\n",
      "see you\n",
      "\n",
      "5\n",
      "619\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-05 22:48\n",
      "@ogrisel do you know anything about the issue in #4421 ? some warnings are not raised / caught ....\n",
      "\n",
      "1\n",
      "620\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-06 00:25\n",
      "cool. say hi to everybody from me ;)\n",
      "\n",
      "1\n",
      "621\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-07 18:02\n",
      "@ragv you had a PR to include some help on git to the dev docs, right?\n",
      "\n",
      "1\n",
      "622\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-08 14:35\n",
      "@ogrisel do you have time for #4362 ?\n",
      "\n",
      "1\n",
      "623\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-09 21:42\n",
      "@rvraghav93 @ragv I know you don't have much time, but if you could point me again to where you did the improvement of the contrib docs, I could maybe work on that. I think having some more comments on git would really help some people.\n",
      "\n",
      "1\n",
      "624\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-09 23:35\n",
      "AFAIK I think the intended framework was just put on the wiki for now - https://github.com/scikit-learn/scikit-learn/wiki/Contributors-Guide\n",
      "\n",
      "1\n",
      "625\n",
      "53135b495e986b0712efc453\n",
      "2015-05-14 20:03\n",
      "@amueller Sorry for the delayed response... as @trevorstephens pointed out that was the proposed structure... I'd love your comments on the same :)\n",
      "\n",
      "1\n",
      "626\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-14 20:30\n",
      "it looks good\n",
      "\n",
      "1\n",
      "627\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-05-15 08:29\n",
      "@rvraghav93 @amueller oh that looks really good. One thing I noticed though is that there's overlap between \"API design\" and our current \"rolling your own estimator\", and users do get confused about what they can and can't do in `__init__`, and why things fail afterwards. Do you have any ideas on how to make it better?\n",
      "\n",
      "1\n",
      "628\n",
      "53135b495e986b0712efc453\n",
      "2015-05-15 12:43\n",
      "We could use ideas from the existing page and make it into a single page or a section \"Custom designed estimators\" under \"API design\"... BTW I think I'll start a PR for the same and it will be easier for you to comment and advice me... :)\n",
      "\n",
      "1\n",
      "629\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-05-15 12:44\n",
      "I was just wondering why we can't add line-comments to github wiki pages :)\n",
      "\n",
      "2\n",
      "630\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-15 18:52\n",
      "@vene I have wondered that myself from time to time ;)\n",
      "\n",
      "1\n",
      "631\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-15 20:54\n",
      "@bdholt1 just showed up on IRC and said he's back :)\n",
      "\n",
      "1\n",
      "632\n",
      "55565ccb15522ed4b3e07834\n",
      "2015-05-15 20:54\n",
      "hi all!\n",
      "\n",
      "2\n",
      "633\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-15 20:58\n",
      "My favourite GSOC students, @rvraghav93 @xuewei4d @Barmaley-exe how are things? Reading up on your projects? ;)\n",
      "\n",
      "1\n",
      "634\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-16 04:49\n",
      "@bdholt1\n",
      "oops\n",
      "haha\n",
      "\n",
      "3\n",
      "635\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-16 04:51\n",
      "what i meant to say, was, @amueller .. sorry, i noticed your tag on #4732 today and then your #4711 comment asking for a new issue. Rather crazy week, will see if I can solve it though.\n",
      "and hi @bdholt1  :smile:\n",
      "\n",
      "2\n",
      "636\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-05-16 16:45\n",
      "Yes. I am currently reading the Murphy's paper 'Fitting a Conditional Gaussian Distribution'. That's the paper described three kinds of co-variance matrix in current  GMM implementation. But it seems a little harder to understand in terms of notations. @amueller\n",
      "\n",
      "1\n",
      "637\n",
      "53135b495e986b0712efc453\n",
      "2015-05-16 16:46\n",
      "@amueller yeah ;) :P working on completing my exiting PRs :)\n",
      "\n",
      "1\n",
      "638\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-18 15:42\n",
      "@rvraghav93 cool :)\n",
      "\n",
      "1\n",
      "639\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-18 15:43\n",
      "@xuewei4d have you talked with your mentors yet?\n",
      "it would be great to get a first blog post from both of you ;)_\n",
      "\n",
      "2\n",
      "640\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-05-18 17:28\n",
      "Not yet. I will email them soon. I have published a post several days ago, which is an introduction to my project and myself.\n",
      "I am trying to derive the updating functions of VBGMM with other three kinds of covariance matrix, sphere, diag and tied following PRML.\n",
      "\n",
      "2\n",
      "641\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-18 17:47\n",
      "Xuewei4d: did you mail your blog post to the mailing list? maybe I overlooked it, sorry\n",
      "xuewei4d: that sounds great. Make sure you have your mentors looped in so they know what is going on!\n",
      "btw @ogrisel are you around?\n",
      "I think it is good practice to post each blog post to the mailing list for maximum visibility\n",
      "I have to admit I didn't rss subscribe, and I don't usually use RSS, so i am likely to miss it if you don't post ;)\n",
      "@rvraghav93 btw why did you change your github handle?\n",
      "@rvraghav93 also: for wrapping up the partial_fit testing, I think the most important part is to simplify _compare_attributes\n",
      "\n",
      "7\n",
      "642\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-05-18 17:54\n",
      "Ohh, I will post it into the mailing list. Sorry.\n",
      "\n",
      "1\n",
      "643\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-18 19:37\n",
      "any one here familiar enough with LDA to do a final review?\n",
      "master is broken with a heisenfailure\n",
      "I don't like it\n",
      "\n",
      "3\n",
      "644\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-18 22:36\n",
      "anyone who has an opinion on how back-links from the references to the user-guide should look like should speak here or be forever silent: https://github.com/scikit-learn/scikit-learn/pull/4723\n",
      "\n",
      "1\n",
      "645\n",
      "53135b495e986b0712efc453\n",
      "2015-05-18 22:37\n",
      "@amueller Sorry for the confusion... I just merged my github profiles ragv and the old one rvraghav93 into one to sync with my email rvraghav93@gmail.com ... :) I earlier created ragv as a temporary profile...  And yea I just pushed it ;) could you pl take a look? :)\n",
      "\n",
      "3\n",
      "646\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-19 14:00\n",
      "@amueller the heisenfailure (the nans in solve_triangular called by OMP) only happens on scipy 0.9.0 from ubuntu precise right? What about skipping that test if scipy is too old?\n",
      "\n",
      "1\n",
      "647\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-19 14:35\n",
      "yeah, but we still support scipy 0.9.0, right?\n",
      "I tried reproducing but failed. I fear it is something silly like last time, where there was an if on the scipy version\n",
      "\n",
      "2\n",
      "648\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-19 14:37\n",
      "Have you tried to reproduce with ubuntu 12.04?\n",
      "\n",
      "4\n",
      "649\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-19 14:42\n",
      "ok. I think I have a vagrant box, I'll have a look\n",
      "btw if you had time to look at the mlp branch, this would be great\n",
      "nice! When are you in NYC? Let's have a beer!\n",
      "next week?\n",
      "ttyl!\n",
      "\n",
      "5\n",
      "650\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-19 15:08\n",
      "@rvraghav93 could you maybe start with a first blog post on your plans for the summer?\n",
      "\n",
      "1\n",
      "651\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-05-19 15:23\n",
      "@amueller @rvraghav93 Maybe we could find the time to sync and discuss an outline of the blog post. It's also a good opportunity to think about the upcoming steps.\n",
      "\n",
      "1\n",
      "652\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-19 15:32\n",
      "Sure\n",
      "\n",
      "1\n",
      "653\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-05-19 15:34\n",
      "BTW, I think it's good to be aware of each other's time zones, so we can plan better. I'm currently on Central European (Summer) Time until Sunday, and then New York time. (We can take this to a private chat too if you want.)\n",
      "\n",
      "2\n",
      "654\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-05-19 15:39\n",
      "I will actually be in New York for about a week, if everything goes well. I have to apply for a Chinese visa.\n",
      "Afterwards it's back to boring Ithaca.\n",
      "\n",
      "2\n",
      "655\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-05-19 15:40\n",
      "Currently in Florence for WWW\n",
      "Very beautiful city!\n",
      "\n",
      "2\n",
      "656\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-05-19 15:45\n",
      "Sure! Yes, Monday-Friday I think.\n",
      "Gotta run for a while, talk to you later!\n",
      "\n",
      "2\n",
      "657\n",
      "53135b495e986b0712efc453\n",
      "2015-05-19 18:19\n",
      "@amueller I just PM-ed @vene about the blog post :) Will make one before 20th and share the link :)\n",
      "\n",
      "2\n",
      "658\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-19 18:57\n",
      "@tomdlt btw we have this thing here were we hang out and chat ;)\n",
      "\n",
      "1\n",
      "659\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-20 09:28\n",
      "@amueller I think I understand part of the pbm for the _gram_omp failure on old scipy although I cannot reproduce it in ubuntu 12.04 docker container with scipy 0.9.0. Will submit a PR.\n",
      "I think we can merge.\n",
      "@amueller #4743\n",
      "\n",
      "3\n",
      "660\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-20 14:06\n",
      "@ogrisel thanks! I tried vagrant but didn't reproduce\n",
      "\n",
      "1\n",
      "661\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-20 14:37\n",
      "damn, yeah I'm stupid of not having seen that @ogrisel\n",
      "\n",
      "1\n",
      "662\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-20 14:40\n",
      "Hopefully, this will fix the issue. But travis is so slow nowadays. I opened #4749 to tackle travis speed on the longer term.\n",
      "\n",
      "2\n",
      "663\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-05-20 14:53\n",
      "#4749 would be really, really nice. I did not know travis was starting to support that - awesome!\n",
      "\n",
      "1\n",
      "664\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-20 15:08\n",
      "@ogrisel so do you definitely want to do 0.16.2? I'm not sure. Maybe if we have the jaccard fix?\n",
      "Btw, @ogrisel if you have time reviews of these should be quick: #4741 #4739 #4714\n",
      "maybe we should have added some more whatsnew entries\n",
      "\n",
      "3\n",
      "665\n",
      "54e07e1715522ed4b3dc0866\n",
      "2015-05-20 18:45\n",
      "This is not necessarily for sklearn (more for say mne-python, nilearn, sklearn-theano, etc): would the change mentioned in the blog post referenced in #4749 make certain data-intense (but fast) test cases possible with datasets stored in S3?\n",
      "\n",
      "1\n",
      "666\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-20 18:49\n",
      "depends on how large, right? I guess we don't know which datacenter, but even between datacenters I think ec2 is pretty fast\n",
      "\n",
      "1\n",
      "667\n",
      "54e07e1715522ed4b3dc0866\n",
      "2015-05-20 18:54\n",
      "ah yes - I thought that data was represented in several locations and always near to where you need it, but that is probably wishful thinking. Cost-wise,  as far as I understand data transfer from S3 to EC2 machines is free, so that shouldn't be a hindrance. Then the only question is whether practically out of these containers one can 'see' S3 as one can from EC2.\n",
      "\n",
      "1\n",
      "668\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-05-20 19:21\n",
      "probably not without some kind of credentials or something - you would have to authorize the EC2 instance that it is \"OK\" to access your s3 data\n",
      "\n",
      "1\n",
      "669\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-20 19:24\n",
      "well that is the same for all s3 access, right? ec2 machines are no different from any other machines on the internet wrt to s3 access iirc\n",
      "@ogrisel do you think we can ask intel for an MKL licence for building our wheels?\n",
      "\n",
      "2\n",
      "670\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-05-20 19:33\n",
      "I am thinking more if your data was in a private S3 bucket - in the case where people are accessing your data publicly it would be fine though it would cost $$\n",
      "but if you wanted to only allow data access for,  say, nightly tests on master branch to cut down cost, you would need some kind of authorization.\n",
      "So if Amazon gives free S3 to open source (or Travis), that would work. But otherwise I don't think we can rely on it without a bill.\n",
      "\n",
      "3\n",
      "671\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-20 21:21\n",
      "I just realized that if I wanted to test the neural nets with mkl I have to wait until we release and continuum builds it for me... that seems slightly silly...\n",
      "\n",
      "1\n",
      "672\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-20 22:14\n",
      "hum, thinking about it again, how do we build this on travis? That uses MKL, right?\n",
      "\n",
      "1\n",
      "673\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-05-21 00:06\n",
      "Yes I think one of the anaconda builds on Travis is MKL - do you have the academic version of Anaconda? It is free for you and would allow a conda env with MKL if I remember right\n",
      "\n",
      "1\n",
      "674\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-21 01:31\n",
      "yeah I do. but what blas does sklearn link against? anaconda doesn't come with the library to link against, right?\n",
      "\n",
      "1\n",
      "675\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-05-21 02:26\n",
      "The academic version does\n",
      "you have to register with .edu address - not sure how Travis has it but I am sure they have some license from Continuum\n",
      "\n",
      "2\n",
      "676\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-21 03:30\n",
      "interesting, I have to check. I thought I had the academic license but I ran into trouble when trying to built sklearn using it\n",
      "@ogrisel would you mind reviewing the mlp? I haven't added the early stopping yet, but mostly because I didn't see a case where it helped\n",
      "Also, I'm not sure how much the adaptive learning rate helps\n",
      "yeah it would be great if you could help\n",
      "do you know what the right gain is in the initialization for the different non-linearities?\n",
      "I tested against validation data currently using warm-start.\n",
      "but what I am implementing right now is doing a split inside fit\n",
      "\n",
      "7\n",
      "677\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-21 07:24\n",
      ">  Yes I think one of the anaconda builds on Travis is MKL - do you have the academic version of Anaconda? It is free for you and would allow a conda env with MKL if I remember right  on travis we just use the one month evaluation period of MKL in anaconda. As a travis build tend to last less than one month, it works :)\n",
      "sklearn probably does not build against MKL on travis with anaconda, but the included numpy and scipy packages are linked against it.\n",
      "\n",
      "2\n",
      "678\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-05-21 18:44\n",
      "SGD is unreasonably effective ;) and I am surprised you didn't find a case where early stopping helped. But how are you testing against validation data inside sklearn API? I have been hacking my own code to take fit(X, y, valid_X=None, valid_y=None)\n",
      "I really should help review that code but NIPS is looming :(\n",
      "\n",
      "2\n",
      "679\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-05-21 19:07\n",
      "what do you mean by gain? initializations are mostly (in my exp) about how tricky it is to get the right settings of the optimizer/avoid gradient explosion or vanishing\n",
      "glorot init is normally the \"right thing\" for basically all nets, though I hear orthogonal is good too.\n",
      "I don't know that you should expect a gain in performance on many tasks due to init settings, but you will certainly tell when you want to apply a net to a brand new dataset\n",
      "\n",
      "3\n",
      "680\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-21 19:51\n",
      "you need to multiply the 1/sqrt(fan_in + fan_out) by a constant that is dependent on whether you do relu, tanh or sigm\n",
      "in lasagna that is called the gain factor\n",
      "\n",
      "2\n",
      "681\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-05-21 23:06\n",
      "Ah ok. I usually go by this, page 15\n",
      "http://arxiv.org/pdf/1206.5533v2.pdf\n",
      "4 * for  sigm, tanh\n",
      "1 * for relu\n",
      "should be sqrt(6/fan_in + fan_out)\n",
      "though I guess you could pull out the 6 and adjust the \"gain\" to compensate. I just stick to the script\n",
      "\n",
      "6\n",
      "682\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-26 18:16\n",
      "Sanders said gaussian / uniform doesn't make a difference and I trust his judgement\n",
      "@kastnerkyle thanks I think I'll go with the script\n",
      "\n",
      "2\n",
      "683\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-27 07:10\n",
      "Well, this is weird: https://travis-ci.org/scikit-learn/scikit-learn/jobs/64079397 . cross_val_score, GridSearchCV and SVC and the iris dataset should all be deterministic.\n",
      "\n",
      "1\n",
      "684\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-27 14:09\n",
      "indeed weird\n",
      "\n",
      "1\n",
      "685\n",
      "541a528b163965c9bc2053de\n",
      "2015-05-27 14:11\n",
      "maybe some corrupted memory on the travis host? we should keep this failure in mind if it ever happen a second time\n",
      "\n",
      "1\n",
      "686\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-27 14:57\n",
      "corrupted memory sounds unlikely\n",
      "btw, why does pip install on OS X do a compile? https://github.com/scikit-learn/scikit-learn/issues/4766 when are the wheels used?\n",
      "fair enough\n",
      "\n",
      "3\n",
      "687\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-27 20:14\n",
      "@rvraghav93 I just started reading your blog post. I wouldn't argue with google search hits, as they are google-bubble dependent. Yours and mine are very likely biased heavily towards sklearn.\n",
      "@rvraghav93 end of may is pretty soon, btw. What is the status of the data-dependend CV iterators?  and why is the SVM infamous?\n",
      "ok thanks.\n",
      "ping me once you incorporated vlads comments\n",
      "\n",
      "4\n",
      "688\n",
      "53135b495e986b0712efc453\n",
      "2015-05-27 20:16\n",
      "Vlad advised me on a few improvements  and I am working on it too :D\n",
      "\n",
      "1\n",
      "689\n",
      "53135b495e986b0712efc453\n",
      "2015-05-27 20:17\n",
      "BTW this is my blog url rvraghav93.blogspot.in (so the rest of the people could take a look too)\n",
      "infamous was a wrong usage :/ correcting it :)\n",
      "And I am working on it... The final consensus was that the code must be duplicated right... one with the deprecations and one without... Will push soon ;)\n",
      "sure :)\n",
      "btw I intentionally aimed it to be biased towards scikit... I wanted to give a view where in the end user might see what they get out of my work... like Vlad had advised in the email... Is that okay?\n",
      "\n",
      "5\n",
      "690\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-27 20:22\n",
      "What I meant is that if you use google, google knows you contribute to scikit-learn, so it will show scikit-learn results on top.\n",
      "if someone else reads your blog and clicks the links, but they are an R user, it might show them links to R libraries, and no scikit-learn anywhere\n",
      "\n",
      "2\n",
      "691\n",
      "53135b495e986b0712efc453\n",
      "2015-05-27 20:26\n",
      "I just checked for \"cross validation\" using an online proxy server... our documentation page for cross validation still ranks at 3 ;)\n",
      "actually 2 if you consider the first 2 wikipedia pages as one\n",
      ":)\n",
      "\n",
      "3\n",
      "692\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-27 20:31\n",
      "woah my google cred is improving\n",
      "\n",
      "2\n",
      "693\n",
      "53135b495e986b0712efc453\n",
      "2015-05-27 20:38\n",
      "and may end as in without counting code reviews and revisions which I could do in parallel with the next goal :)\n",
      "\n",
      "3\n",
      "694\n",
      "54e07e1715522ed4b3dc0866\n",
      "2015-05-27 20:56\n",
      "nice :) It appears you have been there for more than a year http://en.wikipedia.org/w/index.php?title=Andreas_M%C3%BCller&type=revision&diff=597996618&oldid=573147763\n",
      "time to fill in that page\n",
      ";)\n",
      "\n",
      "3\n",
      "695\n",
      "53135b495e986b0712efc453\n",
      "2015-05-27 20:58\n",
      "awesome ;) we are filling it up :P\n",
      "\n",
      "3\n",
      "696\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-28 02:37\n",
      "Hey @amueller , regarding #4767 , just remembered your #4347 now... do you think I should pause work there until that merges? Do you think it will be merged? Also applies to #4215 I suppose... I guess both of these PRs would not necessarily require deprecation since they would not be in a public release, but would be more work for you. Should I press on with @vmichel 's PR comments, or put both on hold pending renewed class_weight naming/implementation?\n",
      "Reviewing these, I begin to feel that I am a one-trick pony :smiley:\n",
      "\n",
      "2\n",
      "697\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-05-28 15:43\n",
      "#4347 should be merged. There were no real reviews yet, though.\n",
      "I'm pretty sure you have more tricks up your leave, though @trevorstephens ;)\n",
      "I'm not sure, I would love some reviews for #4347. I'll rebase now. Maybe @ogrisel has time to have a look?\n",
      "\n",
      "3\n",
      "698\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-05-29 03:04\n",
      "Well I'll leave #4215 alone for now then. If PassAgg gets merged, it should be pretty simple to change it over in your PR I guess. Unless you want me to advertise #4347 there? I'm fine either way.\n",
      "\n",
      "1\n",
      "699\n",
      "53135b495e986b0712efc453\n",
      "2015-06-01 02:16\n",
      "@vene @amueller @ogrisel Hey this is the updated blog post incorporating all your suggestions... I am still a newbie at ML and might have made a few stupid mistakes... Please take a look at the post and feel free to point out if there are any :)\n",
      "http://rvraghav93.blogspot.com/2015/05/gsoc-2015-with-python-software.html\n",
      "\n",
      "2\n",
      "700\n",
      "551418d115522ed4b3dddd7b\n",
      "2015-06-01 10:48\n",
      "@rvraghav93 doesn't Wei Xue work on Gaussian Mixture Models? I thought that bayesian hyperoptimization didn't make it into accepted projects\n",
      "\n",
      "1\n",
      "701\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-01 13:18\n",
      "yeah, that is correct\n",
      "\n",
      "1\n",
      "702\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-01 13:19\n",
      "I'll have a look at the blog post probably tomorrow, I am sprinting with @pprett and @GaelVaroquaux today\n",
      "https://github.com/scikit-learn/scikit-learn/issues/3560\n",
      "\n",
      "2\n",
      "703\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-01 16:03\n",
      "There is a sprint in boston!\n",
      "(like, today)\n",
      "if anyone here wants to join\n",
      "like @llllllllll\n",
      "lol\n",
      "that is the worst name\n",
      "\n",
      "6\n",
      "704\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-01 16:48\n",
      "@GaelVaroquaux do you want to merge this? https://github.com/scikit-learn/scikit-learn/pull/4785\n",
      "\n",
      "1\n",
      "705\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-01 17:36\n",
      "https://github.com/scikit-learn/scikit-learn/issues/4784\n",
      "\n",
      "1\n",
      "706\n",
      "53135b495e986b0712efc453\n",
      "2015-06-02 07:37\n",
      "@Barmaley-exe thanks! :) @xuewei4d apologies for the same ;)\n",
      "\n",
      "1\n",
      "707\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-02 18:28\n",
      "@amueller @llllllllll worst name, or *best* name? :smile:\n",
      "\n",
      "1\n",
      "708\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-02 18:34\n",
      "I think you could go a level further @l1Ill11IIlll111III\n",
      "just totally destroy people with certain fonts\n",
      "\n",
      "2\n",
      "709\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-02 20:45\n",
      "Hi, @rvraghav93 I think you need to update the RSS feed of your blog in Terri's website, (link)[https://github.com/terriko/gsoc/blob/master/blog-aggregator-configs/config2015.ini]\n",
      "\n",
      "1\n",
      "710\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-03 15:16\n",
      "@xuewei4d wow, it's been a while since I've seen an .ini file!\n",
      "Those are great.\n",
      "For users who learn by hacking around in IPython, it was so easy to miss the User Guide completely\n",
      "\n",
      "3\n",
      "711\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-03 15:22\n",
      "haha\n",
      "@vene @llllllllll thinks it is the best. I just feel it is hard to type ;)\n",
      "\n",
      "2\n",
      "712\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-03 15:23\n",
      "@rvraghav93 which PR are you currently working on? I think the data independend CV should have priority.\n",
      "\n",
      "1\n",
      "713\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-03 15:48\n",
      "@rvraghav93 in the blog post, \"improvise\" should probably be \"improve\"\n",
      "\n",
      "1\n",
      "714\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-03 16:08\n",
      "@amueller good thing Gitter has autocompletion\n",
      "\n",
      "1\n",
      "715\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-03 16:10\n",
      "Can I ask a quick joblib question here? https://github.com/joblib/joblib/blame/master/doc/memory.rst#L187 it says r+ and w+ \"will propagate the changes to disk\". Shouldn't that be \"will NOT propagate\"?\n",
      "\n",
      "1\n",
      "716\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-03 17:40\n",
      "ping @ogrisel ?\n",
      "\n",
      "1\n",
      "717\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-03 19:15\n",
      "I am fascinated time and again by which PRs and issues get attention\n",
      "yeah\n",
      "I mean it is neat, but like 6 core devs in a day?\n",
      "\n",
      "3\n",
      "718\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-03 19:16\n",
      "What do you mean? The CallableVectorizer?\n",
      "*CallableTransformer?\n",
      "\n",
      "2\n",
      "719\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-03 19:19\n",
      "I think it's because all of us hacked up one at some point or another\n",
      "\n",
      "3\n",
      "720\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-03 19:21\n",
      "I am unreasonably happy about the backlinks\n",
      "ok\n",
      "\n",
      "2\n",
      "721\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-03 19:30\n",
      "well but from IPython you don't get a clickable link, only a useless reference, right?\n",
      "\n",
      "3\n",
      "722\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-03 20:13\n",
      "@amueller @ogrisel I think I need more time to write down all equations of VBGMM and DPGMM. Currently I have those for full and diag covariance. https://www.dropbox.com/s/8hlbb7dlwllwcry/VBGMM.pdf?dl=0\n",
      "\n",
      "1\n",
      "723\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-03 20:16\n",
      "so you are missing spherical? Shouldn't that be easiest?\n",
      "I'll try to have a look tomorrow, but it would be great if @ogrisel and loic could have a look\n",
      "\n",
      "2\n",
      "724\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-03 20:20\n",
      "Well, I read through  PRML and equations for full covariance. Since there are some repeated routines, I choose to do them in order of 'full', 'diag', 'sphere' and 'tied'.\n",
      "since 'diag' share some similarities with 'full'\n",
      "\n",
      "2\n",
      "725\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-03 20:23\n",
      "@rvraghav93 \"digits\" is not the same dataset as MNIST, it is much smaller. in your blog post, you mention mnist but link to a digits example.\n",
      "@rvraghav93 why do you use alpha when plotting points?\n",
      "@rvraghav93 I feel this sentence is unclear: \"Even when the model is optimized with the constrain of maximizing the score based upon the test set, there is still a chance of overfitting as the information about the test set can leak into the model and hence the model could be optimized for the test set alone.\"  it would be more explicit to say the information leaks via the selection of hyperparameters.\n",
      "@rvraghav93 and cross-validation does not entirely overcome this. Overfitting to cross-validation is harder than overfitting to a single test set, but it is still possible, which is why people do nested cross-validation.\n",
      "@rvraghav93 grid.best_estimator_.score(X_test, y_test) is also not great btw. You can just use grid.score\n",
      "@rvraghav93 talking about gamma=0 for the SVM is also a bit weird. This is an odd way that we used to select the default, which is 1. / n_features ,I think.\n",
      "\n",
      "6\n",
      "726\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-03 20:36\n",
      "\"which is why people do nested cross-validation\" I think that's where @rvraghav93 is trying to lead to.\n",
      "\n",
      "3\n",
      "727\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-04 07:25\n",
      "> Can I ask a quick joblib question here? https://github.com/joblib/joblib/blame/master/doc/memory.rst#L187 it says r+ and w+ \"will propagate the changes to disk\". Shouldn't that be \"will NOT propagate\"?  Not it is correct. Both `r+` and `w+` open the file in read-write mode. `r` is read-only. The difference between `r+` and `w+` is that `w+` will delete any existing file before creating a new one from scratch.\n",
      "\n",
      "1\n",
      "728\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-04 09:10\n",
      "@xuewei4d will have a look thanks. Another thing we could try to address during your GSoC is the ability to add a `partial_fit` function for incremental / out-of-core fitting of (classical) GMM, for instance http://arxiv.org/abs/0712.4273. Of course the priority is fixing issues on the current code base. I can currently familiarizing myself with this part of the code base that I don't know well enough to comment on the open PRs / issues.\n",
      "\n",
      "1\n",
      "729\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-04 14:42\n",
      "Thanks @ogrisel . I just finished the derivations for VBGMM.\n",
      "\n",
      "1\n",
      "730\n",
      "53135b495e986b0712efc453\n",
      "2015-06-04 14:42\n",
      "@xuewei4d Thanks!! I just updated the same :)\n",
      "@amueller thanks for the feedback will update my blog post accordingly :)\n",
      "I think Terri should merge the PR then only it will refresh :)\n",
      "\n",
      "3\n",
      "731\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-04 14:45\n",
      "great, but the webpage http://terri.toybox.ca/python-soc/ seems not updated yet.\n",
      "\n",
      "1\n",
      "732\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-04 15:12\n",
      "@ogrisel Would the partial_fit version be EM style, or SGD? I think EM style is \"easier\" from a convergence perspective but I am not sure how you would do it in minibatch fashion\n",
      "Though I think David Cournapeau had some good papers on it he showed at PyCon\n",
      "I have the links somewhere\n",
      "\n",
      "3\n",
      "733\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-04 15:41\n",
      "I think we should focus on fixing what we have before implementing new algorithms\n",
      "\n",
      "1\n",
      "734\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-04 15:58\n",
      "I agree with @amueller.  May I ask what is EM style @kastnerkyle ?\n",
      "\n",
      "1\n",
      "735\n",
      "551418d115522ed4b3dddd7b\n",
      "2015-06-04 16:12\n",
      "@xuewei4d I think he meant that in order to do partial_fit (update model rather than retrain it) you can just do several Expectation Maximization iterations from the point you've stopped at previously\n",
      "Though I don't get SGD part of it. I'm not sure of how severe expenses of a single EM iteration are, but I don't think they're so huge that we can't afford even a minibatch iteration\n",
      "\n",
      "2\n",
      "736\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-04 17:03\n",
      "Expectation-maximization style instead of gradient descent by minibatches. I am weaker in EM than SGD approaches, but I have done SGD style learning for GMMs in the very recent past. This is why I asked - trying to gauge what I would need to read :)\n",
      "Also, +1 to @amueller comment. Nothing can really happen until there is something that is well documented and understood by several people is there to experiment with/on\n",
      "\n",
      "2\n",
      "737\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-04 20:20\n",
      "there was once a PR that summarized all classifiers, I think. Does anyone know where that went?\n",
      "\n",
      "1\n",
      "738\n",
      "53135b495e986b0712efc453\n",
      "2015-06-04 20:27\n",
      "Hey @amueller Would it not be better to have `model_selection/validation.py` instead of `validate.py`?\n",
      "\n",
      "3\n",
      "739\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-04 21:00\n",
      "@kastnerkyle  I was thinking of online EM as described in http://arxiv.org/abs/0712.4273, but if you have good references for SGD for GMM, that's interesting too. I agree fixing existing stuff is the priority over implementing new incremental solvers but I also think that incremental solvers would make GMMs more practically useful so it would be good to review the literature on that topic.\n",
      "\n",
      "1\n",
      "740\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-04 21:02\n",
      "You know me - partial fit for all the things! But this link looks good - I will read up once I get spare time.\n",
      "\n",
      "1\n",
      "741\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-04 21:04\n",
      "@xuewei4d how different are your derivations from http://scikit-learn.org/dev/modules/dp-derivation.html ?\n",
      "David recommended to read http://leon.bottou.org/publications/pdf/online-1998.pdf as an intro to some of the concept of the online EM paper.\n",
      "\n",
      "2\n",
      "742\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-04 21:08\n",
      "lol. Actually, the doc that the link points is for DPGMM, not VBGMM\n",
      "I am trying to figure out that Blei's paper for DPGMM now.\n",
      "@ogrisel\n",
      "\n",
      "3\n",
      "743\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-04 22:07\n",
      "does anyone have opinions on the heterogeneous feature union interface?\n",
      "3886\n",
      "#3886\n",
      "?\n",
      "\n",
      "4\n",
      "744\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:07\n",
      "hi Andy\n",
      "\n",
      "1\n",
      "745\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:08\n",
      "I also had thought about what you just proposed `(estimator, column_name, weight)`\n",
      "but I think it's not self-documenting enough\n",
      "\n",
      "2\n",
      "746\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:13\n",
      "and also it makes it hard if you don't need weights. Pass explicit `None`s? ugly\n",
      "\n",
      "3\n",
      "747\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:14\n",
      "I'd something like `dict(name='date', estimator=DateExtractor(), column='timestamp', weight=0.5)`\n",
      "but without a dict\n",
      "namedtuple maybe?\n",
      "I never used them, let me check\n",
      "\n",
      "4\n",
      "748\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:16\n",
      "it'd be nice to also support `('date', DateExtractor(), column='timestamp')`, it'd be almost like a pipeline with optional arguments\n",
      "is that even doable in Python?\n",
      "\n",
      "2\n",
      "749\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-04 22:17\n",
      "with namedtuples\n",
      "right?\n",
      "but it would be super cumbersome for the user\n",
      "because they need to import this particular named tuple class\n",
      "\n",
      "4\n",
      "750\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:19\n",
      "if only there existed anonymous namedtuples\n",
      "like ad-hoc, you just write (a, b, d='something'). Something like a function call argument list, but without the function\n",
      "\n",
      "2\n",
      "751\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-04 22:19\n",
      "I think if we want a default argument for weight, we need to define a custom class. But for the user that doesn't look much different from a namedtuple\n",
      "how do you mean anonymous?\n",
      "hehe yeah that's not possible in Python\n",
      "because () calls the tuple constructor\n",
      "I guess we could monkey-patch it :P\n",
      "the other option would be a syntax more like the make_stuff helpers\n",
      "but that would require kwargs\n",
      "\n",
      "7\n",
      "752\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:21\n",
      "I feel dirty just having this conversation\n",
      "\n",
      "2\n",
      "753\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-04 22:22\n",
      "ColumnTransformer(some_name=(CountVectorizer(), column_name, weight), some_name2=(OneHotEncoder(), column_name, weight))\n",
      "still wouldn't document what the weight is, though\n",
      "I think I'm going with\n",
      "ColumnTransformer({'some_name': (CountVectorizer(), column_name), 'some_name2': (OneHotEncoder(), column_name)}, weights=[weight1, weight2])\n",
      "err\n",
      " ColumnTransformer({'some_name': (CountVectorizer(), column_name), 'some_name2': (OneHotEncoder(), column_name)}, weights={'some_name':weight1, 'some_name2':weight2})\n",
      "\n",
      "6\n",
      "754\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:31\n",
      "I wish they could all be grouped in the same place\n",
      "\n",
      "1\n",
      "755\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-04 22:33\n",
      "a quick question. Where should I add deprecation warnings in GMM? Is that good adding warning  just after ```class GMM```, (not within any function)\n",
      "\n",
      "2\n",
      "756\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:35\n",
      "the other extreme would be ColumnTransformer(names=['a', 'b', 'c'], estimators=[CountVectorizer()] * 3, columns=['title', 'content', 'comments'], weights=[1, 1, 2])\n",
      "hey, what if for ColumnTransformer we drop the names completely and use the column as the name?\n",
      "\n",
      "2\n",
      "757\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-04 22:37\n",
      "then you can't grid-search if you have multiple transformers on the same column\n",
      "hm if we change the data structure of how we store the transformers from FeatureUnion, we'll have a lot of code duplication :-/\n",
      "\n",
      "6\n",
      "758\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:42\n",
      "that's how Ward was deprecated\n",
      "looks god\n",
      "good\n",
      "https://github.com/scikit-learn/scikit-learn/pull/4370/files#diff-d7365adec1b76c4ff63051e4d1dd32b0L932\n",
      "why?\n",
      "\n",
      "5\n",
      "759\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-04 22:42\n",
      "hm if we use dicts we also need to sort them every time we use them lol\n",
      "\n",
      "1\n",
      "760\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-04 22:45\n",
      "updated #3886\n",
      "because dictionaries have undefined sorting, and if we iterate over them in transform we might get them in a different order then in fit\n",
      "i.e. the features would be shuffled\n",
      "\n",
      "3\n",
      "761\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:47\n",
      "but aren't they ever only accessed by key?\n",
      "ahhh, I know what you mean now\n",
      "\n",
      "2\n",
      "762\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 22:51\n",
      "yes, this is probably the reason why Pipeline and FeatureUnion aren't {name: Estimator()} dicts, but lists of tuples\n",
      "sorting isn't good, we really want to keep the order the user gives. Otherwise there'll be a world of confusion\n",
      "Now, the question is:  `ColumnTransformer([('name', (Est(), 'col')), ...]), transformer_weights=...)` vs. `ColumnTransformer([('name', Est(), 'col'), ...]), transformer_weights=...)`\n",
      "\n",
      "3\n",
      "763\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-04 22:59\n",
      "I think you can use OrderedDict from collections also.\n",
      "I like the flat one personally\n",
      "\n",
      "2\n",
      "764\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-04 23:01\n",
      "@kastnerkyle yes, but then the user needs to do `ColumnTransformer(OrderedDict(...))`\n",
      "\n",
      "1\n",
      "765\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-04 23:06\n",
      "Was thinking you could do d= OrderedDict(); d.items = user_d.items internally, though I have no idea if this would work. Probably more trouble than it is worth\n",
      "maybe sorting is just as easy. But I like the flat list version better\n",
      "er list of tuple\n",
      "\n",
      "3\n",
      "766\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-05 03:21\n",
      "Pipeline is not a dict because it has a sequence, FeatureUnion is not a dict becaues I'm stupid.\n",
      "Look at the issue, I went with dict.\n",
      "why do you think sorting isn't good?\n",
      "because people don't know which indices correspond to what?\n",
      "damn I just spend two hours on that.\n",
      "\n",
      "5\n",
      "767\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-05 03:28\n",
      "Well if you put it that way\n",
      "\n",
      "1\n",
      "768\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-05 03:29\n",
      "It's not a lot different from classes_ being sorted. Which also is a bit confusing for the user. But it is cleaner.\n",
      "It would be good to support OrderedDicts if e user so wishes\n",
      "\n",
      "2\n",
      "769\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-05 13:00\n",
      "@xuewei4d wouldn't you happen to have a PDF version of PRML? I am working at home today and I left my hardcover copy in our lab.\n",
      "I don't remember...\n",
      "\n",
      "2\n",
      "770\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-05 13:01\n",
      " Yes, I have\n",
      "\n",
      "1\n",
      "771\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-05 13:03\n",
      "https://www.dropbox.com/s/7u13hvokr1lh2fa/Pattern%20Recognition%20and%20Machine%20Learning.pdf?dl=0\n",
      "\n",
      "2\n",
      "772\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-05 13:38\n",
      ":smile: Let me know if there is any problem in the derivation draft.\n",
      "\n",
      "1\n",
      "773\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-05 14:04\n",
      "yeah sorry, I am slow to review your work. I have been busy helping a colleague with a nips submission this week. The deadline is tonight. I hope to be more responsive next week.\n",
      "\n",
      "1\n",
      "774\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-05 17:25\n",
      "@rvraghav93 any update on the generalized CV? It would really be great if you'd report more regularly so we can follow what is happening.\n",
      "\n",
      "1\n",
      "775\n",
      "53135b495e986b0712efc453\n",
      "2015-06-05 17:57\n",
      "Sory sory! :( will update that soon...!! I am unable to pass all the tests :/\n",
      "BTW can I go ahead and add the exceptions module or should I wait for a reply from Lars (#4309)?\n",
      "\n",
      "2\n",
      "776\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-05 18:01\n",
      "even if tests are failing, you can still push\n",
      "\n",
      "5\n",
      "777\n",
      "53135b495e986b0712efc453\n",
      "2015-06-05 18:03\n",
      "Yes sure... apologies for the lack of regular communications :)\n",
      "and yea it would be cleaner to put those in exceptions... coz we are already duplicating a lot of code it would be great if we could group those together into the exceptions module... (not a big issue though)\n",
      "\n",
      "4\n",
      "778\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-05 18:24\n",
      "@vene @ogrisel do you remember the estimator summary PR? I think it was by @mblondel ?\n",
      "is there a more readable alternative to mgrid with complex numbers? this seems like the most horrifying hack.\n",
      "but how do I solve my one-hot-encoding problem now? Should I change OneHotEncoder to work with strings?\n",
      "\n",
      "5\n",
      "779\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-05 19:03\n",
      "maybe I'm imagining things...\n",
      "\n",
      "1\n",
      "780\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-05 19:17\n",
      "@amueller what are you referencing re: mgrid? is it in any scikit-learn code?\n",
      "\n",
      "1\n",
      "781\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-05 19:52\n",
      "I just noticed that the problem setting in the current derivation of DPMM is simpler than mine. You could find it in my new blog post http://xuewei4d.github.io/2015/06/05/gsoc-week2-vbgmm-and-gmm-api.html. @amueller @ogrisel @lesteve.\n",
      "\n",
      "1\n",
      "782\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-05 20:05\n",
      "@vene yes, in the examples. np.mgrid[1:2:10j] means \"do ten steps from 1 to 2\"\n",
      "it is a multi-dimensional version of linspace\n",
      "without the j it means \"step\"\n",
      "so mgrid implements a multi-dimensional arange if the third argument is real, and a multi-dimensional linspace if it is complex.\n",
      "not sure if :rage4: or :trollface:\n",
      "what should the allowed input types for OneHotEncoder be?\n",
      "currently it is integer arrays\n",
      "\n",
      "7\n",
      "783\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-05 20:10\n",
      "So there is no function for this without a slice api?\n",
      "\n",
      "1\n",
      "784\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-05 20:13\n",
      "Thanks for the wrap-up @xuewei4d! I will have a deeper look at it next week.\n",
      "\n",
      "7\n",
      "785\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-05 20:19\n",
      "@xuewei4d about the API issues, we need to make sure that the score API (in particular the shape) is not conflicting with the score method of other models in scikit-learn in particular models that are not density estimators. I think we would have a `density` or `log_density` methods to have more explicit names and avoid conflicts\n",
      "\n",
      "1\n",
      "786\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-05 20:20\n",
      " isinstance(sparse.csr_matrix([[1, 3], [5, 3]]), containers.Mapping) == True. # that is all\n",
      "\n",
      "1\n",
      "787\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-05 20:45\n",
      "@xuewei4d indeed, that is pretty cool\n",
      "hum, if I want to one-hot encode ['paris', 'paris', 'london', 'new york'] I have to use ``LabelBinarizer``, not ``OneHotEncoder``. That seems odd.\n",
      "And if it is ['paris', 'paris', 'london'] I only get a single column....\n",
      "\n",
      "3\n",
      "788\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-05 20:47\n",
      "@amueller the mapping thing is horrifying\n",
      "\n",
      "2\n",
      "789\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-05 20:53\n",
      "Ok so I want to do an example for the heterogeneous feature union / ColumnTransformer that processes `` ['paris', 'paris', 'london', 'new york'] `` as a categorical variable. Is this really not possible with scikit-learn?\n",
      "\n",
      "1\n",
      "790\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-05 20:57\n",
      "Less. CSR is naturally a dict-like structure\n",
      "\n",
      "2\n",
      "791\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-05 21:04\n",
      "We could code a gael-bot for gutter\n",
      "\n",
      "1\n",
      "792\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-05 21:07\n",
      "Anything that can be turned into a set and indexed?\n",
      "\n",
      "2\n",
      "793\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-05 21:54\n",
      "list of strings should work too\n",
      "\n",
      "1\n",
      "794\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-06 17:07\n",
      "and how about lists of lists of strings?\n",
      ";)\n",
      "\n",
      "2\n",
      "795\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-06 18:01\n",
      "@amueller what is the use case? I'd just use CountVectorizer without a tokenizer there :D\n",
      "In your example above, `['paris', 'paris', 'london', 'new york']` is the entire column (4 values) or a single value?\n",
      "if the former, OneHotEncoder would work, right?\n",
      "@amueller try this: ``` x = [['paris', 'paris', 'london'], ['london', 'nyc']] CountVectorizer(analyzer=lambda x: x).fit_transform(x).todense() ```\n",
      "I think OneHotEncoder should support it\n",
      "what I meant yesterday is that it can support anything that's an object\n",
      "as long as you can assign integers to different objects you encounter\n",
      "\n",
      "7\n",
      "796\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-06 18:06\n",
      "the entire column\n",
      "four samples\n",
      "it is just one categorical value\n",
      "OneHotEncoder only works on integers\n",
      "true. It seems odd to me, though.\n",
      "Is that the interface we want?\n",
      "and I feel like I also want to support data that has one column that is city and one column that is color. But maybe ColumnTransformer is for that\n",
      "ok but what is the type / shape of X? Would you support this for X being a list of arbitrary objects? Or for X being an 2d array of arbitrary objects?\n",
      "\n",
      "8\n",
      "797\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-06 18:07\n",
      "LabelBinarizer kind of works, but is not really made for this usecase and doesn't really have a transformer interface\n",
      "\n",
      "1\n",
      "798\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-06 18:08\n",
      "this works: ``` In [12]:  x = [['paris'], ['paris'], ['london'], ['new york']] CountVectorizer(analyzer=lambda x: x).fit_transform(x).toarray()  Out[12]: array([[0, 0, 1],        [0, 0, 1],        [1, 0, 0],        [0, 1, 0]]) ```\n",
      "\n",
      "1\n",
      "799\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-06 18:11\n",
      "I'd say list (or 1d array) of arbitrary objects\n",
      "why 2d array? for city and color?\n",
      "once you have more than one field of categorical variables, you could encode them as dict(city='new york', color='red') in which case dictvectorizer works, right?\n",
      "or encode them as dict(cities=['nyc', 'paris'], colors=['red', 'yellow']) and then ColumnTransformer + OneHotEncoder should work\n",
      "\n",
      "4\n",
      "800\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-06 18:18\n",
      "for your example you could do ``` X = {'post_content': [\"long string 1\", \"long string 2\"...],         'metadata': [{location='nyc', category='misc'}, {location='paris', category='programming'}...] ```\n",
      "and ColumnTransformer(dict(post_content=CountVectorizer(), metadata=DictVectorizer()))\n",
      "\n",
      "2\n",
      "801\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-06 18:28\n",
      "I feel the nested dicts are ugly.\n",
      "I just updated the example but something is wrong :-/\n",
      "ah\n",
      "how do you do the code highlighting again?\n",
      "right\n",
      "\n",
      "5\n",
      "802\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-06 18:36\n",
      "```python x = ['paris', 'paris', 'london', 'new york'] CountVectorizer(analyzer=lambda x: [x]).fit_transform(x).toarray() ```\n",
      "is what I want\n",
      "\n",
      "2\n",
      "803\n",
      "529c6c25ed5ab0b3bf04d824\n",
      "2015-06-06 18:37\n",
      "backticks\n",
      "\n",
      "1\n",
      "804\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-06 18:42\n",
      "that works, but the analyzer lambda is super opaque\n",
      "idea: why not deprecate DictVectorizer and create a CategoricalVectorizer with the same interface\n",
      "except if you pass it a 2d array, it would treat each column as a different implicit key\n",
      "so then you can do `['paris', 'paris', 'new york', 'london']` as well as `[['paris, 'red'], ['paris', 'green'], ...]`\n",
      "or that\n",
      "\n",
      "5\n",
      "805\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-06 18:55\n",
      "hm... maybe just add that?\n",
      "I added a somewhat interesting example to the pr\n",
      "\n",
      "4\n",
      "806\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-06 21:20\n",
      "IThe example is very nice\n",
      "\n",
      "1\n",
      "807\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-07 00:49\n",
      "thanks :)\n",
      "\n",
      "1\n",
      "808\n",
      "53135b495e986b0712efc453\n",
      "2015-06-07 11:50\n",
      "Why do we have the `_check_cv` function? It is called by `check_cv` which does nothing additionally? Am I missing something? https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cross_validation.py#L1431\n",
      "\n",
      "1\n",
      "809\n",
      "53135b495e986b0712efc453\n",
      "2015-06-07 12:11\n",
      "It should be removed... I opened a PR #4829 for that... Please review @vene @amueller ! :)\n",
      "and @ogrisel\n",
      "\n",
      "2\n",
      "810\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-07 21:54\n",
      "@amueller can `sklearn.utils.testing.ignore_warnings` ignore *specific types of warnings*?\n",
      "the code looks odd\n",
      "\n",
      "2\n",
      "811\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-07 21:55\n",
      "(this is in reference to #4824)\n",
      "\n",
      "1\n",
      "812\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-07 22:31\n",
      "so have we decided to just never use the words \"extreme learning machine\" in docstrings/documentation?\n",
      "\n",
      "1\n",
      "813\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-08 00:53\n",
      "That is my opinion. Though it is weird cause that is what people call it. So I am of 2 minds at once\n",
      "But the paper citation stuff is really pretty shady so that is tough too.\n",
      "\n",
      "2\n",
      "814\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-08 02:33\n",
      "Do we care about being googlable for this?\n",
      "\n",
      "1\n",
      "815\n",
      "53135b495e986b0712efc453\n",
      "2015-06-08 11:16\n",
      "Just a general question... Is there a way to track changes in a particular module and get notified if that gets merged? (w.r.t #4294) (ref [comment](https://github.com/scikit-learn/scikit-learn/pull/4829#issuecomment-109952865))\n",
      "Ah its easier to just check the history of the file ;) sorry for the noise!\n",
      "\n",
      "2\n",
      "816\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-08 13:53\n",
      "@kastnerkyle btw I'm going to seriously start on the tutorials now, I was pretty busy with other things before\n",
      "@vene I think we should use the word Extreme Learning Machine in the narrative and maybe as a remark in the docstrings, but not as the name.\n",
      "Like \"This can be used to implement so-called Extreme Learning Machines\" or something like that\n",
      "\n",
      "3\n",
      "817\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-08 14:16\n",
      "That's probably wise.\n",
      "\n",
      "1\n",
      "818\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-08 14:17\n",
      "Also, scikit-learn should have the power to change the way researchers call algorithms. Maybe our naming decision will fix this mess a bit.\n",
      "\n",
      "1\n",
      "819\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-08 14:18\n",
      "I've seen researchers calling svms SMO because that's what the solver class is called in Weka\n",
      "\n",
      "1\n",
      "820\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-08 14:20\n",
      "omg\n",
      "\n",
      "1\n",
      "821\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-08 14:40\n",
      "I remember Fabian had started wrapping liblinear's CD solver. Now that SAG is joining the family of solvers available for logistic regression, we might be able to simplify the codebase by picking that up\n",
      "assuming it matches the interface of scipy optimizers\n",
      "https://github.com/fabianp/pytron/\n",
      "(question triggered by @amueller's [comment](https://github.com/scikit-learn/scikit-learn/pull/4738#issuecomment-108592200))\n",
      "\n",
      "4\n",
      "822\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-08 18:22\n",
      "@kastnerkyle when do you have time to discuss the tutorial? Do you think we should put exercises after each sections or do \"morning lecture, morning exercises, afternoon lecture, afternoon exercises\"? Maybe multiple short breaks would be good? four hours is such a long time. Maybe a four 45 blocks with 15 minutes exercises? Not sure.\n",
      "\n",
      "1\n",
      "823\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-08 20:35\n",
      "Hi, I am almost done the derivation draft. I fixed some typos and errors, and completed the DP part. What is left is the lower bound and predictive distribution for   two cases. I think I could finish it tomorrow. The difference from current derivation includes, the initial parameters and the updating functions for Wishart distribution. The rest of them are the same. I hope I could find out the reason of the latter.\n",
      "With regards to the new names of DPGMM and VBGMM, I think these two names are not suitable, just like someone calls SVM as SMO. Actually, the models are Bayesian GMM, Dirichlet Process Bayesian GMM (DPGMM is often used) respectively. Both of them are solved by variational inference. In other words, VBGMM is not a good name. The new names, I think, should have the meaning of 'Bayesian GMM solved by VB', 'DP(B)GMM solved by VB'.\n",
      "\n",
      "2\n",
      "824\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-08 21:17\n",
      "@tw991 the two PRs with the mlp code are #3939 and #3204. #3204 contains adagrad\n",
      "@xuewei4d I agree wrt VB. It should be \"bayesian GMM\"\n",
      "or something similar\n",
      "\n",
      "3\n",
      "825\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-08 22:24\n",
      "@amueller I have time. Just started my internship today so I should be able to take a look at it now :) I think 45 min blocks + excercises is good but we should also budget for questions *during*. And hopefully we will not be only 2 who can help when questions arise - maybe some advanced sklearn users in audience can help too\n",
      "Though I am expecting more question during advanced section for sure. We could do that in 30/30 blocks\n",
      "if necessary. Also the first day tutorial might inform us of mods to make for second day\n",
      "\n",
      "3\n",
      "826\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-08 22:27\n",
      "@kastnerkyle cool. I basically just copied the 2013 as a start, but I want to do some reorganization. I'm not entirely certain about the structure and the cluster stuff\n",
      "I am now going through the lectures again and I'll draft a new toc tomorrow. I think having a bit more intro about what ML is in the beginning would be good, and I'll try to recycle some of my diagrams\n",
      "Actually the current notebooks are pretty close to the toc in the proposal, with a bit of reorganization\n",
      "@kastnerkyle is there any part you'd like to work on particularly so that we don't conflict? Also, when do you have time or a beer in the city?\n",
      "I think I'd like to work on the intro for now.\n",
      "\n",
      "5\n",
      "827\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-08 22:51\n",
      "I am planning right now to pop in over the weekend. So if you have a day or time which is good I am game. I am probably stronger talking about PCA, GP and friends than random forests, SVM and the like if we are gonna go into theoretical aspects. But if not then I have no preference.\n",
      "Also I am terrible at the text preprocessor stuff. But I think it is important\n",
      "DictVectorizer etc. Might be a crucial thing for examples. Raw text -> classification ala your troll detection model\n",
      "\n",
      "3\n",
      "828\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-08 23:26\n",
      "I don't think we will do much theory, and I don't think we should include GPs. I want to talk a bit more about details about some of the commonly used supervised models.\n",
      "I think Sunday would work for me.\n",
      "\n",
      "2\n",
      "829\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-08 23:45\n",
      "OK cool. I will let you know later in the week what is up but I can try to be in NY on Sunday. What part are you in?\n",
      "\n",
      "1\n",
      "830\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-08 23:57\n",
      "I'm in east village but I'm pretty free on Sunday, so I could travel a bit\n",
      "\n",
      "1\n",
      "831\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-09 13:41\n",
      "That sounds good to me. I will email you and set up something. I will be the country kid in the Big Apple ;)\n",
      "\n",
      "1\n",
      "832\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-09 15:29\n",
      "@kastnerkyle one thing  I noticed about the notebooks is that they are pretty text-heavy. I'm not sure how I feel about it. I guess it depends on whether you think of it more as a presentation or as an \"in your own time\" thing. While someone is speaking, the text seems more distracting than helpful. I'm not sure about it, though.\n",
      "\n",
      "1\n",
      "833\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-09 15:30\n",
      "I agree. The text is useful \"after the fact\". But our presentation should basically be the content contained in that text IMO, so we can focus on the pretty pictures\n",
      "We could make a stripped down version of all, with just images. Or just scroll so that images are the focus\n",
      "\n",
      "2\n",
      "834\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-09 15:32\n",
      "So, you suggest do develop one with text for people to work though it at home afterwards, and drop the text for the presentation? Which one do we distribute then for the tutorial? The no-text one?\n",
      "\n",
      "1\n",
      "835\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-09 15:39\n",
      "do we have a real-world dataset that uses dict-vectorizer? Or any real-world dataset that has categorical features encoded as strings? Btw, what is the recommended method to deal with that? Pandas? Or converting to dict and DictVectorizer?\n",
      "\n",
      "1\n",
      "836\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-09 15:40\n",
      "I would say it depends how much we reuse old tutorials. If it is all new (unlikely) generating that much content will be tough - in that case I would go with pics only. But if we mostly reuse I think we should do text + images in main notebooks we put in tutorial repo, then just have a separate folder of \"presentation ready\" which is basically titles, pictures and key math or whatever else is necessary to talk on. Or vice-versa (text versions in a separate folder called like \"self_teaching\") to avoid confusion\n",
      "\n",
      "2\n",
      "837\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-09 15:46\n",
      "I'd go with the \"self teaching\" approach\n",
      "\n",
      "1\n",
      "838\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-09 15:46\n",
      "yeah. that way it takes extra effort to open the wrong content :) though I am sure it will happen anyways...\n",
      "\n",
      "1\n",
      "839\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-09 21:27\n",
      "@kastnerkyle I don't see where cross-validation is introduced in the notebooks... hum...\n",
      "\n",
      "1\n",
      "840\n",
      "53135b495e986b0712efc453\n",
      "2015-06-10 09:37\n",
      "``` from sklearn.utils.extmath import fast_dot from sklearn.exceptions import EfficiencyWarning import warnings warnings.simplefilter('always', EfficiencyWarning) import numpy as np fast_dot(np.array([1, 2, 3]), np.array([4, 5, 6])) ``` This code which is supposed to raise a warning doesn't raise one!  However this one does -_- ``` from sklearn.utils.extmath import _fast_dot from sklearn.exceptions import EfficiencyWarning import warnings warnings.simplefilter('always', EfficiencyWarning) import numpy as np _fast_dot(np.array([1, 2, 3]), np.array([4, 5, 6])) ```  Ref: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L151\n",
      "\n",
      "1\n",
      "841\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-10 13:35\n",
      "The GMM model in http://scikit-learn.org/stable/modules/dp-derivation.html is not standard. The distribution of \\mu_k  does not have variance depending on the Gamma distribution. It is just a constant. I think that is why the current implementation is kind of weird....\n",
      "\n",
      "1\n",
      "842\n",
      "53135b495e986b0712efc453\n",
      "2015-06-10 23:00\n",
      "The `NonBLASDotWarning` (to be converted to `EfficiencyWarning`) doesn't seem to work as intended!\n",
      "\n",
      "1\n",
      "843\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-10 23:01\n",
      "how do you mean?\n",
      "\n",
      "1\n",
      "844\n",
      "53135b495e986b0712efc453\n",
      "2015-06-10 23:02\n",
      "Please check the above code sample :)\n",
      "The `EfficiencyWarning` in the above code is supposed to be `NonBLASDotWarning`...\n",
      "\n",
      "2\n",
      "845\n",
      "53135b495e986b0712efc453\n",
      "2015-06-10 23:05\n",
      "It is being disabled at [utils/validation.py](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py#L42).. maybe it has something to do with it?\n",
      "Nope I tried removing that line and this issue still persists!\n",
      "\n",
      "2\n",
      "846\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-10 23:07\n",
      "I am not very good with working with the warnings registry.\n",
      "\n",
      "1\n",
      "847\n",
      "53135b495e986b0712efc453\n",
      "2015-06-10 23:08\n",
      "hmm okay! Thanks for looking into it :) Does this issue seem worthy enough  for a new issue on github?\n",
      "\n",
      "5\n",
      "848\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-10 23:35\n",
      "I noticed that gmm code is a little messy. Some functions should be decomposed, and the way of initializing parameters is not completely implemented. According to the derivation draft, different estimators of responsibility, weight, mean, covariance could be combined together to represent GMM, BGMM, DPGMM. I think, in terms of maintainability,  it is good to create bunches of estimators with inheritance and then combine them on demand in  just use one class to represent all three models with four types of covariance. Is that a good idea?\n",
      "\n",
      "1\n",
      "849\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-10 23:51\n",
      "usually we try to restrict inheritance to mixins, so that there is no complex overloading of functions. If you can implement it by providing mixins for the four covariance types, that would be great. I'm not sure how well these factorize, though.\n",
      "@kastnerkyle it would be great if you could give me your feedback on my redoing of the notebooks.\n",
      "\n",
      "2\n",
      "850\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-11 00:11\n",
      "OK looking now. Key question - does StarCluster still work?\n",
      "\n",
      "1\n",
      "851\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-11 00:12\n",
      "I was under the impression that it broke at some point a ways back but I am not super informed on these things\n",
      "The hashing stuff / out-of-core stuff seems like maybe we should do it *before* parallel. It is quite nice I think. Now onto rendered_notebooks\n",
      "I think we should focus on py 3.4 compat - seeing at least a few print blah in there. I can work on that if you want?\n",
      "Also... describing all the different types of sparse matrices may not be pleasant in the intro. Maybe leave them there but gloss over subtleties of different representations? If you know guts I am all for it - but I am definitely a high-level sparse matrix user. I use what works normally and check stack overflow\n",
      "\n",
      "4\n",
      "852\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-11 00:41\n",
      "Should we add an image telling what petal and sepal are? Or just say what it is. For sure *somebody* will ask  :)\n",
      "\n",
      "1\n",
      "853\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-11 07:54\n",
      "@xuewei4d do you think changing the handling of the \\mu_k will have a significant impact on the runtime performance? Besides Bishop's PRML do you have another \"standard\" formulation / derivation of the Variational GMM in mind?\n",
      "+1 for not introducing a deep hierarchy of estimators for GMMs but ok for using mixin class and possibly a _BaseGMM abstract base class if that can help factoring too redundant code.\n",
      "@kastnerkyle I have not tried to launch a StarCluster instance in a long time but the project is still active. The configuration might have changed a bit (e.g. AMIs ids) so it should be possible to adapt it to make it run again. However I would not do that during a tutorial if you are not a regular user yourself. You can just mention that it exists in passing.\n",
      "it works here\n",
      "\n",
      "4\n",
      "854\n",
      "53135b495e986b0712efc453\n",
      "2015-06-11 10:56\n",
      "Is it just me or this page takes forever to load :@ - I've been trying since yesterday ;( https://travis-ci.org/scikit-learn/scikit-learn/jobs/66302262\n",
      "\n",
      "1\n",
      "855\n",
      "53135b495e986b0712efc453\n",
      "2015-06-11 11:33\n",
      "Thanks a lot for checking :)\n",
      "\n",
      "3\n",
      "856\n",
      "53135b495e986b0712efc453\n",
      "2015-06-11 11:35\n",
      "Thankss a ton for this txt log link! Will be really useful for me! :D and yea it does work :)\n",
      "\n",
      "1\n",
      "857\n",
      "53135b495e986b0712efc453\n",
      "2015-06-11 11:37\n",
      "Hey @ogrisel BTW could you check why  ``` from sklearn.utils.extmath import fast_dot from sklearn.exceptions import NonBLASDotWarning import warnings warnings.simplefilter('always', NonBLASDotWarning) import numpy as np fast_dot(np.array([1, 2, 3]), np.array([4, 5, 6])) ``` doesn't raise the intended warning? Am sure I must be missing something simple here :|\n",
      "\n",
      "1\n",
      "858\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-11 11:55\n",
      "@rvraghav93 which PR number is this again?\n",
      "\n",
      "3\n",
      "859\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-11 12:03\n",
      "this warning is only raised for old versions of numpy\n",
      "< 1.7\n",
      "if np_version < (1, 7, 2) and _have_blas_gemm()\n",
      "\n",
      "6\n",
      "860\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-11 13:40\n",
      "@ogrisel @amueller pyspark now deserves a mention along with https://pypi.python.org/pypi/sparkit-learn/0.1 I think.\n",
      "Would be of interest for many of the same people who care about StarCluster\n",
      "\n",
      "2\n",
      "861\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-11 13:50\n",
      "also useful, although not directly related: - http://yelp.github.io/MOE/ - http://pythonhosted.org/airflow/\n",
      "I have not yet found to the time to test any of those\n",
      "\n",
      "2\n",
      "862\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-11 14:26\n",
      "@ogrisel The distinction is that whether \\mu_k depends on \\Lambda_k. PRML and MLAPP(P750) models that dependence. In the literature, some work has that ,some work does not. I think those are two kinds of modeling. Modeling the dependence would give more accurate approximation. The exercise 10.20 in PRML says if you have many data, the pdf of q(\\mu, \\Lambda) will become a delta function, which recover the classic EM algorithm. But the pdf of current variational distribution will not, since the variance of \\mu is fixed.  For refactoring, I would intend to build a BaseGaussianMixtureModel and with different estimators for different variables. For example, there are 8 covariance estimators. full, diag, spherical, tied, times variational or not. Then GaussianMixtureModel could be implemented by inheriting from base class and combined with one of 8 estimators for covariance variables. I don't know estimators should be taken as a mixin class. I would prefer to let GMM includes the estimators.\n",
      "\n",
      "1\n",
      "863\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-11 14:29\n",
      "I would rather keep the covariance choice as an hyper-parameter of the class instead of dedicating it a sub-class.\n",
      "side remark, I think for the full covariance type, it might be interesting to experiment with a shrinkage estimator such as ledoit-wolf, at least in the Maximum likelihood / EM formulation. If that can improve the cross-validated log-likelihood we might consider it for inclusion.\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.covariance.ledoit_wolf.html\n",
      "\n",
      "3\n",
      "864\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-11 14:32\n",
      "so the GMM would be like ```class GMM(FullCovMixin, BaseGMM)```,```class GMM(DiagCovMixin, BaseGMM)``` ?\n",
      "OK. I will try to build some prototypes :)\n",
      "\n",
      "2\n",
      "865\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-11 14:34\n",
      "@xuewei4d please feel free to open a [WIP] PR with that refactoring with mixin / base classes so that we can have a discussion on concrete code.\n",
      "Also private classes should start with a `_`.\n",
      "\n",
      "2\n",
      "866\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-11 14:37\n",
      "So what about the approximation? Do you like to consider the dependence?\n",
      "OK.\n",
      "\n",
      "2\n",
      "867\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-11 14:43\n",
      "@kastnerkyle the \"rendered notebooks\" are still the old ones. I haven't redone them. I agree we should do more out-of-core stuff before doing clusters.\n",
      "@kastnerkyle I am currently somewhere around 3. and 4.\n",
      "\n",
      "2\n",
      "868\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-11 14:46\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.covariance.ledoit_wolf.html The section 'notes' seems has a little problem. '+' sign is interpreted as a list mark.\n",
      "\n",
      "1\n",
      "869\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-11 14:53\n",
      "> So what about the approximation? Do you like to consider the dependence?  It depends how the runtime, memory usage and the  complexity of the code base will evolve if we do so :) I am still clear about the details. Reading PRML at the moment. It would be great if we could have an example that demonstrates that the VBGMM asymptotically joins the solution of the MLE estimate on some toy dataset\n",
      "leveraging ledoit wolf shrinkage is not a priority as we already have the `min_covar` hack to regularize the covariance estimation. But I think we should keep it in mind and it would be great to explore the importance of covariance regularization, especially on high dim data\n",
      "\n",
      "2\n",
      "870\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-11 15:05\n",
      "Great. I would continue to work on the PR #4802\n",
      "\n",
      "1\n",
      "871\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-11 15:11\n",
      "I think we should also improve the examples to discuss model selections for GMMs. I started to run some experiments here: https://github.com/ogrisel/notebooks/blob/master/gmm/Model%20Selection%20for%20GMM.ipynb\n",
      "I have to go offline now, see you later.\n",
      "\n",
      "2\n",
      "872\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-11 15:22\n",
      "I tried some toy experiments with VBGMM before, but it did not work correctly. May I ask why would you like to do model selection for GMM? See you\n",
      "\n",
      "1\n",
      "873\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-11 15:25\n",
      "@amueller I noticed the other ones were shorter and swapped. Only got to 2.x but I can try to do some PRs tonight.\n",
      "\n",
      "1\n",
      "874\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-11 15:28\n",
      "@kastnerkyle cool :) I'm working on the intro to unsupervised currently\n",
      "@xuewei4d could you have a look at #4845 ?\n",
      "\n",
      "3\n",
      "875\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-11 15:48\n",
      "Where is the discussion about #4511?\n",
      "I mean on the ML. Thanks~\n",
      "\n",
      "2\n",
      "876\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-11 15:52\n",
      "the conclusion was that we want to raise a value error\n",
      "that is deprecate it for now and raise a value error in the future\n",
      "@xuewei4d https://sourceforge.net/p/scikit-learn/mailman/scikit-learn-general/thread/20150501175859.GE1362450%40phare.normalesup.org/#msg34075913\n",
      "\n",
      "3\n",
      "877\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-11 20:43\n",
      "@ogrisel what do you think about adding a ``shuffle`` option to cross_val_score and GridSearchCV (@jnothman or anyone else feel also free to chime in;) ?\n",
      "I feel it would be useful for cv=integer\n",
      "\n",
      "2\n",
      "878\n",
      "5537027215522ed4b3df56ab\n",
      "2015-06-12 04:06\n",
      "Hey guys, does anyone else have issues with big memory usage because of Python dicts going crazy with high dimentional data for text vectorizers?\n",
      "I was gonna implement it as a Trie, but it would be a big change and maybe change external API unless we do a dict abstraction\n",
      "i went from 50gb used to 200mb\n",
      "so no need to use hashingvect\n",
      "yeah looks like it's not happening\n",
      "\n",
      "5\n",
      "879\n",
      "5537027215522ed4b3df56ab\n",
      "2015-06-12 04:12\n",
      "another tangentially related question... since the cost of creating a vectorizer with millions of features is so large in terms of memory, there is a need for preemptive feature selection\n",
      "i mean for me... things like forward selection, mcmc based, etc. I am not sure if it belongs in scikit-learn\n",
      "this is to avoid generating the large matrix with unnecessary features when they will likely be discarded anyway\n",
      "\n",
      "3\n",
      "880\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-12 12:11\n",
      "@lqdc I like the idea of the Trie - even if we just called it something else (in case where it can't match external API) it sounds insanely useful. As for pre-emptive feature selection it would be cool if this can be done in a general way, but all the tricks I know are domain specific. Any ideas in that direction seem nice, since it is a real-world issue\n",
      "\n",
      "1\n",
      "881\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-12 14:52\n",
      "@lqdc there is some very simple feature selection based in min_df and max_df. But that needs to built the whole dictionary first.\n",
      "how large is your dictionary? The idea of a Trie came up before. So you actually implemented it?\n",
      "\n",
      "2\n",
      "882\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-12 15:56\n",
      "some discussion on ties here: https://github.com/scikit-learn/scikit-learn/issues/2639\n",
      "\n",
      "1\n",
      "883\n",
      "5537027215522ed4b3df56ab\n",
      "2015-06-12 17:00\n",
      "and yes, the feature selection would be for avoiding building the whole dict which perhaps wouldn't be a problem in the first place if we used a trie for medium-sized datasets\n",
      "\n",
      "1\n",
      "884\n",
      "5537027215522ed4b3df56ab\n",
      "2015-06-12 17:06\n",
      "I don't remember the size of the dictionary exactly but it was taking up 50ish gb\n",
      "\n",
      "1\n",
      "885\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-12 17:10\n",
      "what do you mean by \"looks like it's not happening\"?\n",
      "the question is a bit how complex the code is and the speed and memory compared to C++ dicts and python dicts\n",
      "\n",
      "2\n",
      "886\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-12 17:17\n",
      "in the PR, the memory footprint was 1/3 of CountVectorizer. You reported 1/200 above, right?\n",
      "\n",
      "3\n",
      "887\n",
      "5537027215522ed4b3df56ab\n",
      "2015-06-12 17:20\n",
      "      CountVectorizer(): 94MB;     CountVectorizer(ngram_range=(1,2): 666MB;     MarisaCountVectorizer(): 1.2MB;     MarisaCountVectorizer(ngram_range=(1,2)): 13.3MB;\n",
      "also he was doing it in a way that doesn't help my casse. basically  he made the python dict fist and then populated a trie with the dict then replaced the dict with the trie\n",
      "the use case he has is build the vocab on a beefy machine then when you actually want to use it, unpickle and use\n",
      "\n",
      "3\n",
      "888\n",
      "5537027215522ed4b3df56ab\n",
      "2015-06-12 17:32\n",
      "I was also using MARISA trie\n",
      "\n",
      "1\n",
      "889\n",
      "5537027215522ed4b3df56ab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-06-12 17:44\n",
      "but building it from the streaming input. anyway, as @larsmans put it in that thread, there is understandable hesitation of merging a large c++ dependency that no one fully understands besides people who wrote the trie I assume.\n",
      "\n",
      "1\n",
      "890\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-12 18:32\n",
      "well but allowing it as an optional replacement might work\n",
      "\n",
      "1\n",
      "891\n",
      "544879efdb8155e6700cdb21\n",
      "2015-06-12 19:39\n",
      "Hello, I have a question about pull requesting policy. I have a PR and I made some changes, what I should to do with new changes:  add new commit to PR, amend initial commit? I used to use amended commits, it keeps history look nice, but PR discussions seems ambiguous because of `outdated diffs`.\n",
      "\n",
      "4\n",
      "892\n",
      "544879efdb8155e6700cdb21\n",
      "2015-06-13 01:29\n",
      "Hello, I'm not sure that gitter is proper place to ask this question, if so please point me the right place.  I have a problem with building the whole project, when I do `make` i get this tests result: `FAILED (SKIP=14, errors=2)`  ``` ERROR: sklearn.tests.test_cross_validation.test_cross_val_score_pandas ... IndexError: arrays used as indices must be of integer (or boolean) type ```  pandas.__version__ = 0.13.1 numpy.__version__ = 1.6.2  Thank you\n",
      "\n",
      "1\n",
      "893\n",
      "53135b495e986b0712efc453\n",
      "2015-06-13 12:53\n",
      "Hey @mr0re1 ... do you have a particular PR number for which this test fails... or incase you haven't already, could you just raise one so we can all see the modified code that caused this test failure? :)\n",
      "\n",
      "1\n",
      "894\n",
      "53135b495e986b0712efc453\n",
      "2015-06-13 13:23\n",
      "And btw this is indeed the place to ask such questions :)\n",
      "\n",
      "1\n",
      "895\n",
      "544879efdb8155e6700cdb21\n",
      "2015-06-13 15:44\n",
      "Tests fail in master branch. There is no my changes. It seems like problem is in my environment.\n",
      "\n",
      "1\n",
      "896\n",
      "551418d115522ed4b3dddd7b\n",
      "2015-06-13 15:53\n",
      "@mr0re1 the problem might be in sklearn. Testing doesn't cover all possible combinations of platforms and third-party libraries. I suggest you create an issue and put all debug information about failed testcases\n",
      "\n",
      "1\n",
      "897\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-13 16:32\n",
      "@mr0re1, that test behaves differently whether pandas is installed or not. I assume the error occurs on the Pandas-only code path. Pandas is known to break API often. This is a problem with the test, in my opinion, but your installation seems to be mostly functional. The test might pass if you update pandas. Still, could you raise an issue for this on github? It's something I think we should fix.  What is the other error, by the way? You said you get 2 errors but only showed us one.\n",
      "@rvraghav93, since this is a cross-validation indexing issue, could you look into it? If you use anaconda you can easily set up a virtual environment with a specific version of pandas.\n",
      "also, @rvraghav93, when you said PR number above, did you mean opening a Github issue?\n",
      "Ah I see\n",
      "@mr0re1 what test do you get the error for, I mean?\n",
      "\n",
      "5\n",
      "898\n",
      "53135b495e986b0712efc453\n",
      "2015-06-13 16:33\n",
      "Sure I'll look into it :)\n",
      "\n",
      "1\n",
      "899\n",
      "544879efdb8155e6700cdb21\n",
      "2015-06-13 16:34\n",
      "@vene, will open an issue, the second error is the same.\n",
      "\n",
      "1\n",
      "900\n",
      "53135b495e986b0712efc453\n",
      "2015-06-13 16:35\n",
      "Since @mr0re1 was asking about raising a PR a few chats above, I assumed he had been working on something which had failed the tests ;) So I suggested that he raise a PR... not that the failure is in master... that is moot :P\n",
      "*now\n",
      ":D\n",
      "\n",
      "3\n",
      "901\n",
      "53135b495e986b0712efc453\n",
      "2015-06-14 13:44\n",
      "This [link](http://eprints.pascal-network.org/archive/00006964/01/vedaldi10.pdf) which is a reference to additive chi2 sampler seems to be down!\n",
      "There was an issue regarding broken links right?\n",
      "#4344\n",
      "This [comment](https://github.com/scikit-learn/scikit-learn/issues/4344#issuecomment-77744354) in particular!\n",
      "\n",
      "4\n",
      "902\n",
      "53135b495e986b0712efc453\n",
      "2015-06-14 20:40\n",
      "chatting from the gitter irc bridge!\n",
      "\n",
      "1\n",
      "903\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-14 20:44\n",
      ":)\n",
      "\n",
      "1\n",
      "904\n",
      "53135b495e986b0712efc453\n",
      "2015-06-14 20:49\n",
      "Its cool!! I am planning to resume work on the irc-gitter bridging thing that we discussed long back (when gael wasn't happy with us using gitter over irc) when I find time!\n",
      "i.e apart from my regular work ;)\n",
      "\n",
      "2\n",
      "905\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-14 20:51\n",
      "I actually really like gitter's interface, what were Gael's concerns?\n",
      "\n",
      "1\n",
      "906\n",
      "53135b495e986b0712efc453\n",
      "2015-06-14 21:05\n",
      "That the discussions were getting diluted at multiple platforms... (gitter, irc, ml, github etc...)\n",
      "\n",
      "4\n",
      "907\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-14 21:08\n",
      "i like the auto-links to issues,  and the activity pane in gitter\n",
      "\n",
      "1\n",
      "908\n",
      "53135b495e986b0712efc453\n",
      "2015-06-14 21:09\n",
      "And history, code formatting, markdown too! ;)\n",
      "Speaking of activity pane :p @vene could you take a look at #4860 its a very minor PR! :)\n",
      "\n",
      "2\n",
      "909\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-14 21:28\n",
      "I left a comment there\n",
      "\n",
      "1\n",
      "910\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-15 00:12\n",
      "@rvraghav93 what do you plan to write about in this week's blog post?\n",
      "\n",
      "1\n",
      "911\n",
      "53135b495e986b0712efc453\n",
      "2015-06-15 00:14\n",
      "Nested CV!! Is that okay? Hey BTW do u happen to have a link of the mailing list thread that u had mentioned previously? I am unable to find it :/\n",
      "And I'll resume work on cross validation as soon as I am done with fit reset / partial fit tests... :)\n",
      "Okay and yes sure!!\n",
      "Okay!!\n",
      "Sure I'll do that! :)\n",
      "\n",
      "5\n",
      "912\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-15 00:28\n",
      "@rvraghav93 the discussion was here. I think this thread has some very valuable insight about evaluation in machine learning. http://sourceforge.net/p/scikit-learn/mailman/message/34102242/\n",
      "\n",
      "4\n",
      "913\n",
      "53135b495e986b0712efc453\n",
      "2015-06-15 00:38\n",
      "And I am 2 blog posts behind -_- nested CV was supposed to be last Sunday's ! For this sunday can I post about online learning / `partial_fit` support in sklearn and the usefulness of the proposed tests (in ensuring `partial_fit` behaves as expected)?\n",
      "\n",
      "1\n",
      "914\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-15 00:41\n",
      "That sounds good, I think you can do a smaller post about this mid-next week. This is more internal stuff, it's hard to write too much interesting stuff about it.\n",
      "Maybe talk about how useful it is to have common tests that enforce API conventions to future code as well\n",
      "\n",
      "4\n",
      "915\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-15 00:52\n",
      "also @rvraghav93 could you put down a brief plan for what you think we can get done in the next 2-3 weeks?\n",
      "A bit of structure might be useful, and I think this is better than just going back to the original timeline, since things are always in flow :)\n",
      "\n",
      "2\n",
      "916\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-15 18:06\n",
      "Any comment on #4802? I mean in terms of class definition. Most of methods are not implemented yet, but I hope I am on the right track.\n",
      "\n",
      "1\n",
      "917\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-15 18:42\n",
      "btw irc is totally unused during sprints\n",
      "^^\n",
      "btw @rvraghav93 what is the status of the data-dependent cv?\n",
      "sorry I was out for the weekend\n",
      "it would be great if you could catch up on the blog posts. The IRC bridge is really not that important\n",
      "and I saw there was a lot of discussion on the assert helpers. are they needed for any of the gsoc PRs?\n",
      "\n",
      "6\n",
      "918\n",
      "53135b495e986b0712efc453\n",
      "2015-06-15 19:09\n",
      "I just wanted to finish it off as it was lying around for a long time :/ I'll resume work on it by tomorrow :) Planning to finish off my blog post, the helpers and tests by today :)\n",
      "\n",
      "1\n",
      "919\n",
      "54f0341115522ed4b3dc8e00\n",
      "2015-06-16 06:24\n",
      "hi there\n",
      "\n",
      "1\n",
      "920\n",
      "54f0341115522ed4b3dc8e00\n",
      "2015-06-16 06:26\n",
      "im using gridsearch to train a visual object detection pipeline with a few skimage transformers, pca and svm... since i acquired my larger dataset, gridsearch explodes in a very strange way <unconvertable> deep in python: http://nopaste.ghostdub.de/?1132 <unconvertable> and i cant quite make any sense of that :( ... would anyone happen to have an idea in which direction i could search?\n",
      "\n",
      "1\n",
      "921\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-16 13:49\n",
      "@Nebukadneza try with n_jobs=1 that should make it easier to debug\n",
      "\n",
      "1\n",
      "922\n",
      "54f0341115522ed4b3dc8e00\n",
      "2015-06-16 14:26\n",
      "with single threading it doesnt happen at all :(\n",
      "\n",
      "1\n",
      "923\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-16 14:54\n",
      "does all of it run through? it is maybe one specific parameter setting that does that\n",
      "\n",
      "2\n",
      "924\n",
      "54f0341115522ed4b3dc8e00\n",
      "2015-06-16 15:09\n",
      "trying to dig into the delayed and multiprocessing pool with ipdb now Oo but i just dont see where this format-string-fu even comes from or what its used for Oo\n",
      "\n",
      "1\n",
      "925\n",
      "53135b495e986b0712efc453\n",
      "2015-06-16 20:18\n",
      "@vene Thanks for that ML link! It was really useful!!\n",
      "\n",
      "1\n",
      "926\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-16 21:35\n",
      "TIL: don't try to make a learning curve example with a non-parametric model\n",
      "\n",
      "1\n",
      "927\n",
      "53135b495e986b0712efc453\n",
      "2015-06-16 21:37\n",
      "Why do you say so? :)\n",
      "\n",
      "1\n",
      "928\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-16 21:37\n",
      "because the training error doesn't go up with the number of samples\n",
      "\n",
      "2\n",
      "929\n",
      "53135b495e986b0712efc453\n",
      "2015-06-16 23:08\n",
      "Hey Andy,  Earlier you had given me the feedback for my 1st blogpost! Sorry for the delay in response...  > why do you use alpha when plotting points?  That makes it look a little faded and better... I think this was a suggestion by someone when I was working on silhoute plot example ;) Do you feel I should remove that?  > I feel this sentence is unclear: \"Even when the model is optimized with the constrain of maximizing the score based upon the test set, there is still a chance of overfitting as the information about the test set can leak into the model and hence the model could be optimized for the test set alone.\" it would be more explicit to say the information leaks via the selection of hyperparameters.  Have updated so! Thanks!  > cross-validation does not entirely overcome this. Overfitting to cross-validation is harder than overfitting to a single test set, but it is still possible, which is why people do nested cross-validation.  Like Vlad said a few chats below yours, that was what I was getting at... I have reworded it to make it clear!  > grid.bestestimator.score(X_test, y_test) is also not great btw. You can just use grid.score  fixed :)  > talking about gamma=0 for the SVM is also a bit weird. This is an odd way that we used to select the default, which is 1. / n_features , I think.  I've fixed it to 0.5... which is (1 / n_features)  I am now working on the 2nd blog post (Nested CV) :)\n",
      "\n",
      "1\n",
      "930\n",
      "53135b495e986b0712efc453\n",
      "2015-06-16 23:29\n",
      "@amueller @vene I've also started a ML thread for my GSoC project to interact more! :)\n",
      "http://sourceforge.net/p/scikit-learn/mailman/message/34213648/\n",
      "\n",
      "2\n",
      "931\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-17 14:02\n",
      "@ogrisel do you have a second?\n",
      "\n",
      "1\n",
      "932\n",
      "5581814615522ed4b3e20c6a\n",
      "2015-06-17 14:18\n",
      "How scikit can help beginners?\n",
      "\n",
      "1\n",
      "933\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 14:18\n",
      "@BastinRobin http://scikit-learn.org/stable/tutorial/\n",
      "\n",
      "1\n",
      "934\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-17 14:27\n",
      "@amueller yes, sorry I was on the phone\n",
      "\n",
      "1\n",
      "935\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-17 14:29\n",
      "> TIL: don't try to make a learning curve example with a non-parametric model > because the training error doesn't go up with the number of samples  this is actually an interesting example if you constrast it with the learning curve of a parametric model\n",
      "\n",
      "1\n",
      "936\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-17 14:36\n",
      "yeah, but maybe to advanced for an introductory course.\n",
      "never mind, I was confused by the timing of https://github.com/scikit-learn/scikit-learn/pull/4844, but I think the code is just overly complicated\n",
      "\n",
      "2\n",
      "937\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 17:16\n",
      "Anybody around? :) I have a few (probably lame) questions regarding nested CV!\n",
      "\n",
      "1\n",
      "938\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 17:46\n",
      "hi @rvraghav93\n",
      "\n",
      "1\n",
      "939\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-17 17:49\n",
      "yeah I'm around\n",
      "I have a meeting in 10 minutes but I'll answer stuff here\n",
      "\n",
      "2\n",
      "940\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 17:53\n",
      "Why is nested CV not generically possible with our current API... I understand what needs to be done to make the iterators data indep. but I don't get why nested cv is difficult as Mathieu had commented in that issue description... `cross_val_score(GridSearchCV(est, ....))` would be enough right?\n",
      "\n",
      "1\n",
      "941\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 17:55\n",
      "this works currently if you use e.g. `cv=3` in the inner one\n",
      "\n",
      "6\n",
      "942\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-17 17:56\n",
      "or even n_samples. If the size of your dataset is not divisible by the cv in the outer loop, the training sets in the inner loop will have different lengths\n",
      "\n",
      "3\n",
      "943\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 17:58\n",
      "I can't even get my emoji right, I'm useless\n",
      "\n",
      "1\n",
      "944\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 17:59\n",
      "This is a general apology for all lame questions that may follow :P please bear with me!\n",
      "Ok so the inner loop does hyper param optimization using a separate cross validation and finds the best model which the outer loop uses to find the cross validated score... why should these two affect each other? ( Like apologized earlier, please bear with me :( )\n",
      "\n",
      "2\n",
      "945\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:00\n",
      "don't even think that you have a GridSearch on the inside\n",
      "consider the simpler case with just a cross_val_score inside a cross_val_score\n",
      "hi @xuewei4d\n",
      "what do you mean by current code?\n",
      "if it's in the context of GMMs I think commenting in your PR would work well\n",
      "since most people who could answer are already following the PR\n",
      "also you could use `git blame` and ping the author(s) of the piece of code\n",
      "\n",
      "7\n",
      "946\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 18:01\n",
      "[offtopic - I think gitter supports only a subset of the emojis ;) ]\n",
      "\n",
      "1\n",
      "947\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:01\n",
      "[the bracket messed it up I think: :baby: :baby_bottle: ]\n",
      "\n",
      "8\n",
      "948\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-17 18:01\n",
      "If I need to discuss  the current code, should I open a new issue or add comments on my working PR which is addressing the issues?\n",
      "\n",
      "1\n",
      "949\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 18:02\n",
      "and if its not too much trouble could you give me a line or two of code that will help me clearly understand this?\n",
      "\n",
      "3\n",
      "950\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 18:06\n",
      "you can edit gitter chat messages :)\n",
      "\n",
      "1\n",
      "951\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:07\n",
      "``` clf = GridSearchCV(grid, cv=KFold(n_samples=?, n_folds=3)) cross_val_score(clf, X, y, cv=KFold(n_samples=?, n_folds=3)) ```\n",
      "(ignore the fact that the parameter is currently called `n` and not `n_samples`)\n",
      "how would you fill the question marks?\n",
      "the one on the 2nd line is easy\n",
      "\n",
      "4\n",
      "952\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 18:10\n",
      "won't both be 100?\n",
      ":P\n",
      "ahhhh I get it !\n",
      "yes.. sory.. this is too lame :|\n",
      "thanks a lot :)\n",
      "\n",
      "6\n",
      "953\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:10\n",
      "no, because the inner `clf` (the GridSearchCV) only gets a fold each time\n",
      "if `len(X)` were `99`, you could set the first `?` to `66` I think\n",
      "\n",
      "2\n",
      "954\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:11\n",
      "but 1) this would be hacky 2) users shouldn't need to do this kind of error-prone math 3) this doesn't work at all for many cross-val strategies\n",
      "also the fact that you can delete and edit messages is a bit worrysome\n",
      "\n",
      "4\n",
      "955\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 18:12\n",
      "This is clear now!! Will finish up my blog and resume the work :) Thanks!!\n",
      "\n",
      "5\n",
      "956\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 18:15\n",
      "sure!! I'll publish in an hour or two! and also post it to our ML...\n",
      "btw if you have time could you check if the current implementation of the assert helpers look okay?\n",
      "\n",
      "5\n",
      "957\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:17\n",
      "EST (I think)\n",
      "it's 2pm\n",
      "2:17\n",
      "\n",
      "3\n",
      "958\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:28\n",
      "@amueller for #4844 do you get the same timing on your computer?\n",
      "\n",
      "1\n",
      "959\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-17 18:45\n",
      "@rvraghav93 btw, midterm is rather soon...\n",
      "\n",
      "1\n",
      "960\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-17 18:47\n",
      "@vene yeah. The student just showed me that doing array ** 2 is super-linear in the array size\n",
      "\n",
      "1\n",
      "961\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:47\n",
      "how about array *= array?\n",
      "\n",
      "2\n",
      "962\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:48\n",
      "even with chunks of size 2?\n",
      "how about size 1? That would just be a loop over the items\n",
      "\n",
      "5\n",
      "963\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:51\n",
      "I was thinking in #4844 there might be some indexing thing, but if you narrowed it down to np.pow, that's very wrong\n",
      "how about np.square?\n",
      "This works though (but adds the overhead of a copy)\n",
      "\n",
      "3\n",
      "964\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 18:54\n",
      "``` In [11]: %timeit Xco = X.copy(); Xco *= Xco The slowest run took 4.31 times longer than the fastest. This could mean that an intermediate result is being cached 1000 loops, best of 3: 969 <unconvertable> s per loop  In [12]: %timeit Xco = X.copy(); Xco ** 2 1000 loops, best of 3: 1.25 ms per loop  In [13]: %timeit Xco = X.copy() The slowest run took 4.42 times longer than the fastest. This could mean that an intermediate result is being cached 1000 loops, best of 3: 525 <unconvertable> s per loop ```\n",
      "`[elem * elem for row in Xco for elem in row]` is much slower\n",
      "\n",
      "2\n",
      "965\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 19:08\n",
      "@amueller you mean Gsoc midterm? it usually comes in July only right?\n",
      "\n",
      "3\n",
      "966\n",
      "53135b495e986b0712efc453\n",
      "2015-06-17 19:12\n",
      "Ah its from 26 - 3 july... How much would you like to see me completed with my goals before the midterm? I am thinking  1. model_selection refactoring 2. Data independent CV Iterators. 3. Multiple Metric support - I won't be able to complete this though :/  Does it look okay?\n",
      "\n",
      "1\n",
      "967\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-17 19:15\n",
      "I think 1&2 completed (in such a way in which there's a MRG branch where all tests pass and one can do nested CV nicely) would be good. But by MRG I mean MRG :)\n",
      "@amueller, what do you think?\n",
      "\n",
      "2\n",
      "968\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-17 20:12\n",
      "yeah 1 & 2 merged would be great\n",
      "I also thought there was something wrong with the slicing but I'm very confused now :-/\n",
      "and the problem is both in the squaring and in the outer product computation\n",
      "I feel stupid for not seeing what is happening\n",
      "\n",
      "4\n",
      "969\n",
      "53135b495e986b0712efc453\n",
      "2015-06-18 00:33\n",
      "@vene @amueller My second blog post - http://rvraghav93.blogspot.com/2015/06/gsoc-2015-psf-scikit-learn-nested-cross.html\n",
      "Please take a look and let me know your views!\n",
      "I'll also mail it to the ML as soon as I get a +1 from either of you! :)\n",
      "\n",
      "3\n",
      "970\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 03:15\n",
      "Thanks. I'll have a look tomorrow first thing in  the morning\n",
      "\n",
      "1\n",
      "971\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 14:57\n",
      "@tw991 I'm still not sure about #4844, I hope I have time to look at it soon. In the meantime, once #4874 is done, you could either look to finish https://github.com/scikit-learn/scikit-learn/pull/4539 which is boring but straight-forward, or investigate how to make sure the two k-means algorithms always give the same results in #2008, which is more interesting but also more involved\n",
      "\n",
      "1\n",
      "972\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:10\n",
      "@rvraghav93 I think the book in your blog post is just by Petersohn, and John Vogt Verlag is the publisher (Verlag means something like publisher in German I think)\n",
      "\n",
      "4\n",
      "973\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 15:13\n",
      "yeah\n",
      "if anything I'd try to reference esl\n",
      "(which is what I always reference for everything)\n",
      "\n",
      "3\n",
      "974\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:13\n",
      "for cross-val stuff I started referencing Gilles' thesis :)\n",
      "\n",
      "2\n",
      "975\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:16\n",
      "Me neither, not fully, just the first two parts, but what I've read is great, very readable. Better than most ML textbooks.\n",
      "\n",
      "8\n",
      "976\n",
      "5582dd0915522ed4b3e21b33\n",
      "2015-06-18 15:18\n",
      "Hi, can anyone point me in the direction of a explanation of how to do model ensembling?\n",
      "\n",
      "1\n",
      "977\n",
      "53135b495e986b0712efc453\n",
      "2015-06-18 15:20\n",
      "Okay! thanks for the feedback :) @Callipygian0 http://scikit-learn.org/stable/modules/ensemble.html\n",
      "\n",
      "1\n",
      "978\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:20\n",
      "Sure! I'm still reading through the blog post so give me a few more minutes please\n",
      "\n",
      "3\n",
      "979\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:22\n",
      "@Callipygian0 If you mean ensembling heterogeneous models, you might want the [VotingClassifier](http://scikit-learn.org/dev/modules/ensemble.html#votingclassifier) in the current development version, not available in the last stable release.\n",
      "so, @rvraghav93 I'd actually not manually calculate `n_samples` that way\n",
      "If you want to show an example that's currently possible, I would use `cv=3` instead\n",
      "this currently does stratified kfold if given a classification dataset\n",
      "(I think)\n",
      "and then you can say something like...\n",
      "if you needed more customization, or a different CV such as LeaveOneLabelOut, it wouldn't work anymore\n",
      "oh I just noticed you do nested CV with a for loop\n",
      "\n",
      "9\n",
      "980\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 15:29\n",
      "@rvraghav93 I wouldn't say cleaner way in the first sentence. Many things are straight up not possible. I would say to enable cross-validation with cross-validation objects. Or make it more flexible....\n",
      "@Callipygian0 do you want to ensemble heterogeneous models? Then there is nothing to help you. Though implementing a voting classifier is pretty trivial.\n",
      "@rvraghav93 : \" In each iteration (split), the dataset (X, y) is partitioned into training, validation set.` \" should be \"into a training and a validation set\" also there is a backtick\n",
      "the no should be number.\n",
      "I feel the split in bullet points is not very clear\n",
      "well that could work...\n",
      "@vene not sure what you mean\n",
      "@rvraghav93 I'm not sure it is necessary do mention RandomizedSearchCV, it is pretty unrelated to the issue, right?\n",
      "well yeah\n",
      "\n",
      "9\n",
      "981\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:31\n",
      "also this thing \"This becomes necessary especially when the dataset is too small to split it into three\"\n",
      "it kind of depends on your audience, but technically, it's never too small (unless you have two samples)\n",
      "I fully agree, I just want to phrase it more clearly\n",
      "technically if the data is truly IID, it wouldn't matter, would it?\n",
      "\n",
      "4\n",
      "982\n",
      "5582dd0915522ed4b3e21b33\n",
      "2015-06-18 15:32\n",
      "I had all my models vote but it didnt really seem to work very well. This is my first machine learning experience so i'm very new! There is a kaggle style competition at my work. I was doing very well with GBM but everyone is overtaking me with ensemble now!\n",
      "\n",
      "4\n",
      "983\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:36\n",
      "So, say I have a bunch of data that I want to model. I'll leave out, say, 25% as a test set, do GridSearchCV on the train set, and report the score on the test set. This is probably the most straightforward way, right?\n",
      "But what if I got lucky with my choice of test set?\n",
      "usually yes\n",
      "well if your model is perfect and if your data is IID you can have small variance, right?\n",
      "\"The final result of the nested CV is the collection of n best Models\"\n",
      "unless the 3rd point is exactly in the middle of the two test points\n",
      "hmm\n",
      "you're right\n",
      "anyway I was only trying to say that it might not be clear what \"too few samples to leave a test set out\" means\n",
      "it is indeed a question of variance\n",
      "and because of the variance, you can get particularly lucky or unlucky if you do one single outer fold\n",
      "on small data\n",
      "\n",
      "12\n",
      "984\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 15:37\n",
      "the variance will just be very high with a small dataset, right?\n",
      "\n",
      "1\n",
      "985\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 15:38\n",
      "@rvraghav93 The example with the code is pretty clear, why I feel the initial explanation is not.\n",
      "\n",
      "1\n",
      "986\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 15:39\n",
      "@rvraghav93 However, I wouldn't say you need to estimate n_samples. It is not something you are guessing or using statistical methods on. You just need to know it. but you can't with the current setup\n",
      "@vene I don't think it is. If you have three data points sampled from a Gaussian, you use two for training and one for test, and your model is fitting a gaussian to it, the variance of the log-likelyhood will be high\n",
      "*likelihood\n",
      "even though the data is perfectly iid and you are using the true model\n",
      "well if you have three points in a line and do 3-fold cross valiation ....\n",
      "variance of the estimate scales with the number of samples and cross-validation gives an unbiased estimate of the variance iirc\n",
      "\n",
      "6\n",
      "987\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:52\n",
      "so iris is small, and the outer CV variance in @rvraghav93's example is not large\n",
      "\n",
      "1\n",
      "988\n",
      "53135b495e986b0712efc453\n",
      "2015-06-18 15:54\n",
      "By variance do you mean to say the variance in the hyper param points or the performance of the best models?\n",
      "\n",
      "3\n",
      "989\n",
      "53135b495e986b0712efc453\n",
      "2015-06-18 15:55\n",
      "> \"The final result of the nested CV is the collection of n best Models\"  You were going to say something about this right?\n",
      "\n",
      "2\n",
      "990\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:56\n",
      "I'm fond of LeaveOneLabelOut :)\n",
      "but that works if you do cv=4, no?\n",
      "\n",
      "2\n",
      "991\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 15:56\n",
      "if you use StratifiedKFold with k=4 on iris\n",
      "it will already be impossible\n",
      "yes\n",
      "let's say you want to shuffle, though ;)\n",
      "or you want KFold\n",
      "\n",
      "12\n",
      "992\n",
      "53135b495e986b0712efc453\n",
      "2015-06-18 15:59\n",
      "what would stratification look like on regression?\n",
      "\n",
      "1\n",
      "993\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 15:59\n",
      "that might suggest that the problem is not shuffling by default\n",
      "\n",
      "2\n",
      "994\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 16:01\n",
      "I'm just saying the phrasing should be such that it doesn't lead to \"just make shuffling default\"\n",
      "but instead to \"oh, we need to be able to tweak parameters of the CV object\"\n",
      "you can't?\n",
      "can't you use cv=4 both outside and inside?\n",
      "ah, yes\n",
      "so say you want more iterations for the inner loop?\n",
      "first show an example with cv=4 both outside and inside\n",
      "?\n",
      "btw, what's the intuition when choosing between StratifiedKFold with [Stratified?]ShuffleSplit?\n",
      "the docs just say \"finer control on the number of iterations and the proportion of samples\"\n",
      "I just shuffle first and do StratifiedKFold usually\n",
      "\n",
      "11\n",
      "995\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 16:01\n",
      "I was thinking about adding a shuffle keyword to GridSearchCV and cross_val_score at multiple points\n",
      "alright\n",
      "but wait\n",
      "no\n",
      "the argument should be different\n",
      "if you use cv=4 on the outside with iris\n",
      "you can't use shuffle split on the inside\n",
      "I'm stupid\n",
      "yes. But you can't use shuffle split on the inside\n",
      "because shuffle split needs the number of samples\n",
      "I think that is the right example\n",
      "alright\n",
      "\n",
      "12\n",
      "996\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 16:26\n",
      "well, you can control the number of repetitions and the test set size independently\n",
      "because having too small a test-set will also give you a bad estimate\n",
      "\n",
      "2\n",
      "997\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 18:06\n",
      "oh man, perfect post for a coffee break: http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-neural.html\n",
      "\n",
      "1\n",
      "998\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-18 18:09\n",
      "loving the low level ones\n",
      "\n",
      "1\n",
      "999\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-18 18:11\n",
      "I think I'll get a print of this for the office: http://1.bp.blogspot.com/-XZ0i0zXOhQk/VYIXdyIL9kI/AAAAAAAAAmQ/UbA6j41w28o/s1600/building-dreams.png\n",
      "\n",
      "1\n",
      "1000\n",
      "5576063e15522ed4b3e19cc3\n",
      "2015-06-18 18:28\n",
      "@amueller I updated the #4874. would you like to take a look?\n",
      "\n",
      "1\n",
      "1001\n",
      "53135b495e986b0712efc453\n",
      "2015-06-18 23:57\n",
      "@amueller @vene I've revised the blog post based on all your suggestions... However the example is still an illustration of a working nested CV. I thought once the data indep iterator work is over... I will show an example using LOLO to highlight the benefit of having data independence! Please take a look now :) http://rvraghav93.blogspot.in/2015/06/gsoc-2015-psf-scikit-learn-nested-cross.html\n",
      "\n",
      "1\n",
      "1002\n",
      "53135b495e986b0712efc453\n",
      "2015-06-19 00:03\n",
      "Okay I'll sleep and finish this data indep before sunday :punch:   My next blog on mon/tue should be on nested CV using the data indep LOLO!! :) good night!!\n",
      "BTW is the 3 part explanation still verbose?\n",
      " Okay :) will add one!!\n",
      "\n",
      "3\n",
      "1003\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-19 14:17\n",
      "@rvraghav93 I'll have a look in a bit. I still think you should definitely also show an example that is currently not working. I feel the current breakage is pretty bad and showing it emphasizes how important your work is\n",
      "\n",
      "1\n",
      "1004\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-19 15:19\n",
      "Hi, I adjusted the displaying of verbose messages in ```GaussianMixture```, what do you think? The extra messages displayed when verbose=2 compared to those when verbose=1 are put the same line with 'Iteration'. I think that would be more clear. @ogrisel @amueller\n",
      "[Comment](https://github.com/scikit-learn/scikit-learn/pull/4802#issuecomment-113303471)\n",
      "\n",
      "2\n",
      "1005\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-19 15:30\n",
      "I have finished the most methods of ```_MixtureBase``` and methods of ```GaussianMixture``` related to full covariances. I think it's time to write tests before further implementation. In the master branch, the tests is implemented with ```unittest.TestCase```, while simple testing functions for other modules. Which one should I use?\n",
      "I think displaying verbose messages makes ```fit``` a little messy. Any better idea?\n",
      "\n",
      "2\n",
      "1006\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-19 15:35\n",
      "@xuewei4d We usually prefer simple functions for testing. If you feel that using TestCase would simplify the testing code or make it more clear, you can use that, too.\n",
      "\n",
      "1\n",
      "1007\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-19 16:05\n",
      "Okay.\n",
      "\n",
      "1\n",
      "1008\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-22 17:39\n",
      "@xuewei4d btw, please also ping @lesteve when asking gmm questions, he is @ogrisel's co-mentor ;)\n",
      "\n",
      "1\n",
      "1009\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-22 18:00\n",
      "Okay. I forgot to ping him in the last comment.\n",
      "\n",
      "1\n",
      "1010\n",
      "53135b495e986b0712efc453\n",
      "2015-06-22 18:12\n",
      "@vene @amueller status update... the main part of data independent refactor is done.... all the new split / validate tests pass... working on grid search tests :) Will do the documentations and examples after pushing the grid search tests and an initial round of reviews (so as to finalize implementation) :)\n",
      "one minor question\n",
      "will ask at the PR itself...\n",
      "\n",
      "3\n",
      "1011\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-22 18:20\n",
      "Thanks @rvraghav93. I am super busy right now but I'll review your blog post and PR in the afternoon\n",
      "\n",
      "2\n",
      "1012\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-22 18:40\n",
      "Hi @rvraghav93, thanks for the update! I'll do my best and try to review tonight as well.\n",
      "\n",
      "2\n",
      "1013\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-22 21:02\n",
      "I sent you detailed comments on the blog post per pm so as not to flood the channel. I still think it would be good to add an example that is currently impossible and will be possible with your contribution\n",
      "Has anyone ever seen gcc errors with printf in cython code?\n",
      "sklearn/manifold/_barnes_hut_tsne.c:7211:5: error: format not a string literal and no format arguments [-Werror=format-security]      printf(__pyx_k_t_SNE_Checking_tree_consistency);\n",
      "I get that on my box but not on travis.\n",
      "\n",
      "4\n",
      "1014\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-22 21:17\n",
      "@rvraghav93 so I should review #4294, right?\n",
      "no wait, that is not the right one\n",
      "\n",
      "2\n",
      "1015\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-22 21:19\n",
      "@rvraghav93 you should reference the issues you are working on in the blog post ;)\n",
      "\n",
      "1\n",
      "1016\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-22 21:54\n",
      "any reviewers for https://github.com/scikit-learn/scikit-learn/pull/4621 btw?\n",
      "or maybe https://github.com/scikit-learn/scikit-learn/pull/4840 ?\n",
      "\n",
      "2\n",
      "1017\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-22 22:46\n",
      " Sorry, tonight is a bit tough for me for a more thorough review. I'll be on top of things in the morning.\n",
      "\n",
      "2\n",
      "1018\n",
      "53135b495e986b0712efc453\n",
      "2015-06-22 23:52\n",
      "Thanks for the reviews! Will address them ASAP :)\n",
      "\n",
      "1\n",
      "1019\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-23 14:32\n",
      "@ogrisel and after barnes-hut and LDA you review the GP rewrite? ;)\n",
      "\n",
      "1\n",
      "1020\n",
      "556705cb15522ed4b3e10f84\n",
      "2015-06-23 14:39\n",
      "Hi @amueller  and @ogrisel, I tried to adress all comments in #4444 if you have time for a quick review :)\n",
      "\n",
      "1\n",
      "1021\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 14:45\n",
      "@amueller @vene Could I trouble you for a small doubt? - https://github.com/scikit-learn/scikit-learn/pull/4294#issuecomment-114523116\n",
      "\n",
      "3\n",
      "1022\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 14:49\n",
      "yes I think @amueller is right, returning cv.split(...) should do it\n",
      "is check_cv in utils?\n",
      "\n",
      "7\n",
      "1023\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 14:51\n",
      "Is there any reason why we can't have it to return the iterator?\n",
      "\n",
      "1\n",
      "1024\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 14:54\n",
      "so there are instances when the len(new_cv.split(X, y)) doesn't work?\n",
      "\n",
      "9\n",
      "1025\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 14:56\n",
      "not without expanding it right? I mean `new_cv.split(X, y)` returns an iterator...\n",
      "\n",
      "6\n",
      "1026\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 15:00\n",
      "and you only need to support the new classes. I get it now, this is good.\n",
      "\n",
      "1\n",
      "1027\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 15:00\n",
      "Sorry to pester again! but why do you think we can't have `cv, len_cv = check_cv(...)`... is it so we can find the length without generating the cv?\n",
      "That will make it support old cv classes too right?\n",
      "yes... but  check_cv(..) would... is that why you are suggesting so? :)\n",
      "oh! so you don't want it to `return cv.split(X, y, ...)`? Then fine!\n",
      "ahh! okay! thanks :)\n",
      "\n",
      "5\n",
      "1028\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-23 15:01\n",
      "what do you mean without generating the cv? the ``cv.n_folds(X, y, labels)`` desn't generate the cv, right?\n",
      "and the wrapper of the old classes can just call ``__len__``\n",
      "yeah\n",
      "well what do you mean by \"generate\"?\n",
      "\n",
      "10\n",
      "1029\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 15:02\n",
      "no, check_cv doesn't generate it either\n",
      "not until you iterate over what it returns\n",
      "right?\n",
      "\n",
      "3\n",
      "1030\n",
      "556705cb15522ed4b3e10f84\n",
      "2015-06-23 15:08\n",
      "@amueller  at least it is very similar. If I get it right for bin-packing we want to spread different weights into the smallest possible amount of bags (all bags having the same size). Here the number of bags (folds) is fixed and we want them to have (approximately) the same weight at the end.\n",
      "@amueller thanks for the review :)\n",
      "Yes, I had the same problem when I first needed that functionality.\n",
      "\n",
      "3\n",
      "1031\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-23 15:12\n",
      "I think you can show equivalence with bin-packing by binary searching the bin-size\n",
      "your heuristic was always adding to the smallest bin, right?\n",
      "yeah seems good. I suggested minor refactorings\n",
      "should we try to merge #4444 and #4583 before #4294?\n",
      "\n",
      "4\n",
      "1032\n",
      "556705cb15522ed4b3e10f84\n",
      "2015-06-23 15:14\n",
      "yes, starting with the biggest weights\n",
      "Yes, I'm correcting it at the moment, thanks a lot!\n",
      "\n",
      "4\n",
      "1033\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 15:23\n",
      "Sure\n",
      "\n",
      "1\n",
      "1034\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 15:24\n",
      "This gives me an idea for renaming the ugly `p` in `LeavePLabelOut` into `test_size`, WDYT?\n",
      "\n",
      "1\n",
      "1035\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-23 15:27\n",
      "I suggested n_labels\n",
      "(or n_groups if we rename)\n",
      "why test-size?\n",
      "n_test_labels?\n",
      "but ok, makes sense\n",
      "\n",
      "5\n",
      "1036\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 15:28\n",
      "that's the meaning of `test_size` in #4583\n",
      "if it's an integer\n",
      "it means you'll have `test_size` labels in the test set\n",
      "well, what I mean is, if `test_size` isn't right, we should change it in #4583\n",
      "`test_size` is a bit ambiguous (people could expect it to refer to samples, not labels)\n",
      "\n",
      "5\n",
      "1037\n",
      "556705cb15522ed4b3e10f84\n",
      "2015-06-23 15:30\n",
      "@amueller in the current implementation I sort tuples of (weight, corresponding labels). if I use np.bincout I will have to sort it anyway and keep the correspondence to the original labels, so the complexity would be the same, right?\n",
      "\n",
      "1\n",
      "1038\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 15:31\n",
      "`n_labels` seems good!!\n",
      "\n",
      "1\n",
      "1039\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 15:33\n",
      "How about for LeavePOut, would you suggest `n_samples`?\n",
      "I wouldn't\n",
      "wait, both have their problems\n",
      "@rvraghav93 you're very quick to agree with things :) let's think about it\n",
      "in #4583 `train_size` and `test_size` are natural because they're delegated up to `ShuffleSplit`. They're not the best names but they're the friendliest parametrization.\n",
      "(Also, these parameter names are kind of mixing up test and validation...)\n",
      "\n",
      "6\n",
      "1040\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 15:35\n",
      "ah! okay `test_size` it is!\n",
      ":P\n",
      "`p_samples` ?\n",
      "\n",
      "3\n",
      "1041\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-23 15:36\n",
      "@JeanKossaifi you have to sort it, the correspondence would be stored in ``unique_labels``\n",
      "the quadratic comes from looping over unique_labels and then doing fancy indexing. I want to avoid the fancy indexing\n",
      "\n",
      "2\n",
      "1042\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 15:38\n",
      "(It's hard to teach someone and ramble for 10min about validation vs test, and then you code test_size=0.25)\n",
      "But ignoring this, `test_size` makes sense for LeavePOut (one could imagine even supporting fractions and `train_size`, but that's not important)\n",
      "\n",
      "2\n",
      "1043\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 15:47\n",
      "There are currently no common tests for cross-validation generators, right?\n",
      "\n",
      "1\n",
      "1044\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 15:48\n",
      "there are! `tests/test_cross_validation`?\n",
      "which I've refactored  into `test_split.py` and `test_validate.py`\n",
      "\n",
      "6\n",
      "1045\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 16:10\n",
      "Do you find the name of [`_check_is_partition`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cross_validation.py#L1107) appropriate? I just got totally misled by it.\n",
      "\n",
      "1\n",
      "1046\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 17:01\n",
      "How about `_is_permutation_of_arange_n`?\n",
      "\n",
      "1\n",
      "1047\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-23 17:08\n",
      "from the docstring it checks whether it is a permutation\n",
      "so ``_is_permutation`` seems appropriate?\n",
      "\n",
      "15\n",
      "1048\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 17:11\n",
      "The thing is, after all the refactoring, we'll want it to return the new-style CV object, right?\n",
      "\n",
      "5\n",
      "1049\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-23 17:16\n",
      "will the API of the new CV objects ever be used outside of check_cv then?\n",
      "passing weights, labels (sample_props in general) to the cv generator\n",
      "\n",
      "2\n",
      "1050\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 17:17\n",
      "or we could have `get_cv_safe`?\n",
      "\n",
      "1\n",
      "1051\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 17:19\n",
      "I mean rename `check_cv` to `get_cv_safe` so ppl could use that?\n",
      "I have a crazy suggestion... why don't we takle `sample_weights` along with this? will it make the PR too big?\n",
      "tackle\n",
      "\n",
      "3\n",
      "1052\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 17:23\n",
      "The main todo yet to be done is `check_cv`... other are minor right? documentation must be the next big thing... fixing examples should be quite easy...\n",
      "\n",
      "1\n",
      "1053\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-23 17:30\n",
      "well yes, the way we now discussed check_cv there will be no changes to the estimatorCV code\n",
      "\n",
      "3\n",
      "1054\n",
      "53135b495e986b0712efc453\n",
      "2015-06-23 17:31\n",
      "yep! on it!\n",
      "\n",
      "2\n",
      "1055\n",
      "556705cb15522ed4b3e10f84\n",
      "2015-06-23 17:54\n",
      "@amueller Currently the shuffle parameter is not used given the heuristic used (I could shuffle the labels having a same weight but this case might not appear and the result could be misleading).\n",
      "--sorry with the new notation it would be the labels having the same number of samples :)\n",
      "Great, thanks a lot! :)\n",
      "@vene since you already saw the code, would you have time to take a look? :)\n",
      "\n",
      "4\n",
      "1056\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-23 19:38\n",
      "oh, right. sorry. brainfart. Then please remove the shuffle parameter.\n",
      "for shuffling people could use #4583\n",
      "\n",
      "2\n",
      "1057\n",
      "556705cb15522ed4b3e10f84\n",
      "2015-06-23 19:42\n",
      "Thanks, done!\n",
      "\n",
      "1\n",
      "1058\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-23 20:03\n",
      "I think it looks good now apart from minor cosmetic things that I commented on. Maybe find another reviewer ;)\n",
      "\n",
      "1\n",
      "1059\n",
      "53135b495e986b0712efc453\n",
      "2015-06-24 03:05\n",
      "@vene I am asking since you had raised the question on whether we had common tests for CV... could I add \"Raise an issue to add common tests for CV iterators\" as a todo to be done after `MRG+2`?\n",
      "\n",
      "1\n",
      "1060\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-24 03:07\n",
      "I don't think it's important at the moment\n",
      "\n",
      "2\n",
      "1061\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-24 15:33\n",
      "Hi, it looks like we don't have Old Faithful data set, right?\n",
      "\n",
      "1\n",
      "1062\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-24 17:26\n",
      "@xuewei4d no we don't. As it's very small I think we could include a copy in the `sklearn.datasets` folder as we do for iris (just check that the copyright allow that but I am pretty sure it does).\n",
      "\n",
      "1\n",
      "1063\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-24 19:42\n",
      "OK. I will check it out. I would like to repeat some experiments described on PRML. @ogrisel\n",
      "\n",
      "1\n",
      "1064\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-24 20:00\n",
      "Would be a good sanity check indeed.\n",
      "\n",
      "1\n",
      "1065\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-24 20:02\n",
      "If you include the old faithful dataset, please do so in a separate PR and rebase your GMM PR on top of it to be able to merge the dataset PR first (without waiting for the end of the GMM work) while still being able to use it in your GMM examples.\n",
      "\n",
      "1\n",
      "1066\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-24 20:07\n",
      "Got it.\n",
      "\n",
      "1\n",
      "1067\n",
      "53135b495e986b0712efc453\n",
      "2015-06-24 21:09\n",
      "@vene @amueller @ogrisel If you have a few mins to spare... could you take a look at #4294 and let me know if it looks okay so I can proceed with updating documentation and fixing examples in parallel with the main review? In particular please let me know if you feel `len_cv` `iter_cv` and the new impl of `check_cv` look okay?\n",
      "\n",
      "1\n",
      "1068\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-24 23:37\n",
      "There is a copy of data in R, which has GPL license. It definitely doesn't work. The original data is publish on JSTOR. Terms and Conditions of Use of JSTOR prohibits the commercial use, but which is permitted under sickout-learn's BSD-license. So I don't think we could add old-faithful data. @ogrisel\n",
      "[Terms and Conditions of Use of JSTOR](http://www.jstor.org/page/info/about/policies/terms.jsp)\n",
      "\n",
      "2\n",
      "1069\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-25 07:08\n",
      "too bad.\n",
      "\n",
      "1\n",
      "1070\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-25 07:34\n",
      "Maybe you could try to send an email to the original author of the paper / dataset to ask for explicit permission for inclusion of the data in the scikit-learn toolkit (mention explicitly that its license is the BSD license) if they have the copyright on this data? We will off-course credit their paper in the description of the dataset.\n",
      "@amueller @vene quickfix in #4893 by @jmschrei for the windows test failures that causes appveyor to be all red since recently: https://ci.appveyor.com/project/sklearn-ci/scikit-learn\n",
      "\n",
      "2\n",
      "1071\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-25 07:54\n",
      "BTW, good news: I was contacted by the appveyor maintainer and we were upgraded to a much faster infrastructure (still for free). The builds run in 3-5 mins instead of 15-20min. This means that we might be able to run appveyor on the pull requests as we do for travis. Right now we only run them post-merge to master as it would have been too slow to process the full PR queues on the slow infrastructure.\n",
      "\n",
      "1\n",
      "1072\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-25 14:35\n",
      "OK. I will send the author an email, and cc to @ogrisel, @amueller. Who else should I cc to @ogrisel ?\n",
      "\n",
      "1\n",
      "1073\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-25 16:35\n",
      "it would be great to put the mailing list in CC but it's not possible to reply if there are not subscribed. So just us is fine. In case of positive outcome we can forward there authorization to the mailing list for the record.\n",
      "\n",
      "1\n",
      "1074\n",
      "53135b495e986b0712efc453\n",
      "2015-06-27 01:40\n",
      "I have a question! Should the examples in the old classes (at `cross_validation.py` et al.) use imports from `model_selection` or should I leave it as such?\n",
      "Any reviews for #4826 would be awesome! It will help in moving those Warnings out of the model_selection pr :)\n",
      "\n",
      "2\n",
      "1075\n",
      "53135b495e986b0712efc453\n",
      "2015-06-28 05:40\n",
      "virtualenvwrapper is cool! ;) Just incase any one is interested... This is a simple tutorial - http://simononsoftware.com/virtualenv-tutorial-part-2/\n",
      "@ogrisel thats great!! btw why doesn't appveyor do the doc tests? only travis does it? (not that it should be of any concern)\n",
      "\n",
      "2\n",
      "1076\n",
      "53135b495e986b0712efc453\n",
      "2015-06-28 23:14\n",
      "Also is there a way I could debug appveyor failures without having a win build? virual box? any suggestions?\n",
      "\n",
      "1\n",
      "1077\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-29 13:18\n",
      "http://dev.modern.ie/tools/vms/\n",
      "\n",
      "1\n",
      "1078\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-29 13:22\n",
      "> @ogrisel thats great!! btw why doesn't appveyor do the doc tests? only travis does it? (not that it should be of any concern)  Because on windows some type `repr` are slightly different (I think for int vs long on Python 2 IIRC) and some doctests would have a different representation. Porting the doctests to be cross-platform would be painful. doctests should be considered a way to check that the documentation is up to date with the code. So if it passes on one platform, we know that the doc is up to date. For unit tests on the other hand we want to test on all the supported platforms to make sure that the code is fully cross-platform.\n",
      "\n",
      "1\n",
      "1079\n",
      "53135b495e986b0712efc453\n",
      "2015-06-29 13:23\n",
      "Thanks :)\n",
      "@vene thanks for the link :)\n",
      "\n",
      "2\n",
      "1080\n",
      "541a528b163965c9bc2053de\n",
      "2015-06-29 13:35\n",
      "Note that last time I checked, the modern.io vms were 32 bit only.\n",
      "That might be a limitation in some cases.\n",
      "\n",
      "2\n",
      "1081\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-29 13:46\n",
      "good point. @rvraghav93 you might be able to get a free Windows license through MSDN Academic Alliance or dreamspark, if your university is partnered with them.\n",
      "I had a dreamspark win7 for a while\n",
      "I\n",
      "\n",
      "3\n",
      "1082\n",
      "53135b495e986b0712efc453\n",
      "2015-06-29 14:37\n",
      "I too got one via the IEEE comsoc offer!! :D but never used it ;/  Anyway the current failure is in 32 bit/ Python 2.6.. so this should suffice for now :)\n",
      "wow! @vene The setup is super fast... All I had to do was extract and open the ova file :O\n",
      "\n",
      "2\n",
      "1083\n",
      "53135b495e986b0712efc453\n",
      "2015-06-29 16:27\n",
      "BTW could I start resurrecting #2759 (multiple metric support) in parallel? But since this depends on the data indep PR, I am not sure how to proceed... Should I maybe start a ML thread to collect ideas?\n",
      "\n",
      "1\n",
      "1084\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-30 03:05\n",
      "@rvraghav93 let's finish the data dependent cv first\n",
      "sorry I have been offline for the last 4 days\n",
      "\n",
      "2\n",
      "1085\n",
      "53135b495e986b0712efc453\n",
      "2015-06-30 03:06\n",
      "No issues :) Would you be able to spare a few mins to take a look at that? :)\n",
      "\n",
      "3\n",
      "1086\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-30 03:07\n",
      "I have 300+ unread emails and it is 23:06\n",
      "\n",
      "8\n",
      "1087\n",
      "556705cb15522ed4b3e10f84\n",
      "2015-06-30 14:57\n",
      "@amueller @jnothman @ogrisel are you happy with the changes in #4444? :)\n",
      "\n",
      "1\n",
      "1088\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-30 18:10\n",
      "@JeanKossaifi sorry I can not review any time soon\n",
      "\n",
      "1\n",
      "1089\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-30 18:22\n",
      "A friendly reminder to all the mentors that mid-terms are coming up this Friday!\n",
      "\n",
      "1\n",
      "1090\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-30 18:55\n",
      "I am writing the mid term evaluation, but I don't think my PR could merge right now. Do you think I need to pick ```GaussianMixture``` and ```_MixtureBase``` out of my current PR and make it ready to merge?  @amueller\n",
      "\n",
      "1\n",
      "1091\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-06-30 19:00\n",
      "@xuewei4d no, I don't think you should do that\n",
      "Its fine if there is good progress\n",
      "\n",
      "2\n",
      "1092\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-06-30 19:28\n",
      "Just submitted the evaluation form. Hope everything is fine.\n",
      "\n",
      "1\n",
      "1093\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-06-30 19:33\n",
      "Mentors... does anyone have access to Artem's GSoC evaluation? Apparently I am still not added correctly as a mentor! Working on getting the right access but would like to put an initial evaluation even if it means someone else copy-pastes in my words.\n",
      "\n",
      "1\n",
      "1094\n",
      "5425a933163965c9bc206e53\n",
      "2015-06-30 21:14\n",
      "Is there a reason RMSLE isn't in the metrics module, or would a PR for that be welcome?\n",
      "\n",
      "1\n",
      "1095\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-30 22:18\n",
      "@zacstewart what does RMSLE stand for?\n",
      "\n",
      "1\n",
      "1096\n",
      "5425a933163965c9bc206e53\n",
      "2015-06-30 22:18\n",
      "Root mean squared logarithmic error.\n",
      "Same as RMSE, but take the log(y_pred + 1) and log(y_true + 1) instead of just the values\n",
      "I don't know if it is. I am bringing it up because it is the evaluation metric for a competition I am starting on today\n",
      "\n",
      "8\n",
      "1097\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-30 22:21\n",
      "is this a mainstream thing? I see the first result is from Kaggle :/\n",
      "\n",
      "1\n",
      "1098\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-30 22:24\n",
      "If you want you can prepare a PR\n",
      "Of course it would need tests and docs (with references) and maybe a motivating example.\n",
      "\n",
      "2\n",
      "1099\n",
      "5425a933163965c9bc206e53\n",
      "2015-06-30 22:26\n",
      "Okay. That's reasonable. Not sure I can provide a motivating reason to use it, other than it's the metric for the competition I'm working on\n",
      "\n",
      "1\n",
      "1100\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-30 22:27\n",
      "I don't know whether it belongs in scikit-learn by default. But I'm pessimistic because I personally never heard of this metric before. I'm rather clueless though. It also may be the case that it's been around under a different name\n",
      "I mean PartitionIterator, not CVIterator\n",
      "\n",
      "2\n",
      "1101\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-06-30 23:07\n",
      "Ugh. I just noticed that the CVIterator, when given a class that implements _iter_test_indices, converts the indices to masks and then back into indices.\n",
      "\n",
      "1\n",
      "1102\n",
      "53135b495e986b0712efc453\n",
      "2015-07-01 04:42\n",
      "seq of seq support for `LabelBinarizer` is [earmarked for removal in 0.17](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/multiclass.py#L195)... should we remove it??\n",
      "\n",
      "1\n",
      "1103\n",
      "53135b495e986b0712efc453\n",
      "2015-07-01 04:54\n",
      "@vene Thanks heaps for the patient reviews!!! I'll address them and push before tomorrow along with the documentation and examples... with that #4294 is done!!! (except for further rounds of reviews, ofcourse!!)\n",
      "tomorrow (in NYC time)\n",
      "Yeaa :/ Thanks anyway :) After addressing your comments I'll split it into one commit per file... this should make it marginally easy to review?\n",
      "\n",
      "3\n",
      "1104\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-01 05:11\n",
      "I still have, like, half of the PR to go through.  it's a big one! :)\n",
      "Don't worry about label binarizer right now. Probably yes, it should be removed.\n",
      "I use  git diff to compare to the old files, so I don't really care how the commits are grouped. I just look at the head\n",
      "\n",
      "4\n",
      "1105\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-01 05:15\n",
      "Anyway, before I go to bed, I'm not sold on using labels for the predefined split. It shouldn't stray too much from the old api\n",
      "I'll think about it some more tomorrow\n",
      "Happy hacking!\n",
      "\n",
      "4\n",
      "1106\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-01 13:49\n",
      "@zacsteward  I never heard of that metric before either. which community is it from? Google told me it is used in a couple of kaggle competitions, but it doesn't have a wikipedia entry\n",
      "i guess it is for exponential regression tasks. Basically you log the target and then to rmse? For most models it would probably be better to log the target before training as regression models are more likely to minimize rmse\n",
      "\n",
      "8\n",
      "1107\n",
      "5425a933163965c9bc206e53\n",
      "2015-07-01 13:49\n",
      "Kaggle is the only place I've seen it as well\n",
      "\n",
      "1\n",
      "1108\n",
      "5425a933163965c9bc206e53\n",
      "2015-07-01 13:54\n",
      "Hm, I suppose then I would just have to exp my predictions for submission\n",
      "\n",
      "6\n",
      "1109\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-01 13:59\n",
      "np. I wouldn't worry about optimizing this, though using log1p and expm1 are probably a good idea\n",
      "\n",
      "1\n",
      "1110\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-01 14:00\n",
      "For stability yes. For speed, this is just pre and post processing, so I doubt it matters\n",
      "http://www.johndcook.com/blog/2010/06/07/math-library-functions-that-seem-unnecessary/\n",
      "\n",
      "2\n",
      "1111\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-01 15:29\n",
      "if curious, here's the numpy implementation of log1p if the math.h one is not available https://github.com/numpy/numpy/blob/master/numpy/core/src/npymath/npy_math.c.src#L86\n",
      "\n",
      "1\n",
      "1112\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-02 14:45\n",
      "@amueller did you see this notebook ~~re~~implementing the google blog post from earlier? https://github.com/google/deepdream/blob/master/dream.ipynb\n",
      "\n",
      "1\n",
      "1113\n",
      "53135b495e986b0712efc453\n",
      "2015-07-02 14:49\n",
      "wow! :O\n",
      "\n",
      "1\n",
      "1114\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-02 14:50\n",
      "it generates things that look like this https://slack-files.com/files-tmb/T04T7B1ST-F074J6EDD-5630fb79d2/frames_360.png\n",
      "\n",
      "2\n",
      "1115\n",
      "53135b495e986b0712efc453\n",
      "2015-07-02 14:56\n",
      "@vene Do you have any suggestions for a practical example for nested cv using LOLO... ? :) Any dataset / public domain problem (like iris classification)?\n",
      "Okay!!\n",
      "\n",
      "2\n",
      "1116\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-02 14:57\n",
      "I can't think of any datasets with meaningful groups\n",
      "you could generate one yourself\n",
      "\n",
      "2\n",
      "1117\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-02 15:00\n",
      "like a regression problem with multiple noisy observations per subject\n",
      "if you accidentally leave observations of the same subject in both train and test folds, you'll get overly optimistic results\n",
      "\n",
      "2\n",
      "1118\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-02 15:11\n",
      "(I'm just thinking out loud, this might not be right) say you have 2 features and the target is `y = w_1 + w_2 + noise` but `w1` is very correlated within the same subject and w2 is essentially independent.\n",
      "like if `w1` is essentialy the label of the subject + noise, and `w2` is just noise\n",
      "say maybe `w_1`is the person's weight (fluctuates slightly but not a lot) and `w_2` is how many minutes the person walked outside today\n",
      "\n",
      "3\n",
      "1119\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-02 15:22\n",
      "@vene yeah saw the blog post. pretty cool\n",
      "I have to work on the scipy tutorial today, sorry\n",
      "\n",
      "2\n",
      "1120\n",
      "5425a933163965c9bc206e53\n",
      "2015-07-02 20:37\n",
      "Is there a good way to use an LabelEncoder to encode several columns of categorical variables?\n",
      "This is exactly what I want. But I need to encode those string categorical values to integers so I can use it :)\n",
      "I am actually using pandas. I've done this various hacky ways in the past\n",
      "Would making the OneHotEncoder accept strings be relatively straight forward or are there design problems blocking it? I'd love to issue a PR for that\n",
      "\n",
      "4\n",
      "1121\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-02 20:38\n",
      "you probably want OneHotEncoder\n",
      "(only it doesn't do strings at the moment which makes me sad)\n",
      "you could make it a dict and use dict vectorizer... or use pandas?\n",
      "I was really surprised when I recently realized that there is no good way to do this in sklearn. you can open an issue if you like\n",
      "\n",
      "4\n",
      "1122\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-02 20:55\n",
      "it seemed slightly non-trivial but didn't look blocking. maybe open an issue and ping @jnothman what he thinks of it\n",
      "\n",
      "1\n",
      "1123\n",
      "5425a933163965c9bc206e53\n",
      "2015-07-02 20:55\n",
      ":+1:\n",
      "\n",
      "2\n",
      "1124\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-07-04 02:08\n",
      "[![download (1).jpg](https://files.gitter.im/scikit-learn/scikit-learn/6cOi/thumb/download-_1_.jpg)](https://files.gitter.im/scikit-learn/scikit-learn/6cOi/download-_1_.jpg)\n",
      "Deep dreaming on sklearn...\n",
      "My assessment: blue blob is pure machine, and orange blob is learning.\n",
      "FWIW, the crazy stuff in the background was imagined from a blank white canvas\n",
      "\n",
      "4\n",
      "1125\n",
      "53135b495e986b0712efc453\n",
      "2015-07-04 08:11\n",
      "Cool!! How\n",
      "*how'd u do that??\n",
      "BTW andyy happy birthday!! :)\n",
      "\n",
      "3\n",
      "1126\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-07-04 15:39\n",
      "@rvraghav93 I spent several hours trying to get caffe installed :smiley: and then played with a few of the params at https://github.com/google/deepdream\n",
      "And then, things got weird... http://i.imgur.com/5QA87gU.gif\n",
      "\n",
      "2\n",
      "1127\n",
      "53135b495e986b0712efc453\n",
      "2015-07-05 13:27\n",
      "if there were no eyes this would probably look even better! eyes pop out almost everywhere... :anguished:\n",
      "\n",
      "1\n",
      "1128\n",
      "53135b495e986b0712efc453\n",
      "2015-07-06 15:06\n",
      "@trevorstephens  you might want to take a took at this :laughing:\n",
      "http://i.imgur.com/OPbPA4M.gif\n",
      "\n",
      "2\n",
      "1129\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-07-07 20:10\n",
      "that's terrifying\n",
      "\n",
      "1\n",
      "1130\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-08 14:18\n",
      "@xuewei4d I think you should have a look at https://bitbucket.org/michaelchughes/bnpy/\n",
      "It looks like it might be a good reference for the dp gmm\n",
      "\n",
      "2\n",
      "1131\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-08 14:50\n",
      "Thanks. I am looking into it.\n",
      "\n",
      "1\n",
      "1132\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-08 20:30\n",
      "Hi I have a question about git/github. How can I create a branch from a branch of other's forked repository?\n",
      "\n",
      "1\n",
      "1133\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-08 21:15\n",
      "You can add the fork as a remote with git remote add <name> <uri>\n",
      "\n",
      "1\n",
      "1134\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-08 21:22\n",
      "I just tried, but there are too many conflicts. What should I do?\n",
      "\n",
      "1\n",
      "1135\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-08 22:11\n",
      "I don't see how \"git remote add\" could trigger conflicts, what exactly is the problem?\n",
      "the way I'd do it is:  ``` git remote add xuewei4d https://github.com/xuewei4d/scikit-learn.git git fetch xuewei4d git checkout xuewei4d/<whatever-branch> git checkout -b <my-new-local-branch> ```\n",
      "if the branch you want is a PR you can also use [this trick](https://gist.github.com/piscisaureus/3342247)\n",
      "HTH\n",
      "\n",
      "4\n",
      "1136\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-10 11:58\n",
      "@rvraghav93 there was a question on the mailing list very relevant to what you are working on\n",
      "\n",
      "1\n",
      "1137\n",
      "53135b495e986b0712efc453\n",
      "2015-07-10 12:16\n",
      "Thanks for pinging me!! I'll look into it and reply :)\n",
      "\n",
      "1\n",
      "1138\n",
      "53135b495e986b0712efc453\n",
      "2015-07-10 12:17\n",
      "Also if you could spare a few mins could you let me know if there is anything left to be done for the #4294 :) I wish to get started on the next goal (sample props) asap :)\n",
      "\n",
      "1\n",
      "1139\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-10 12:30\n",
      "I need to review it, it will take a bit more than a few minutes\n",
      "I hope I can do it today\n",
      "\n",
      "2\n",
      "1140\n",
      "53135b495e986b0712efc453\n",
      "2015-07-10 12:43\n",
      "Thanks :)) also do you feel it is an apt time to send a mail on the mailing list requesting for the comments on sample props? (ML instead of issue since it will reach a wider audience)\n",
      "\n",
      "1\n",
      "1141\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-10 12:51\n",
      "I'd discuss in the issue first\n",
      "\n",
      "1\n",
      "1142\n",
      "53135b495e986b0712efc453\n",
      "2015-07-10 12:57\n",
      "Okay! Thanks!\n",
      "\n",
      "1\n",
      "1143\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-10 12:58\n",
      "There is already discussion in progress there\n",
      "It seems like the appropriate place\n",
      "\n",
      "2\n",
      "1144\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-10 13:00\n",
      "That's just my 2c\n",
      "After polling dev opinion, you can summarize and post on the ML too\n",
      "\n",
      "2\n",
      "1145\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-10 13:02\n",
      "As for Luca's question, you could advertise how that sort of thing is possible with your branch, introduce the new API and see whether it suits his needs. It's good to get the users' point of view.\n",
      "Also it might attract another round of review, which would be great\n",
      "\n",
      "2\n",
      "1146\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-10 14:33\n",
      "Thanks @vene. I had an internet connection problem.\n",
      "I cannot login gitter yesterday.\n",
      "\n",
      "2\n",
      "1147\n",
      "53135b495e986b0712efc453\n",
      "2015-07-10 15:12\n",
      "@vene okay :) thanks!!\n",
      "\n",
      "1\n",
      "1148\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-10 19:34\n",
      "Hi, when I am using %timeit on one function, how to disable caching intermediate results?\n",
      "\n",
      "1\n",
      "1149\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-11 15:56\n",
      "good morning everyone! sprints are starting now!\n",
      "\n",
      "1\n",
      "1150\n",
      "53135b495e986b0712efc453\n",
      "2015-07-11 16:06\n",
      "Good morning!! Scipy sprints??\n",
      "\n",
      "1\n",
      "1151\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-11 16:33\n",
      "yes!\n",
      "\n",
      "1\n",
      "1152\n",
      "53135b495e986b0712efc453\n",
      "2015-07-11 16:35\n",
      "Awesome! good luck with that! :)\n",
      "\n",
      "1\n",
      "1153\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-11 19:21\n",
      "Hi @amueller, sprint still going?\n",
      "\n",
      "1\n",
      "1154\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-11 21:11\n",
      "yeahg\n",
      "\n",
      "1\n",
      "1155\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-11 21:12\n",
      "@rvraghav93 @vene so what do we want to tackle next? I feel the sample_props is not clear enough to go ahead\n",
      "Another possibility would be the multiple metrics grid-search\n",
      "or nesting GridSearchCV and EstimatorCV\n",
      "\n",
      "3\n",
      "1156\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-11 21:14\n",
      "is #1626 where I should read about the second suggestion?\n",
      "multiple metrics seems more straightforward to me right now\n",
      "I agree sample_props is not ripe yet\n",
      "\n",
      "3\n",
      "1157\n",
      "53135b495e986b0712efc453\n",
      "2015-07-11 23:17\n",
      "Okay!! multiple metric it is! BTW the reason I suggested sample props to be done next was because it seemed more framework-ish...\n",
      "\n",
      "2\n",
      "1158\n",
      "53135b495e986b0712efc453\n",
      "2015-07-11 23:19\n",
      "yes indeed!! BTW I heard from Joel that he wouldn't be available for the next few weeks... too bad since he vouched for multiple metric support very much!\n",
      "My reply to Luca was brief... will expand it as a blog post and reply to that mail like you had suggested...\n",
      "\n",
      "3\n",
      "1159\n",
      "53135b495e986b0712efc453\n",
      "2015-07-11 23:22\n",
      "Also I scoured through the notification email and found bits and pieces of the lost conversation on `classifier` param... have commented it there!!\n",
      "\n",
      "1\n",
      "1160\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-11 23:33\n",
      "I'm finally resuming my review! sorry for the delay\n",
      "thanks for finding the e-mails, I thought there had been more discussion that I missed\n",
      "\n",
      "2\n",
      "1161\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-12 00:38\n",
      "@rvraghav93: didn't we agree to not duplicate the code that doesn't need to be duplicated? eg the old gridsearch, can't it be just imported from the new path?\n",
      "basically we'll have duplicated functionality in the old and new CV classes for a while, but the rest of the things that were moved should just be imported from the new place, assuming their behavior with old-style CV classes doesn't change.\n",
      "\n",
      "2\n",
      "1162\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-12 16:45\n",
      "+1\n",
      "\n",
      "1\n",
      "1163\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-12 16:47\n",
      "@amueller what do you think about the placement on the `labels` argument in `cross_val_score` and the like?\n",
      "If we leave it at the end, we don't need to duplicate that code either, right?\n",
      "\n",
      "2\n",
      "1164\n",
      "53135b495e986b0712efc453\n",
      "2015-07-12 16:54\n",
      "(About reducing duplicates) already done at https://github.com/rvraghav93/scikit-learn/pull/2 :)\n",
      "I also reused a few docstrings... That is a bit hacky not sure if that's correct!!\n",
      "@amueller discussion abt the labels arg here - https://github.com/scikit-learn/scikit-learn/pull/4294#discussion_r34417412\n",
      "\n",
      "3\n",
      "1165\n",
      "55a13e9b5e0d51bd787b0bb6\n",
      "2015-07-12 21:23\n",
      "I'm curious about the [MRG] convention I see being used on pull requests. Does that just indicate that the author believes it's ready to merge?\n",
      "I think I wrote a good test for PR #4961. I hesitated before because I thought it would be weird to test something that should be somewhere around 0.5. But then I taught myself about assert_almost_equal.\n",
      "It was fun sprinting with some of you, I'll be back on later today or this week to hopefully finish that k-means example.\n",
      "\n",
      "3\n",
      "1166\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-12 21:52\n",
      "@mrphilroth exactly! It's a way for reviewers to tell at a glance what state a PR is in.\n",
      "\n",
      "1\n",
      "1167\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-15 21:22\n",
      "https://cloud.githubusercontent.com/assets/1180956/8710149/a6437600-2b15-11e5-9524-d503200f01a5.gif\n",
      "\n",
      "1\n",
      "1168\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-15 21:23\n",
      "Hi all. Here is a ```BayesianGaussianMixture``` demo on rescaled old faith data.\n",
      "\n",
      "1\n",
      "1169\n",
      "54a2cde7db8155e6700e4190\n",
      "2015-07-17 16:44\n",
      "Andy uploaded the docs here: http://scikit-learn.github.io/dev/\n",
      "\n",
      "1\n",
      "1170\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-20 09:28\n",
      "please everybody check scikit-learn.github.io/ and stable and dev and 0.16 subfolders\n",
      "https://github.com/scikit-learn/scikit-learn/issues/4993\n",
      "report issues there\n",
      "ok looking good now.\n",
      "waiting for the dns move\n",
      "\n",
      "5\n",
      "1171\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-20 09:58\n",
      "added 0.15\n",
      "\n",
      "1\n",
      "1172\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-20 15:17\n",
      "dns is up over here, thanks @amueller!\n",
      "hi @rvraghav93\n",
      "\n",
      "2\n",
      "1173\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-20 19:27\n",
      "@xuewei4d sorry for the lack of feedback, but would you have time to look at this: https://github.com/scikit-learn/scikit-learn/pull/1292#issuecomment-122038677 ?\n",
      "@rvraghav93 I'll have more time on wednesday to look at your code\n",
      "\n",
      "2\n",
      "1174\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-20 19:40\n",
      "Yes. He is right. I have fixed this problem in https://github.com/scikit-learn/scikit-learn/pull/4802/files#diff-a498a8ef6ad37ebc525591d722e0a7ceR220\n",
      "\n",
      "1\n",
      "1175\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-20 19:43\n",
      "The kmeans initialization in GMM is not good. It only initializes the means_. My PR will initialize the responsibilities first, then all parameters including means, weights, covars.\n",
      "\n",
      "1\n",
      "1176\n",
      "555b8aa615522ed4b3e0a160\n",
      "2015-07-21 13:02\n",
      "@ogrisel AppVeyor is blocked on a build\n",
      "\n",
      "1\n",
      "1177\n",
      "5583486615522ed4b3e220e5\n",
      "2015-07-21 22:10\n",
      "Hi, I'm fairly new to machine learning/deep learning, but I'm currently working on a project where I want to classify some biological images. I want to initially run an unsupervised clustering of similar images that don't have  labels. Later on I want to take a set of labeled images and run some type of deep learning algorithm to aid in classifying the images. What are some ways i can get started? Any help would be really appreciated, thank you!\n",
      "\n",
      "1\n",
      "1178\n",
      "53135b495e986b0712efc453\n",
      "2015-07-21 22:42\n",
      "Hi @vene @amueller sorry! Got a bit busy :/ will resume work in a few hours!! :)\n",
      "\n",
      "1\n",
      "1179\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-23 00:18\n",
      "Hi @rvraghav93. Do you have a draft of your next blog post?\n",
      "I will be on a 16h flight tomorrow, maybe I can make myself useful.\n",
      "\n",
      "2\n",
      "1180\n",
      "53135b495e986b0712efc453\n",
      "2015-07-25 09:16\n",
      "Hey! I'll add one tonight! :)\n",
      "\n",
      "1\n",
      "1181\n",
      "53135b495e986b0712efc453\n",
      "2015-07-25 14:29\n",
      "Vlad is choosing k in KNN a good example for LOLO? (About labeled data I was thinking like some N samples from M patients and we leave one patient out for validation to make sure we are able to generalise to other patients too)\n",
      "Sounds good?\n",
      "I am unable to find a nice example for LOLO otherwise :/\n",
      "any suggestions?\n",
      "\n",
      "4\n",
      "1182\n",
      "53135b495e986b0712efc453\n",
      "2015-07-25 15:47\n",
      "Also if you find time could you take a look at #4826\n",
      "\n",
      "1\n",
      "1183\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-07-28 03:29\n",
      "just to let u know, cluster.DBSCAN (and I guess lots more) is not working with scipy 0.16.0 on OSX, at least not with a conda install (maybe the library is missing in the conda binary?) https://github.com/scipy/scipy/issues/5092\n",
      "well, others worked it, im just happy it does work again. ;) Its really scary if scipy doesnt work... ;)\n",
      "but thankfully, its so easy to quickly downgrade with conda: `conda install scipy=0.15`\n",
      "\n",
      "3\n",
      "1184\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-28 14:08\n",
      "@rvraghav93 sorry for my proonged abscense. I am back from my travels and will catch up with all your activity\n",
      "michaelaye: what is the error you get?\n",
      "@michaelaye what is the error? I'm pretty sure that is working for other people.\n",
      "kinda ;) well np.dot \"doesn't work\" so ....\n",
      "\n",
      "4\n",
      "1185\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-28 20:38\n",
      "master is failing?\n",
      "was that me?!\n",
      "is there a way to see when master started failing?\n",
      "\n",
      "3\n",
      "1186\n",
      "54a2cde7db8155e6700e4190\n",
      "2015-07-28 20:44\n",
      "https://travis-ci.org/scikit-learn/scikit-learn/builds\n",
      "May not just be master\n",
      "\n",
      "2\n",
      "1187\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-07-28 20:44\n",
      "@amueller i was not alone, e.g. https://github.com/ContinuumIO/anaconda-issues/issues/392 but its all fixed now.\n",
      "\n",
      "2\n",
      "1188\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-28 20:46\n",
      "@TomAugspurger which makes it very hard to find out which are actual master commits and which are not\n",
      "\n",
      "1\n",
      "1189\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-28 22:25\n",
      "great, the error was first here: 24e962cfe1c348d0c1de95f546b2091fe75a2c06\n",
      "failure: https://travis-ci.org/scikit-learn/scikit-learn/jobs/72957365 success: https://travis-ci.org/scikit-learn/scikit-learn/jobs/72439348\n",
      "and all the versions seem to be  the same\n",
      "\n",
      "3\n",
      "1190\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-28 22:42\n",
      "ah, new scipy\n",
      "\n",
      "1\n",
      "1191\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-29 01:49\n",
      "@rvraghav93 you should have pinged me using my username, I didn't see your message. I'm behind the Great Firewall so my connectivity is poor until the 31st\n",
      "To answer, I don't see why knn would be better or worse than anything else. The gist of the problem is the estimation of the score. If you use kfold instead of lolo you will overestimate.\n",
      "The idea of validation is to estimate how your model would do in a realistic setting. If your observations are grouped and they arrive in groups, it's not realistic to assume that you can be able to train on some samples from the same groups that you will run it on.\n",
      "I guess knn is likely to overestimate. But it's not a question of cchoosing k. It's one of methodology\n",
      "Think of search queries. If real life users would look for exactly the same queries you have in your training set, 1nn can return perfect results. But that's not a realistic of interesting case.\n",
      "\n",
      "5\n",
      "1192\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-29 01:59\n",
      "If you \"contaminate\" your evaluation with this kind of data you can think your system generalizes much better than it really does.\n",
      "Because the model can implicitly learn to recognize the latent group label, and then get some of the test points predicted really well\n",
      "I hope this makes sense. I gtg\n",
      "\n",
      "3\n",
      "1193\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-29 15:06\n",
      "Excuse me, what kind of test cases should I write for some computation functions?\n",
      "\n",
      "1\n",
      "1194\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-29 15:32\n",
      "which functions?\n",
      "\n",
      "1\n",
      "1195\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-29 15:40\n",
      "like update functions...\n",
      "https://github.com/scikit-learn/scikit-learn/pull/4802/files#diff-47bf98f4dd63f89baa089da3ffe28652R650\n",
      "https://github.com/scikit-learn/scikit-learn/pull/4802/files#diff-47bf98f4dd63f89baa089da3ffe28652R197\n",
      "\n",
      "7\n",
      "1196\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-07-29 18:07\n",
      "OK.. I will try it against simple cases.\n",
      "\n",
      "1\n",
      "1197\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-29 19:01\n",
      "reviews for #5049 and #5047 would be very welcome so we can fix travis\n",
      "\n",
      "1\n",
      "1198\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-30 15:19\n",
      "do we have a way to check if a regressor or classifier supports multi-output / multi-label? cc @arjoly\n",
      "\n",
      "1\n",
      "1199\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-07-31 16:18\n",
      "not a lot happening here at the moment ^^\n",
      "\n",
      "1\n",
      "1200\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-07-31 16:20\n",
      "I just got out from behind the Great Firewall. But I'm coming after a 35h trip. It'll be a little while before I can be of any help :(\n",
      "\n",
      "2\n",
      "1201\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-01 20:56\n",
      "According to the emails from Prof.  Azzalini and Prof. Bowman, I think, we cannot use old-faithful data set without the permission of Royal Statistical Society. We cannot use data from R either, right?\n",
      "@amueller @ogrisel\n",
      "\n",
      "2\n",
      "1202\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-02 15:29\n",
      "@xuewei4d It probably depends, what dataset do you mean in particular?\n",
      "\n",
      "1\n",
      "1203\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-02 21:23\n",
      "I think based on their mails we shouldn't worry too much about including it.\n",
      "\n",
      "1\n",
      "1204\n",
      "53135b495e986b0712efc453\n",
      "2015-08-02 21:27\n",
      "could someone merge #5077? :)\n",
      "\n",
      "1\n",
      "1205\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-02 22:32\n",
      "why is that imporant?\n",
      "but sure\n",
      "\n",
      "2\n",
      "1206\n",
      "53135b495e986b0712efc453\n",
      "2015-08-02 23:18\n",
      "When attempting to import from the old `cross_validation` module which is deprecated... I noticed a weird behaviour in ipython... simply tabcompleting brings up the deprecation warning... is that normal...?  ``` from sklearn.cross_validation import KF/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:40: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. Refer model_selection.split for more info.   \"Refer model_selection.split for more info.\", DeprecationWarning) ```\n",
      "notice the warning start after `KF<tab>`\n",
      "@amueller That was failing #4294 ;) Thanks for the merge!\n",
      "\n",
      "3\n",
      "1207\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-02 23:59\n",
      "That's expected if the deprecation warning is at module level AFAIK\n",
      "\n",
      "2\n",
      "1208\n",
      "53135b495e986b0712efc453\n",
      "2015-08-03 01:42\n",
      "Just realised we crossed 7k stars!! :beers:\n",
      "\n",
      "1\n",
      "1209\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-03 11:58\n",
      "It is old-faithful data set. @vene\n",
      "\n",
      "1\n",
      "1210\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-03 14:36\n",
      "wb @ogrisel :)\n",
      "\n",
      "1\n",
      "1211\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-03 16:37\n",
      "@vene can you have a look at the discussion topics at the top of #4294 ?\n",
      "\n",
      "1\n",
      "1212\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-03 18:33\n",
      "sure thing\n",
      "\n",
      "1\n",
      "1213\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-03 20:00\n",
      "I'm slightly confused by the PR that to @rvraghav93 's branch that removes the code reuse.\n",
      "Is only the newest commit relevant? That commit seems to have some extra things squashed in it.\n",
      "\n",
      "2\n",
      "1214\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-03 20:15\n",
      "I haven't really looked at that recently\n",
      "I think we should first get the one with all the duplication merged\n",
      "what do you think about making the submodules private?\n",
      "\n",
      "3\n",
      "1215\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-03 20:30\n",
      "with underscores?\n",
      "I think it's a good idea\n",
      "\n",
      "2\n",
      "1216\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-03 20:32\n",
      "yes, with underscores.\n",
      "ok please say in the PR, then he can do that. apart from the docs /examples that is the only major thing left.\n",
      "\n",
      "2\n",
      "1217\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-05 18:26\n",
      "@rvraghav93 how are things looking?\n",
      "\n",
      "1\n",
      "1218\n",
      "53135b495e986b0712efc453\n",
      "2015-08-05 18:32\n",
      "Hey thanks a lot for the reviews!! Ive fixed most... Ill fix the rest (mostly trivial) and We are merging  that pr this weekend ;) u guys will be available right?? So we could have last minute reviews?? Also do u feel that or should have +3 since its a major refactpr??\n",
      "Pr*\n",
      "(Apologies for typo - using my mobile :( )\n",
      "\n",
      "3\n",
      "1219\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-05 23:36\n",
      "Hey @amueller @ogrisel , I got some medical issues, and probably cannot work for GSoC project in the recent few days. Sorry. :worried:\n",
      "\n",
      "1\n",
      "1220\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-05 23:57\n",
      "@xuewei4d sorry to hear that, get well soon!\n",
      "\n",
      "1\n",
      "1221\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-06 00:00\n",
      "@rvraghav93 I will be around this week. I agree a 4th person would be nice, maybe @jnothman can take a look. PS: Don't forget about the blog!\n",
      "\n",
      "1\n",
      "1222\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-06 02:30\n",
      "@rvraghav93 I'll be around but I'll also have to work on some other things. When are you going to Paris.\n",
      "@xuewei4d get well soon!\n",
      "\n",
      "2\n",
      "1223\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-06 16:40\n",
      "@ogrisel are you project owner on appveyor? we need to do something: http://help.appveyor.com/discussions/problems/2721-getting-message-error-creating-build-entry-please-contact-appveyor-support-in-every-build  otherwise all PRs have failing tests.\n",
      "That was easy: https://github.com/scikit-learn/scikit-learn/pull/5093\n",
      "\n",
      "2\n",
      "1224\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-06 17:24\n",
      "ah, I found the appveyor integration stuff ^^ @ogrisel sent me that at some point\n",
      "so the appveyor builds are running again and I'm excited to see what happens with #5093 ^^\n",
      "\n",
      "2\n",
      "1225\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-06 17:42\n",
      "hum.. @ogrisel I still need help :-/ https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.1490/job/26vknwf7qa70r868\n",
      "\n",
      "1\n",
      "1226\n",
      "5478876cdb8155e6700d907b\n",
      "2015-08-07 11:24\n",
      "@amueller is waiting the thing to do or is there something missing from #5037 that I can do to get it towards MRG+n's?\n",
      "\n",
      "1\n",
      "1227\n",
      "555b8aa615522ed4b3e0a160\n",
      "2015-08-07 16:22\n",
      "@betatim you can change the name of the PR with [MRG], to show you ask for some review\n",
      "\n",
      "1\n",
      "1228\n",
      "53135b495e986b0712efc453\n",
      "2015-08-09 09:06\n",
      "I feel we could add more to the doc string of the dataset loaders... (perhaps a description or at least a link?)  For example in the diabetes documentation no domain related info is given...\n",
      "domain related as in no info as to what the targets represent or what the various attributes are?\n",
      "@amueller @agramforte said there were delays in the visa process ;( I've tried pestering him too :P I really hope I'd be there by September at the least :|\n",
      "\n",
      "3\n",
      "1229\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-09 13:02\n",
      "Most of them have readmes after loading, I guess.\n",
      "\n",
      "1\n",
      "1230\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-09 18:14\n",
      "wasn't the diabetes fixed recently?\n",
      "@rvraghav93 last time I checked there were still unresolved comments on the model selection PR.\n",
      "\n",
      "2\n",
      "1231\n",
      "53135b495e986b0712efc453\n",
      "2015-08-09 18:19\n",
      "No it is still the same AFAIK... Yes a few... I'm working on them :) I'm actually finishing up on my blog post! Will publish the same in a few mins... Would you be around? If so I'll trouble you for a review!\n",
      "\n",
      "8\n",
      "1232\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-09 18:28\n",
      "?\n",
      "ah ^^\n",
      "\n",
      "2\n",
      "1233\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-09 22:55\n",
      "Can this get another review: https://github.com/scikit-learn/scikit-learn/pull/4924 so we can merge it before  https://github.com/scikit-learn/scikit-learn/pull/4421 ?\n",
      "also we should really work on https://github.com/scikit-learn/scikit-learn/pull/4421\n",
      "\n",
      "2\n",
      "1234\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-10 14:03\n",
      "@o\n",
      "\n",
      "1\n",
      "1235\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-10 14:09\n",
      "Sorry for the interruption. I am back, although not fully recovered.\n",
      "Hi @amueller @ogrisel , I just finished test cases for DP. Where should I update the mixture documentation and the mixture examples?\n",
      "\n",
      "2\n",
      "1236\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-10 18:35\n",
      "@xuewei4d great :) yeah, that sounds like a plan.\n",
      "\n",
      "1\n",
      "1237\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-10 18:37\n",
      "I think I finished what I must do.  @ogrisel mentioned the \"mixture documentation\" and \"mixture examples\" several days ago, but I don't know where should I put.\n",
      "\n",
      "3\n",
      "1238\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-10 18:41\n",
      "I don't think this is a priority but it would be kinda nice. The actual implementation and tests are much more important obviously. sorry for the slow reviews :-/\n",
      "\n",
      "1\n",
      "1239\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-10 18:43\n",
      "I agree. I think I could do the examples after the soft pencil down deadline. What else should I begin right now before next Monday?\n",
      "Never mind. :sweat_smile: The equations for BGMM are really cumbersome.\n",
      "\n",
      "3\n",
      "1240\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-10 19:36\n",
      "@rvraghav93  \"in the year 2010\" I'd just say \"in 2010\"\n",
      "\n",
      "1\n",
      "1241\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-10 19:40\n",
      "I think there's a methodological issue with the set up in your blog post. If you do plain Kfold in the outer loop, you can still have the same group label in the outer train and test. Why not use leave-one-label-out in both inner and outer?\n",
      "So at a \"big picture\" level I don't see the point of the final section that looks at the folds.  I think that's common practice anyway, doesn't seem specific to nested or group-aware CV.\n",
      "I would prefer if you showed an actual example of overfitting without LOLO. Your explanation of why it can happen is good, but it would be much better to illustrate it.\n",
      "You suggested earlier using KNN and generating some data based on the patient id.\n",
      "\n",
      "4\n",
      "1242\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-10 20:00\n",
      "@xuewei4d btw were you aware that semantics of np.diag are changing? in new versions, np.diag returns a view. Not sure that is relevant, I haven't reviewed this parts in detail\n",
      "\n",
      "1\n",
      "1243\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-10 21:43\n",
      "also I cann't load kernelsvm.tripod.com\n",
      "\n",
      "1\n",
      "1244\n",
      "554e8cf715522ed4b3e02ab6\n",
      "2015-08-11 01:16\n",
      "'\n",
      "\n",
      "1\n",
      "1245\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-11 14:16\n",
      "@amueller I checked np.diag usages in my code, I think it is OK. I didn't write sth into the diagonal elements of an array.\n",
      "I just added the verbose flag back into mixture modules. What else should I do before soft pencil down? @amueller @ogrisel\n",
      "\n",
      "2\n",
      "1246\n",
      "53135b495e986b0712efc453\n",
      "2015-08-11 17:08\n",
      "@vene Thanks for the review!! I couldn't find a nice dataset to illustrate LOLO :/ There is one dataset inside the proprietary `perClass` package (small_medical... I'll have to mail them and ask if it can be used in a blog post... moreover it is a matlab package so I'll have to convert it to a csv file)   Do you have any suggestions?? Earlier I recall yourself suggesting search query... could you expand a bit on that pl??\n",
      "And I can load kernelsvm.tripod.com :O That seems to be a pretty famous reference for SVR...\n",
      "Appveyor doesn't seem to test the model selection module :/ Do I have to add something somewhere?\n",
      "@amueller @ogrisel\n",
      "\n",
      "4\n",
      "1247\n",
      "5478876cdb8155e6700d907b\n",
      "2015-08-11 17:18\n",
      "does someone know how to \"restart\" a appveyor build? The build for a PR failed with some weird errors that I am pretty confident have nothing to do with the contents of the PR #5037.\n",
      "thanks, appveyor is doing its thing :)\n",
      "\n",
      "3\n",
      "1248\n",
      "53135b495e986b0712efc453\n",
      "2015-08-11 17:25\n",
      "simplest way would be to push a commit / make some amends to your previous comment, squash and force push it...\n",
      "\n",
      "1\n",
      "1249\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-11 17:30\n",
      "@rvraghav93 you can always create a syntethic dataset\n",
      "\n",
      "1\n",
      "1250\n",
      "53135b495e986b0712efc453\n",
      "2015-08-11 18:11\n",
      "Okay! I'll look on how to create one... Any tips?? :)\n",
      "Also could you take a look at #4919 ?\n",
      "\n",
      "2\n",
      "1251\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-11 23:56\n",
      "@rvraghav93 regarding the dataset: you can take the diabetes dataset and invent arbitrary group ids (as you already have), but generate a new `y` based on a formula you come up with (a function of one or two features and also of the group_id)\n",
      "or you could just use `make_blobs` with a large number of blobs and arbitrarily assign half of them to the positive and half to the negative class. Then the blob assignment is the group_id.  Imagine if half a blob is in training and half in testing, a classifier like KNN will predict really well.\n",
      "Makes sense?\n",
      "\n",
      "3\n",
      "1252\n",
      "53135b495e986b0712efc453\n",
      "2015-08-12 14:31\n",
      "@amueller @vene Could you confirm if [this](https://github.com/scikit-learn/scikit-learn/pull/4294/files#r35808353) can be left untouched??\n",
      "@vene Thanks!!! And yes it helps! I'll modify the example that way!! Using `make_blobs`, I will also be able to illustrate it graphically :)\n",
      "\n",
      "2\n",
      "1253\n",
      "53135b495e986b0712efc453\n",
      "2015-08-12 14:45\n",
      "Also could anyone help me with [this](https://travis-ci.org/scikit-learn/scikit-learn/jobs/75260268#L1509) failure :/ I am unable to comprehend why this test should fail :|\n",
      "\n",
      "1\n",
      "1254\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-12 15:58\n",
      "@rvraghav93 regarding the failure: maybe the doctest should have the random seed fixed?\n",
      "\n",
      "1\n",
      "1255\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-12 17:03\n",
      "@xuewei4d btw were you aware that semantics of np.diag are changing? in new versions, np.diag returns a view. Not sure that is relevant, I haven't reviewed this parts in detail\n",
      "(hm sorry sent that before)\n",
      "@xuewei4d I'm sorry, I don't think I'll be able to review before the pencil down. I'm pretty busy and have to look at @rvraghav93's work. I'm not sure where @ogrisel is.\n",
      "Where is loic btw?\n",
      "\n",
      "4\n",
      "1256\n",
      "53135b495e986b0712efc453\n",
      "2015-08-12 19:24\n",
      "@vene No it passes in master... it used to pass in my branch too... since I squashed a few commits I am unable to get the exact point at which it broke :sob:\n",
      "\n",
      "1\n",
      "1257\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-12 20:42\n",
      "you can always git bisect\n",
      "but have you tried that piece of code locally in ipython with random_state to None vs other fixed values?\n",
      "\n",
      "2\n",
      "1258\n",
      "53135b495e986b0712efc453\n",
      "2015-08-13 10:46\n",
      "Thanks!! git bisect is cool... This seems to happen after the OnlinLDA pr.. (#3659)  I'll send a PR to fix the random state...\n",
      "\n",
      "1\n",
      "1259\n",
      "53135b495e986b0712efc453\n",
      "2015-08-13 10:55\n",
      "Wait thats not right... Fixing random_state fixes this but I am unable to figure out why master doesn't fail..\n",
      "\n",
      "1\n",
      "1260\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-13 13:15\n",
      "where is the random state not fixed?\n",
      "\n",
      "1\n",
      "1261\n",
      "53135b495e986b0712efc453\n",
      "2015-08-13 13:46\n",
      "https://github.com/rvraghav93/scikit-learn/commit/97d4f3eaba284c07406b82a5d75d9da8196e95e7\n",
      "\n",
      "1\n",
      "1262\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-13 16:42\n",
      "That's not in the LDA branch..\n",
      "did you sent a PR, I didn't see it.\n",
      "\n",
      "2\n",
      "1263\n",
      "53135b495e986b0712efc453\n",
      "2015-08-13 16:54\n",
      "No... I just added it as a commit to #4294.. and yea it has nothing to do with LDA... sorry I misread git bisect output...\n",
      "Also appveyor is not testing model selection... why is that??\n",
      "\n",
      "2\n",
      "1264\n",
      "5478876cdb8155e6700d907b\n",
      "2015-08-13 17:10\n",
      "\n",
      "2\n",
      "1265\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-13 17:36\n",
      "ah, but the commit is not in the addition to the examples. I'm not entirely certain but maybe it's better to merge these two so they are self-contained\n",
      " s/tests/examples/\n",
      "\n",
      "2\n",
      "1266\n",
      "53135b495e986b0712efc453\n",
      "2015-08-13 17:45\n",
      "`E486: Pattern not found: tests` :p\n",
      "Ok so you mean we can merge https://github.com/rvraghav93/scikit-learn/pull/3 into #4294 right?\n",
      "\n",
      "7\n",
      "1267\n",
      "53135b495e986b0712efc453\n",
      "2015-08-13 17:48\n",
      "For the last few failures... Only travis seemed to be unhappy... appveyor didn't raise any errors... I'll confirm with a dummy failing test in a moment...\n",
      "merged...\n",
      "Is there a way to make travis build the documentation automatically and host it at a temporary place somewhere? (maybe using pythonanywhere.com + additional travis build just for docs)?\n",
      "Sorry :/\n",
      "http://rvraghav93.github.io/scikit-learn/doc/_build/html/stable/\n",
      "\n",
      "5\n",
      "1268\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-13 17:56\n",
      "we have a setup hosted on rackspace but it is non-trivial and you don't have access, sorry\n",
      "we should improve that.\n",
      "\n",
      "2\n",
      "1269\n",
      "53135b495e986b0712efc453\n",
      "2015-08-13 18:02\n",
      "Could you review [this commit](https://github.com/rvraghav93/scikit-learn/commit/b5077d2f817b7c78782da3703c7cf4847809092a) which tests `_CVIterableWrapper` alone? Its a minor one... (And I did this since you said that there were no tests covering it.. (your comment got hidden..)...)\n",
      "\n",
      "2\n",
      "1270\n",
      "53135b495e986b0712efc453\n",
      "2015-08-13 22:46\n",
      "This is the documentation - http://rvraghav93.github.io/scikit-learn/ Once this gets an OK I think #4294 is finally done :D\n",
      "\n",
      "1\n",
      "1271\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-13 22:48\n",
      "@rvraghav93 that 404s for me\n",
      "\n",
      "1\n",
      "1272\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-14 13:53\n",
      "I'll give it a (hopefully) final review this afternoon :)\n",
      "\n",
      "1\n",
      "1273\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-14 15:03\n",
      "Hi, how could I preview the html generated by rst file ?\n",
      "\n",
      "1\n",
      "1274\n",
      "53135b495e986b0712efc453\n",
      "2015-08-14 15:34\n",
      "Open doc/_build/html/stable/index.html in your browser :)\n",
      "@amueller thanks!!\n",
      "\n",
      "2\n",
      "1275\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-14 15:43\n",
      "Thanks @rvraghav93\n",
      "after ```make```?\n",
      "\n",
      "2\n",
      "1276\n",
      "550f53e215522ed4b3dda5f6\n",
      "2015-08-14 15:51\n",
      "Thanks. I got it.\n",
      "\n",
      "1\n",
      "1277\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-14 15:54\n",
      "after make in the doc folder\n",
      "@rvraghav93 there are no changes to doc/ or examples/ in #4294. did you forget to push or something?\n",
      "oh, wait, I thought you merged https://github.com/rvraghav93/scikit-learn/pull/3 and https://github.com/rvraghav93/scikit-learn/pull/4 but you didn't?\n",
      "well there were no changes in the grid-search part?\n",
      "it still had the header grid-search and was referencing grid_search.*\n",
      "\n",
      "6\n",
      "1278\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-14 17:45\n",
      "the built at http://rvraghav93.github.io/scikit-learn/doc/_build/html/stable/modules/grid_search.html doesn't seem to be using the documentation change branch?\n",
      "I'll review rvraghav93/scikit-learn#4 now, but it would be good to have a built of it\n",
      "\n",
      "2\n",
      "1279\n",
      "53135b495e986b0712efc453\n",
      "2015-08-14 17:58\n",
      "2 mins! :)\n",
      "\n",
      "2\n",
      "1280\n",
      "53135b495e986b0712efc453\n",
      "2015-08-14 18:05\n",
      "It should be updated now!! How did you detect it was not updated? (asking so I could check that part of doc and see if it reflects the change :) )\n",
      "\n",
      "1\n",
      "1281\n",
      "53135b495e986b0712efc453\n",
      "2015-08-14 18:12\n",
      "And yes I didn't merge https://github.com/rvraghav93/scikit-learn/pull/4... I only merged https://github.com/rvraghav93/scikit-learn/pull/3 since I got a +1 from yourself and Vlad... Do you suggest that I merge https://github.com/rvraghav93/scikit-learn/pull/4 too?\n",
      "\n",
      "1\n",
      "1282\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-14 18:21\n",
      "well I wanted to give it a final pass as a whole. but I'll review the 4 now on its own\n",
      "\n",
      "1\n",
      "1283\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-14 18:27\n",
      "I can help\n",
      "\n",
      "1\n",
      "1284\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-14 18:50\n",
      "thanks @vene. I think the main question on rvraghav93/scikit-learn#4 is whether to keep the old stuff in the references\n",
      "and whether to \"fix\" the references in the whatsnew (loads of non-links otherwise?)\n",
      "I'd actually like at least one more feedback on keeping the old methods in the references or not.\n",
      "@rvraghav93 are you around?\n",
      "I know ;)\n",
      "did you fix the other examples, though?\n",
      "if so, you should push...\n",
      "cool\n",
      "and the doc pr was rebased on top?\n",
      "\n",
      "14\n",
      "1285\n",
      "53135b495e986b0712efc453\n",
      "2015-08-14 18:58\n",
      "@vene thanks!! yayy with all 3 of us online... its getting merged by today I suppose ;) :D\n",
      "\n",
      "1\n",
      "1286\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-14 19:07\n",
      "@rvraghav93 did you do \"make html\" in the doc folder to run all the examples? That shouldn't give any deprecation warnings wrt the move\n",
      "same goes for running the tests. the old classes should be tested, but all deprecations should be caught.\n",
      "\n",
      "2\n",
      "1287\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-14 19:32\n",
      "@rvraghav93 does rvraghav93/scikit-learn#4  include the example fixes?\n",
      "\n",
      "1\n",
      "1288\n",
      "53135b495e986b0712efc453\n",
      "2015-08-14 19:51\n",
      "2 mins... the make html takes a looooong time :(\n",
      "Example fixes were merged into #4294\n",
      "of `model_selection` branch which includes example fixes.. yes :)\n",
      "\n",
      "3\n",
      "1289\n",
      "53135b495e986b0712efc453\n",
      "2015-08-14 20:18\n",
      "something got screwed up :sob: now I deleted the merged branch too :/ will have to redo the work... give me a few mins... its quite trivial only...\n",
      "yay I was able to recover it as a patch from [here](https://github.com/rvraghav93/scikit-learn/commit/beec231002e722ea19a494dfc411140ac6327842) :D\n",
      "\n",
      "2\n",
      "1290\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-14 20:24\n",
      "with git, things are rarely truly lost\n",
      "\n",
      "1\n",
      "1291\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-14 22:40\n",
      "@rvraghav93 sorry I gotta run. I'll continue reviewing tomorrow\n",
      "\n",
      "1\n",
      "1292\n",
      "555e110715522ed4b3e0bd50\n",
      "2015-08-18 10:26\n",
      "Anyone interested in giving some feedback on https://github.com/scikit-learn/scikit-learn/pull/5123 ?\n",
      "\n",
      "1\n",
      "1293\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-18 14:15\n",
      "@basveeling sorry, feedback often takes some time. It sounds like a good idea to me, though I'm not the authority on this subject.\n",
      "When designing the classifiers, we didn't have \"most entries are zero all the time\" in mind, which is the case for hashing.\n",
      "\n",
      "2\n",
      "1294\n",
      "555e110715522ed4b3e0bd50\n",
      "2015-08-18 16:36\n",
      "@amueller no worries, thanks for the feedback! I'll spend some more time on benchmarking the sparse structure\n",
      "\n",
      "1\n",
      "1295\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-08-18 19:06\n",
      "@rvraghav93: gentle reminder that Friday is the firm GSoC pencils-down date.\n",
      "\n",
      "1\n",
      "1296\n",
      "53135b495e986b0712efc453\n",
      "2015-08-19 13:04\n",
      "Apologies for the delay!! I got my French work permit and am shopping some stuff and also applying for the visa :D I'll finish the documentation comments by today!!\n",
      "\n",
      "1\n",
      "1297\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-19 15:40\n",
      "sweet @rvraghav93 ! congratulations!\n",
      "\n",
      "1\n",
      "1298\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-25 17:34\n",
      "ping @vighneshbirodkar we often hang out here for discussions, too [though the more we stay on github the better]\n",
      "\n",
      "1\n",
      "1299\n",
      "53810862048862e761fa2887\n",
      "2015-08-25 18:15\n",
      "Cool\n",
      "\n",
      "1\n",
      "1300\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-08-25 18:50\n",
      "@ogrisel btw I didn't have time to look into the doc build server. do you think you'll find time?\n",
      "\n",
      "1\n",
      "1301\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-01 15:21\n",
      "@rvraghav93 how are things? I haven't heard from you in a while...\n",
      "\n",
      "1\n",
      "1302\n",
      "53135b495e986b0712efc453\n",
      "2015-09-04 18:42\n",
      "Hey!! Looking for apartments in Paris ;( Looks like I'll get the Visa only after I confirm the accom... and I won't be getting accom from univ... Hoping to be there atleast by Sept end/Oct 1st\n",
      "I'll have to finish up the documentation no? Sorry It won't be delayed anymore... I'll do this in a day or two... Only a very few comments to be fixed up...\n",
      "\n",
      "2\n",
      "1303\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-08 14:49\n",
      "@ogrisel are you around?\n",
      "\n",
      "1\n",
      "1304\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-09 20:49\n",
      "@vighneshbirodkar I think this would be interesting to work on: https://github.com/scikit-learn/scikit-learn/issues/4920\n",
      "\n",
      "1\n",
      "1305\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-09 21:14\n",
      "does anyone have opinions about including issue numbers into whatsnew?\n",
      "\n",
      "1\n",
      "1306\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-09-09 22:33\n",
      "Just that sklearn will outlive github\n",
      ":)\n",
      "\n",
      "2\n",
      "1307\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-09 22:33\n",
      "well we can stop using them then ;) Also, outlive the existence of github or the use?\n",
      "As long as the links still work it might be useful, even if we transition to the next version control system ;)\n",
      "\n",
      "2\n",
      "1308\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-09-09 22:41\n",
      "Possibly the existence. But we could always save a copy of the issue text.\n",
      "It seems useful to have the numbers.\n",
      "\n",
      "2\n",
      "1309\n",
      "53135b495e986b0712efc453\n",
      "2015-09-10 00:27\n",
      "I just got a stop gap accommodation till Dec 31 thanks to Mainak Jas and Airbnb... I am back... @amueller is there anything I need to worry about w.r.t this comment? (https://github.com/scikit-learn/scikit-learn/pull/4270#issuecomment-136450796)\n",
      "\n",
      "1\n",
      "1310\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-10 00:37\n",
      "@rvraghav93 Great to have you back!\n",
      "No, it's just that we won't include the model selection changes in the upcoming release.\n",
      "Still it would be great to get them done soon\n",
      "thanks cool :)\n",
      "Do you maybe want to have a look at #4924 ?\n",
      "cool. have fun tonight :)\n",
      "\n",
      "6\n",
      "1311\n",
      "53135b495e986b0712efc453\n",
      "2015-09-10 00:38\n",
      "Okay!! :D and thanks :)\n",
      "\n",
      "1\n",
      "1312\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-10 13:27\n",
      "@amueller I have rebased the LDA deprecation in #5245. Let's wait for CI to check that I did not break anything in the process.\n",
      "\n",
      "1\n",
      "1313\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-10 14:11\n",
      "also #5236\n",
      "\n",
      "1\n",
      "1314\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-10 14:15\n",
      "For #4924, it will need to be updated to work on top of #5245.\n",
      "\n",
      "3\n",
      "1315\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-10 14:29\n",
      "I gave my +1\n",
      "> do you have any opinion on adding PR links to whatsnew? I am fine with it. It would be great to add the PR numbers on all of the items but that sounds tedious to do :)\n",
      "I will have to run soon to take my shuttle. I will be busy tonight so I don't think will be able to work on the release much more today. I should be able to focus on that tomorrow though.\n",
      "You might want to have a look at #5104 as well.\n",
      "\n",
      "4\n",
      "1316\n",
      "53810862048862e761fa2887\n",
      "2015-09-10 18:54\n",
      "`LabelEncoder` right now is doing a binary search using `np.searchsorted`. Can't we speed that up by using a dictionary ?\n",
      "\n",
      "1\n",
      "1317\n",
      "53135b495e986b0712efc453\n",
      "2015-09-11 02:10\n",
      "@amueller @vene Do we need `LabelKFold` in #4294? Why can't we just pass the labels in `split(X, y, labels)` instead of having a new class? (Same for `LabelShuffleSplit`)\n",
      "@ogrisel appveyor doesn't test model selection\n",
      "[![appveyor.png](https://files.gitter.im/scikit-learn/scikit-learn/x9Q7/thumb/appveyor.png)](https://files.gitter.im/scikit-learn/scikit-learn/x9Q7/appveyor.png)\n",
      "\n",
      "3\n",
      "1318\n",
      "53135b495e986b0712efc453\n",
      "2015-09-11 02:41\n",
      "[![appveyor2.png](https://files.gitter.im/scikit-learn/scikit-learn/gKbL/thumb/appveyor2.png)](https://files.gitter.im/scikit-learn/scikit-learn/gKbL/appveyor2.png)\n",
      "1st image you can see one of appveyor builds passing successfully despite a failing test in model_selection, in the 2nd you can see travis working correctly...\n",
      "https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.2085/job/qd27oykpd2ubc1yr#L3734 - After `sklearn.mixture` `sklearn.model_selection` should have been tested...\n",
      "Any clues to debug this?\n",
      "\n",
      "4\n",
      "1319\n",
      "53135b495e986b0712efc453\n",
      "2015-09-11 05:27\n",
      "[![appveyorfinal.png](https://files.gitter.im/scikit-learn/scikit-learn/KW30/thumb/appveyorfinal.png)](https://files.gitter.im/scikit-learn/scikit-learn/KW30/appveyorfinal.png)\n",
      "\n",
      "1\n",
      "1320\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-11 14:13\n",
      "The newest merge seems to have modified _tree.c\n",
      "\n",
      "1\n",
      "1321\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-11 14:13\n",
      "which merge?\n",
      "\n",
      "1\n",
      "1322\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-11 14:14\n",
      "\"ENH add sag solver in LinearRegression and Ridge\"\n",
      "\n",
      "7\n",
      "1323\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-11 14:27\n",
      "pushed the cythonized tree code to master directly\n",
      "\n",
      "1\n",
      "1324\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-11 14:28\n",
      "appveyor is failing?\n",
      "or is that from the previous tree code\n",
      "ah, appveyor is really behind\n",
      "\n",
      "3\n",
      "1325\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-11 14:29\n",
      "here is the state of the queue: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/history\n",
      "\n",
      "4\n",
      "1326\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-11 15:06\n",
      "@ogrisel I'm in ;)\n",
      "sorry meetings stuff\n",
      "\n",
      "2\n",
      "1327\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-11 15:07\n",
      "no pbm\n",
      "\n",
      "1\n",
      "1328\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-11 15:09\n",
      "I'll do #5104 and then the tsne example\n",
      "the mlp is starting to look good btw #5214\n",
      "\n",
      "5\n",
      "1329\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-11 15:14\n",
      "> have you seen this: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.2086/job/drvmlx86c9swelx4 ?  Hum that's bad\n",
      "\n",
      "5\n",
      "1330\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-11 15:19\n",
      "That's weird that it only fails on 32 bit Python, both 2 and 3. It's seems completely unrelated to the architecture.\n",
      "maybe it's just random?\n",
      "the fact that we get it with Python 2 is really weird: it means that it cannot be caused by the use of the multiprocessing context /  start method in  joblib because this does not exist under Python 2.\n",
      "\n",
      "3\n",
      "1331\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-11 15:22\n",
      "here the failing pattern is different: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.2090\n",
      "\n",
      "1\n",
      "1332\n",
      "53135b495e986b0712efc453\n",
      "2015-09-11 15:23\n",
      "@ogrisel #4294 And no it was not a doctest... I specifically made a failing test inside model selection to confirm my observation... :)\n",
      "\n",
      "1\n",
      "1333\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-11 15:24\n",
      "maybe this is caused by a change in the way nose run the tests.\n",
      "I will open an issue to track this problem\n",
      "@rvraghav93 your issue is probably not related to the appveyor problem we are discussing (which has to do with multiprocessing #5254)\n",
      "\n",
      "3\n",
      "1334\n",
      "53135b495e986b0712efc453\n",
      "2015-09-11 15:25\n",
      "~~Okay but this is localized to model_selection right? maybe I am doing something incorrectly?~~\n",
      "Also @amueller @vene do we need the new `LabelKFold` and `LabelShuffleSplit` as separate classes or can we specify the labels in `split(X, y, labels)` in the (new) `KFold` class itself?\n",
      "\n",
      "4\n",
      "1335\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-11 15:30\n",
      "@rvraghav93 do you have a windows machine at hand? you can try to replicate it locally by following the install stepts in appveyor.yml\n",
      "\n",
      "1\n",
      "1336\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-11 15:35\n",
      "@rvraghav93 they do somewhat different things. What is the benefit of putting them in the same class?\n",
      "\n",
      "1\n",
      "1337\n",
      "53135b495e986b0712efc453\n",
      "2015-09-11 15:42\n",
      "`LabelKFold` is `KFold` with `labels` (somewhat like group labels) specifying  that the points in the same label should not be used for both testing and training right? Would it benefit from grouping together?\n",
      "\n",
      "1\n",
      "1338\n",
      "541a528b163965c9bc2053de\n",
      "2015-09-11 16:39\n",
      "@amueller unfortunately I will have to leave soon and won't be able to work on the release this WE. I think we should fix the appveyor issue before cutting the branch. I have opened a PR there #5255 to try a quickfix even though I don't understand the problem.\n",
      "I can work on that on monday if that does not work\n",
      "I added some quick benchmark in the comments of #5253.\n",
      "appveyor has a network problem on the fast infra and so the queue is running on the old Azure based infra\n",
      "this is why the build are slower than usual\n",
      "I don't know if that explains the weird multiprocessing issue though (it seems unlikely)\n",
      "ok I have to go\n",
      "see you later\n",
      "\n",
      "8\n",
      "1339\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-11 17:27\n",
      "ok ttyl. Can you work on the release next week?\n",
      "\n",
      "1\n",
      "1340\n",
      "53135b495e986b0712efc453\n",
      "2015-09-11 17:50\n",
      "@amueller @vene Actually that won't make sense especially when we want to group `Stratified{KFold|ShuffleSplit}` and `{KFold|ShuffleSplit}` together making stratify an option as suggested by Joel! Sorry for the noise... I'll add it as such :)\n",
      "\n",
      "1\n",
      "1341\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-12 17:54\n",
      "AppVeyor tweeted about slow performance recently; I guess that's what is holding up all the tests\n",
      "\n",
      "1\n",
      "1342\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-13 00:58\n",
      "@ogrisel the MLP is good to go, too, I think. do you want to merge it after release or before? I don't really see a reason not to merge now.\n",
      "\n",
      "1\n",
      "1343\n",
      "553e8e1015522ed4b3df97f7\n",
      "2015-09-13 14:02\n",
      "Hey guys, can we merge this? https://github.com/scikit-learn/scikit-learn/pull/4525\n",
      "\n",
      "1\n",
      "1344\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-13 15:25\n",
      "I think so. You have my +!\n",
      "+1\n",
      "I think we should fix these. @ogrisel didn't want to fix them I think.\n",
      "I mean it does add clutter to the docstrings, but it is not rendered on the webpage\n",
      "\n",
      "4\n",
      "1345\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-13 15:28\n",
      "I'm taking a quick look as well\n",
      "Looks great, thanks for the work!\n",
      "\n",
      "2\n",
      "1346\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-13 15:32\n",
      "@rvraghav93 what is the status of your issue? I have a windows machine, I can take a quick look.\n",
      "\n",
      "1\n",
      "1347\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-13 15:40\n",
      "which issue is that?\n",
      "\n",
      "1\n",
      "1348\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-13 15:43\n",
      "I'm not sure, I just saw Olivier asked if he had a windows machine to reproduce an Appveyor issue,.\n",
      "\n",
      "1\n",
      "1349\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-13 15:46\n",
      "ah that is the joblib one that keeps appveyor from failing\n",
      "just run the test suite\n",
      "\n",
      "5\n",
      "1350\n",
      "53135b495e986b0712efc453\n",
      "2015-09-13 16:01\n",
      "@jmschrei thanks a lot!! In my PR #4294 the tests in the new module `model_selection` are not being run in appveyor... Any help would be really awesome!! :) I could use virtual box... but earlier I had little success doing so :(\n",
      "\n",
      "1\n",
      "1351\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-13 16:03\n",
      "That is a lot of files changed.\n",
      "\n",
      "3\n",
      "1352\n",
      "53135b495e986b0712efc453\n",
      "2015-09-13 16:06\n",
      "You can replicate that by simply creating a foo folder and a tests directory with a simple failing test :)\n",
      "Yes that's a whole lot of commits ;) I'll probably squash it to less than 5\n",
      "It will help tracking things easier....\n",
      "\n",
      "3\n",
      "1353\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-13 16:09\n",
      "I don't understand, is this a PR meant to refactor CV or reorganize the modules?\n",
      "\n",
      "4\n",
      "1354\n",
      "53135b495e986b0712efc453\n",
      "2015-09-13 16:11\n",
      "Refractor as in making then data dependent... and reorganise into `model_selection` folder\n",
      "*independent\n",
      "\n",
      "2\n",
      "1355\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-13 16:30\n",
      "After months of developing in Ubuntu, I am remembering why Windows is such a pain.\n",
      "\n",
      "2\n",
      "1356\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-09-13 16:43\n",
      "@rvraghav93 could you clarify what about the labels param I should comment on?\n",
      "\n",
      "1\n",
      "1357\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-13 16:48\n",
      "I am getting 12 failures\n",
      "Mostly related to string format\n",
      "Is this what you are getting, @rvraghav93\n",
      "\n",
      "3\n",
      "1358\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-13 17:15\n",
      "These errors seem to stem from getting longs instead of ints\n",
      "This manifests as getting (10L, 2L) instead of (10, 2)\n",
      "\n",
      "2\n",
      "1359\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-13 18:19\n",
      "Is there any way to force numpy to use ints? I'm not finding anything, unfortunately.\n",
      "In the shape, I mean.\n",
      "\n",
      "2\n",
      "1360\n",
      "53135b495e986b0712efc453\n",
      "2015-09-14 00:32\n",
      "@jmschrei No my concern is that the model_selection tests are not at all run by appveyor... only travis seems to detect the module and run the tests...\n",
      "Is that because there are no public python sources in `model_selection`? (all three are private)\n",
      "@vene You had earlier said that the doc for labels param was not apt... Does `\" Class labels to be assigned to the samples and used while splitting the dataset into test/train set.\"` sound like a good doc for `labels` param?\n",
      "(of `split(X, y, labels)`?)\n",
      "\n",
      "4\n",
      "1361\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-14 07:16\n",
      "The unit tests are failing on my machine. raise self.failureException(\"ImportError('No module named sag_fast'm) != None\")\n",
      "Ubuntu 64 bit, they were all running fine on Friday.\n",
      "\n",
      "2\n",
      "1362\n",
      "55e5c37d0fc9f982beaf4d61\n",
      "2015-09-14 07:34\n",
      "did you miss a make in ?\n",
      "\n",
      "2\n",
      "1363\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-14 13:19\n",
      "@rvraghav93 it shouldn't be \"class labels\" they are not class labels. maybe group labeles?\n",
      "also not really assigned to the samples?\n",
      "\n",
      "2\n",
      "1364\n",
      "553e8e1015522ed4b3df97f7\n",
      "2015-09-14 16:25\n",
      "Guys, I opened this PR https://github.com/scikit-learn/scikit-learn/pull/5271 to fix a minor typo in doc/modules/neighbors.rst L470\n",
      "I'll do a git grep to check for more typos, if there are any\n",
      ":sweat_smile:\n",
      "\n",
      "3\n",
      "1365\n",
      "53135b495e986b0712efc453\n",
      "2015-09-19 13:03\n",
      "how about `\"Group labels for the samples used while splitting the dataset into test/train set.\"`?\n",
      "@amueller @vene\n",
      "This would be for all the `labels` parameter in `.*Label.*` classes...\n",
      "\n",
      "3\n",
      "1366\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-19 18:07\n",
      "yes that sounds good\n",
      "\n",
      "1\n",
      "1367\n",
      "53135b495e986b0712efc453\n",
      "2015-09-20 06:59\n",
      "@amueller @vene I've fixed the documentation and the `labels` param... I've hosted the doc (with examples and all the new changes but without plots) [here](http://rvraghav93.github.io/doc_builds)\n",
      "The doc with plots is building... It will hopefully get over in a few hours and I'll host it once its done...\n",
      "Ah one more thing... the appveyor not testing the `model_selection` is not yet fixed... :/ I suspect its because that module has not public python files? That would be the only thing left to investigate apart from your final reviews on rvraghav93/scikit-learn#4\n",
      "And moving `grid_search.rst` to `search.rst` once the review is over :)\n",
      "\n",
      "4\n",
      "1368\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-21 15:44\n",
      "Thanks @rvraghav93. I'll try to review soon. I have a couple of hundred emails I need to read, and we want to release this week, though\n",
      "@ogrisel (or anyone else) do you remember the recent blog post that explained tree-based models as linear combinations ?\n",
      "I don't think that was it. there was a way to explain the prediction made for a single point as a simple function of the features somehow\n",
      "\n",
      "3\n",
      "1369\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-09-21 17:01\n",
      "@amueller do you mean Tianqis?\n",
      "https://github.com/scikit-learn/scikit-learn/pull/5222\n",
      "\n",
      "2\n",
      "1370\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-22 00:43\n",
      "https://github.com/scikit-learn/scikit-learn/pull/5293 reviews for 0.17?\n",
      "@vighneshbirodkar This one would be good to fix: https://github.com/scikit-learn/scikit-learn/issues/5089\n",
      "\n",
      "2\n",
      "1371\n",
      "53810862048862e761fa2887\n",
      "2015-09-22 16:22\n",
      "Can @amueller or someone else OK this ? https://github.com/scikit-learn/scikit-learn/pull/5234\n",
      "It is a minor change and can be easily included in the upcoming release\n",
      "\n",
      "2\n",
      "1372\n",
      "53810862048862e761fa2887\n",
      "2015-09-22 16:35\n",
      "@amueller There are some warnings due to PIL not being there. I can either install PIL on travis or skip those tests like this https://github.com/scikit-image/scikit-image/blob/master/skimage/future/graph/tests/test_rag.py#L51\n",
      "\n",
      "1\n",
      "1373\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-22 17:59\n",
      " scikit-learn/scikit-learn#5234 already has my +1\n",
      "\n",
      "1\n",
      "1374\n",
      "53135b495e986b0712efc453\n",
      "2015-09-26 20:48\n",
      "@vene would you be coming for the sprint?? :) - https://github.com/scikit-learn/scikit-learn/wiki/Upcoming-events\n",
      ";(\n",
      "May I ask - funding or busy?? :)\n",
      "\n",
      "3\n",
      "1375\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-09-26 20:49\n",
      "Unfortunately not :(\n",
      "\n",
      "1\n",
      "1376\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-09-26 20:52\n",
      "Both... I'm in a slightly awkward position in my PhD\n",
      "I might visit Paris in the winter though\n",
      "No clue, it's just hope for now.\n",
      "\n",
      "4\n",
      "1377\n",
      "53135b495e986b0712efc453\n",
      "2015-09-26 20:54\n",
      "Thats great!! when??\n",
      "\n",
      "1\n",
      "1378\n",
      "53135b495e986b0712efc453\n",
      "2015-09-27 00:38\n",
      "@amueller reviews for  rvraghav93/scikit-learn#4 please? :grin:\n",
      "(2 comments of Joel are yet  to be addressed there)\n",
      "\n",
      "2\n",
      "1379\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-09-27 17:21\n",
      "Sorry I'm still quite sick and stressed. I'll get to it asap\n",
      "\n",
      "1\n",
      "1380\n",
      "53135b495e986b0712efc453\n",
      "2015-09-28 18:12\n",
      "Oh!! Okay get well soon :)\n",
      "\n",
      "1\n",
      "1381\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-10-02 21:45\n",
      "I'm toying with the idea of doing a PyData NYC pilgrimage next month. Anyone planning on being there? Chance for a sklearn sprint perhaps?\n",
      "\n",
      "1\n",
      "1382\n",
      "53810862048862e761fa2887\n",
      "2015-10-02 21:51\n",
      "Count me in\n",
      "\n",
      "1\n",
      "1383\n",
      "53135b495e986b0712efc453\n",
      "2015-10-05 14:30\n",
      "Do we need to have the notifications for test failures in gitter activity bar... wouldn't notifications on PR activity suffice?\n",
      "@ogrisel\n",
      "\n",
      "2\n",
      "1384\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-05 14:33\n",
      "@rvraghav93 we can try adding them in gitter. If it's too noisy, we can remove it.\n",
      "\n",
      "1\n",
      "1385\n",
      "53135b495e986b0712efc453\n",
      "2015-10-06 14:06\n",
      "Okay :)\n",
      "\n",
      "1\n",
      "1386\n",
      "53135b495e986b0712efc453\n",
      "2015-10-06 14:22\n",
      "BTW from discussions at #4254 I don't think #4225 will be in soon... Should it still be tagged 1.7?\n",
      "\n",
      "1\n",
      "1387\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-06 17:00\n",
      "I agree, re-tagged.\n",
      "\n",
      "1\n",
      "1388\n",
      "5612e60ad33f749381a8600b\n",
      "2015-10-06 19:59\n",
      "Where is the best place to find help on deprecation issues for scikit?  I'm having trouble using Multilabelbinarizer and neither stackoverflow nor googling are helping.\n",
      "\n",
      "2\n",
      "1389\n",
      "5612e60ad33f749381a8600b\n",
      "2015-10-06 20:05\n",
      "I'm trying to use the accuracy_score function (or even the Confusion matrix function) and I've transformed my Ytest using an multilabel binarizer so it's type is \"multilabel-indicator\" but the predicted values are in the form of binary.\n",
      "so I get a \"ValueError: Can't handle mix of multilabel-indicator and binary\" error, but multilabel-indicator seems to be supported according to line 93 in classification.py\n",
      "\n",
      "2\n",
      "1390\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-07 09:11\n",
      "@TracyMRohlin please write this a small reproduction code snippet on small random data (e.g. using `np.random.randn` and such) and post it as a question on stackoverflow. If you think this is a bug, post it as an issue on the scikit-learn issue tracker instead.\n",
      "\n",
      "1\n",
      "1391\n",
      "53135b495e986b0712efc453\n",
      "2015-10-07 14:37\n",
      "#3123 can be closed!\n",
      "\n",
      "1\n",
      "1392\n",
      "53135b495e986b0712efc453\n",
      "2015-10-07 14:44\n",
      "@ogrisel Manoj wants you to review his #4242 if possible :P\n",
      "\n",
      "1\n",
      "1393\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-07 14:45\n",
      "I would like to focus on bug fixing for the release this week...\n",
      "\n",
      "1\n",
      "1394\n",
      "53135b495e986b0712efc453\n",
      "2015-10-07 14:54\n",
      "Okay :)\n",
      "\n",
      "1\n",
      "1395\n",
      "53135b495e986b0712efc453\n",
      "2015-10-07 16:24\n",
      "@TomDLT Can I take #4523 up?\n",
      "\n",
      "1\n",
      "1396\n",
      "555b8aa615522ed4b3e0a160\n",
      "2015-10-07 16:47\n",
      "yes sure!\n",
      "\n",
      "1\n",
      "1397\n",
      "53135b495e986b0712efc453\n",
      "2015-10-09 14:56\n",
      "@ogrisel I feel #4826 can be included in 0.17?? It was already [reviewed by Andy](https://github.com/scikit-learn/scikit-learn/pull/4826#issuecomment-125715318) a second review should make it merge-able?\n",
      "\n",
      "1\n",
      "1398\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-09 16:43\n",
      "opening my sklearn inbox now\n",
      "everybody brace themselves for spam\n",
      "(by me)\n",
      "\n",
      "3\n",
      "1399\n",
      "53135b495e986b0712efc453\n",
      "2015-10-09 16:54\n",
      "I'm eagerly waiting ;)\n",
      "\n",
      "1\n",
      "1400\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-09 16:58\n",
      "did https://github.com/scikit-learn/scikit-learn/pull/4826\n",
      "\n",
      "1\n",
      "1401\n",
      "530c03e25e986b0712efafb8\n",
      "2015-10-09 17:13\n",
      "What is the right way to get a nice unique and consistent hash value for an estimator?\n",
      "can I hash something like `type(est), est.get_params(), ...`?\n",
      "\n",
      "2\n",
      "1402\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-09 18:10\n",
      "with or without the part that is estimated from data? Without that should cover it.\n",
      "\n",
      "4\n",
      "1403\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-09 19:59\n",
      "well with is harder. I think we opted for storing the data along, right? I think storing the class, get_params, and the data is enough. With a fixed random_state that is.\n",
      "actually, what is the definition of unique consistent hash?\n",
      "should they be the same if a) they are the same object [probably not] b) they behave the same way? c) they are the same in memory? I guess the answer is c)?\n",
      "\n",
      "3\n",
      "1404\n",
      "530c03e25e986b0712efafb8\n",
      "2015-10-09 20:15\n",
      "I'm trying to solve this more generally and in isolation from the dasklearn project.  Is there a consistent set of attributes on a BaseEstimator that define it?  After I call `estimator.fit(X)`is there a set of attributes on the object that I can consistently check?  Or does this vary estimator-to-estimator?\n",
      "is there a way to check if a model is fitted?\n",
      "or to revert it to a non-fitted state?\n",
      "\n",
      "3\n",
      "1405\n",
      "53135b495e986b0712efc453\n",
      "2015-10-09 20:38\n",
      "If I understand you correctly, What you require is partly similar to the model similarity checking problem at #4841 AFAIK you can only have a relative equality check and not a(n) (absolute) hash value that can uniquely identify a fit-model...\n",
      "\n",
      "1\n",
      "1406\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-09 22:17\n",
      "@mrocklin beware of `est.get_params(deep=True)` that includes both the subestimator instances and their params\n",
      "e.g.: ```python >>> from sklearn.svm import SVC >>> from sklearn.decomposition import PCA >>> from sklearn.pipeline import make_pipeline >>> p = make_pipeline(PCA(3), SVC()) >>> p.get_params(deep=True) {'svc__probability': False, 'svc__decision_function_shape': None, 'svc__degree': 3, 'pca__copy': True, 'svc__tol': 0.001, 'svc': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',   max_iter=-1, probability=False, random_state=None, shrinking=True,   tol=0.001, verbose=False), 'steps': [('pca', PCA(copy=True, n_components=3, whiten=False)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',   max_iter=-1, probability=False, random_state=None, shrinking=True,   tol=0.001, verbose=False))], 'svc__cache_size': 200, 'svc__max_iter': -1, 'pca__n_components': 3, 'svc__coef0': 0.0, 'pca__whiten': False, 'svc__shrinking': True, 'pca': PCA(copy=True, n_components=3, whiten=False), 'svc__gamma': 'auto', 'svc__verbose': False, 'svc__C': 1.0, 'svc__kernel': 'rbf', 'svc__class_weight': None, 'svc__random_state': None} ```\n",
      "\n",
      "2\n",
      "1407\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-09 22:20\n",
      "> After I call estimator.fit(X)is there a set of attributes on the object that I can consistently check? Or does this vary estimator-to-estimator?  It varies on an per-estimator basis.\n",
      "Attributes learned from data (by the call to fit) ends in `_`.\n",
      "`get_params` only returns constructor parameters (aka hyperparameters) not the fitted parameters\n",
      "we don't have a good abstraction to introspect / serialize / deserialize fitted models.\n",
      "to revert to a non-fitted state you can use:  ```python >>> from sklearn.base import clone >>> unfitted_est = clone(fitted_est) ```\n",
      "the `clone`  name is not necessarily a good name...\n",
      "\n",
      "6\n",
      "1408\n",
      "530c03e25e986b0712efafb8\n",
      "2015-10-09 22:36\n",
      "Is there an equivalent for to ask if it is fitted?\n",
      "\n",
      "1\n",
      "1409\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-10-10 01:59\n",
      "Have not read any history on this, but https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py#L636 might help? Need to know what attributes get set when fitted for a given est though I think. I think if you grep'd the repo you could find an exhaustive list of those being used to check.\n",
      "\n",
      "1\n",
      "1410\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-12 17:37\n",
      "@mrocklin there is no way currently to know if something has been fitted. The \"idiomatic\" way is to try and predict. If it hasn't been fitted, it will raise an appropriate error. But you need to know the number of features is might have been fitted with.\n",
      "@ogrisel did you have time to set up the doc build server yet?\n",
      "\n",
      "2\n",
      "1411\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-13 00:04\n",
      "wow this is the worst: https://github.com/scikit-learn/scikit-learn/issues/5267\n",
      "\n",
      "1\n",
      "1412\n",
      "53135b495e986b0712efc453\n",
      "2015-10-13 09:19\n",
      "Is having an issue for splitting the current utils into private/public worth it?\n",
      "\n",
      "1\n",
      "1413\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-13 19:33\n",
      "not sure. we should get the model_selection stuff done first.\n",
      "I'm catching up right now\n",
      "ping @GaelVaroquaux are you here?\n",
      "also ping @ogrisel\n",
      "I might be able to help with releasing this week. should we? Or wait for the sprint?\n",
      "we cam\n",
      "we can't really merge model selection before releasing\n",
      "\n",
      "7\n",
      "1414\n",
      "53135b495e986b0712efc453\n",
      "2015-10-13 19:56\n",
      "If we have the model selection merged by this week I can work on the multiple metric thing during sprint :)\n",
      "You are coming right?\n",
      "Gael is not in gitter ;)\n",
      "\n",
      "3\n",
      "1415\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-13 22:46\n",
      "I just updated to numpy 1.10.1 and now I get a lot of test failures. Anyone else?\n",
      "TypeError: Cannot cast ufunc subtract output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n",
      "running conda\n",
      "\n",
      "3\n",
      "1416\n",
      "53135b495e986b0712efc453\n",
      "2015-10-14 08:48\n",
      "I can confirm that! (numpy pip installed)\n",
      "http://stackoverflow.com/a/14270230/3109769\n",
      "\n",
      "2\n",
      "1417\n",
      "53135b495e986b0712efc453\n",
      "2015-10-14 09:08\n",
      "`np.cancast(np.float64, np.int64)` is `False` from `numpy 1.10.1`\n",
      "\n",
      "1\n",
      "1418\n",
      "53135b495e986b0712efc453\n",
      "2015-10-14 10:53\n",
      "Setting the `dtype` at `check_array` stage fixes these failures... Do we need a travis build for 1.10 or should we update one to check for numpy 1.10.1? (@ogrisel?) - PR at #5398\n",
      "@vene la multi ani!! :P\n",
      "\n",
      "2\n",
      "1419\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-14 12:19\n",
      "I finally took the time to fix the docbuilder machine to update the dev/ website.\n",
      "It seems to work correctly but let me know if you spot problems. There is a couple of broken example in master.\n",
      "\n",
      "2\n",
      "1420\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-14 12:28\n",
      "@amueller I would like to release the beta tomorrow. @lesteve is working on the joblib 0.9.0 release right now.\n",
      "\n",
      "1\n",
      "1421\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-10-14 12:38\n",
      "Thanks @rvraghav93 :)\n",
      "\n",
      "1\n",
      "1422\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-14 14:46\n",
      "@ogrisel cool. Anything I should look at in partticular?\n",
      "@ogrisel the numpy 1.10.1 looks bad. can you confirm?\n",
      "\n",
      "2\n",
      "1423\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-14 14:55\n",
      "@ogrisel I'm still catching up with github notifications and my health sucks :-/\n",
      "\n",
      "1\n",
      "1424\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-14 17:04\n",
      "numpy 1.10.1 need fixes but seemingly not too complicated\n",
      "I have not checked the LogisticRegressionCV issue\n",
      "\n",
      "2\n",
      "1425\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-14 18:09\n",
      "Ok. just got to sklearn notification inbox zero. I'll have a celebratory dirty chai latte and then look at LogisticRegressionCV and other issues that we enthusiastically tagged for 0.17\n",
      "the doc build seems to be working. pushed 30 seconds ago! Awesomeness!! Thanks @ogrisel !\n",
      "(and sorry for the constant nagging about it )\n",
      "and scikit-learn.org/dev/auto_examples/preprocessing/plot_function_transformer.html\n",
      "that would be sweet. sure\n",
      "@rvraghav93 do you want to build the docs and see if you find the errors?\n",
      "or I'll do it.\n",
      "I haven't checked doc build errors or testing errors recently\n",
      "\n",
      "8\n",
      "1426\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-14 18:11\n",
      "> the doc build seems to be working. pushed 30 seconds ago:  I just did a fix :)\n",
      "\n",
      "3\n",
      "1427\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-14 18:12\n",
      "there are errors in some examples\n",
      "I need to deploy an HTTP server to publish the doc build log.\n",
      "+1 a DNS\n",
      "don't have time to do that tonight though\n",
      "+1 for the dirty chai latte :)\n",
      "I'll go and get some dinner now, see you later.\n",
      "\n",
      "6\n",
      "1428\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-14 18:38\n",
      "sure :) It would be good to get https://github.com/scikit-learn/scikit-learn/pull/4478 merged. I'll add a whatsnew now\n",
      "also https://github.com/scikit-learn/scikit-learn/pull/5395\n",
      "\n",
      "2\n",
      "1429\n",
      "53135b495e986b0712efc453\n",
      "2015-10-15 12:19\n",
      "@amueller Sorry I saw the chat just now! I'll build the docs and look for errors  :)\n",
      "\n",
      "1\n",
      "1430\n",
      "5571fe1015522ed4b3e17d90\n",
      "2015-10-15 12:50\n",
      "@rvraghav93 I was planning to look at the broken examples too. Let me know if we can split the work between the two of us.\n",
      "\n",
      "1\n",
      "1431\n",
      "53135b495e986b0712efc453\n",
      "2015-10-15 12:53\n",
      "Yes sure! I'm looking at http://scikit-learn.org/dev/auto_examples/model_selection/plot_roc.html\n",
      "Let me know if you want to take over...\n",
      "yea!\n",
      "\n",
      "3\n",
      "1432\n",
      "5571fe1015522ed4b3e17d90\n",
      "2015-10-15 12:55\n",
      "Do you have a list of all the broken examples? I am generating the doc right now but if you already have the full list I could look at another broken example.\n",
      "\n",
      "4\n",
      "1433\n",
      "5571fe1015522ed4b3e17d90\n",
      "2015-10-15 13:11\n",
      "OK I'll take a look at http://scikit-learn.org/dev/auto_examples/model_selection/plot_roc.html\n",
      "\n",
      "1\n",
      "1434\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-15 13:16\n",
      "> Is there a way to work with 2 branches at once?  clone the scikit-learn repo twice, create 2 conda env or 2 virtualenvs and `pip install -e .` each repo in each env.\n",
      "\n",
      "1\n",
      "1435\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-15 13:22\n",
      "@amueller any review for the joblib sync #5399? It reverts a broken experimental change in the pickle format that was in introduced in joblib 0.9.0b2 (hence not part of scikit-learn 0.16.1). See:  https://github.com/joblib/joblib/blob/master/CHANGES.rst#release-092\n",
      "RandomTreesEmbedding looks like a real regression :(\n",
      "\n",
      "2\n",
      "1436\n",
      "5571fe1015522ed4b3e17d90\n",
      "2015-10-15 14:03\n",
      "I generated the doc locally so I am going to create one ticket by broken example. FWIW I found 5 broken examples: examples/applications/plot_tomography_l1_reconstruction.py examples/ensemble/plot_random_forest_embedding.py examples/manifold/plot_lle_digits.py examples/model_selection/plot_roc.py examples/svm/plot_rbf_parameters.py\n",
      "Note the tomography one is broken only for numpy 1.10\n",
      "\n",
      "2\n",
      "1437\n",
      "53135b495e986b0712efc453\n",
      "2015-10-15 14:05\n",
      "you did doc with plots right? How did it get over so fast? It takes forever on my machine :/ Anyway let me know if you want me to look into any of those while you work on other things...\n",
      "\n",
      "2\n",
      "1438\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-15 14:13\n",
      "Let's split the work on fixing the examples.\n",
      "\n",
      "5\n",
      "1439\n",
      "5571fe1015522ed4b3e17d90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-10-15 14:17\n",
      "@rvraghav93 the plot_roc one is due to the roc_curves not all having the same shape, not sure why ...\n",
      "\n",
      "1\n",
      "1440\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 14:35\n",
      "I'm back\n",
      "\n",
      "1\n",
      "1441\n",
      "5571fe1015522ed4b3e17d90\n",
      "2015-10-15 14:39\n",
      "I did the plot_tomography_l1_reconstruction fix while I was at it.\n",
      "\n",
      "6\n",
      "1442\n",
      "53135b495e986b0712efc453\n",
      "2015-10-15 15:10\n",
      "Is there a reason why we don't have the `requirements.txt` in our repo?\n",
      "\n",
      "1\n",
      "1443\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 15:17\n",
      "yeah because it would need to include numpy and scipy and we don't want people to install this via pip on linux\n",
      "is anyone doing #5407 ?\n",
      "\n",
      "2\n",
      "1444\n",
      "53135b495e986b0712efc453\n",
      "2015-10-15 15:19\n",
      "Okay.. and yes I am...\n",
      "\n",
      "1\n",
      "1445\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 15:26\n",
      "http://scikit-learn.org/dev/modules/classes.html is entirely broken\n",
      "it's the sphinx version with the fun\n",
      "\n",
      "2\n",
      "1446\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 15:38\n",
      "btw, do we want to fix the \"FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\" ?\n",
      "@ogrisel which version of sphinx is on the doc build bot?\n",
      "\n",
      "2\n",
      "1447\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 16:52\n",
      "@ogrisel I mean it is not really release related but for the build bot we need to fix the sphinx version to 1.2.3 not sure how to do that with a salt state\n",
      "oh wait, just  name = sphinx == 1.2.3\n",
      "yeah that is no good. I just sent you a PR to fix it to 1.2.3\n",
      "current stable doesn't build the api docs\n",
      "website build is fixed. Thanks @ogrisel\n",
      "\n",
      "5\n",
      "1448\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-15 17:03\n",
      "> @ogrisel are you planning on uploading the website for the rc? No, right?  Updating the /stable/ part? No I don't think we should do it for the beta.\n",
      "\n",
      "11\n",
      "1449\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 17:20\n",
      "ok. gotta grab some lunch now\n",
      "@rvraghav93 if you're bored you can try to bisect https://github.com/scikit-learn/scikit-learn/issues/5267\n",
      "\n",
      "2\n",
      "1450\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-15 17:20\n",
      "do you want me to do the sklearn-docbuilder stuff?\n",
      "you can always give it a try later\n",
      "guten Appetit!\n",
      "ok\n",
      "\n",
      "4\n",
      "1451\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 17:23\n",
      "merci\n",
      "I can try it later\n",
      "I don't see any fires at the moment\n",
      "\n",
      "3\n",
      "1452\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 17:24\n",
      "fixing more warnings in master and fixing the doc-build would be nice\n",
      "\n",
      "2\n",
      "1453\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 17:24\n",
      "how about the \"FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\" ? from numpy?\n",
      "anyhow, my stomach demands attention\n",
      "cool.\n",
      "I'll fix \"FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\" in a couple of minutes. I'd like to include that in the RC. Then you can cut it tomorrow morning?\n",
      "\n",
      "4\n",
      "1454\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 19:13\n",
      "what is the timeline? how long will you be around today?\n",
      "\n",
      "1\n",
      "1455\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 19:30\n",
      "many of the the \"elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\"  are due to us comparing parameters that might be arrays to strings. like ``if init == \"something\"``\n",
      "\n",
      "1\n",
      "1456\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-15 19:36\n",
      "I will soon logout. I wanted to do the RandomTreeEmbedding example fix but we can do that after the cut of the 0.17.X branch.\n",
      "Do you want to cut it today? Otherwise I can do it tomorrow morning (Paris time).\n",
      "I just merged the joblib upgrade\n",
      "\n",
      "3\n",
      "1457\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-15 20:10\n",
      "https://github.com/scikit-learn/scikit-learn/pull/5413\n",
      "\n",
      "1\n",
      "1458\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-17 00:37\n",
      "@ogrisel do you understand this https://travis-ci.org/MacPython/scikit-learn-wheels/jobs/85847030 ?\n",
      "install_scripts failed\n",
      "\n",
      "2\n",
      "1459\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-17 08:51\n",
      "@amueller it's weird I just did it (bdist_wheel) on my mac and I don't get the error.\n",
      "python 3.5.0 as well.\n",
      "\n",
      "2\n",
      "1460\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-17 08:55\n",
      "Maybe the version of the `wheel` project pre-installed on the travis host is too old for Python 3.5.0. Will change the config to make it upgrade it.\n",
      "> Requirement already satisfied (use --upgrade to upgrade): wheel in ./venv/lib/python3.5/site-packages\n",
      "\n",
      "2\n",
      "1461\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-17 16:42\n",
      "@ogrisel I saw you commited something. did it fix it?\n",
      "the weird gcc flags were... weird but I didn't see it come up again after I added the flags\n",
      "ah, wheels work. we only need to sync master for the 3.3 fix\n",
      "\n",
      "3\n",
      "1462\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-17 16:48\n",
      "For 3.5 on the OSX  wheel builder, yes it's fixed (just ugraded the `wheel` package on travis).\n",
      "For your 3.3 fix that I merged I plan to only include it for the next release.\n",
      "I would like to have the tag match the content of the archive.\n",
      "\n",
      "3\n",
      "1463\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-17 17:57\n",
      "What do you mean by that? that the version tag in the sklearn repo is the same as the wheel with the same name? makes sense.\n",
      "I have to work on other stuff today. You are busy with Pycon FR right? it looks like #5008 would be good to have / review\n",
      "\n",
      "2\n",
      "1464\n",
      "560731310fc9f982beb1f438\n",
      "2015-10-17 20:55\n",
      "I'd like to start contributing and work on #5089, so I was wondering if it's better to submit one PR with a lot of warning fixes or PRs in chunks that deal with one specific type of warning?\n",
      "\n",
      "1\n",
      "1465\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-18 07:56\n",
      "@oolongtea  Start with one that fix related stuff if it's your first PR. It's easier to give you feedback to get started.\n",
      "\n",
      "1\n",
      "1466\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-18 19:23\n",
      "does anyone know a good \"real world\" dataset for regression on which regularization helps? In diabetes and boston linear regression does as well as any other linear model, which is sad. the only examples for regularization we have are using ``make_regression``\n",
      "so diabetes is from the lars paper, but lars doesn't actually make better predictions than ols on diabetes.....\n",
      "\n",
      "2\n",
      "1467\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-10-18 20:00\n",
      "How about high dim. data, say movie review stars from text\n",
      "\n",
      "1\n",
      "1468\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-18 20:06\n",
      "yeah that would work. but I don't want to go too high dim. I settled for polynomial features on boston, which works well\n",
      "It's for the book, and I don't want to explain bag of words at this point. Not sure I want to explain polynomial features, but it's a little easier.\n",
      "might be an interesting example for non-synthetic data\n",
      "(for the examples folder I mean)\n",
      "\n",
      "4\n",
      "1469\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-20 07:21\n",
      "\n",
      "6\n",
      "1470\n",
      "53135b495e986b0712efc453\n",
      "2015-10-20 08:37\n",
      "should there be need contrib tag in #4687?\n",
      "Thanks :beers:\n",
      "\n",
      "2\n",
      "1471\n",
      "53135b495e986b0712efc453\n",
      "2015-10-20 08:58\n",
      "@zermelozf #3846 ? There are like 90 odd examples to be added ;) (@ogrisel and others - Do you feel this one would be useful or should he pick something else from the recently tagged pool of issues to work on?)\n",
      "here too (need contrib to be removed) - #5322\n",
      "@glouppe If you are able to find time could you review #4294 ? ;)\n",
      "Lol okay :D I am eagerly waiting for Andy ;)\n",
      "\n",
      "4\n",
      "1472\n",
      "53135b495e986b0712efc453\n",
      "2015-10-20 09:44\n",
      "@glouppe Could you please remove the \"Need Contributor\" tag from #5474, #4687, #5455, #5447, #5432, #5380 ?\n",
      "\n",
      "1\n",
      "1473\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-20 09:46\n",
      "all done\n",
      "\n",
      "1\n",
      "1474\n",
      "53135b495e986b0712efc453\n",
      "2015-10-20 10:04\n",
      "@glouppe also from this one - 5290 and maybe assign it to Arthur? (sorry for repeatedly pinging you ;) )\n",
      "\n",
      "1\n",
      "1475\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-20 11:07\n",
      "done\n",
      "\n",
      "1\n",
      "1476\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-20 11:08\n",
      "unfortunately, I cannot assign to people outside of the scikit-learn team\n",
      "dunno why\n",
      "\n",
      "2\n",
      "1477\n",
      "53135b495e986b0712efc453\n",
      "2015-10-20 11:32\n",
      "This one can be closed - #5060\n",
      "(It was fixed by #5084)\n",
      "\n",
      "2\n",
      "1478\n",
      "555b8aa615522ed4b3e0a160\n",
      "2015-10-20 11:37\n",
      "@ogrisel in sklearn.metrics.pairwise._parallel_pairwise, the joblib Parallel loop is slowed by thread locking, which is weird with the multiprocessing backend\n",
      "\n",
      "1\n",
      "1479\n",
      "55e5c37d0fc9f982beaf4d61\n",
      "2015-10-20 11:41\n",
      "issue #5481 is actually a batch of small issues on estimators that fails on read only memory map data once check_array process memory map without copying their content unnecessarily. I guess it could be labelled as easy as it could be addressed by new contributors\n",
      "\n",
      "1\n",
      "1480\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-20 11:47\n",
      "wow, this is poisoned gift you are giving me there\n",
      "\n",
      "3\n",
      "1481\n",
      "54d4ae8cdb8155e6700f858d\n",
      "2015-10-20 12:25\n",
      "hi guys, not sure if that's the place to ask but I'm looking for a way to use a function-call instead of already having the target value next to the features. So I'd like to put in the features as usual but don't know the results yet. This is because I don't know them and also don't want to run them since it would take to long to do this with a grid-brute. I'm hoping to save time by using a more advanced search mechanism instead that I can feed a function that then puts out the results. I've only found examples so far where the target values are already known. Any help on what to look for?\n",
      "another thing I'm looking for is a function that I can give a sample of numbers, let's say a np array of 1000 numbers and then have that function create a sample of N numbers with the same distribution characteristics as the input numbers...\n",
      "\n",
      "3\n",
      "1482\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-20 12:27\n",
      "can anyone get me? I'm in the lobby\n",
      "\n",
      "1\n",
      "1483\n",
      "53135b495e986b0712efc453\n",
      "2015-10-20 12:29\n",
      "Olivier is coming!\n",
      "\n",
      "1\n",
      "1484\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-20 13:10\n",
      "wohoo wifi\n",
      "\n",
      "1\n",
      "1485\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-20 13:18\n",
      "\\o/\n",
      "\n",
      "1\n",
      "1486\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-20 13:43\n",
      "@ogrisel can we trigger circleci on https://github.com/scikit-learn/scikit-learn/pull/5451 again?\n",
      "\n",
      "1\n",
      "1487\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-20 16:21\n",
      "Btw, if you add an entry to whatsnew.rst, please include a link to the github issue / pull request!\n",
      "\n",
      "1\n",
      "1488\n",
      "53810862048862e761fa2887\n",
      "2015-10-20 17:38\n",
      "Hello, I am trying to understand the current Gradient Boosting code\n",
      "Can someone point out where step 3 is being performed ? According to the this https://en.wikipedia.org/wiki/Gradient_boosting#Algorithm\n",
      "\n",
      "2\n",
      "1489\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-21 06:11\n",
      "@rvraghav93 In addition to finding issues to fix at the sprint (which is great!), could you also try to promote reviewing :) You can tell people to look for PRs with the [MRG] tag, these are the ones ready for review\n",
      "\n",
      "1\n",
      "1490\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 06:56\n",
      "@vighneshbirodkar you mean 2.3,  right?\n",
      "\n",
      "1\n",
      "1491\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-21 07:31\n",
      "if anyone has a PR that needs review that is what I am focusing on - though some stuff may be outside my wheelhouse\n",
      "\n",
      "1\n",
      "1492\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-21 07:41\n",
      "@kastnerkyle #4294 would need some love, but this is huge\n",
      "(though both arnaud and andy have been reviewing it)\n",
      "#5291 maybe?\n",
      "or any help with all the mrg+1 PRs is welcome, so that we can have some of those merged\n",
      "\n",
      "4\n",
      "1493\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 07:52\n",
      "@kastnerkyle https://github.com/scikit-learn/scikit-learn/pull/5358 ;)\n",
      "\n",
      "1\n",
      "1494\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 07:58\n",
      "Anyone wanna weigh in on #5319 (adding k modes to related projects)\n",
      "Anything anyone want's me to have a look at?\n",
      "\n",
      "2\n",
      "1495\n",
      "53135b495e986b0712efc453\n",
      "2015-10-21 08:16\n",
      "I have an urgent bank work :( I'll be coming in the afternoon. Apologies :grin:\n",
      "@glouppe sure!! Will do :)\n",
      "\n",
      "2\n",
      "1496\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-21 08:17\n",
      "@rvraghav93 no problem.\n",
      "@rvraghav93 hopefully by then I will have finished reviewing your CV PR ;)\n",
      "\n",
      "2\n",
      "1497\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 08:18\n",
      "can we discuss #5023\n",
      "\n",
      "1\n",
      "1498\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-21 08:18\n",
      "@amueller #5358 looks like a +2 now with @ogrisel\n",
      "\n",
      "3\n",
      "1499\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 08:19\n",
      "@ogrisel opinions on #5023 ?\n",
      "I wanna ask gael and arnaud, too\n",
      "lol https://github.com/scikit-learn/scikit-learn/pull/5498/files#r42594166\n",
      "I think this is a first, Gael telling me to be more pragmatic\n",
      "not sure I should screenshot\n",
      "\n",
      "5\n",
      "1500\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 08:50\n",
      "@ogrisel you agree that GaussianProcesses should be removed in 0.20, right?\n",
      "then we need to fix and backport\n",
      "\n",
      "2\n",
      "1501\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-21 08:54\n",
      "old GP is deprecated in 0.18, removed in 0.20\n",
      "indeed\n",
      "\n",
      "2\n",
      "1502\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-21 08:56\n",
      "ok my bad then\n",
      "i'll fix that\n",
      "done\n",
      "\n",
      "3\n",
      "1503\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-21 09:14\n",
      "#5504 and #5505 need contributors: those are documentation related issues. @rvraghav93: @amueller told me to tell you to leave those issues for others ;)\n",
      "\n",
      "2\n",
      "1504\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 09:16\n",
      "hm similarly here: #5452 we should change the version. or backport the deprecation to 0.17\n",
      "\n",
      "1\n",
      "1505\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-21 09:17\n",
      "#5500 as well\n",
      "documentation fixes\n",
      "\n",
      "2\n",
      "1506\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 09:21\n",
      "ah needs contributor as well...\n",
      "\n",
      "1\n",
      "1507\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 09:39\n",
      "@kastnerkyle if you are bored, maybe look at #5008 (needs reviews)\n",
      "\n",
      "1\n",
      "1508\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 10:21\n",
      "I though #5141 was merged, whoops (randomized_svd default parameters and normalization)\n",
      "reviews appreciated\n",
      "\n",
      "2\n",
      "1509\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-21 10:33\n",
      "I am +1 on 5141\n",
      "so it is +2 now - if tests pass you can go ahead and hit the button @amueller\n",
      "I thought those were addressed, but we can let him hit the button instead\n",
      "\n",
      "3\n",
      "1510\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-21 10:40\n",
      "you can go ahead and merge Kyle :)\n",
      "\n",
      "1\n",
      "1511\n",
      "53810862048862e761fa2887\n",
      "2015-10-21 10:40\n",
      "Yes @amueller , I mean 2.3\n",
      "\n",
      "1\n",
      "1512\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-21 10:46\n",
      "@glouppe traaaaaaaaaaaaavis. so slow. it needs to hurry up before andy stops eating... he is faster than me :D\n",
      "I'll never win the \"merge button shootout\"\n",
      "\n",
      "2\n",
      "1513\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 10:47\n",
      "@vighneshbirodkar in ``update_terminal_region`` I think. not sure though\n",
      "@ogrisel had some thoughts about doing only two power iterations, I think\n",
      "\n",
      "2\n",
      "1514\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 10:56\n",
      "does anyone at the sprint have ibuprofene?\n",
      "@vig\n",
      "@vighneshbirodkar about the one hot encoder: we are fine if dtype=object, right?\n",
      "\n",
      "3\n",
      "1515\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 11:24\n",
      "can people still look at travis logs? https://travis-ci.org/scikit-learn/scikit-learn/builds/86595504 for example?\n",
      "I think travis just throttled our IP\n",
      "\n",
      "2\n",
      "1516\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-21 11:25\n",
      "keeps \"loading\" here\n",
      "\n",
      "3\n",
      "1517\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-21 11:32\n",
      "H99 errors\n",
      "https://devcenter.heroku.com/articles/error-codes#h99-platform-error\n",
      "could be anything\n",
      "\n",
      "3\n",
      "1518\n",
      "53135b495e986b0712efc453\n",
      "2015-10-21 11:49\n",
      "@ogrisel Lol okay ;) and thanks a lot for the reviews :)\n",
      "\n",
      "2\n",
      "1519\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 12:19\n",
      "I remember there was a discussion about how to call a function that gives uncertainty estimates on regression values. Where was that? I can't find it any more\n",
      "We need reviews on #4490 and want to backport it.\n",
      "\n",
      "2\n",
      "1520\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 12:28\n",
      "I'm going to lie on the couch in front of the room if anyone is looking for me. I'm beginning to question my decision to take a flight yesterday ^^\n",
      "\n",
      "1\n",
      "1521\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-21 12:51\n",
      "> I remember there was a discussion about how to call a function that gives uncertainty estimates on regression values. Where was that? I can't find it any more  `return_std` on the `predict` method of old GPs\n",
      "I thought we discussed the use of the same parameter for another non-GP regressor (I checked RidgeCV but it's not it apparently).\n",
      "\n",
      "2\n",
      "1522\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 13:56\n",
      "That is doing interval predictions. that is slightly different. sometimes you want to evaluate the density at a certain point\n",
      "\n",
      "1\n",
      "1523\n",
      "53810862048862e761fa2887\n",
      "2015-10-21 14:11\n",
      "@amueller While you are there, can you have a discussion with the other about the `OneHotEncoder` issue and in general how we can handle strings with it ?\n",
      "others*\n",
      "\n",
      "2\n",
      "1524\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 14:13\n",
      "the joblib fun on windows is back: https://ci.appveyor.com/project/amueller/scikit-learn/build/1.0.1408/job/7sldjuplbqf2t31u\n",
      "@vighneshbirodkar what was the open questions? if there are  columns with mixed strings and numbers?\n",
      "so the problem is only if you pass a list?\n",
      "but maybe the new code based on unique would work for both integers and objects? well ok that is not that important\n",
      "\n",
      "4\n",
      "1525\n",
      "53810862048862e761fa2887\n",
      "2015-10-21 14:15\n",
      "Yes, specially an array with `1` and `\"1\"` in the same column\n",
      "@amueller What is your definition of \"fun\" ? :D\n",
      "\n",
      "2\n",
      "1526\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-21 14:17\n",
      "something that you cannot reproduce on your dev environment\n",
      "Maybe @TomDLT can give it a try.\n",
      "\n",
      "2\n",
      "1527\n",
      "53810862048862e761fa2887\n",
      "2015-10-21 14:22\n",
      "I have faced similar issues before somewhere else.\n",
      "\n",
      "1\n",
      "1528\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-21 14:29\n",
      "Yes: it used to be very frequent in the past and at some point it stopped appearing. But now it's back. I am not sure whether it reveals a true problem in the way we use multiprocessing under windows or is caused by a problem on the appveyor CI infrastructure.\n",
      "\n",
      "4\n",
      "1529\n",
      "55e5c37d0fc9f982beaf4d61\n",
      "2015-10-21 15:02\n",
      "PR #5492 is ready for review. Caching + removal of .c file seems to be working\n",
      "It's a bit hackish though, waiting for some proper build system...\n",
      "\n",
      "2\n",
      "1530\n",
      "53810862048862e761fa2887\n",
      "2015-10-21 15:18\n",
      "@amueller In `2.3` when they say \"argmin\" I assume there will be some sort of a loop ? I can't seem to find that anywhere\n",
      "\n",
      "2\n",
      "1531\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 15:42\n",
      "@ogrisel I can not reproduce the issue with conda locally\n",
      "\n",
      "1\n",
      "1532\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-21 15:46\n",
      "FYI the https://ci.appveyor.com/project/amueller/scikit-learn/build/1.0.1408/job/7sldjuplbqf2t31u failure might be caused by the fact that this was deployed on @amueller's appveyor account which runs of a different infra than the one we should usually run on (that is using the @sklearn-ci account). I reconfigured the appveyor webhook on the scikit-learn repo and hopefully the future appveyor builds will run on the correct infra and not fail this way anymore. If this is not the case we will have to investigate.\n",
      "> @ogrisel I can not reproduce the issue with conda locally  So it might be a consequence of the past travis outage\n",
      "\n",
      "5\n",
      "1533\n",
      "53810862048862e761fa2887\n",
      "2015-10-21 16:03\n",
      "If you see this https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L254 for example,  only the learning late is multiplied.\n",
      "\n",
      "1\n",
      "1534\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 16:15\n",
      "@vighneshbirodkar I'm not sure. Maybe ping @pprett ?\n",
      "@ogrisel can we talk about handling strings in OneHotEncoder ?\n",
      "\n",
      "2\n",
      "1535\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 16:29\n",
      "@ogrisel have you asked arnaud about the greater is better issue?\n",
      "\n",
      "1\n",
      "1536\n",
      "53810862048862e761fa2887\n",
      "2015-10-21 16:52\n",
      "Yes\n",
      "\n",
      "1\n",
      "1537\n",
      "53810862048862e761fa2887\n",
      "2015-10-21 16:55\n",
      "Yes, and I am assuming based on the dtype of the input, we will have 2 different pieces of code to process them (object and dtype)\n",
      "\n",
      "4\n",
      "1538\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-21 17:00\n",
      "maybe we do ``check_array`` and if we get back a string dtype, we do the conversion again with dtype object. Does that solve all problems?\n",
      "\n",
      "3\n",
      "1539\n",
      "53810862048862e761fa2887\n",
      "2015-10-21 17:36\n",
      "Now that I think about it, since the `LabelEncoder` has a lot of the functionality needed, instead have a subclass of `Pipeline` called `OneHotObjectEncoder`\n",
      "\n",
      "1\n",
      "1540\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-10-21 19:09\n",
      "So the KNN docstrings mention from place to place the idea that `metric` can be `\"precomputed\"`, but I can't get that to work, the current code doesn't seem to implement it.\n",
      "I can't seem to find any documentation or any open issues about this.\n",
      "hmm I think it's fixed in master actually, sorry about the noise\n",
      "\n",
      "3\n",
      "1541\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-22 07:26\n",
      "good morning :)\n",
      "\n",
      "1\n",
      "1542\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-22 07:27\n",
      "@vighneshbirodkar For the one hot encoder: that\n",
      "that's why I said try to do check_array, and if we get a string type back, instead make it an object type.\n",
      "your example will give a string type\n",
      "if we detect that and instead convert with an explicit object dtype, it'll work\n",
      "\n",
      "4\n",
      "1543\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-22 07:44\n",
      "good morning\n",
      "\n",
      "1\n",
      "1544\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-22 07:44\n",
      "good morning\n",
      "\n",
      "1\n",
      "1545\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-22 07:44\n",
      "hey, by any chance, what would you recommend for a good introduction to linear models? (to be recommended as reading materials for a workshop with an audience of physicists)\n",
      "(covering linear regression, lasso, svm, etc)\n",
      "okay, thanks Kyle\n",
      "=(\n",
      "Given my +1 and partial reviews from @amueller, @ngoix and @jmschrei, can we merge #5487?\n",
      "thanks alex :)\n",
      "\n",
      "6\n",
      "1546\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-22 08:18\n",
      "good morning - @ogrisel might comment more but I would probably say elements of statistical learning\n",
      "or the intro version of that (can't remember the name)\n",
      "\"Introduction to Statistical Learning\"\n",
      "\n",
      "3\n",
      "1547\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-22 08:22\n",
      "I don't have a better suggestion\n",
      "\n",
      "1\n",
      "1548\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-22 08:56\n",
      "is it me or github is very slow at the moment?\n",
      "\n",
      "2\n",
      "1549\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-22 10:40\n",
      "@glouppe you just missed the introduction of the scikit-learn advancement proposal\n",
      "\n",
      "1\n",
      "1550\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-22 12:48\n",
      "@glouppe I think it is important to note that SLAP is our new acronym...\n",
      "\n",
      "1\n",
      "1551\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-22 13:13\n",
      "@kastnerkyle @GaelVaroquaux called the repo \"enhancement\" :-/ https://github.com/scikit-learn/enhancement_proposals\n",
      "stupid git question: how to I update a local branch that is a pr/1234 branch?\n",
      "i.e. that comes from a pull request\n",
      "I think I'll have to head out soon\n",
      "@kastnerkyle that is not working yet, there is a PR\n",
      "\n",
      "5\n",
      "1552\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-22 14:36\n",
      "does anyone know if CircleCI is pushing the doc to github? Or just creating an artifact that some other thing can get\n",
      "I am trying to make CircleCI push a doc (for another project) after succesful build\n",
      "\n",
      "2\n",
      "1553\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-22 15:02\n",
      "https://skll.readthedocs.org/en/latest/run_experiment.html#param-grids-optional\n",
      "https://github.com/EducationalTestingService/skll/blob/5ea61b8dfc23570e661468457a262b6c2242daa9/skll/learner.py#L62\n",
      "\n",
      "2\n",
      "1554\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-22 16:55\n",
      "ok - I got the build part working on sklearn-theano pretty easily, and I think Fred will take a look at it for Theano as well. This is a lot better than a cron job...\n",
      "\n",
      "3\n",
      "1555\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-10-23 02:59\n",
      "@amueller ... http://opendatascicon.com/scikit-learn-code-sprint/ ... Looking forward to it!\n",
      "\n",
      "1\n",
      "1556\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-23 07:13\n",
      "@trevorstephens cool :) glad to have you there!\n",
      "\n",
      "1\n",
      "1557\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-23 07:28\n",
      "will we have MLP by today? :)\n",
      "and isolation forest?\n",
      "that would be very nice\n",
      "\n",
      "3\n",
      "1558\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-23 07:40\n",
      "sadly mlp seems unlikely IMO - lots of hard choices but it is getting closer. if there are other things to be reviewed I can take a look - I am also hopeful the PCA fixes by Giorgio will go today (if they didn't last night!)\n",
      "\n",
      "1\n",
      "1559\n",
      "54bd0a4fdb8155e6700ed136\n",
      "2015-10-23 07:53\n",
      "it's been more than a year since the MLP PRs have started :/\n",
      "if these decisions are only about internals, we should try to be pragmatic at some point\n",
      "\\o/\n",
      "\n",
      "3\n",
      "1560\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-23 08:41\n",
      "yes it is mostly logical - some of the core logic is spread over several classes which makes it hard to reason about its behavior especially w.r.t to stopping criterion. But Andy has a student on it full-time (ish) who is quite good so I don't think it will be much longer\n",
      "and the documentation is basically done IMO - which was a huge chunk of the work\n",
      "@glouppe things are looking positive for the MLP - you might get your wish :D\n",
      "#5299 is very close if anyone wants to review\n",
      "\n",
      "4\n",
      "1561\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-23 13:22\n",
      "anyone wants to review #5540 ?\n",
      "@kastnerkyle but if it's only refactoring, we don't really need to do that for merging if it's not public API, right?\n",
      "\n",
      "2\n",
      "1562\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-23 14:34\n",
      "we need to do ##5502 if anyone is looking for an issue to pick up\n",
      "it's needed for 0.17\n",
      "\n",
      "2\n",
      "1563\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-23 15:33\n",
      "https://github.com/scikit-learn/scikit-learn/pull/5274#discussion-diff-42377195\n",
      "https://github.com/scikit-learn/scikit-learn/pull/5565#issuecomment-150609391\n",
      "\n",
      "2\n",
      "1564\n",
      "54e07d4015522ed4b3dc0856\n",
      "2015-10-23 16:52\n",
      "@glouppe it's gonna happen. If Travis ever runs\n",
      "\n",
      "1\n",
      "1565\n",
      "5615bee3d33f749381a8a4f5\n",
      "2015-10-26 09:21\n",
      "I have a stupid general question here: What are possible ways to do key words/sentenses extraction from a large text corpora?\n",
      "\n",
      "1\n",
      "1566\n",
      "553e8e1015522ed4b3df97f7\n",
      "2015-10-26 10:40\n",
      "@bawongfai have you tried textrank?\n",
      "\n",
      "1\n",
      "1567\n",
      "5615bee3d33f749381a8a4f5\n",
      "2015-10-26 11:39\n",
      "@vortex-ape not really. How does it compare to document vectorisation?\n",
      "Then is it good bad for a short sentence that contains potential keyword?\n",
      "bad i meant\n",
      "So what do you suggest as an alternative?\n",
      "\n",
      "4\n",
      "1568\n",
      "53810862048862e761fa2887\n",
      "2015-10-26 11:41\n",
      "Text rank tries to order sentences by their importance\n",
      "A sentence is of more importance  if it talks about a large number of things and thus gets a higher score.\n",
      "\n",
      "2\n",
      "1569\n",
      "53810862048862e761fa2887\n",
      "2015-10-26 11:44\n",
      "From what I've used it, bad\n",
      "\n",
      "1\n",
      "1570\n",
      "53810862048862e761fa2887\n",
      "2015-10-26 11:45\n",
      "It generally gives a long sentence talking about lots kf things\n",
      "Can you give me an example of an output you would expect?\n",
      "You can give this a try, but I have no idea what kind of output to expect.\n",
      "http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html\n",
      "\n",
      "4\n",
      "1571\n",
      "5615bee3d33f749381a8a4f5\n",
      "2015-10-26 11:48\n",
      "@vighneshbirodkar I would like to have a summarisation of conversation\n",
      "conversation, by nature, could be short\n",
      "I guess the summary would be about text summarisation and keyword extraction\n",
      "\n",
      "4\n",
      "1572\n",
      "53810862048862e761fa2887\n",
      "2015-10-26 11:49\n",
      "For example we are having one, what would you expect the summary to be ?\n",
      "\n",
      "1\n",
      "1573\n",
      "553e8e1015522ed4b3df97f7\n",
      "2015-10-26 11:52\n",
      "would you want an abstractive summary or an extractive one? I used textrank once to get extractive summaries of a conversation and it seemed to work well, though I didn't compare it with other methods\n",
      "\n",
      "4\n",
      "1574\n",
      "553e8e1015522ed4b3df97f7\n",
      "2015-10-26 11:58\n",
      "You could experiment with both these methods and explore about other ones too, and see which one gives you the desired results, I used textrank as a quick hack in a hackathon so I'm no expert in this regard\n",
      ":smile:\n",
      "\n",
      "2\n",
      "1575\n",
      "5615bee3d33f749381a8a4f5\n",
      "2015-10-26 12:01\n",
      "@vortex-ape thanks, i will try textrank first\n",
      "I would like to go into deep learning approach later\n",
      "\n",
      "2\n",
      "1576\n",
      "5615bee3d33f749381a8a4f5\n",
      "2015-10-26 15:26\n",
      "@vortex-ape tried textrank, not too bad\n",
      "\n",
      "1\n",
      "1577\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-29 23:18\n",
      "@ogrisel you think we can release weekend or Monday? Should we talk to conda folks?\n",
      "hm was about to reach out to asmeurer but I guess he is not the right contact any more ^^ http://asmeurer.github.io/blog/\n",
      "trying ilanschnell now\n",
      "\n",
      "3\n",
      "1578\n",
      "53810862048862e761fa2887\n",
      "2015-10-29 23:27\n",
      "@amueller are you back ?\n",
      "\n",
      "1\n",
      "1579\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-29 23:31\n",
      "@vighneshbirodkar https://www.youtube.com/watch?v=Q2J9F2sJMT4\n",
      "\n",
      "1\n",
      "1580\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-10-29 23:32\n",
      "sweet I didn't realize gitter embedded youtube videos. really important for github collaborations.\n",
      "\n",
      "1\n",
      "1581\n",
      "53810862048862e761fa2887\n",
      "2015-10-29 23:33\n",
      "https://youtu.be/FJbmB9k2Y88\n",
      "\n",
      "2\n",
      "1582\n",
      "541a528b163965c9bc2053de\n",
      "2015-10-31 14:42\n",
      "@amueller some appveyor builds seems to be fast again, especially on 64 bit for some reason: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.3621\n",
      "maybe there is some variability in the platform\n",
      "\n",
      "2\n",
      "1583\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-02 00:34\n",
      "@amueller #5164 re-opened by accident?\n",
      "\n",
      "1\n",
      "1584\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-02 00:35\n",
      "@trevorstephens thanks for the mail. #5164 reports more issues\n",
      "\n",
      "4\n",
      "1585\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-02 00:37\n",
      "i thought that one was limited to graphviz?\n",
      "there are a couple of other issues with other 32-bit fails\\\n",
      "\n",
      "2\n",
      "1586\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-02 00:39\n",
      "there's tracking\n",
      "click 'edits'\n",
      "sorry 'revisions'\n",
      "under the title\n",
      "\n",
      "4\n",
      "1587\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-02 00:41\n",
      "i moved the recent paris sprint info to 'past sprints'\n",
      "do core contribs get notified of the changes to wiki?\n",
      "\n",
      "2\n",
      "1588\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-02 00:44\n",
      "not that I know of.  maybe I could subscribe\n",
      "yeah I saw, looks good :)\n",
      "\n",
      "2\n",
      "1589\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-02 00:47\n",
      "reviews for https://github.com/scikit-learn/scikit-learn/pull/5661 would also be welcome\n",
      "@trevorstephens the review suggestions are not explicitly for you, I just like to spam the channel ;)\n",
      "\n",
      "2\n",
      "1590\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-02 00:59\n",
      "haha ok. i have no familiarity with tsne anyhow :-/ but int64? i didn't even know that existed! number of ants to stack to get to the moon?\n",
      "\n",
      "4\n",
      "1591\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-02 01:03\n",
      "anyone who has a spare cycle and cares about package organization, comments about the location of `partial_dependence` in #5653 welcome :-)\n",
      "\n",
      "1\n",
      "1592\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-02 03:50\n",
      "anyone who as spare cycles, I just opened 7 pull requests, 4 of which are release blockers ^^\n",
      "I just commented\n",
      "also, don't worry about it tonight ;)\n",
      "\n",
      "3\n",
      "1593\n",
      "53810862048862e761fa2887\n",
      "2015-11-02 03:52\n",
      "@amueller Where did you see the `OneHotEncoder` warnings ?\n",
      "\n",
      "1\n",
      "1594\n",
      "53810862048862e761fa2887\n",
      "2015-11-02 03:54\n",
      "No warnings on python 2.7.6 and numpy 1.8.2\n",
      "\n",
      "3\n",
      "1595\n",
      "53810862048862e761fa2887\n",
      "2015-11-02 04:42\n",
      "Reproduced it, but yeah, I am better of looking at this tomorrow :D\n",
      "\n",
      "1\n",
      "1596\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-02 18:04\n",
      "I would really like to get some feedback on how to treat the 32bit test failures\n",
      "\n",
      "1\n",
      "1597\n",
      "53810862048862e761fa2887\n",
      "2015-11-02 18:09\n",
      "Could you tell me more about them ?\n",
      "\n",
      "1\n",
      "1598\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-02 18:18\n",
      "the 32bit failures? They are precision issues and I don't know whether to reduce the precision or ignore the tests or what else to do.\n",
      "\n",
      "1\n",
      "1599\n",
      "53810862048862e761fa2887\n",
      "2015-11-02 18:21\n",
      "Do you have some logs somwhere ?\n",
      "somewhere*\n",
      "\n",
      "2\n",
      "1600\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-02 18:40\n",
      "@amueller , on #5682, would a test to ensure an index error is not thrown for the second example i gave be sufficient?\n",
      "or test the value error string ?\n",
      "\n",
      "2\n",
      "1601\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-02 18:43\n",
      "Testing the value error string would be good\n",
      " @vighneshbirodkar logs are here: https://github.com/scikit-learn/scikit-learn/issues/5534 but it is more a question to the other core devs on how we handle this.\n",
      "\n",
      "2\n",
      "1602\n",
      "53810862048862e761fa2887\n",
      "2015-11-02 18:50\n",
      "Ok\n",
      "\n",
      "1\n",
      "1603\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-03 17:29\n",
      "hm... anyone have an idea why github stopped notifying me for comments on pull requests I created?\n",
      "ok I was just hallucinating, never mind\n",
      "\n",
      "2\n",
      "1604\n",
      "53135b495e986b0712efc453\n",
      "2015-11-03 20:01\n",
      ".\n",
      "\n",
      "1\n",
      "1605\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-03 20:14\n",
      "?\n",
      "I'm trying to check if we missed anything in whatsnew for 0.17 by running a diff against 0.16\n",
      "some idiot changed the docstrings of all the classes so all files are changed\n",
      "\n",
      "3\n",
      "1606\n",
      "53135b495e986b0712efc453\n",
      "2015-11-03 20:21\n",
      "sorry that . was a typo.. (gitter android app sucks... :/) and lol `*`silently hopes I don't show up in the git blame`*` :p\n",
      "\n",
      "1\n",
      "1607\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-03 20:50\n",
      "there is an android gitter app? hm...\n",
      "meh\n",
      "\n",
      "2\n",
      "1608\n",
      "53135b495e986b0712efc453\n",
      "2015-11-03 20:51\n",
      "yea its just the mobile site wrapped as an app... too slow and buggy... :/\n",
      "\n",
      "1\n",
      "1609\n",
      "53810862048862e761fa2887\n",
      "2015-11-03 20:54\n",
      "They updated it\n",
      "\n",
      "1\n",
      "1610\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-03 21:27\n",
      "anyone want to make the pdf docs build on 0.17.X ? ^^\n",
      "\n",
      "1\n",
      "1611\n",
      "53810862048862e761fa2887\n",
      "2015-11-04 06:09\n",
      "http://cs.nyu.edu/~vnb222/temp/user_guide.pdf\n",
      "I can see figures overflowing in some pages ?\n",
      "\n",
      "2\n",
      "1612\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-04 18:50\n",
      "thanks for having a look\n",
      "when I called \"make dist\" there were latex errors and the build didn't finish\n",
      "\n",
      "2\n",
      "1613\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-04 21:05\n",
      "does anyone have matplotlib 1.5 to test the examples?\n",
      "I'm trying to pip install it in my conda env. I'm sure that's going to go great\n",
      "@MechCoder if you're bored, you can have a look at https://github.com/scikit-learn/scikit-learn/pull/5721\n",
      "if anyone wants to help me find out why \"clustering\" isn't properly linked on the website, that would also be sweet ^^\n",
      "\n",
      "4\n",
      "1614\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-04 21:52\n",
      "https://github.com/scikit-learn/scikit-learn/issues/5724 is also fun\n",
      "\n",
      "1\n",
      "1615\n",
      "562aa0c916b6c7089cb80cd7\n",
      "2015-11-06 00:06\n",
      "Do I need to have great knowledge about machine learning and AI to start with.\n",
      "or knowing some basic about it will help.\n",
      "I'm confused as of now I know python and some basic of machine learning and AI\n",
      "\n",
      "3\n",
      "1616\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-06 00:47\n",
      "@manipalsingh013 knowing some basics is good but even that is not necessary to get started\n",
      "\n",
      "2\n",
      "1617\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-06 03:42\n",
      "Thanks for all your work @amueller, congrats on the release! :beers:\n",
      "\n",
      "1\n",
      "1618\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-06 03:47\n",
      "btw sorry I missed your responses, I'll have a bit more of a tinker with MLP at some point\n",
      "\n",
      "1\n",
      "1619\n",
      "54b4f2d1db8155e6700e99c0\n",
      "2015-11-08 20:07\n",
      "Hey Folks, congrats for the release! Awesome work!\n",
      "Quick question, why does KMeans accept y in its fit() ?\n",
      "\n",
      "2\n",
      "1620\n",
      "53135b495e986b0712efc453\n",
      "2015-11-08 20:12\n",
      "For API compatibility with other supervised learning algorithms... (and to safely pass y through kmeans in a pipeline)\n",
      "\n",
      "1\n",
      "1621\n",
      "54b4f2d1db8155e6700e99c0\n",
      "2015-11-08 20:13\n",
      "Thanks a lot @rvraghav93 :)\n",
      "\n",
      "1\n",
      "1622\n",
      "53135b495e986b0712efc453\n",
      "2015-11-10 16:23\n",
      "Firefox users might find this handy ;) - https://addons.mozilla.org/en-US/firefox/addon/git-done/?src=search\n",
      "\n",
      "1\n",
      "1623\n",
      "5571fe1015522ed4b3e17d90\n",
      "2015-11-12 08:46\n",
      "@rvraghav93 out of interest what does it look like in the PR, does it just add a \"Done\" comment when you click on the \"Done\" button? Not sure how useful this is to be honest. Generally when you do something following a comment, it gets hidden into an \"outdated diff\" section anyway.\n",
      "\n",
      "1\n",
      "1624\n",
      "53135b495e986b0712efc453\n",
      "2015-11-12 09:14\n",
      "for some reason it doesn't work for me ;( It might be useful for those comments that aren't hidden after the change I think... ;) Apart from that you are right... its not really that useful since the comment gets hidden anyways...\n",
      "\n",
      "1\n",
      "1625\n",
      "5571fe1015522ed4b3e17d90\n",
      "2015-11-12 10:30\n",
      "I'd love if someone did a firefox extension which shows \"outdated diff\" comments. I know there are some bookmarklets on the internet but it never worked for me somehow, I always end up having to copy and paste the code in the Javascript console, not great.\n",
      "\n",
      "1\n",
      "1626\n",
      "53135b495e986b0712efc453\n",
      "2015-11-12 10:39\n",
      "as as in does not hide them by default u mean huh?\n",
      "\n",
      "1\n",
      "1627\n",
      "5571fe1015522ed4b3e17d90\n",
      "2015-11-12 10:43\n",
      "see for example https://coderwall.com/p/akdgoq/expand-all-outdated-diff-comments-in-a-github-pull-request\n",
      "\n",
      "1\n",
      "1628\n",
      "53135b495e986b0712efc453\n",
      "2015-11-12 10:53\n",
      "Install [greasemonkey](https://addons.mozilla.org/en-US/firefox/addon/greasemonkey/) and add this script to it, (you can enable or disable greasemonkey if you aren't using it for anything else) to enable/disable expanding the diff comments... ``` // ==UserScript== // @name        github expand outdated diff comments // @namespace   rvraghav93@gmail.com // @include     https://github.com/scikit-learn/scikit-learn/pull/* // @version     1 // @grant       none // ==/UserScript== $(\".outdated-diff-comment-container\").addClass('open') ```\n",
      "One thing I wish the github PR page to show is a list of all the referred links and PRs or Issues in a neat side bar like stackoverflow has at the right, next to the question area... I am planning to write a greasemonkey script for that... I think it will be super useful to have all the links at the right instead of searching through the PR...\n",
      "\n",
      "3\n",
      "1629\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-13 19:49\n",
      "Hey @amueller , I'm trying to decide between heckling the presenter at the scikit-learn tutorial @ ODSC or learning some d3.js ... Do you have a strong opinion on the matter? :smile:\n",
      "\n",
      "1\n",
      "1630\n",
      "553e8e1015522ed4b3df97f7\n",
      "2015-11-14 10:59\n",
      "I hope all of you in Paris are safe :worried:\n",
      "\n",
      "1\n",
      "1631\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-14 21:42\n",
      "master does not compile on py35 for me:\n",
      "\n",
      "1\n",
      "1632\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-14 21:42\n",
      "\n",
      "0\n",
      "1633\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-14 21:48\n",
      "This also did not go away by checking out `0.17`.\n",
      "\n",
      "1\n",
      "1634\n",
      "53135b495e986b0712efc453\n",
      "2015-11-14 22:03\n",
      "Could you try \"make clean\" then make...\n",
      "\n",
      "1\n",
      "1635\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-15 06:51\n",
      "`make clean; make` finishes fine, but then? should i `make install` as usual for C-progs?\n",
      "\n",
      "1\n",
      "1636\n",
      "53135b495e986b0712efc453\n",
      "2015-11-15 11:53\n",
      "yes.. does that succeed now?\n",
      "\n",
      "1\n",
      "1637\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-15 16:46\n",
      "```bash $ make install make: *** No rule to make target `install'.  Stop. ```\n",
      "\n",
      "1\n",
      "1638\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-15 19:34\n",
      "do you want to globally install? or user install or build in the folder?\n",
      "python setup.py install or python setup.py develop or python setup.py build_ext -i do some of these\n",
      "\n",
      "2\n",
      "1639\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-15 21:41\n",
      "yes, but i have been advised above not to use that.\n",
      "i want to just install for my user account, what is the advised sequence of commands (as written above, `python setup.py install` failed).\n",
      "\n",
      "2\n",
      "1640\n",
      "5634e8e116b6c7089cb8fa99\n",
      "2015-11-15 21:49\n",
      "@michaelaye it would be\n",
      "python setup.py install --user\n",
      "\n",
      "2\n",
      "1641\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-15 21:51\n",
      "hah, after the full run of make clean; make the `python setup.py install` actually worked now. thanks for your help.\n",
      "in case u wonder, i still need to self-compile even so im a conda user,  b/c i have pinned my numpy to 1.9.* b/c numpy 1.10 has performance probs with character arrays.\n",
      "and theres no scikit-learn for conda py3.5 numpy  1.9 on offer in conda.\n",
      "\n",
      "3\n",
      "1642\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-16 17:11\n",
      "welcome everybody to the sprint!\n",
      "\n",
      "1\n",
      "1643\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-16 17:19\n",
      " hey all!\n",
      "\n",
      "1\n",
      "1644\n",
      "564a144c16b6c7089cbaebe3\n",
      "2015-11-16 17:43\n",
      "Hello folks! I'm having an issue building sklearn from source. I've included my full logs here (http://pastebin.com/13LKxi55). I'm using pyenv with anaconda. Has anyone seen any issues with sklearn/cluster/_k_means.c failing to compile?  This looks a lot like my error https://github.com/scikit-learn/scikit-learn/issues/3114, but I don't understand the resolution.\n",
      "\n",
      "1\n",
      "1645\n",
      "564a0e2916b6c7089cbaead6\n",
      "2015-11-16 17:44\n",
      "Which version of python are you using?\n",
      "\n",
      "1\n",
      "1646\n",
      "564a144c16b6c7089cbaebe3\n",
      "2015-11-16 17:45\n",
      "``` (root) <unconvertable>  scikit-learn git:(master) pyenv version anaconda3-2.4.0 (set by /Users/maxlikely/src/scikit-learn/.python-version) ```\n",
      "which is 3.5\n",
      "\n",
      "2\n",
      "1647\n",
      "564a0e2916b6c7089cbaead6\n",
      "2015-11-16 17:49\n",
      "I think that might be your issue\n",
      "Possible to build against an 3.4?\n",
      "comment on it\n",
      "\n",
      "3\n",
      "1648\n",
      "564a0dc816b6c7089cbaeacd\n",
      "2015-11-16 17:49\n",
      "suppose I've found an Easy issue to address, having a Needs Contributor label. how do I claim this as my own?\n",
      "\n",
      "1\n",
      "1649\n",
      "564a144c16b6c7089cbaebe3\n",
      "2015-11-16 17:52\n",
      "@joshuacook I tried building against 3.4.3, I had the same issue. I can try 3.4.0.\n",
      "\n",
      "1\n",
      "1650\n",
      "54c084dbdb8155e6700eed4c\n",
      "2015-11-16 17:57\n",
      "http://scikit-learn.org/stable/developers/contributing.html\n",
      "\n",
      "1\n",
      "1651\n",
      "564a18f416b6c7089cbaec93\n",
      "2015-11-16 17:57\n",
      "Here is the documentation on contributing code, for the person who just asked: http://scikit-learn.org/dev/developers/contributing.html#contributing-code\n",
      "\n",
      "1\n",
      "1652\n",
      "564a0e2916b6c7089cbaead6\n",
      "2015-11-16 17:58\n",
      "@maxlikely I had issues with required libs being built against different versions. But I'm not using `conda`\n",
      "\n",
      "1\n",
      "1653\n",
      "564a185916b6c7089cbaec66\n",
      "2015-11-16 18:02\n",
      "When I run `make` on a forked branch of master, I get 1 error and 1 failure.  Here is the traceback for the error http://pastebin.com/C8EpQT75 It involves reading a jpeg with PIL, which hasn't been working for me the past few weeks.  I have Pillow 3.0.0 installed\n",
      "\n",
      "1\n",
      "1654\n",
      "564a0d8116b6c7089cbaeabf\n",
      "2015-11-16 18:07\n",
      "Looking at https://github.com/scikit-learn/scikit-learn/issues/5686\n",
      "\n",
      "1\n",
      "1655\n",
      "564a0dc816b6c7089cbaeacd\n",
      "2015-11-16 18:10\n",
      "looking at 5581, and need to reconcile the built path mentioned in the bug with the path in the source. bug mentions file:///home/andy/checkout/scikit-learn/doc/_build/html/stable/modules/neural_networks_supervised.html#more-control-with-warm-start, but the source path is actually doc/modules/neural_networks_supervised.rst. After updating such a doc, what is to be done to verify all is well before submitting a PR?\n",
      "\n",
      "1\n",
      "1656\n",
      "564a18f416b6c7089cbaec93\n",
      "2015-11-16 18:19\n",
      "When I run '''make''' on my branch, the build fails. Here is the traceback: http://pastebin.com/VgHFvhPF# Any help appreciated.\n",
      "\n",
      "1\n",
      "1657\n",
      "5648fd3216b6c7089cbad1e3\n",
      "2015-11-16 18:30\n",
      "I got the same error as @hallr   Any ideas why it doesn't \"make\"?\n",
      "\n",
      "1\n",
      "1658\n",
      "564a116816b6c7089cbaeb95\n",
      "2015-11-16 18:34\n",
      "Me too I have the same problem @hallr  @lazarillo\n",
      "\n",
      "1\n",
      "1659\n",
      "564a0e2916b6c7089cbaead6\n",
      "2015-11-16 18:38\n",
      "any restructured text gurus?\n",
      "\n",
      "1\n",
      "1660\n",
      "564a0dc816b6c7089cbaeacd\n",
      "2015-11-16 18:46\n",
      "answering my own question, `make doc` builds html docs under doc/_build/html/stable\n",
      "\n",
      "1\n",
      "1661\n",
      "564a0d8116b6c7089cbaeabf\n",
      "2015-11-16 18:50\n",
      "Any traction on that make test error with the infinite value?\n",
      "\n",
      "1\n",
      "1662\n",
      "564a264d16b6c7089cbaee0f\n",
      "2015-11-16 18:55\n",
      "hi all, if anybody would like to review my pull request for 'adding cython to requirements' in documentation, it is here:\n",
      "https://github.com/scikit-learn/scikit-learn/pull/5834\n",
      "\n",
      "2\n",
      "1663\n",
      "5648fd3216b6c7089cbad1e3\n",
      "2015-11-16 18:57\n",
      "Hi @joshuacook  I don't know if I'd say guru, but I understand it fairly well.\n",
      "\n",
      "1\n",
      "1664\n",
      "564a144c16b6c7089cbaebe3\n",
      "2015-11-16 19:00\n",
      "I'm going to take a stab at https://github.com/scikit-learn/scikit-learn/issues/5606.\n",
      "\n",
      "1\n",
      "1665\n",
      "564a0d8116b6c7089cbaeabf\n",
      "2015-11-16 19:00\n",
      "I have a PR for an error that happens in the tests: https://github.com/scikit-learn/scikit-learn/pull/5836\n",
      "\n",
      "3\n",
      "1666\n",
      "53135b495e986b0712efc453\n",
      "2015-11-16 19:00\n",
      "@amueller #4533 can be closed...\n",
      "\n",
      "1\n",
      "1667\n",
      "564a11fc16b6c7089cbaeb9e\n",
      "2015-11-16 19:12\n",
      "Has anyone found an issue that they are working on that is simple and would enjoy more collaboration?\n",
      "\n",
      "1\n",
      "1668\n",
      "564a176d16b6c7089cbaec44\n",
      "2015-11-16 19:22\n",
      "Is there a norm against implementing visualization methods directly within an sklearn module?\n",
      "\n",
      "1\n",
      "1669\n",
      "564a11fc16b6c7089cbaeb9e\n",
      "2015-11-16 19:24\n",
      "@hallr @NTBlok Can i work on issue #5827 with you?\n",
      "\n",
      "1\n",
      "1670\n",
      "53135b495e986b0712efc453\n",
      "2015-11-16 19:25\n",
      "What kind of visualisation methods do you mean? @jonoleson\n",
      "\n",
      "1\n",
      "1671\n",
      "564a176d16b6c7089cbaec44\n",
      "2015-11-16 19:31\n",
      "@rvraghav93 Like I was considering adding a plotting method to the RFECV module that would graph the cross-validation scores for each subset of features. Just a simple line graph with num_features on the x-axis and cv_score on the y-axis.\n",
      "\n",
      "3\n",
      "1672\n",
      "564a27c016b6c7089cbaee48\n",
      "2015-11-16 19:39\n",
      "I'm looking into #5364\n",
      "\n",
      "1\n",
      "1673\n",
      "564a183f16b6c7089cbaec5b\n",
      "2015-11-16 19:42\n",
      "we're looking into #5804\n",
      "\n",
      "1\n",
      "1674\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-16 19:42\n",
      "for someone interested in figuring out a segfault, you can take a stab at https://github.com/scikit-learn/scikit-learn/issues/5724\n",
      "btw, fixing any errors in the build of the documentation (running make or make html in the doc folder) or fixing any warnings in the tests is also very welcome\n",
      "@MrChristophRivera @hugobowne just asked me about it.\n",
      "it is probably not as easy as the other issues. please check with him if he is working on it\n",
      "is @maxlikely in SF at the sprint?\n",
      "\n",
      "6\n",
      "1675\n",
      "564a18f416b6c7089cbaec93\n",
      "2015-11-16 20:05\n",
      "@NTBlok @fluxtransport and I submitted PR for #5827 - add contributors for 0.16 and 0.17 to docs\n",
      "\n",
      "1\n",
      "1676\n",
      "564a11fc16b6c7089cbaeb9e\n",
      "2015-11-16 20:12\n",
      "@lazarillo and I are taking a stab at #4920\n",
      "\n",
      "1\n",
      "1677\n",
      "564a0d8116b6c7089cbaeabf\n",
      "2015-11-16 20:40\n",
      "Is the problem with the news group downloads b/c the underlying data has moved to a new web site? https://github.com/scikit-learn/scikit-learn/issues/4711\n",
      "\n",
      "1\n",
      "1678\n",
      "564a0d8116b6c7089cbaeabf\n",
      "2015-11-16 21:00\n",
      "I had a look at the segfault -- I cannot reproduce when using %run inside an ipython notebook. Using either of the new matplotlib methods to fetch the yahoo data.\n",
      "\n",
      "1\n",
      "1679\n",
      "564a0e2916b6c7089cbaead6\n",
      "2015-11-16 21:03\n",
      "Pull request for the single dead link: https://github.com/scikit-learn/scikit-learn/pull/5829#partial-pull-merging\n",
      "\n",
      "1\n",
      "1680\n",
      "564a11fc16b6c7089cbaeb9e\n",
      "2015-11-16 21:08\n",
      "@amueller Is issue Meta-estimators for multi-output learning #5824 still available?\n",
      "\n",
      "1\n",
      "1681\n",
      "564a264d16b6c7089cbaee0f\n",
      "2015-11-16 21:18\n",
      "@MrChristophRivera i'm just looking into it now -- not sure whether i'll attack it . are you at the code sprint. thanks @amueller\n",
      "\n",
      "1\n",
      "1682\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-16 21:31\n",
      "@MrChristophRivera probably is ;) you can also use the physical audio channel\n",
      "\n",
      "1\n",
      "1683\n",
      "564a4d2b16b6c7089cbaf2ee\n",
      "2015-11-16 21:52\n",
      "@amueller made changes you requested: https://github.com/scikit-learn/scikit-learn/pull/5841\n",
      "\n",
      "1\n",
      "1684\n",
      "564a144c16b6c7089cbaebe3\n",
      "2015-11-16 22:02\n",
      "Hey, I just wanted to check if anyone has claimed https://github.com/scikit-learn/scikit-learn/issues/5851 yet?\n",
      "\n",
      "1\n",
      "1685\n",
      "564a183f16b6c7089cbaec5b\n",
      "2015-11-16 22:48\n",
      "git@github.com:RubyW/scikit-learn.git\n",
      "oops\n",
      "\n",
      "2\n",
      "1686\n",
      "564a0d8116b6c7089cbaeabf\n",
      "2015-11-16 22:49\n",
      "can someone review my change here: https://github.com/scikit-learn/scikit-learn/pull/5858\n",
      "\n",
      "1\n",
      "1687\n",
      "564a0dc816b6c7089cbaeacd\n",
      "2015-11-16 23:18\n",
      "coupla PRs here: https://github.com/scikit-learn/scikit-learn/pull/5833 https://github.com/scikit-learn/scikit-learn/pull/5856\n",
      "\n",
      "1\n",
      "1688\n",
      "564a0dc816b6c7089cbaeacd\n",
      "2015-11-16 23:34\n",
      "probably\n",
      "\n",
      "1\n",
      "1689\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-17 06:46\n",
      "looks like the dataset for the scipy2015 tutorial sessions is not coming down? http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip download does not start.\n",
      "im using the fetch_data script in your github repo.\n",
      "ah worked now, just took ages for the 81 MB with no bytes flowing for minutes...\n",
      "\n",
      "3\n",
      "1690\n",
      "53135b495e986b0712efc453\n",
      "2015-11-17 13:24\n",
      "@jonoleson I am unable to gauge how useful that usecase is... Once you convince @amueller it is useful enough (:P) you can add a plotting function similar to the `plot_partial_dependence` (in `ensemble/partial_dependence.py`)...  (But IMHO that particular usecase seems simple enough to not warrant a plotting helper function!)\n",
      "\n",
      "1\n",
      "1691\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 10:47\n",
      "I have data with a mix of categorical and numerical values. It classifies very well using one-hot encoding + random forests but terribly using any non-tree method I have tried. I am looking for a clustering method that might work on this sort of data.  What might be suitable?\n",
      "I tried some standard methods that rely on euclidean distance but they are a disaster it seems\n",
      "I noticed that http://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering.htm exists\n",
      "\n",
      "3\n",
      "1692\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 12:27\n",
      "#5765 can be closed...\n",
      "\n",
      "1\n",
      "1693\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 12:28\n",
      "hi @rvraghav93\n",
      "\n",
      "10\n",
      "1694\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 12:29\n",
      "Yes I will be working on Trees and RFs :)\n",
      "\n",
      "2\n",
      "1695\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 12:30\n",
      "For me the priority is https://github.com/scikit-learn/scikit-learn/issues/5212#issuecomment-155387289 :)\n",
      "Do you have any suggestions in mind?\n",
      "\n",
      "2\n",
      "1696\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 12:35\n",
      "good point.. because there is a risk my ideas are already done or just plain stupid, do you mind being a first stage filter?\n",
      "\n",
      "1\n",
      "1697\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 12:35\n",
      "And about GLM what does it do? Generalized Linear Models? translates roughly to our `linear_model` module?\n",
      "Oh sure a stupidity filter... I'm in ;)\n",
      "\n",
      "2\n",
      "1698\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 12:38\n",
      "the full title of the randomGLM talk is \"Random generalized linear model:  a highly accurate and interpretable  ensemble predictor\" . http://labs.genetics.ucla.edu/horvath/RGLM/TalkRGLM.pdf   In more detail it is an ensemble predictor based on  bootstrap  aggregation (bagging) of  generalized linear models  whose  covariates are selected using  forward regression  according to  AIC criteria.\n",
      "maybe we already have something that is equivalent to that?\n",
      "@rvraghav93  https://followthedata.wordpress.com/2013/10/10/random-generalized-linear-models/ has an explanation of randomglm too\n",
      "\n",
      "3\n",
      "1699\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 12:43\n",
      "We got `bagging` and `boosting` in the `ensemble` module! (Is that what randomGLM does?)... I will read the links in a moment...\n",
      "\n",
      "3\n",
      "1700\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 12:50\n",
      "But the paper - http://www.ncbi.nlm.nih.gov/pubmed/23323760 - has only 12 citations... I am not sure the core devs might want to take this into scikit learn, since this is not popular/old/academically established enough... See [this FAQ](http://scikit-learn.org/stable/faq.html#can-i-add-this-new-algorithm-that-i-or-someone-else-just-published) :)\n",
      "\n",
      "3\n",
      "1701\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 13:09\n",
      "ok back to my real suggestions :)\n",
      "\n",
      "1\n",
      "1702\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 13:12\n",
      "for random forests, it would be great if we had a method to make a small interpretable versions.  One of the main drawbacks of random forests is that they end up like a black box. Can you read http://link.springer.com/chapter/10.1007/978-3-319-18356-5_20 ?\n",
      "or even to infer a single decision tree\n",
      "http://scikit-learn.org/stable/faq.html doesn't mention neural networks!\n",
      "@rvraghav93  see above\n",
      "\n",
      "4\n",
      "1703\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 13:18\n",
      "ah..200 references.. I will work on that :)\n",
      "it's not a bad rule\n",
      "\n",
      "2\n",
      "1704\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 13:23\n",
      "@rvraghav93  under the 200+ citations rule I withdraw all my suggestions :)\n",
      "\n",
      "1\n",
      "1705\n",
      "564a264d16b6c7089cbaee0f\n",
      "2015-11-19 13:43\n",
      "#5834 ready for merge. minor changes to documentation. straightforward.\n",
      "\n",
      "1\n",
      "1706\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 13:54\n",
      "200+ citations is not a hard and fast rule... if the suggestion is for an improvement/technique that is not fundamentally different from a well established algorithm and gives a very significant performance improvement, it would be worthwhile to implement the same... Basically, the idea is that we don't want code that might rot over time without a substantial userbase or maintainers to support or both... Essentially u can compress that rule to this --  `((Will a lot of people who already use sklearn benefit from this?) || (Will it help bring *a lot* of new people to sklearn?, if its a completely new feature)) && (Does it fit well within our API?) && !(Will it make life tougher for the existing users)` ;)\n",
      "\n",
      "8\n",
      "1707\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 13:57\n",
      "@rvraghav93  I realise it's a little cheeky to ask here but.. do you have a view on my earlier question? That is ... I have data with a mix of categorical and numerical values. It classifies very well using one-hot encoding + random forests but terribly using any non-tree method I have tried. I am looking for a clustering/unsupervised method that might work on this sort of data. What might be suitable? I tried some standard methods that rely on euclidean distance but they are a disaster it seems\n",
      "That is what took me to the unsupervised random forest method\n",
      "\n",
      "2\n",
      "1708\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 14:23\n",
      "Another suggestion, this one I hope uncontroversial. We seem not to have the Gower distance implemented. As in http://stats.stackexchange.com/a/15313/53128\n",
      "\n",
      "1\n",
      "1709\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 15:10\n",
      "@rvraghav93  Thanks! I am impressed again :)\n",
      "\n",
      "4\n",
      "1710\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 15:14\n",
      "@rvraghav93  is daisy https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/daisy.html something that been discussed (this is relevant to the Gower coefficient). I have  a very vague memory of seeing it in some scikit learn discussion but I may well have that wrong\n",
      "ah no.. I think I was remembering http://stackoverflow.com/a/26387936/2179021\n",
      "Although there is a very interesting and related PR about LambdaMART I see where the conclusion of the discussion seems to be that we are better off using GBRT\n",
      "\n",
      "3\n",
      "1711\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 16:04\n",
      "I just realised that scikit learn has no support at all for ordinals currently.. is that right?\n",
      "\n",
      "1\n",
      "1712\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 16:14\n",
      "I think so... By ordinals you mean something like `{\"small\", \"medium\", \"large\"}` correct?\n",
      "@glouppe Your PhD thesis is awesome! Thanks... I am loving it...\n",
      "\n",
      "2\n",
      "1713\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 16:17\n",
      "@rvraghav93  more 1,2,3,4,5,6,7 where all you know is that 1<2<3<4<5<6<7\n",
      "that is you just know the order\n",
      "but you can't do 1+3 = 4\n",
      "we could call them ranks\n",
      "\n",
      "4\n",
      "1714\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 16:38\n",
      "@amueller If you come online I have a list of minor PRs for you to review/merge ;) BTW was #5883 discussed during the sprint??\n",
      "\n",
      "1\n",
      "1715\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-19 16:50\n",
      "@lesshaste scikit-learn doesn't really have any support for categorical variables at the moment.\n",
      "#5883 wasn't discusses during the sprint afaik\n",
      "@rvraghav93 I have a loooot to review at the moment. Trying to catch up\n",
      "\n",
      "3\n",
      "1716\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 16:51\n",
      "@lesshaste Oh!! Sorry I haven't followed that PR... :(\n",
      "@amueller Do you have anything in mind that I could be of help (in reviewing)?\n",
      "\n",
      "2\n",
      "1717\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-19 16:54\n",
      "things by tw991, the neural network improvements, the huber regressor. Anything by vignesh, tian or manoj (I'm supervising them). Your stuff is after that ;)\n",
      "\n",
      "1\n",
      "1718\n",
      "53135b495e986b0712efc453\n",
      "2015-11-19 16:56\n",
      "Do you mean to say I can review any of these or are u just listing ur todo list? :P\n",
      "\n",
      "1\n",
      "1719\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 17:00\n",
      "@amueller right.. well I suppose the tree improvements will be  the first major support for categorical variables.. ? I am referring to https://github.com/scikit-learn/scikit-learn/pull/4899\n",
      "@amueller and my suggestion for the Gower coefficient gives an easy way for mixed types including categorical variables if all you need is a distance. This could really help for clustering I suspect.\n",
      "\n",
      "2\n",
      "1720\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-19 17:17\n",
      "@amueller  in relation to our previous conversation about using shufflesplit to subsample for CV, I realised the use case I really had in mind is doing this out of core\n",
      "so the 10^9 feature vectors stay out of core and shufflesplit samples 6 10^5 feature vectors for training and 4 10^5 feature vectors for testing from the large out of core data set.  This may all be better done by bespoke code however rather than something built into scikit learn\n",
      "@amueller  That sounds like a good idea to me\n",
      "@amueller  maybe http://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html or https://docs.python.org/2/library/mmap.html ?\n",
      "Is it worth opening an issue? I see that some support for out of core processing is entering into scikit-learn\n",
      "or are you thinking the mmap'ing can happen in the user code and shufflesplit will just use it efficiently as is?\n",
      "\n",
      "6\n",
      "1721\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-19 18:36\n",
      "I was listing my todo list ;)\n",
      "@lesshaste you could possibly use memory mapped arrays to do this out of core. Not sure though\n",
      "yes\n",
      "that might be possible\n",
      "yes\n",
      "all fit methods do\n",
      "in particular to make pipelines easier to understand\n",
      "\n",
      "7\n",
      "1722\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-19 20:37\n",
      "Why would the PCA fit methods accept target vectors `y` if they dont do anything with it? Just for API exchange-a-bility ?\n",
      "\n",
      "1\n",
      "1723\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-19 22:27\n",
      "Thanks. So, if I may ask one more: Im a bit confused how to reduce dimensionality for a regression task? It is clear to me that some features are more related to variance in the target vector than others, so I believe to understand that applying a PCA should make sense in general, but I dont understand how to apply `sklearn` for this. Should I make the target vector part of the X-matrix before handing it to the PCA? I want to learn which of my features are related to variability in the target vector, and how much. All your examples and very nice tutorials only ever mention how to apply PCA to classification but never a case for regression, it seems.\n",
      "\n",
      "1\n",
      "1724\n",
      "54a2cde7db8155e6700e4190\n",
      "2015-11-19 22:43\n",
      "Youre probably better of with something like Lasso or RidgeRegression if youre interested in knowing which features are related to your target vector.\n",
      "PCA will mix together your inputs, meaning your regression coefficients wont have a direct interpretation.\n",
      "But if you do want to use PCA, then making a pipeline something like `pipeline.make_pipeline(PCA(n_components=N), LinearRegression()).fit(X, y)` ought to work (untested code)\n",
      "\n",
      "3\n",
      "1725\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-19 22:55\n",
      "reviews for #5728 would be cool\n",
      "\n",
      "1\n",
      "1726\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2015-11-19 22:58\n",
      "I thought PCA might be the right tool, as it provided this percentage-wise composition of the feature vectors for the resulting decomposition which I find very neat, also the way the explained variance results tells you how many new components are needed to explain variety. Your pipeline code will not consider the target vector in the PCA so I wonder how the PCA can be helpful that way.\n",
      "Ill have a look at how Lasso/Ridge Regressions work, thanks.\n",
      "ok, gotcha, thks.\n",
      "\n",
      "3\n",
      "1727\n",
      "564e507e16b6c7089cbb6551\n",
      "2015-11-19 23:17\n",
      "Is fuzzy c-means clustering implemented in scikit-learn?\n",
      "\n",
      "1\n",
      "1728\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-19 23:28\n",
      "@h4k1m0u currently not. Why do you want that and not GMMs?\n",
      "It is just optimizing GMMs with spherical covariance and optimized using hard EM, right?\n",
      "@michaelaye PCA never uses y, it is an unsupervised method. Therefor it is likely not a good choice for your application.\n",
      "\n",
      "3\n",
      "1729\n",
      "564e507e16b6c7089cbb6551\n",
      "2015-11-20 02:15\n",
      "@amueller because I'm trying to cluster an image and for that I've found in the literature that it's possible to take into account the spatial context (neighbourhood) when using fuzzy c-means. But, I would be really interested to know if GMM offer this possibility as well in image clustering (considering the spatial dimension besides the intensity), although I had issues with the GMM on matlab when operating on a multi-dimensional data (multivariate gaussians).\n",
      "\n",
      "1\n",
      "1730\n",
      "53135b495e986b0712efc453\n",
      "2015-11-20 09:08\n",
      "The number of open PRs are slowly increasing... From 300 to 368 :O\n",
      "We should have one sprint just for reviewing ;) That would be awesome!\n",
      "\n",
      "2\n",
      "1731\n",
      "53135b495e986b0712efc453\n",
      "2015-11-20 09:18\n",
      "BTW github's new repo layout is good! ;)\n",
      "everything on top...\n",
      "\n",
      "2\n",
      "1732\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-20 11:08\n",
      "@rvraghav93 That's a great idea!\n",
      "\n",
      "1\n",
      "1733\n",
      "564e507e16b6c7089cbb6551\n",
      "2015-11-20 13:14\n",
      "When clustering an image (distribution of pixel intensities), is it possible using GMM (implemented in scikit-learn) to include the spatial context besides the pixel brightness during the clustering (GMM using multivariate gaussian distribution)?\n",
      "\n",
      "1\n",
      "1734\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-20 13:15\n",
      "@h4k1m0u  That's quite a tricky question (I am not an expert).  I suspect your best bet for getting a high quality answer would be actually implement a solution using GMM on some publicly available data and show the problem. Can you do that?\n",
      "even better if you can find another system that does exactly what you want and compare them\n",
      "There is nothing as compelling to a developer as a worked example :)\n",
      "@h4k1m0u  It's very useful you have code that does it. Your question is good but I wonder if this the right place for it. If you are not asking for a new feature or asking about an existing one maybe a stackexchange site would work better\n",
      "@h4k1m0u  sorry I wasn't very helpful!\n",
      "please feel free to post the url to the question here\n",
      "\n",
      "6\n",
      "1735\n",
      "564e507e16b6c7089cbb6551\n",
      "2015-11-20 14:04\n",
      "@lesshaste I know that what I'm trying to do has already been achieved with fuzzy c-means (including the neighborhood context) to the clustering (see [Zhang and Chen 2004] A novel kernelized fuzzy C-means algorithm with application in medical image segmentation). But, since the fuzzy c-means is not implemented in scikit-learn, I'm looking for something similar but with GMM.\n",
      "@lesshaste yes in python (https://github.com/scikit-fuzzy/scikit-fuzzy)\n",
      "@lesshaste ok I will try to test the GMM implemented in scikit-learn, to cluster images (taking into account the spatial context). Otherwise, I will ask my question in stackoverflow... thanks..\n",
      "\n",
      "3\n",
      "1736\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-20 14:13\n",
      "is there code in some other language for fuzzy c-means?\n",
      "\n",
      "1\n",
      "1737\n",
      "564e507e16b6c7089cbb6551\n",
      "2015-11-20 20:12\n",
      "@lesshaste it's okay, my issue is not really related to machine learning (clustering). just an image processing issue (maybe I should ask to the scikit-image community): http://stackoverflow.com/questions/33834883/include-the-spatial-context-of-pixels-during-image-clustering\n",
      "\n",
      "1\n",
      "1738\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-20 21:08\n",
      "@h4k1m0u the standard is to also include the x and y coordinates. slic for example is just k-means on x,y,lab\n",
      "same for quickshift\n",
      "look at the image segmentation module in scikit-image. it does what you want.\n",
      "\n",
      "3\n",
      "1739\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-20 21:14\n",
      "@h4k1m0u answert on SO, too\n",
      "@MechCoder starting to review your HuberEstimator. Teaches you to be careful what you wish for ;)\n",
      "\n",
      "2\n",
      "1740\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-20 21:33\n",
      "@rvraghav93 please don't ping joel, he is offline at the moment.\n",
      "\n",
      "1\n",
      "1741\n",
      "53135b495e986b0712efc453\n",
      "2015-11-20 21:39\n",
      "Yes I heard about him... Thats so sad... :/ Hope he stays strong!! And yes sure I won't... (BTW are you telling me in the context of any particular comment where I accidentally pinged him or are you just letting me know?)\n",
      "BTW I just realized you are down to your last but one task in your TODO... That means I am next :smiling_imp:\n",
      "(Don't worry I've a very small list - #5823 #5703 #5568 #4115)\n",
      ":P\n",
      "\n",
      "4\n",
      "1742\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-20 22:23\n",
      "No, there is like three more tasks :P Still on Huber, then Vighnes and Tian\n",
      "you pinged Joel 5 days ago somewhere. How did you hear. He did not post publicly, so I think we should not make it overtly public.\n",
      "\n",
      "2\n",
      "1743\n",
      "53135b495e986b0712efc453\n",
      "2015-11-20 22:34\n",
      "Check PM :)\n",
      "well then `amueller._todo_queue.put_nowait([5823, 5703, 5568, 4115])` ^_^\n",
      "\n",
      "2\n",
      "1744\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-20 22:42\n",
      "http://i.imgur.com/aOChOa2.png\n",
      "(note the last line)\n",
      "\n",
      "2\n",
      "1745\n",
      "53135b495e986b0712efc453\n",
      "2015-11-20 22:46\n",
      "In my defence `put_no wait` fails when the thread is busy... so u dont have to worry about it :p I'll work on something else at the moment :grin:\n",
      "\n",
      "1\n",
      "1746\n",
      "55e8e8690fc9f982beaf992f\n",
      "2015-11-21 01:12\n",
      "Hey guys! What's the best way to perform Kernel Logistic Regression or Import Vector Machine (or anything that will do binary classification + output probabilities) with SkLearn or some other python package? I can't seem to find anything. Does no one ever use KLR or IVM ?\n",
      "Thanks, but I don't have that much data, so I wanted to use it since I really need probabilistic outputs. I don't care about scaling for now.\n",
      "\n",
      "12\n",
      "1747\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-21 01:13\n",
      "I have not heard of IVM. and yes, no-one ever uses KernelLogisticRegression because it scales even worse than SVMs ;)\n",
      "\n",
      "1\n",
      "1748\n",
      "55e8e8690fc9f982beaf992f\n",
      "2015-11-21 01:19\n",
      "Interesting, thanks.  Also this is the IVM; just heard of it recently: http://www.ipb.uni-bonn.de/ivm/?L=1\n",
      "\n",
      "1\n",
      "1749\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-21 01:21\n",
      "interesting. that's my almer mater but I haven't heard of that professor\n",
      "paper is here: http://papers.nips.cc/paper/2059-kernel-logistic-regression-and-the-import-vector-machine.pdf\n",
      "you can set n_components to a smaller number in Nystroem for an approximation of kernel logistic regression\n",
      "ah the improved paper is foerstner\n",
      "makes sense\n",
      "\n",
      "5\n",
      "1750\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-11-21 01:23\n",
      "if you have benchmarks that show IVM is superior to svm + platt let me know ;)\n",
      "ah IVM is kernel logistic regression with one-step look-ahead greedy selection of the non-zero dual coefficients.\n",
      "I think people are just not so excited about kernels any more, so people don't really care for practical implementations of kernels\n",
      "\n",
      "4\n",
      "1751\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-21 07:00\n",
      "@amueller I didn't realise people don't really care about kernels any more. Is this because everyone has moved on to random forests and deep learning?\n",
      "\n",
      "1\n",
      "1752\n",
      "5576063e15522ed4b3e19cc3\n",
      "2015-11-22 04:03\n",
      "This looks interesting :) https://github.com/google/skflow\n",
      "\n",
      "1\n",
      "1753\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-11-22 05:16\n",
      "People certainly do care about kernels, @lesshaste\n",
      "\"Random forests and deep learning\" are certainly not the solution to everything.\n",
      "\n",
      "2\n",
      "1754\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-22 07:07\n",
      "@tw991  Thanks!\n",
      "\n",
      "1\n",
      "1755\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-22 07:08\n",
      "@jmschrei  It would lovely to see a blog post investigating that question more deeply.\n",
      "\n",
      "1\n",
      "1756\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-11-22 08:51\n",
      "Investigating what question?\n",
      "If people use other methods than RF and deep learning?\n",
      "That's pretty interesting, I didn't know that was a thing.\n",
      "\n",
      "3\n",
      "1757\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-22 11:08\n",
      "@jmschrei  I didn't quite mean that :) If you look at kaggle winners their main tools are quite consistent  (GBRT and/or deep learning).  I was thinking of a blog post titled \"Where kernels methods still rule\" explaining with examples where they are still the best approach\n",
      "@jmschrei  that's very interesting... although I am a little surprised by the trees being slow\n",
      "@jmschrei I think you would be a great person to write a blog post on this. It's very interesting and not universally understood\n",
      "\n",
      "3\n",
      "1758\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-11-22 20:47\n",
      "My understanding is high frequency trading uses kernel methods pretty extensively, because they need speed and tree based approaches are rather slow\n",
      "So they'll usually use some form of logistic kernel regression\n",
      "\"Rather slow\" in the HFT sense, not in the normal person sense, at making predictions\n",
      "Kernel methods are also pretty good for variable length sequences. For example in bioinformatics, the 'spectrum kernel SVM' is frequently used to compare protein sequences to each other to do domain classification or such.\n",
      "I mean, kernels extend far past just matrices of data. There are kernels to compare tree based structures, or graphs, to each other.\n",
      "Why?\n",
      "Doing an inner product is super fast, compared to traversing n binary trees.\n",
      "I'm not trying to demean either RF or deep learning, which are super powerful, but kaggle competitions are a small subset of ML problems out there.\n",
      "You haven't even touched my favorite models, probabilistic graphical models. People use Bayes nets, HMMs, GMMs all the time.\n",
      "Very fast for a normal person, but it's still orders of magnitude slower than an inner product using BLAS, and in the HFT sense, microseconds count.\n",
      "I also imagine you can put a logistic kernel machine on a GPU, but can't put trees on a gpu easily.\n",
      "What do you think it should cover?\n",
      "\n",
      "12\n",
      "1759\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-22 20:52\n",
      "@jmschrei  only because there is a line of research on producing minimum and forests or even a single decision tree with similar performance to a random forest and all you are doing is comparisons. 1000 comparisons is very fast\n",
      "\n",
      "1\n",
      "1760\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-22 20:59\n",
      "I am trying and failing to find some papers on inferring a singe decision tree from a random forest currently\n",
      "I can't seem to find the papers now... :(\n",
      "@jmschrei  Well... that would be up to the author :)  But how about a set of topics intersecting with... timings for prediction using random forests versus kernel methods, spectrum kernel SVM and how it is applied to variable length sequences. This would be even cooler if there were a test dataset and we could see how well a straightforward application of GBRT does in comparison,  practical examples with real data for kernels to compare tree based structures, or graphs and a comparison with what one would have to do using GBRT\n",
      "that sort of thing :)\n",
      "Basically, concrete classification or regression tasks where there is  a clearly understandable objective function and we can see how kernel methods are easier or just do better\n",
      "\n",
      "5\n",
      "1761\n",
      "55901c1b15522ed4b3e2f949\n",
      "2015-11-22 21:16\n",
      "I don't think it's possible to infer a single tree from an entire random forest, except in special circumstances\n",
      "That also doesn't look exactly the same as turning a RF into a single decision tree\n",
      "I thought you meant turn a RF into a single tree which mimiced it identically\n",
      "\n",
      "3\n",
      "1762\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-22 21:18\n",
      "@jmschrei  there is work on this. I am just struggling to find the papers again!\n",
      "\n",
      "8\n",
      "1763\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-22 21:23\n",
      "@jmschrei  here is one https://www.researchgate.net/profile/Ulf_Johansson5/publication/221008645_One_tree_to_explain_them_all/links/0deec52ff78d51398e000000.pdf\n",
      "@jmschrei  here is another http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.6595&rep=rep1&type=pdf\n",
      "@jmschrei  this is a copy and paste of the related work section https://bpaste.net/show/54f0d4433bca\n",
      "let me know if you want any of the papers\n",
      "\n",
      "4\n",
      "1764\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-22 21:56\n",
      "I don't know how well known it is.. or how useful  :)\n",
      "It would be great to understand how practically important http://jmlr.org/proceedings/papers/v37/beygelzimer15.pdf is too!\n",
      "section 5 claims it does better than vowpal wabbit\n",
      "\n",
      "3\n",
      "1765\n",
      "53810862048862e761fa2887\n",
      "2015-11-23 03:16\n",
      "Hello  I have been trying a lot of stuff to get #5689 to pass the tests. I am not able to reproduce the failures locally using `conda`.  I have narrowed the failure down to one line Any ideas why this commit https://github.com/vighneshbirodkar/scikit-learn/commit/02bf4df3ccd9f2eec5f1c0519caff7fbe7257969 causes this test https://travis-ci.org/scikit-learn/scikit-learn/builds/92637403 to fail ?\n",
      "\n",
      "1\n",
      "1766\n",
      "5653208916b6c7089cbbd390\n",
      "2015-11-23 15:12\n",
      "Hi anyone know word2vec ?\n",
      "\n",
      "1\n",
      "1767\n",
      "5653208916b6c7089cbbd390\n",
      "2015-11-23 15:13\n",
      "I cant import Word2vec in python. Am facing error File \"/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/models/word2vec.py\", line 690, in train     raise RuntimeError(\"you must first build vocabulary before training the model\") RuntimeError: you must first build vocabulary before training the model\n",
      "Kindly any one help me\n",
      "\n",
      "2\n",
      "1768\n",
      "53135b495e986b0712efc453\n",
      "2015-11-23 15:13\n",
      "Could you post a full code snippet?\n",
      "\n",
      "3\n",
      "1769\n",
      "5653208916b6c7089cbbd390\n",
      "2015-11-23 15:16\n",
      "Traceback (most recent call last):   File \"<pyshell#0>\", line 1, in <module>     import word2vec   File \"word2vec.py\", line 14, in <module>     model = word2vec.Word2Vec(sentences, size=100, window=4, min_count=1, workers=4)   File \"/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/models/word2vec.py\", line 432, in __init__     self.train(sentences)\n",
      "File \"/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/models/word2vec.py\", line 690, in train     raise RuntimeError(\"you must first build vocabulary before training the model\") RuntimeError: you must first build vocabulary before training the model\n",
      "\n",
      "2\n",
      "1770\n",
      "53135b495e986b0712efc453\n",
      "2015-11-23 15:22\n",
      "Oh I am sorry I didn't realize that you were asking about the library `word2vec`... This chat room is about scikit-learn :) This is not the correct place to ask... You would probably be better off, asking them at their mailing list... sorry :)\n",
      "\n",
      "3\n",
      "1771\n",
      "53135b495e986b0712efc453\n",
      "2015-11-23 15:25\n",
      "You could try - here - https://groups.google.com/forum/#!forum/gensim or here - https://radimrehurek.com/gensim/support.html :)\n",
      "\n",
      "1\n",
      "1772\n",
      "5653208916b6c7089cbbd390\n",
      "2015-11-23 15:26\n",
      "Thank you so much :+1:  :)\n",
      "\n",
      "2\n",
      "1773\n",
      "54e07d6515522ed4b3dc0858\n",
      "2015-11-23 15:33\n",
      "@Rahulvks if you follow the directions in this blog post it should work: http://rare-technologies.com/word2vec-tutorial/\n",
      "in particular, I suspect you are missing the middle step here: ``` >>> model = gensim.models.Word2Vec() # an empty model, no training >>> model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator >>> model.train(other_sentences)  # can be a non-repeatable, 1-pass generator ```\n",
      "\n",
      "2\n",
      "1774\n",
      "53135b495e986b0712efc453\n",
      "2015-11-23 19:34\n",
      "@vigneshbirodkar  did u try pdb?? Its good sometimes to debug frame by frame... Its a bit irritating to get started with but its worth it... (Just a humble suggestion ;) )\n",
      "\n",
      "1\n",
      "1775\n",
      "53810862048862e761fa2887\n",
      "2015-11-23 20:12\n",
      "@rvraghav93 I would have done that, but the tests don't fail on my system.\n",
      "\n",
      "1\n",
      "1776\n",
      "53135b495e986b0712efc453\n",
      "2015-11-23 20:16\n",
      "Ah that's a drat... Only otherway is to skip all other tests temporarily and also skip 2.6, 2.7 tests and ram Travis till u figure it out :p (if u run only this one... u can run it in a minute I think)\n",
      "\n",
      "4\n",
      "1777\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-25 17:59\n",
      "I am wondering if I have misunderstood https://github.com/scikit-learn/scikit-learn/issues/5870#issuecomment-159511717 .  Is the question if isnan() is fast in C ?\n",
      "\n",
      "1\n",
      "1778\n",
      "53135b495e986b0712efc453\n",
      "2015-11-25 18:08\n",
      "The thing is NaN in IEEE std has two possible representations (qNaN, which is the quiet NaN where we explicitly specify values to be NaN and sNaN where NaN is a signal NaN and is a (possibly unexpected) result of  numeric computation, like in Gael's comment...) So Gael was wondering if that would make `is_nan` computation in python/cython less efficient... Your pandas point was correct in that pandas does use a consistent NaN representation for both q/sNaN (atleast that is what I understood from that link), whereas numpy doesn't have one... I think it won't really affect the speed... but I am not sure... I am currently working on benchmarking that...\n",
      "\n",
      "1\n",
      "1779\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-25 18:14\n",
      "@rvraghav93 interesting! In C you really just have to do x!=x which is true iff x = NaN. This is exactly one comparison\n",
      "@rvraghav93  I also looked at the assembly that you get from isnan() from gcc which is quite interesting too :)\n",
      "@rvraghav93  I looked it up.. it seems any C99 compliant C compiler is guaranteed to do x!=x correctly That is x!=x iff x is NaN\n",
      "\n",
      "3\n",
      "1780\n",
      "53135b495e986b0712efc453\n",
      "2015-11-25 18:18\n",
      "Could you share?\n",
      "\n",
      "1\n",
      "1781\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-25 18:19\n",
      "@rvraghav93  sure.. using math.h,  isnan() compiles to  jmp __isnanf  .  However return x != x compiles to xor eax, eax ucomiss xmm0, xmm0 setp    al ret\n",
      "@rvraghav93  however it turns out gcc had a performance bug and bleeding edge isnan()  compiles to something closer to the latter assembly.. Does this make sense?\n",
      "\n",
      "2\n",
      "1782\n",
      "53135b495e986b0712efc453\n",
      "2015-11-25 18:33\n",
      "I am not sure if the latter x!=x will work in all compilers... but that is the most effective way to check for nan AFAIK... In general, IIRC `ucomiss` will handle the nan(s) effectively (i.e not distinguish between multiple nan representations)\n",
      "\n",
      "1\n",
      "1783\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-25 19:05\n",
      "@rvraghav93  I think you are right C99 mandates the use of a macro for this.  x!=x certainly works in gcc however and I assume all sensible compilers\n",
      "@rvraghav93  if you are interesting.. this was the gcc performance bug https://sourceware.org/bugzilla/show_bug.cgi?id=17441 . Fixed on 2015-09-18\n",
      "\n",
      "2\n",
      "1784\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-25 19:38\n",
      "@rvraghav93  it is specified in http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf#page=525&zoom=auto,-193,767 section F.9.3 Relational operators\n",
      "but in any case...isnan() is much easier to read and gcc will compile it properly soon :) (I don't really understand the math.h versus cmath.h point in any case)\n",
      "\n",
      "2\n",
      "1785\n",
      "561a58f7d33f749381a8ff2f\n",
      "2015-11-27 09:57\n",
      "I always wonder what is the best way to apply transformers to a dataset\n",
      "e.g. I want to OneHotEncode only certain variables\n",
      "X = [categorical_column, continuous_column, continuous_column]\n",
      "then throw it in a pipeline\n",
      "where a onehotencoder would only apply to the categorical column\n",
      "(similar with a standardscaler, only to columns where it \"makes sense\")\n",
      "how do you guys solve this issue?\n",
      "\n",
      "7\n",
      "1786\n",
      "53135b495e986b0712efc453\n",
      "2015-11-27 12:27\n",
      "@ogrisel @amueller Why does sorceforge not show 0.17 as the latest version??\n",
      "http://sourceforge.net/projects/scikit-learn/files/\n",
      "\n",
      "2\n",
      "1787\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-27 17:31\n",
      "@rvraghav93  hi\n",
      "\n",
      "1\n",
      "1788\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-27 17:32\n",
      "I was attempting to read http://arxiv.org/abs/1504.04595 . There is still a big gap between stats and machine learning!\n",
      "This part in particular where they explain which classifiers they will compare with: \"For comparison, we also present results for several state-of-the-art methods for high-dimensional classification, namely Penalized LDA (Witten and Tibshirani, 2011), Nearest Shrunken Centroids (Tibshirani et al., 2003), Shrunken Centroids Regularized Discriminant Analysis (Guo, Hastie and Tibshirani, 2007), and Independence Rules (IR) (Bickel and Levina, 2004), as well as for the base classi- fier applied in the original space\"\n",
      "does scikit-learn have any of those?\n",
      "oh..maybe shrunken centroids are here? http://scikit-learn.org/stable/modules/neighbors.html#nearest-shrunken-centroid\n",
      "is our LDA implementation the same as  Penalized LDA (Witten and Tibshirani, 2011) ?\n",
      "on another note.. the two images at http://scikit-learn.org/stable/auto_examples/neighbors/plot_nearest_centroid.html#example-neighbors-plot-nearest-centroid-py look identical to me\n",
      "are they meant to be different?\n",
      "\n",
      "7\n",
      "1789\n",
      "53135b495e986b0712efc453\n",
      "2015-11-27 22:13\n",
      "we also have LDA ^^\n",
      "\n",
      "1\n",
      "1790\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-27 23:15\n",
      "@rvraghav93  thanks.. I don't even know what  Shrunken Centroids Regularized Discriminant Analysis and Independence Rules are\n",
      "but the author of the paper is absolutely at the top of his field so I assume the new method is important\n",
      "@rvraghav93  do we have *penalized* LDA as in https://faculty.washington.edu/dwitten/Papers/JRSSBPenLDA.pdf ?\n",
      "\n",
      "3\n",
      "1791\n",
      "53135b495e986b0712efc453\n",
      "2015-11-27 23:26\n",
      "Ah no... I am not sure how useful that is?\n",
      "And there is something wrong with the example that you had posted... Mind raising it as an issue for someone else to look into it...? `shrinkage` is supposed to have an effect... ping @robertlayton and @MechCoder in your issue...\n",
      "\n",
      "2\n",
      "1792\n",
      "53135b495e986b0712efc453\n",
      "2015-11-27 23:37\n",
      "And lol no I don't either... I am hoping it gets named to LDA on steroids... must be easier to rememeber... on a serious note it seems to be a combination of regular LDA with shrunken centroids method (thought you must have figured that out already ;))\n",
      "\n",
      "1\n",
      "1793\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-11-28 08:23\n",
      "@rvraghav93  thanks. I opened an issue. It would be very interesting to know what exactly scikit-learn is missing from that list and if the things that are missing are worthwhile.\n",
      "\n",
      "1\n",
      "1794\n",
      "561a58f7d33f749381a8ff2f\n",
      "2015-11-28 12:19\n",
      "Guys what about adding an interface to OneHotEncoder and other transformers that can take a matrix and only operate on certain columns? E.g. if max_unique_values < 5 per column or whatever....\n",
      "I'm not sure how to use this encoder directly on a matrix mixed of continuous variables and categorical variables.\n",
      "Some others must have battled this?\n",
      "Also, is pd.get_dummies a solution here? I don't see though how it would work with train + test together....\n",
      "\n",
      "4\n",
      "1795\n",
      "53135b495e986b0712efc453\n",
      "2015-11-28 20:59\n",
      "@kootenpv I don't think we have a workaround for this ATM ;( We are working on making the pipeline objects more flexible...\n",
      "\n",
      "1\n",
      "1796\n",
      "561a58f7d33f749381a8ff2f\n",
      "2015-11-28 23:39\n",
      "@rvraghav93  How are you solving that situation then? I understand if there is no solution, but there must be some way to \"work around\" it, no? Just a list of transformers for each variable or something?\n",
      "\n",
      "1\n",
      "1797\n",
      "565b339a16b6c7089cbc9958\n",
      "2015-11-29 17:20\n",
      "Hey guys!\n",
      "I was trying to work on an easy bug regarding RobustScaler under sklearn.preprocessing\n",
      "when i checked the data.py file in the given file hierarchy, the robust scaler class does exist and i'm not able to figure out why is it giving me an import error\n",
      "\n",
      "3\n",
      "1798\n",
      "565b339a16b6c7089cbc9958\n",
      "2015-11-29 17:23\n",
      "I've even tried to run the plot_robust_scaling.py in the examples folder but then again I end up with the same import error.\n",
      "\n",
      "1\n",
      "1799\n",
      "554c47fe15522ed4b3e01823\n",
      "2015-11-30 14:18\n",
      "Hey Guys, I want to contribute to this project, How do i start?\n",
      "\n",
      "1\n",
      "1800\n",
      "553e8e1015522ed4b3df97f7\n",
      "2015-11-30 14:23\n",
      "Hi @chinmoysam, just dive into the issue tracker and see issues that are tagged Easy and Need Contributor.\n",
      "take a look at the developer guide too: http://scikit-learn.org/stable/developers/\n",
      "\n",
      "2\n",
      "1801\n",
      "554c47fe15522ed4b3e01823\n",
      "2015-11-30 14:25\n",
      "@vortex-ape Thanks is this issue tracker in the github page?? and i have just started learning scikit i have some knowledge of programming in python, Can i directly go to issue tracker and try to solve something??\n",
      "\n",
      "3\n",
      "1802\n",
      "554c47fe15522ed4b3e01823\n",
      "2015-11-30 16:03\n",
      "thanks a lot  @vortex-ape\n",
      "\n",
      "1\n",
      "1803\n",
      "5653208916b6c7089cbbd390\n",
      "2015-11-30 16:30\n",
      "Hi, Anyone having sample svm text classification code in sklearn ?\n",
      "\n",
      "1\n",
      "1804\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-01 07:06\n",
      "@Rahulvks have you looked at the text classification tutorial? http://scikit-learn.org/dev/tutorial/text_analytics/working_with_text_data.html\n",
      "you can also find several examples in the examples gallery: http://scikit-learn.org/dev/auto_examples/index.html\n",
      "\n",
      "2\n",
      "1805\n",
      "565b339a16b6c7089cbc9958\n",
      "2015-12-01 13:39\n",
      "@amueller, can you please help me out with this issue??\n",
      "I was trying to work on an easy bug regarding RobustScaler under sklearn.preprocessing  when i checked the data.py file in the given file hierarchy, the robust scaler class does exist and i'm not able to figure out why is it giving me an import error I've even tried to run the plot_robust_scaling.py in the examples folder but then again I end up with the same import error.\n",
      "I currently use a mac operating on OS X 10.11\n",
      "and tried importing the code on python3\n",
      "[![Screen Shot 2015-12-01 at 7.13.59 PM.png](https://files.gitter.im/scikit-learn/scikit-learn/KIc6/thumb/Screen-Shot-2015-12-01-at-7.13.59-PM.png)](https://files.gitter.im/scikit-learn/scikit-learn/KIc6/Screen-Shot-2015-12-01-at-7.13.59-PM.png)\n",
      "\n",
      "5\n",
      "1806\n",
      "561a58f7d33f749381a8ff2f\n",
      "2015-12-01 15:23\n",
      "@SumedhArani  Old version, it is really new\n",
      "\n",
      "1\n",
      "1807\n",
      "561a58f7d33f749381a8ff2f\n",
      "2015-12-01 18:42\n",
      "@SumedhArani  What I meant was... you're importing an old version of scikit, while you might be looking at some other source code. Try to see the path of sklearn.preprocessing module after importing sklearn.preprocessing\n",
      "    import sklearn.preprocessing\n",
      "    sklearn.preprocessing\n",
      "\n",
      "3\n",
      "1808\n",
      "565b339a16b6c7089cbc9958\n",
      "2015-12-02 05:33\n",
      "@kootenpv Thanks for the reply!!\n",
      "<module 'sklearn.preprocessing' from '/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/sklearn/preprocessing/__init__.py'>\n",
      "This is the output that I got on trying out what you told me!\n",
      "\n",
      "3\n",
      "1809\n",
      "565b339a16b6c7089cbc9958\n",
      "2015-12-02 05:35\n",
      "I've recently upgraded my scikit version and prior to which I had installed 0.16 version whose source code I have been referring to.\n",
      "I'll check out once again.\n",
      "I used pip install -U scikit-learn and it says the requirement is up to date.\n",
      "\n",
      "3\n",
      "1810\n",
      "541d52b1163965c9bc205cf3\n",
      "2015-12-02 18:53\n",
      "hi anyone up here\n",
      "\n",
      "1\n",
      "1811\n",
      "53135b495e986b0712efc453\n",
      "2015-12-02 19:01\n",
      "Yes?\n",
      "\n",
      "1\n",
      "1812\n",
      "541d52b1163965c9bc205cf3\n",
      "2015-12-02 19:06\n",
      "https://github.com/scikit-learn/scikit-learn/issues/5947\n",
      "i would like to implement this  if it is not already implemented (I couldnt find this in scikit library )\n",
      "The closest was  spectral embedding\n",
      "I havent explained the idea of the algorithm in the issue  but i can do that\n",
      "and the applications of the same too , as in recent years its gainig some popularity\n",
      "\n",
      "5\n",
      "1813\n",
      "53135b495e986b0712efc453\n",
      "2015-12-02 19:18\n",
      "How popular is it?\n",
      "\n",
      "1\n",
      "1814\n",
      "53135b495e986b0712efc453\n",
      "2015-12-02 19:21\n",
      "Also what kind of problems does it solve? You may want to include the answer to the prev questions in your issue :)\n",
      "I haven't looked into it exactly... will do so and let you know my view :)\n",
      "\n",
      "2\n",
      "1815\n",
      "541d52b1163965c9bc205cf3\n",
      "2015-12-02 19:30\n",
      "as already mentioned some very popular uses are web graph minning , page ranking algorithms\n",
      "let me include some application based papers in the area\n",
      "http://cseweb.ucsd.edu/~atsiatas/pr_diffusion.pdf\n",
      "http://ictactjournals.in/paper/IJSC_Vol3_Iss3_P5_544_548.pdf\n",
      "\n",
      "4\n",
      "1816\n",
      "53135b495e986b0712efc453\n",
      "2015-12-02 19:33\n",
      "I'll let you know my views after I look into it. You may have to wait for core devs to respond to you in that issue before you can proceed. You can also make a detailed email to our mailing list linking your issue to attract more comments.\n",
      "Could you also add these questions and answers to the issue? (So that this discussion could reach a larger audience)\n",
      "\n",
      "2\n",
      "1817\n",
      "541d52b1163965c9bc205cf3\n",
      "2015-12-02 19:35\n",
      "fot the other part of the questions :: it is used   in  a semi-supervised setup  and can be used at problems requiring local and global scales of information using some sort of  label transfer but not limited to this\n",
      "\n",
      "1\n",
      "1818\n",
      "55f2a92f0fc9f982beb05d85\n",
      "2015-12-02 19:40\n",
      "Hey Scikiters,\n",
      "\n",
      "1\n",
      "1819\n",
      "55f2a92f0fc9f982beb05d85\n",
      "2015-12-02 19:41\n",
      "I would like to know if someone has a parallel implementation of DBSCAN .. or Knows how to use it on top of Apache Spark .. Thanks in Advance\n",
      "\n",
      "1\n",
      "1820\n",
      "53135b495e986b0712efc453\n",
      "2015-12-02 19:45\n",
      "I may be wrong but isn't our implementation parallel?\n",
      "\n",
      "3\n",
      "1821\n",
      "55f2a92f0fc9f982beb05d85\n",
      "2015-12-02 19:54\n",
      "Do u mean, i should try to edit the scikit implementation to utilize Joblib, or u already using Joblib ?\n",
      "\n",
      "1\n",
      "1822\n",
      "541d52b1163965c9bc205cf3\n",
      "2015-12-02 19:59\n",
      "i will do that\n",
      "\n",
      "1\n",
      "1823\n",
      "55f2a92f0fc9f982beb05d85\n",
      "2015-12-02 20:02\n",
      "@halwai  .. if u mean DBSCAN, would u please let me know then ?\n",
      "\n",
      "1\n",
      "1824\n",
      "541d52b1163965c9bc205cf3\n",
      "2015-12-02 20:11\n",
      "@Elbehery   i am sorry  i was not reffering to DBSCAN\n",
      "\n",
      "1\n",
      "1825\n",
      "55f2a92f0fc9f982beb05d85\n",
      "2015-12-02 20:27\n",
      "no problem\n",
      "\n",
      "1\n",
      "1826\n",
      "53135b495e986b0712efc453\n",
      "2015-12-03 01:13\n",
      "Could someone remove the need contrib tags from issues which have a PR open? @amueller @ogrisel @glouppe ?\n",
      "#5943 #4639 #4808 #4883 #5029 #5298 #5318 #5952\n",
      "\n",
      "2\n",
      "1827\n",
      "53135b495e986b0712efc453\n",
      "2015-12-03 01:19\n",
      "Also since there was sufficient interest in mailing list (by sufficient interest I mean the +1 from Joel and +0(?) from Andy ;)), could some one add the \"Need Review\" tag pl? Once added I have a list of PRs to be tagged ;)\n",
      "\n",
      "1\n",
      "1828\n",
      "561a58f7d33f749381a8ff2f\n",
      "2015-12-04 10:19\n",
      "I'm really wondering if there would be ways to \"cache\" things in the Pipeline\n",
      "I mean... when you're doing a grid search... if you use some kind of static transformation in it\n",
      "E.g. you use a CountVectorizer that has no variation in parameters and it is put in the pipeline, I suspect that if you have some alpha values changing in a Ridge() in the pipeline, it still does the static CountVectorizer endlessly?\n",
      "    Pipeline({       'countvectorizer': CountVectorizer,       'ridge': Ridge})          grid = {'ridge__alpha': [0.1, 1, 10]}\n",
      "\n",
      "4\n",
      "1829\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-07 19:10\n",
      "@kootenpv not currently unfortunately https://github.com/scikit-learn/scikit-learn/pull/3951\n",
      "dask-sklearn tries to get rid of that, but that's more of a prototype\n",
      "see http://blaze.pydata.org/blog/2015/10/19/dask-learn/\n",
      "\n",
      "3\n",
      "1830\n",
      "561a58f7d33f749381a8ff2f\n",
      "2015-12-08 00:43\n",
      "@amueller nice catch\n",
      "\n",
      "1\n",
      "1831\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-08 10:55\n",
      "hi.. I was wondering if http://arxiv.org/pdf/1109.0887.pdf \"Learning Nonlinear Functions Using Regularized Greedy Forest\" is of interest? It was the method that came second in the kaggle higgs boson competition\n",
      "\n",
      "1\n",
      "1832\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-08 11:01\n",
      "hmm.. I see that despite this it hasn't been cited many times\n",
      "\n",
      "1\n",
      "1833\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-08 17:46\n",
      "@kootenpv well it is a known longstanding issue\n",
      "\n",
      "1\n",
      "1834\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-09 00:05\n",
      "man @agramfort is killing it on the issue tracker\n",
      "\n",
      "1\n",
      "1835\n",
      "53135b495e986b0712efc453\n",
      "2015-12-09 00:09\n",
      "xD\n",
      "\n",
      "1\n",
      "1836\n",
      "53135b495e986b0712efc453\n",
      "2015-12-09 01:28\n",
      "Btw can u create the \"Need Review\" Tag??\n",
      "\n",
      "1\n",
      "1837\n",
      "565b49ee16b6c7089cbc9b44\n",
      "2015-12-09 09:03\n",
      "Hey! I've found that http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold if called under `print` shows first argument as named `labels`, but you actually cannot pass it this way. Is it intentional?\n",
      "\n",
      "1\n",
      "1838\n",
      "53135b495e986b0712efc453\n",
      "2015-12-09 12:57\n",
      "A lot of issues need the `\"Need Contributor\"` tag removed - https://github.com/scikit-learn/scikit-learn/labels/Need%20Contributor\n",
      "\n",
      "1\n",
      "1839\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-09 14:57\n",
      "@vighneshbirodkar maybe ping @ogrisel or @lesteve about the joblib thing\n",
      "rvraghav93: can you list the issues that need the \"need contributor\" removed?\n",
      "\n",
      "2\n",
      "1840\n",
      "53810862048862e761fa2887\n",
      "2015-12-09 15:06\n",
      "Hello @ogrisel , @lesteve , I can see a fix for #5956 by setting `copy_cov=True` on this line https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/dict_learning.py#L296\n",
      "Also, passing `mmap_mode=readwrite' to `joblib.Parallel` gives a file descriptor error, but passing `mmap_mode=c' works,\n",
      "I am yet to find out what 'c' means for it,  I wasn't able to find any documentation\n",
      "Ok, see stands for Copy on write, is passing that ok ?\n",
      "\n",
      "4\n",
      "1841\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-09 16:41\n",
      "added the needs review tag\n",
      "\n",
      "1\n",
      "1842\n",
      "53135b495e986b0712efc453\n",
      "2015-12-09 16:43\n",
      "Yay yay :p I'll just send u a list for both (removal of need cont and addtn of need rev)\n",
      "\n",
      "1\n",
      "1843\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-09 17:10\n",
      "sent where?\n",
      "ah sorry future\n",
      "yes\n",
      "\n",
      "3\n",
      "1844\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-09 17:36\n",
      "@rvraghav93 sorry I'm a bit out of touch with what is happening at the moment. Have you worked on the multiple metrics stuff yet?\n",
      "@vighneshbirodkar would you be fine with a non-ml project? I think doing the website build would be cool #5578 #4986\n",
      "Otherwise the multiple metrics would be very high priority to me unless @rvraghav93 worked on it. It might be that we need to first merge ragav's improvements / code deletions though\n",
      "\n",
      "3\n",
      "1845\n",
      "53135b495e986b0712efc453\n",
      "2015-12-09 17:44\n",
      "Hey no I am breaking my head over tree code ;(\n",
      "Sorry I will try my best to start it by this weekend :)\n",
      "There are a few minor PRS of mine which u might want to consider for review xD\n",
      "\n",
      "3\n",
      "1846\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-09 17:57\n",
      "sorry, not much time for reviews for the rest of the year :-/\n",
      "\n",
      "1\n",
      "1847\n",
      "53810862048862e761fa2887\n",
      "2015-12-09 17:58\n",
      "Are Transformers also expected not to do anything in place ?\n",
      "\n",
      "1\n",
      "1848\n",
      "53135b495e986b0712efc453\n",
      "2015-12-10 15:13\n",
      "`\"Need Review\"` - #5907, #5823, #5703, #5568, #4115, #5883, #5983, #5971, #5971, #5945, #5929, #5815, #5920  and PRs ready for merge - #5981, #5946, #5925 and Issues (or PRs) which need a removal of `\"Need Contributor\"` - #5986, #5876, #5868, #5824, #5789, # 5738, #5583, #5367, #5298, #5269, #4804  (Going a bit overboard with the usage of tags - take with a grain of salt - * a \"Stalled Work\" tag (hoping Gael doesn't oppose ;) ) -  #5316 (and a lot more...) (this tag could be added after the author of the PR  says he is no longer interested / have time to work on it or doesn't respond with the status in a month's time, this will help people who wish to work on the related issue, understand that their new PR is welcome... we could just add the `\"Need Contributor\"` back to the related issue... but people get confused why there is an existing PR for that issue) * a \"Action Needed\" tag - #4804 Along with `\"Need Review\"` to denote that review has been done and it needs the author to respond to the review... (So we can filter PRs for review like \"Need Review\" and !\"Action Needed\")\n",
      "\n",
      "1\n",
      "1849\n",
      "53810862048862e761fa2887\n",
      "2015-12-10 15:20\n",
      "@rvraghav93 2 of my PRs also need reviews, you think they need to be tagged ?\n",
      "\n",
      "1\n",
      "1850\n",
      "53135b495e986b0712efc453\n",
      "2015-12-10 15:50\n",
      "\"Need Review\" of course ;) Could you list them here??\n",
      "Nevermind! @amueller #5414, #5270\n",
      "\n",
      "2\n",
      "1851\n",
      "53135b495e986b0712efc453\n",
      "2015-12-10 15:52\n",
      "and awesome you've fixed #5689... sorry I tried and gave up...\n",
      "\n",
      "1\n",
      "1852\n",
      "53810862048862e761fa2887\n",
      "2015-12-10 15:58\n",
      "That's ok, that issue still hasn't been fixed, though. I was making a mistake by not passing randon_state, but there is no reason why that test should fail\n",
      "\n",
      "1\n",
      "1853\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-10 16:22\n",
      "should we tag all \"MRG\" ones with \"needs review\"?\n",
      "\n",
      "1\n",
      "1854\n",
      "53135b495e986b0712efc453\n",
      "2015-12-10 16:23\n",
      "I was wondering that... but that would be too much... So I was thinking maybe we could have only 20 odd `\"Need Review\"` tags at a time? How does that sound?\n",
      "We have at most 10 serious reviewers right?\n",
      "The number of `[MRG*` issues are over 150\n",
      "By most recently commented...\n",
      "\n",
      "4\n",
      "1855\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-10 16:25\n",
      "I tagged all of them, but we should untag those that actually wait for the person doing the review to get back\n",
      "well how do you choose the 20 ?\n",
      "how does that make sense?\n",
      "if someone created the perfect pull request and no dev reviewed it for 2 years, then it will be never reviewed\n",
      "\n",
      "4\n",
      "1856\n",
      "53135b495e986b0712efc453\n",
      "2015-12-10 16:29\n",
      "Well rules are meant to be broken... :P We could have 10 more for such PRs? Or not... probably that was a stupid suggestion... nevermind ;)\n",
      "But there will be a question on how do we get to decide that 10 probably.. so its better that all `[MRG*` is \"need review\" tagged...\n",
      "\n",
      "2\n",
      "1857\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-10 16:32\n",
      "we should remove the ones where we are waiting on the contributor to address comments\n",
      "\n",
      "1\n",
      "1858\n",
      "53135b495e986b0712efc453\n",
      "2015-12-10 16:35\n",
      "Okay that sounds better :) If I catch something like that I'll just ping u here... and how about `\"Work Stalled\"` for stalled PRs?\n",
      "\n",
      "4\n",
      "1859\n",
      "53135b495e986b0712efc453\n",
      "2015-12-10 16:42\n",
      "For example in [this comment](https://github.com/scikit-learn/scikit-learn/issues/5229#issuecomment-149628243) the author of #5316 said that he is not working on it anymore... There are quite a few PRs like that and a few more where the author has stopped responding... I was wondering out aloud if we could tag those with `\"Work stalled\"`...\n",
      "\n",
      "1\n",
      "1860\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-10 17:04\n",
      "or close them?\n",
      "\n",
      "1\n",
      "1861\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-10 17:05\n",
      "well how do you know if something is stalled and what would be the benefit of the tag?\n",
      "\n",
      "1\n",
      "1862\n",
      "53135b495e986b0712efc453\n",
      "2015-12-10 17:06\n",
      "that is better... but I feel it should be done only for stalled PRs which have a related issue open and we should include a comment in the issue that \"There was a stalled PR ##### That was closed due to inactivity\"... but I am afraid that closing might be a bit rude?\n",
      "I was thinking of doing a random search from time to time ;)\n",
      "for comments  by authors saying so or comments which ask for status that go unresponded for more than a month...\n",
      "\n",
      "3\n",
      "1863\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-10 17:28\n",
      "@rvraghav93 is waterponey colocated with you?\n",
      "\n",
      "6\n",
      "1864\n",
      "53135b495e986b0712efc453\n",
      "2015-12-10 17:32\n",
      "Ah he was sitting with us... but I am unable to recollect who he is...\n",
      "\n",
      "2\n",
      "1865\n",
      "561a58f7d33f749381a8ff2f\n",
      "2015-12-10 21:06\n",
      "any Dutch :P?\n",
      "\n",
      "1\n",
      "1866\n",
      "5525b91815522ed4b3deb7d6\n",
      "2015-12-10 22:27\n",
      "@amueller @rvraghav93 In sympy we have \"PR: Author's turn\". It is put once the PR is reviewed and we are waiting on the author to address the comments. This way it is sometime easier to identify stalled one's.  :smile:\n",
      "\n",
      "1\n",
      "1867\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-10 22:31\n",
      "@vighneshbirodkar multi-class AUC might also be interesting: #3298\n",
      "@vighneshbirodkar the ada-grad stuff here #3729 would be cool but is likely to be a bigger project\n",
      "@leosartaj how often do you reassign them? if the PR is quite active, it changes often who's turn it is. Or do you only do that if it is stalled for a while?\n",
      "it might be helpful\n",
      "\n",
      "4\n",
      "1868\n",
      "5525b91815522ed4b3deb7d6\n",
      "2015-12-10 22:34\n",
      "I have seen it to work best: 1. It is a big PR. Reviews take time. 2. Author replies in a while. Probably not a good idea when the PR is quite active. Works well for slower one's.\n",
      "\n",
      "1\n",
      "1869\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-11 00:03\n",
      "reviews on #4936 would be good ....\n",
      "thanks @leosartaj :)\n",
      "\n",
      "2\n",
      "1870\n",
      "5525b91815522ed4b3deb7d6\n",
      "2015-12-11 00:04\n",
      ":smile:\n",
      "\n",
      "1\n",
      "1871\n",
      "53135b495e986b0712efc453\n",
      "2015-12-11 15:26\n",
      "Is #5995 an easy one? If so could you tag it with \"Need contribs\". I found a contributor looking for an issue ;) ([ref](https://github.com/scikit-learn/scikit-learn/issues/5879#issuecomment-162382187) )\n",
      "\n",
      "1\n",
      "1872\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-11 15:34\n",
      "we need to merge https://github.com/scikit-learn/scikit-learn/pull/5578 first\n",
      "\n",
      "1\n",
      "1873\n",
      "53135b495e986b0712efc453\n",
      "2015-12-11 16:54\n",
      "I have a probably lame question regarding cython code - Why is that we don't delete all the free the memory of all the members in the destructor? (Ref [the destructor of `Splitter` class](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_splitter.pyx#L105))\n",
      "\n",
      "1\n",
      "1874\n",
      "53810862048862e761fa2887\n",
      "2015-12-11 16:58\n",
      "What do you think should be freed and isn't ?\n",
      "\n",
      "1\n",
      "1875\n",
      "53135b495e986b0712efc453\n",
      "2015-12-11 17:05\n",
      "Any other attribute say `sample_weight`? (Fair warning this could be a very dumb question but I am quite new to cython ;))\n",
      "\n",
      "1\n",
      "1876\n",
      "53810862048862e761fa2887\n",
      "2015-12-11 17:08\n",
      "It should be freed, but where it is freed depends on the context of the code. For example, any object declared in Python should not get freed with `free` and Python's GC will pick it up. If an object is declared here and not freed, it might be getting freed elsewhere\n",
      "\n",
      "2\n",
      "1877\n",
      "53135b495e986b0712efc453\n",
      "2015-12-11 17:17\n",
      "so basically `sample_weight` of this splitter will hold the reference to the mem block managed elsewhere (or probably by the python GC) correct?\n",
      "\n",
      "3\n",
      "1878\n",
      "53810862048862e761fa2887\n",
      "2015-12-11 17:17\n",
      "I won't be surprised if that is infact a numpy array\n",
      "\n",
      "2\n",
      "1879\n",
      "54b4f2d1db8155e6700e99c0\n",
      "2015-12-13 19:33\n",
      "Hey Folks, with PCA, if you fit with all the components, you should be able to transform with a subset of them only. If Im not wrong, this is currently not possible. Should I open an issue?\n",
      "\n",
      "1\n",
      "1880\n",
      "561a58f7d33f749381a8ff2f\n",
      "2015-12-13 22:47\n",
      "@Djabbz  Sounds very strange. I don't think that's possible with PCA. Do you have literature where it says? As far as I know if you go compress 40 features into 10 features, you'd need measurements on all 40 features to result into 10.\n",
      "\n",
      "1\n",
      "1881\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-14 18:58\n",
      "@Djabbz we could add a parameter to the ``transform`` method but usually we don't like to do that. Can you give an example application?\n",
      "You could just replace ``components_`` by ``components_[:n_features_you_want]``\n",
      "\n",
      "2\n",
      "1882\n",
      "5363a92c048862e761fa03c3\n",
      "2015-12-16 20:18\n",
      "@rvraghav93  [here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/multiclass.py#L193) This line assumes  `Y` to be a sparse matrix. But if target variable `y` has only one class then  in  `Y = self.label_binarizer_.fit_transform(y)` `Y` becomes `numpy.ndarray` which makes `Y.tocsc()` fail. Is this intended?\n",
      "\n",
      "1\n",
      "1883\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-16 20:35\n",
      "there was some code to plot a neural network somewhere. does anyone remember where?\n",
      "\n",
      "1\n",
      "1884\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-17 14:32\n",
      "@amueller  I am sure this isn't helpful but... http://www.texample.net/tikz/examples/neural-network/ is at least pretty\n",
      "\n",
      "1\n",
      "1885\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-17 16:03\n",
      "thanks @lesshaste. I'd prefer python code but I have something similar than that now\n",
      "\n",
      "1\n",
      "1886\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-17 17:07\n",
      "@amueller  I know already this is a dim question but.. I notice scikit-learn has said no to adding deep learning. However we do already have MLP and some people working on improving that code (e.g. adding dropout). Is there a clean line between MLPs and deep learning?\n",
      "I assume that an MLP with 20 layers is not deep learning for example?\n",
      "it seems that the three main types of neural networks are feed-forward, convolutional and recurrent.  If we just look at feed-forward neural networks, I am wondering if there is a clear view of what is in and what is out of scope?\n",
      "no is a perfectly acceptable answer :)\n",
      "\n",
      "4\n",
      "1887\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-17 17:19\n",
      "@lesshaste neural networks are by now basically synonymous with deep learning. convnets are actually feed-forward nets\n",
      "I think the agreed scope is feed forward without convolutions\n",
      "\n",
      "2\n",
      "1888\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-17 17:20\n",
      "@amueller  ok.. I ask as a recent mailing list question was \"I'm interested in deep learning and wanna contribute to scikit-learn and try out for GSoC next summer. I was wondering if scikit-learn is looking to expand its neural nets package.\"\n",
      "\n",
      "7\n",
      "1889\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-17 17:21\n",
      "have you read sanders posts on the galazy zoo and plankton competitions?\n",
      "\n",
      "3\n",
      "1890\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-17 17:22\n",
      "on the topic of gsoc.. it would be great if someone could fix/rewrite the variational Bayes module\n",
      "\n",
      "5\n",
      "1891\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-17 17:26\n",
      "@amueller  I read that article.. it's still not clear to me how much worse you would do with a simpler architecture. But I do think that image tasks seem uniquely well suited to convolutional networks\n",
      "however I notice that neural networks are now being used more for non-image based tasks on kaggle\n",
      "@amueller  I think you told me that VBGMM was basically broken.. but I may have remembered that wrong\n",
      "sounds wonderful\n",
      "\n",
      "4\n",
      "1892\n",
      "54d4a1d6db8155e6700f853b\n",
      "2015-12-17 17:28\n",
      "there is a pull request with a rewrite of the vbgmm\n",
      "but it is not finished\n",
      "and the status is unclear to me\n",
      "I'm a bit preoccupied with writing a book\n",
      "that I want to finish early spring\n",
      "it's a machine learning book for programmers without a lot of math (because there are many good stats / ml books out there already)\n",
      "it aims to be very practical\n",
      "thanks :)\n",
      "(and I'll go back to that now ;)\n",
      "check out the GMM rewrite pull request if you are interested\n",
      "\n",
      "10\n",
      "1893\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-17 17:29\n",
      "oh great! On machine learning?\n",
      "\n",
      "3\n",
      "1894\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-17 17:30\n",
      "I look forward to it!\n",
      "\n",
      "1\n",
      "1895\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-17 17:31\n",
      "although I am a little worried that ML is going to be overtaken by neural network mumbo jumbo :)\n",
      "I will do.. thanks\n",
      "good luck!\n",
      "(p.s. did you consider cloning yourself? :) )\n",
      "(as everyone wants your attention)\n",
      "\n",
      "5\n",
      "1896\n",
      "53135b495e986b0712efc453\n",
      "2015-12-19 17:54\n",
      "I sometimes have a feeling he already has.... ^^\n",
      "\n",
      "1\n",
      "1897\n",
      "53135b495e986b0712efc453\n",
      "2015-12-19 18:00\n",
      "@kaichogami Apologies for the late response! That indeed looks like a bug... Apparently `_fit_binary` seems to handle the case of constant `y`. Could you also fix this and add a test too?\n",
      "\n",
      "1\n",
      "1898\n",
      "5363a92c048862e761fa03c3\n",
      "2015-12-20 12:37\n",
      "@rvraghav93 no problem. I'll do that. Just to be sure, I am supposed to open another pull request for this issue right?\n",
      "\n",
      "1\n",
      "1899\n",
      "53135b495e986b0712efc453\n",
      "2015-12-20 12:39\n",
      "If you have'nt raised one for that already you could just do it in a single pr...\n",
      "Or maybe multiple PRS ur choice :)\n",
      "\n",
      "2\n",
      "1900\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-21 11:06\n",
      "hi @rvraghav93\n",
      "\n",
      "1\n",
      "1901\n",
      "53135b495e986b0712efc453\n",
      "2015-12-21 16:24\n",
      "Hey :)\n",
      "\n",
      "1\n",
      "1902\n",
      "53135b495e986b0712efc453\n",
      "2015-12-21 16:50\n",
      "your suggestion of `rpart` is great! `rpart` seems to do surrogate splits and handles the missing values pretty well.. but I guess its a bit computationally intensive, so I am going with Gilles' suggestion of finding the best split, with all the missing values sent to either side of the split (left or right).... Lets see how it works... This one modifies a loooot of code and I'm struggling with refactoring - procrastination - refactoring - giving up - getting back up and all other cycles in between ;P Hope I can gift a missing value supporting tree for christmas ;)\n",
      "(rant w.r.t #5870)\n",
      "\n",
      "2\n",
      "1903\n",
      "564789be16b6c7089cbab8b7\n",
      "2015-12-21 17:28\n",
      "@rvraghav93  great!\n",
      "I look forward to it :)\n",
      "\n",
      "2\n",
      "1904\n",
      "544906e2db8155e6700cdd16\n",
      "2015-12-22 00:09\n",
      "Hi everyone!  I'm trying to run a grid search on a dataset that is stored in a Pandas DataFrame (something very similar to this example http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#example-hetero-feature-union-py).  When I run my code I get the next error message: \"ValueError: cannot label index with a null key\" I've tried different approaches but I didn't be able to fix it. I have a working example that I can share with you if needed.\n",
      "\n",
      "1\n",
      "1905\n",
      "53135b495e986b0712efc453\n",
      "2015-12-22 00:58\n",
      "@mac2bua Could you paste your code into github gist/ pastebin and post the link here please?\n",
      "\n",
      "1\n",
      "1906\n",
      "544906e2db8155e6700cdd16\n",
      "2015-12-22 03:33\n",
      "Yes, of course I can!\n",
      "https://gist.github.com/mac2bua/94f0f15bc327684d16ba\n",
      "let me know if you need anything else\n",
      "\n",
      "3\n",
      "1907\n",
      "53135b495e986b0712efc453\n",
      "2015-12-26 18:07\n",
      "@aron-bordin Welcome to scikit-learn! Please check if any of [these](https://github.com/scikit-learn/scikit-learn/labels/Need%20Contributor) issues interest you and start working on it. Let me know if I can be of any help!\n",
      "\n",
      "1\n",
      "1908\n",
      "567ed70316b6c7089cc03bc2\n",
      "2015-12-26 18:11\n",
      "@rvraghav93 Thx, I'll check them\n",
      "\n",
      "1\n",
      "1909\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-01-03 16:38\n",
      "Hi , First of all, wish you all a happy new year. I am new to scikit learn. I wanted to ask for help in working on issues which I have the ability to contribute. The issue regarding [meta-estimators](https://github.com/scikit-learn/scikit-learn/issues/5824) seemed nice thing to work on. Please let you know about your thoughts. I would be happy to work any other issue if this is beyond my scope. Thanks :)\n",
      "\n",
      "1\n",
      "1910\n",
      "53135b495e986b0712efc453\n",
      "2016-01-03 16:40\n",
      "Could you ping @hugobowne and ask him if he's still working on it? If not please feel free to take it up.\n",
      "\n",
      "2\n",
      "1911\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-01-03 16:54\n",
      "It would also help if some idea can be given regarding how to proceed. Sorry for my doubts on trivial issues, but I am new to the API. AFAIK, I see that the multi class classifiers are right now implemented to turn a binary classifier into a multiclass classifier. This issue is intended to implement meta estimators to turn binary classifiers to multioutput classifiers. Are multi output classifiers same as the multioutput-multiclass classifiers like dt, rf ? And also let me know if it is better to ask the doubts in here or do it at some other place ?\n",
      "\n",
      "1\n",
      "1912\n",
      "53135b495e986b0712efc453\n",
      "2016-01-03 21:21\n",
      "The outline of what needs to be done here is -   * Make `n_outputs` numbers of single output estimators * Train using `X (n_samples x n_features)`, `y (n_samples x n_outputs)` * Predict using `X(n_samples x n_features)` * For `output_i` in `range(n_outputs)` --> `y_predicted_i = estimator_i`, where `y_predicted_i` is of shape `(n_samples x 1)` * Vertically stack all the `y_predicted_i` -s to get the final `y_predicted` of shape `(n_samples x n_outputs)`\n",
      "and as far are `multioutput.py` is concerned, it should provide a meta estimator that changes single/multiclass single output to single/multiclass multioutput...\n",
      "So essentially if you want binary_single_output_estimator to be made a multiclass multioutput you should be able to do both -  * `OVOClassifier(estimator=MultiOutputEstimator(estimator=binary_single_output_estimator))` * `MultiOutputEstimator(estimator=OVOClassifier(estimator=binary_single_output_estimator))`\n",
      "You could add a test making sure both of them return the same predictions\n",
      "I copied this to the issue so @mblondel or anyone else can correct me if I am wrong.\n",
      "Also, I am unable to edit, we require it to be horizontally stacked, not vertically\n",
      "\n",
      "6\n",
      "1913\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-01-04 05:03\n",
      "Thanks @rvraghav93 for the detailed explanation :-)\n",
      "\n",
      "1\n",
      "1914\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-04 23:48\n",
      "does someone have a face recognition example where using eigenfaces actually works better than raw pixels? I played around with lfw, but it seems to make results worse :-/\n",
      "\n",
      "1\n",
      "1915\n",
      "567d7eca16b6c7089cc02a05\n",
      "2016-01-05 18:17\n",
      "I am new to scikit and also want to contribute to, where should i start!\n",
      "\n",
      "1\n",
      "1916\n",
      "55f3a7830fc9f982beb071a9\n",
      "2016-01-05 18:53\n",
      "http://scikit-learn.org/stable/developers/\n",
      "\n",
      "1\n",
      "1917\n",
      "53135b495e986b0712efc453\n",
      "2016-01-05 20:43\n",
      "Start with easy issues that are tagged \"Need Contributor\"\n",
      "\n",
      "1\n",
      "1918\n",
      "53135b495e986b0712efc453\n",
      "2016-01-06 03:36\n",
      "Apparently we can protect branches from force push... hmmm... (https://help.github.com/articles/about-protected-branches)\n",
      "\n",
      "1\n",
      "1919\n",
      "54e6371215522ed4b3dc3718\n",
      "2016-01-06 12:30\n",
      "@rvraghav93 Can you have a look at my pull request #6114 or someone else? I have made changes to all the cython files which had descripancies.\n",
      "*discrepancies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "1920\n",
      "541d52b1163965c9bc205cf3\n",
      "2016-01-10 14:28\n",
      "@amueller  what exactly is wrong with VBGMM\n",
      "\n",
      "1\n",
      "1921\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2016-01-12 20:53\n",
      "I get a 404 on http://scikit-learn.org/stable/faq/\n",
      "\n",
      "1\n",
      "1922\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-01-12 22:17\n",
      "@rvraghav93  I'll finish reviewing soon\n",
      "I have two talks coming up soon which I need to work on\n",
      "\n",
      "2\n",
      "1923\n",
      "53135b495e986b0712efc453\n",
      "2016-01-12 22:19\n",
      "@jmschrei Please take your time! Thanks heaps for your reviews!!\n",
      "\n",
      "1\n",
      "1924\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-01-12 22:20\n",
      "I'm going to take a stab at parallelizing trees after yours is merged.\n",
      "I've never liked the criterion objects so I'm considering getting rid of them.\n",
      "But I think Gilles might not approve. <_<\n",
      "\n",
      "6\n",
      "1925\n",
      "53135b495e986b0712efc453\n",
      "2016-01-12 22:23\n",
      "Really? I found it organized ;) Anyway do let me know if you feel I can be of any help. Alex has asked me to work on the trees for the next few months!!\n",
      "\n",
      "3\n",
      "1926\n",
      "5571fe1015522ed4b3e17d90\n",
      "2016-01-13 08:16\n",
      "@michaelaye Looks like the / at the end is the problem. http://scikit-learn.org/stable/faq.html works. I'll take a look at fixing the link on the main page.\n",
      "@michaelaye the FAQ link at the bottom of the page is now fixed on the dev doc: http://scikit-learn.org/dev/\n",
      "\n",
      "2\n",
      "1927\n",
      "5395efa3a9176b500d1cd7fb\n",
      "2016-01-13 21:09\n",
      "Ok, but the link on /stable is still broken, FYI.\n",
      "but good to know that it actually works without the /\n",
      "\n",
      "2\n",
      "1928\n",
      "5571fe1015522ed4b3e17d90\n",
      "2016-01-14 07:38\n",
      "> Ok, but the link on /stable is still broken, FYI.\n",
      "Yep this will be fixed on the next stable release\n",
      "\n",
      "2\n",
      "1929\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-14 16:37\n",
      "@halwai do you mean the old or the new?\n",
      "\n",
      "1\n",
      "1930\n",
      "53135b495e986b0712efc453\n",
      "2016-01-14 18:43\n",
      "\n",
      "1\n",
      "1931\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-15 20:44\n",
      "@MechCoder https://github.com/scikit-learn/scikit-learn/issues/4497\n",
      "\n",
      "1\n",
      "1932\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-01-15 21:54\n",
      "Is there an easy way to see what the new features etc might be for the upcoming 0.18 by searching https://github.com/scikit-learn/scikit-learn/pulls ?\n",
      "clearly this would be subject to change but I haven't found an appropriate tag to search for yet\n",
      "\n",
      "2\n",
      "1933\n",
      "53810862048862e761fa2887\n",
      "2016-01-15 22:43\n",
      "@amueller I have started working on https://github.com/vighneshbirodkar/sklearn-stub\n",
      "\n",
      "1\n",
      "1934\n",
      "53810862048862e761fa2887\n",
      "2016-01-19 05:00\n",
      "@amueller I have deployed Travis, and Coveralls and CircleCI is now building the doumentation\n",
      "http://vighneshbirodkar.github.io/sklearn-stub/docs/\n",
      "\n",
      "2\n",
      "1935\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-19 16:20\n",
      "@vighneshbirodkar sweet!\n",
      "@vighneshbirodkar that is actually totally aweseom\n",
      "can you add a \"user guide\" like page to the documentation that explains what exactly a user has to do to make this work for them?\n",
      "@vighneshbirodkar this here can maybe help, too: https://github.com/uwescience/shablona\n",
      "\n",
      "4\n",
      "1936\n",
      "541d52b1163965c9bc205cf3\n",
      "2016-01-19 16:49\n",
      "@amueller   old one\n",
      "\n",
      "1\n",
      "1937\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-19 16:55\n",
      "@halwai I think the algorithm is wrong. The update doesn't conform to the literature, and it doesn't seem to work very well in many settings.\n",
      "\n",
      "3\n",
      "1938\n",
      "541d52b1163965c9bc205cf3\n",
      "2016-01-19 17:04\n",
      "@amueller  which pull request are you talking about can u share the link\n",
      "\n",
      "1\n",
      "1939\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-19 17:11\n",
      "#4802\n",
      "\n",
      "1\n",
      "1940\n",
      "53810862048862e761fa2887\n",
      "2016-01-19 17:37\n",
      "@amueller I'm on it\n",
      "\n",
      "2\n",
      "1941\n",
      "53810862048862e761fa2887\n",
      "2016-01-19 17:53\n",
      "@amueller I noticed one thing. Currently the CircleCI script installs python packages via both apt-get and pip. We could simplify it to only use pip. The cache in CircleCI caches pip packages and their subsequent installation will happen in no time.\n",
      "\n",
      "1\n",
      "1942\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-19 18:05\n",
      "@vighneshbirodkar but installing numpy and scipy by pip is discouraged and will take forever and apt-get is also cached.\n",
      "\n",
      "1\n",
      "1943\n",
      "53810862048862e761fa2887\n",
      "2016-01-19 18:08\n",
      "@amueller I ran into some PYTHONPATH issues on CircleCI. Is it ok if I install numpy and scipy through pip for the stub package?\n",
      "It does need compliing. But only for the first time. Doing it this way let's us keep the configuration to a minimum. Do you think I should switch to apt-get ?\n",
      "\n",
      "2\n",
      "1944\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-19 18:13\n",
      "that requires compiling, right? that will take very long. And the people that copy the stub will have trouble.\n",
      "one option would be to just use conda\n",
      "and not test a non-conda environment\n",
      "\n",
      "3\n",
      "1945\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-19 18:28\n",
      "how do you mean for the first time? the first time in master? or the first time for any pull request?\n",
      "\n",
      "1\n",
      "1946\n",
      "53810862048862e761fa2887\n",
      "2016-01-19 18:35\n",
      "CircleCI is only built over master to deploy the documentation. So the first time on master. PRs will be built by travis which uses apt-get\n",
      "\n",
      "1\n",
      "1947\n",
      "53810862048862e761fa2887\n",
      "2016-01-19 19:06\n",
      "I'm sorry. Travis uses conda right now.\n",
      "\n",
      "1\n",
      "1948\n",
      "53135b495e986b0712efc453\n",
      "2016-01-19 19:11\n",
      "@amueller could you take a look at #5568?\n",
      "\n",
      "1\n",
      "1949\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-19 19:11\n",
      "@rvraghav93 It's on my list of the more urgent things ;)\n",
      "\n",
      "2\n",
      "1950\n",
      "569f27d8e610378809bd3960\n",
      "2016-01-20 06:49\n",
      "hello\n",
      "<unconvertable>\n",
      "\n",
      "2\n",
      "1951\n",
      "5496a05adb8155e6700e1a4b\n",
      "2016-01-21 05:05\n",
      "Does sklearn kmeans uses Linde Buzo Gray algorithm for codebook generation?\n",
      "\n",
      "1\n",
      "1952\n",
      "53135b495e986b0712efc453\n",
      "2016-01-21 13:22\n",
      "have a look at this - http://docs.scipy.org/doc/scipy-0.14.0/reference/cluster.vq.html\n",
      "\n",
      "1\n",
      "1953\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-21 18:25\n",
      "@rvraghav93 how does that answer the question? @rajathkumarmp no it's just using lloyd's algorithm\n",
      "\n",
      "1\n",
      "1954\n",
      "53135b495e986b0712efc453\n",
      "2016-01-21 20:03\n",
      "Sorry I assumed he wanted vector quantisation\n",
      "\n",
      "1\n",
      "1955\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-21 22:59\n",
      "well yeah he asked which algorithm is implemented in kmeans. scipy's vq implements the same algorithm as sklearn does\n",
      "\n",
      "1\n",
      "1956\n",
      "53135b495e986b0712efc453\n",
      "2016-01-21 23:05\n",
      "Okay :grin: (I stupidly assumed since it was a vq module, maybe it does it using lbg as lbg seems to be the preferred alg for vq)\n",
      "\n",
      "1\n",
      "1957\n",
      "560d8599d33f749381a7fa7c\n",
      "2016-01-22 02:19\n",
      "\n",
      "1\n",
      "1958\n",
      "5496a05adb8155e6700e1a4b\n",
      "2016-01-22 04:17\n",
      "@amueller thank you for clarifying.\n",
      "\n",
      "1\n",
      "1959\n",
      "5496a05adb8155e6700e1a4b\n",
      "2016-01-22 04:19\n",
      "\"The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features\". This is mentioned in the docs. What is \"k\" here ? Also, is it O(KnT), Multiplication of all 3 parameters?\n",
      "\n",
      "1\n",
      "1960\n",
      "561d08d0d33f749381a937bf\n",
      "2016-01-22 04:40\n",
      "Hi guys, im going to bombard this forum with a lot of newbee questions. Feel free to kick me out anytime. Ive got several IDs just in case.\n",
      "\n",
      "1\n",
      "1961\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-22 16:19\n",
      "@Fredilly lol. Maybe try stackoverflow with the sklearn tag\n",
      "that will get you replies more quickly and more willingly ;)\n",
      "@rajathkumarmp k is the number of clusters, knt is the multiplication\n",
      "\n",
      "3\n",
      "1962\n",
      "53135b495e986b0712efc453\n",
      "2016-01-24 11:38\n",
      "@jmschrei (This is probably a stupid idea), can we compute the accumulated sample_weights for all the samples and store it. (a double array of `n_samples` size)  Later we can compute the weighted n_samples for right or left by subtracting the last index's accumulated value from the first? (Would that speed up the impurity computation and also take us a step closer to having a state-less criterion?)\n",
      "(There is also the class count that is being stored as a state, that also could be accumulated and stored but not sure if it would take up a lot of space.)\n",
      "(Also we need not compute this accumulated sample_weights for all the samples, we could expand this 'cache' when we encounter new samples to avoid computing for samples well away any decision boundary?)\n",
      "\n",
      "3\n",
      "1963\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-25 22:33\n",
      "@ogrisel if you need reviews for 0.17.1 let me know. I'm a bit out of the loop right now\n",
      "\n",
      "1\n",
      "1964\n",
      "5537027215522ed4b3df56ab\n",
      "2016-01-26 04:10\n",
      "Hey guys, anyone has an idea why logistic regression doesn't give realistic probability estimates?\n",
      "with the number of features is large?\n",
      "random forest on the other hand gives more realistic estimates\n",
      "\n",
      "3\n",
      "1965\n",
      "5537027215522ed4b3df56ab\n",
      "2016-01-26 04:16\n",
      "it is basically overfitting on the things that seem to happen only a few times in the dataset, because they are \"good predictors\" mostly by chance. More aggressive L1 regularization always results in poorer performance\n",
      "\n",
      "1\n",
      "1966\n",
      "5537027215522ed4b3df56ab\n",
      "2016-01-26 09:25\n",
      "perhaps if we had bayesian logistic regression\n",
      "\n",
      "1\n",
      "1967\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-26 18:15\n",
      "@lqdc have you tried the calibration module?\n",
      "\n",
      "1\n",
      "1968\n",
      "53810862048862e761fa2887\n",
      "2016-01-28 19:16\n",
      "How does scikit-learn currently upload packages to PyPI ?\n",
      "\n",
      "1\n",
      "1969\n",
      "53135b495e986b0712efc453\n",
      "2016-01-29 16:04\n",
      "I recall @ogrisel or @amueller using twine for that I think. (Ref: https://gitter.im/scikit-learn/scikit-learn?at=552d71150e3138bb6be81ef4)\n",
      "\n",
      "1\n",
      "1970\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-29 16:12\n",
      "I've been using setuptools but twine is also an option\n",
      "\n",
      "1\n",
      "1971\n",
      "53810862048862e761fa2887\n",
      "2016-01-29 16:13\n",
      "I added instructions for setuptools in the stub project\n",
      "\n",
      "1\n",
      "1972\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-01-29 18:28\n",
      "hm anyone wanna review #5270?\n",
      "I think we should really move forward with OneHotEncoder\n",
      "\n",
      "2\n",
      "1973\n",
      "53135b495e986b0712efc453\n",
      "2016-01-29 19:16\n",
      "Is it possible to cythonize and test a single module alone without building scikit-learn fully?\n",
      "\n",
      "1\n",
      "1974\n",
      "53810862048862e761fa2887\n",
      "2016-01-29 19:37\n",
      "If you compile scikit-learn it only compiles changed files.\n",
      "\n",
      "1\n",
      "1975\n",
      "5537027215522ed4b3df56ab\n",
      "2016-01-30 02:01\n",
      "@amueller thanks for the tip, but calibrating doesn't help (http://i.imgur.com/k8pF5p7.png)\n",
      "perhaps, it's an issue with sparse datasets. I wonder what the general solution to this kind of thing is.. adding noise?\n",
      "I was thinking of adding support for bayesian logistic regression since it's commonly used in the R world, but it  doesn't seem to scale to large number of features, because of MCMC + then we need pymc as a dependency.\n",
      "\n",
      "3\n",
      "1976\n",
      "5624bbb016b6c7089cb77b2a\n",
      "2016-01-31 11:26\n",
      "Hi, lately I've been trying to think for a solution to project structure and organization, I've asked at [SO](http://stackoverflow.com/questions/35067412/python-machine-learning-data-science-project-structure) and [Reddit](https://www.reddit.com/r/Python/comments/43ima5/project_template_for_data_scienceanalysis/), since I'm using sklearn and creating new interfaces to the classes, I think this chat would be better. So... how do you guys organize the entire project folder? Also, do you use Pipelines, if so, wqhere do you place all the different transformer?\n",
      "\n",
      "1\n",
      "1977\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-01 15:28\n",
      "@davidgasquez this channel is more for scikit-learn project development. SO is usually good.\n",
      "@davidgasquez I don't think there is any best practice for analysis code. I usually have a module that lives somewhere with the code. but you don't usually want your data under version control...\n",
      "\n",
      "2\n",
      "1978\n",
      "5624bbb016b6c7089cb77b2a\n",
      "2016-02-01 16:36\n",
      "@amueller Sorry for the question in the wrong place. Thanks for the reply!\n",
      "\n",
      "1\n",
      "1979\n",
      "53135b495e986b0712efc453\n",
      "2016-02-02 16:48\n",
      "@vighneshbirodkar Ah yes. Thanks for the response :) When cythonize.dat is rewritten (for some reason, say build clean etc) and I have a build error at say the 5th cython file the cythonize.dat is not updated with the hash records of first 4. This makes it to cythonize the first 4 again and again until 5th (and all subsequent) cython file compiles without error...\n",
      "Also could anyone review https://github.com/scikit-learn/scikit-learn/pull/6254 please?\n",
      "@ogrisel ?\n",
      "@amueller We should add this :P - https://github.com/domgetter/NCoC\n",
      "\n",
      "4\n",
      "1980\n",
      "5572bf2d15522ed4b3e182a1\n",
      "2016-02-02 18:15\n",
      "Can I use a plain `linear_model.RANSACRegressor()` in `cross_validation.cross_val_score`?  Currently I am getting a `ValueError: No inliers found, possible cause is setting residual_threshold (None) too low.`\n",
      "\n",
      "1\n",
      "1981\n",
      "5572bf2d15522ed4b3e182a1\n",
      "2016-02-02 18:30\n",
      "Nevermind, seems to be a problem with my data\n",
      "\n",
      "1\n",
      "1982\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-03 17:58\n",
      "@rvraghav93 we already have no code of conduct :P\n",
      "\n",
      "1\n",
      "1983\n",
      "53135b495e986b0712efc453\n",
      "2016-02-03 18:47\n",
      "Haha\n",
      "\n",
      "1\n",
      "1984\n",
      "53810862048862e761fa2887\n",
      "2016-02-03 20:03\n",
      "@amueller Can you take another look at #5270 ?\n",
      "\n",
      "1\n",
      "1985\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-03 20:18\n",
      "@vighneshbirodkar probably not today, but should be possible tomorrow\n",
      "\n",
      "1\n",
      "1986\n",
      "53810862048862e761fa2887\n",
      "2016-02-03 20:42\n",
      "Ok, thanks\n",
      "\n",
      "1\n",
      "1987\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-04 15:53\n",
      "https://github.com/scikit-learn/scikit-learn/pull/4899 is this just waiting review?\n",
      "and this lovely PR seems to have stalled https://github.com/scikit-learn/scikit-learn/issues/5736\n",
      "\n",
      "2\n",
      "1988\n",
      "53135b495e986b0712efc453\n",
      "2016-02-04 16:51\n",
      "#4899 is waiting for more tests + reviews too.\n",
      "\n",
      "1\n",
      "1989\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-04 22:24\n",
      "I'm super behind, sorry\n",
      "\n",
      "1\n",
      "1990\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-05 14:17\n",
      "I don't think anyone can say sorry! You are doing an amazing job.\n",
      "\n",
      "1\n",
      "1991\n",
      "56a34c16e610378809bdc988\n",
      "2016-02-06 04:12\n",
      "Hi fellows, please read the abstract of this page and have a look at it quickly :\n",
      "http://arxiv.org/pdf/1211.1513.pdf\n",
      "Can this be a good addition to the lib ?\n",
      "And I can see 401 Open PRs. They are not being merged quickly. Do they need some reviewing ? I can assist in best possible ways.\n",
      "\n",
      "4\n",
      "1992\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-06 12:22\n",
      "How can you tell that the input and response for the the iris data set are .data and .target?\n",
      "For digits, its something else.  Where does that information come from?\n",
      "\n",
      "2\n",
      "1993\n",
      "5525b91815522ed4b3deb7d6\n",
      "2016-02-06 12:26\n",
      "@Fredilly Take a look [here](http://scikit-learn.org/stable/datasets/index.html#datasets)\n",
      "\n",
      "1\n",
      "1994\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-06 12:50\n",
      "@leosartaj <unconvertable>\n",
      "\n",
      "1\n",
      "1995\n",
      "56a34c16e610378809bdc988\n",
      "2016-02-07 16:39\n",
      "Veteran contributors, please have a look at the link above ^^^\n",
      "If you think this can be a good addition to the lib, I will try to implement it. I have read it quickly once, if I get a yes, I will thoroughly go through it and start implementing it as a new class.\n",
      "it is named the \"K-Plane Regression\"\n",
      "Oh,ok. I haven't see the specifications of the paper, will check it whether it meets the requirements. If not, I will upload it separate l\n",
      "It can get it reviewed and added as related projects.\n",
      "\n",
      "5\n",
      "1996\n",
      "5363a92c048862e761fa03c3\n",
      "2016-02-07 17:45\n",
      "@karandesai-96 Have a look at [FAQs](http://scikit-learn.org/stable/faq.html#can-i-add-this-new-algorithm-that-i-or-someone-else-just-published)\n",
      "\n",
      "1\n",
      "1997\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-08 08:43\n",
      "K-plane regression appears to have 2 citations which makes it too early for scikit learn if I understand correctly (IIUC?)\n",
      "\n",
      "1\n",
      "1998\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-02-08 14:11\n",
      "Not just 2 citations. 2 citations since 2013...\n",
      "\n",
      "1\n",
      "1999\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-02-08 14:23\n",
      "Just curious, @karandesai-96, why are you interested in this method?\n",
      "\n",
      "1\n",
      "2000\n",
      "56a34c16e610378809bdc988\n",
      "2016-02-08 14:39\n",
      "It has better results than mine, on a public data set I used once.\n",
      "Yeah, it seems too early for sklearn\n",
      "^^ This was the only reason why I was curious to know how it yielded better results... xD\n",
      "\n",
      "3\n",
      "2001\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-08 17:19\n",
      "@karandesai-96  what works well is if you make a separate scikit learn compatible implementation and it can be added to http://scikit-learn.org/stable/related_projects.html potentially\n",
      "\n",
      "1\n",
      "2002\n",
      "56a34c16e610378809bdc988\n",
      "2016-02-09 14:08\n",
      "Yes, even I am thinking about it. Also, I will try to implement ths on certain public datasets I know. I'll see if it gives consistent results.\n",
      "It will consume sometime though.\n",
      "\n",
      "2\n",
      "2003\n",
      "55a7b5b08a7b72f55c3f96ef\n",
      "2016-02-09 14:29\n",
      "Hey all!\n",
      "\n",
      "1\n",
      "2004\n",
      "55a7b5b08a7b72f55c3f96ef\n",
      "2016-02-09 14:29\n",
      "Just started thinking about building up Machine Learning for a problem I'm facing and hopefully SciKit-Learn would help solve it\n",
      "Is this the best place to ask for it?\n",
      "\n",
      "2\n",
      "2005\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-09 18:39\n",
      "does anyone know any good software for clustering big graphs? It seems scikit learn doesn't have stochastic block model support yet sadly\n",
      "\n",
      "1\n",
      "2006\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-09 20:22\n",
      "what's happening with appveyor?\n",
      "@ogrisel?\n",
      "https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.5177/job/tjveqfpn8bcdyrks\n",
      "\n",
      "3\n",
      "2007\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-09 21:06\n",
      "@amueller  You mentioned a time series CV object in the mailing list. I would love to see that too\n",
      "\n",
      "4\n",
      "2008\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-09 21:09\n",
      "to be honest I am not even 100% sure of the best way to do CV on time series data. If you sample randomly from the series you are likely ruin your feature vectors. What is the right approach?\n",
      "oh actually I think I see\n",
      "\n",
      "2\n",
      "2009\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-09 21:38\n",
      "what do you mean by using a time index?\n",
      "\n",
      "1\n",
      "2010\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-09 21:43\n",
      "lets say each datapoint is a day. and you want to use 5 fold on 100 days\n",
      "then training set 1 is days 0-20, test set 1 is 20-40\n",
      "training set 2 is 0-40, test set 2 is 40-60 etc\n",
      "but say for each day you have some arbitrary number of datapoints. Then you need to know which day a datapoint belongs to.\n",
      "actually, it's not that hard, it just needs a \"label\" attribute...\n",
      "\n",
      "5\n",
      "2011\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-09 21:50\n",
      "see http://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection for example\n",
      "\n",
      "1\n",
      "2012\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-09 21:51\n",
      "thanks\n",
      "so in short.. it would be great :)\n",
      "\n",
      "2\n",
      "2013\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-09 21:52\n",
      "I am not sure what you call it but there is also event data which is slightly different from time series data. That is you have a sequence of times when events happen as opposed to labels at every second, say.\n",
      "is there any support for that sort of data?\n",
      "\n",
      "5\n",
      "2014\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-09 21:53\n",
      "with \"having an arbitrary number of datapoint for every day\"\n",
      "\n",
      "3\n",
      "2015\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-09 21:55\n",
      "people who do machine learning on neuronal firing data are particularly interested in this\n",
      "\n",
      "1\n",
      "2016\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-09 22:00\n",
      "the feature vector is a different problem ;)\n",
      "I think most people that do time series analysis are not necessarily in the sciences ;)\n",
      "\n",
      "4\n",
      "2017\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-09 22:05\n",
      "on a different note, when I pip install scikit-learn, are all the C and Fortran dependencies compiled from source or are any binaries used?\n",
      "\n",
      "1\n",
      "2018\n",
      "55a7b5b08a7b72f55c3f96ef\n",
      "2016-02-10 04:31\n",
      "I don't know much about Python but it seems that it works close to how .bat executables work. Can scikit-learn be used to create new files based on input?\n",
      "E.g. input customer profile and it generates a recommended real estate proposal\n",
      "\n",
      "2\n",
      "2019\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-10 09:09\n",
      "@Qoyyuum  yes but this is the wrong place to ask basic python questions\n",
      "\n",
      "1\n",
      "2020\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-02-10 14:06\n",
      "Hi everyone !!!  @amueller I was watching the pull request #4802. It seems blocked for several months now. Do you know what's the problem ?  I've seen that a lot of new codes was submitted at the same time. I image it's a real problem for the reviewers. Maybe it could be easier for them to divide the work of the GSoC in several pull request. Do you think it's a good solution ? I can work on it if you want.\n",
      "\n",
      "1\n",
      "2021\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-10 22:13\n",
      "@tguillemot @ogrisel was mostly working on this. There was some bug there. I'm not sure about the exact status at the moment\n",
      "\n",
      "1\n",
      "2022\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-10 22:13\n",
      "has anyone seen this error on travis before? https://travis-ci.org/scikit-learn/scikit-learn/jobs/108346382 @ogrisel ?\n",
      "\n",
      "1\n",
      "2023\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-02-11 08:40\n",
      "Ok. @ogrisel I'm the new engineer of Telecom and I'm working full time on scikit-learn, so tell me what I can do to help.\n",
      "\n",
      "1\n",
      "2024\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-11 12:38\n",
      "Hi all @tguillemot , I replied to your email. Won't have time to follow up on that before next week though.\n",
      "@amueller merge #6260 ?\n",
      "\n",
      "2\n",
      "2025\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-02-11 12:42\n",
      "@ogrisel No problem. Thx\n",
      "\n",
      "1\n",
      "2026\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-11 15:38\n",
      "@amueller  Do you think it is worth my opening an issue about CV and time series data?\n",
      "\n",
      "1\n",
      "2027\n",
      "56b80528e610378809c05a48\n",
      "2016-02-11 15:40\n",
      "Hello @lesshaste , here is one https://github.com/scikit-learn/scikit-learn/issues/6322\n",
      "\n",
      "1\n",
      "2028\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-11 15:54\n",
      "@ogrisel merged #6260\n",
      "@ogrisel I can work on #6332 and the bug fixes in ~2 hours, not earlier\n",
      "\n",
      "2\n",
      "2029\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-11 16:09\n",
      "@yenchenlin1994  thanks! How do I upvote it :)\n",
      "@yenchenlin1994  there were two interesting cases I think. One where you have some value for each time tick and one where you can have different numbers of events per day\n",
      "I have joined it! Can you upvote there?\n",
      "@yenchenlin1994  I see both cases are already listed by amueller\n",
      "\n",
      "10\n",
      "2030\n",
      "56b80528e610378809c05a48\n",
      "2016-02-11 16:10\n",
      "@lesshaste  Join Github!\n",
      "\n",
      "1\n",
      "2031\n",
      "56b80528e610378809c05a48\n",
      "2016-02-11 16:14\n",
      "Oh about upvote ... I mean join Github as a software engineer and add this function :)\n",
      "\n",
      "2\n",
      "2032\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-11 19:04\n",
      "loool @yenchenlin1994\n",
      "(and I approve ;)\n",
      "\n",
      "2\n",
      "2033\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-11 19:15\n",
      "although it's hard to visualise, is it true that the clustering algorithms perform roughly as the examples in https://github.com/scikit-learn/scikit-learn/pull/6305 when you increase the dimension?\n",
      "Maybe there is some numerical score that could be given on higher dimension data sets?\n",
      "\n",
      "2\n",
      "2034\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-12 00:11\n",
      "@lesshaste there is a PR for stability scores, but evaluating clustering is really hard\n",
      "@lesshaste yes\n",
      "\n",
      "2\n",
      "2035\n",
      "56b80528e610378809c05a48\n",
      "2016-02-12 02:13\n",
      "Hello @amueller ,\n",
      "\n",
      "1\n",
      "2036\n",
      "56b80528e610378809c05a48\n",
      "2016-02-12 02:15\n",
      "Would you please answer the question I asked in this issue lately? https://github.com/scikit-learn/scikit-learn/issues/6322  Oh and homogeneous and heterogeneous should be separated into two classes, right?\n",
      "\n",
      "1\n",
      "2037\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-12 06:17\n",
      "@amueller  thanks. I hope that PR works out. I seem to remember it had made quite a lot of progress\n",
      "\n",
      "1\n",
      "2038\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-12 07:30\n",
      "@amueller  https://github.com/scikit-learn/scikit-learn/pull/4301 ?\n",
      "\n",
      "1\n",
      "2039\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-12 16:23\n",
      "@ogrisel are you around?\n",
      "\n",
      "1\n",
      "2040\n",
      "53135b495e986b0712efc453\n",
      "2016-02-12 17:42\n",
      "Does anyone know if it is a better idea to form the `neighbors/binary_tree.pxi` into a proper `pyx` + `pxd` file rather than importing the pxi file at `kd_tree.pyx` and `ball_tree.pyx`. From [this](https://github.com/cython/cython/wiki/FAQ#what-is-the-difference-between-a-pxd-and-pxi-file-when-should-either-be-used) (Thanks  to @tomdlt) article, I understand that pxi file gets included at both the places. Would it be better to import than to include or am I missing something?\n",
      "\n",
      "1\n",
      "2041\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-12 17:48\n",
      "ok I set up the travis correctly and reproduced locally, so I'll spend the rest of the day fixing numpy dev compatibility\n",
      "\n",
      "8\n",
      "2042\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-12 20:04\n",
      "Anyone who knows the sequential dataset well? I'm stumped on #6334\n",
      "ok, all \"fixed\"\n",
      "\n",
      "2\n",
      "2043\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-15 11:52\n",
      "@yenchenlin1994  hi.. In https://github.com/scikit-learn/scikit-learn/pull/6351 which of the variants listed in the R example are you doing for the homogeneous case?\n",
      "or all of them?\n",
      "\n",
      "2\n",
      "2044\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-15 13:49\n",
      "Anyone familiar with the amazon employee access challenge on kaggle? http://bit.ly/1oj7FtX   It provides a training set and testing set. The training set contains target response but the testing set doesnt. Do I still perform train_test_split on training set?\n",
      "\n",
      "1\n",
      "2045\n",
      "56b80528e610378809c05a48\n",
      "2016-02-15 16:53\n",
      "Hello @lesshaste   Yes, Ive only committed the homogeneous part by now. Actually, Im a little confused about the heterogeneous case.\n",
      "\n",
      "1\n",
      "2046\n",
      "56b80528e610378809c05a48\n",
      "2016-02-15 16:59\n",
      "If I got 1 sample for 1st day, 2 samples for 2nd day and 3 samples for 3rd day, then I decide to do 2 folds heterogeneous cv, what will happen?\n",
      "ping @amueller @lesshaste\n",
      "Do all samples we collect in the same day should be in the same fold?\n",
      "\n",
      "3\n",
      "2047\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-15 17:40\n",
      "sorry, I'm pretty busy right now\n",
      "\n",
      "2\n",
      "2048\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-15 17:50\n",
      "@yenchenlin1994  even in the homogeneous part there are various options for how to do the cross-validation\n",
      "\n",
      "4\n",
      "2049\n",
      "56b80528e610378809c05a48\n",
      "2016-02-16 08:02\n",
      "@lesshaste Hello,\n",
      "Can you annser my question above?\n",
      "If I got 1 sample for 1st day, 2 samples for 2nd day and 3 samples for 3rd day, then I decide to do 2 folds heterogeneous cv, what will happen?\n",
      "\n",
      "3\n",
      "2050\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-16 08:53\n",
      "@yenchenlin1994  I am not sure exactly how you are doing the heterogeneous case I have to admit. Why are you splitting by days?\n",
      "maybe @amueller has a better idea what is going on in this case\n",
      "does R have anything for the heterogeneous case?\n",
      "\n",
      "3\n",
      "2051\n",
      "53135b495e986b0712efc453\n",
      "2016-02-16 09:26\n",
      "@lesshaste Do you have any good datasets in mind with missing values for benching?\n",
      "\n",
      "1\n",
      "2052\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-16 09:29\n",
      "@rvraghav93  is this for time series data or something else?\n",
      "\n",
      "13\n",
      "2053\n",
      "53135b495e986b0712efc453\n",
      "2016-02-16 09:30\n",
      "MAR is one where the missingness is dependent on either the missing values or the observed values (`X`). MCAR is where the missingness is totally random... and NMAR is where the missingness is correlated with the target...\n",
      "for our case we can assume MAR and MCAR are similar for they both will perform better with imputation...\n",
      "or Not Missing At Random ;)\n",
      "hehe both mean the same... some papers use MNAR and some NMAR I think\n",
      "Okay thanks!!\n",
      "\n",
      "5\n",
      "2054\n",
      "53135b495e986b0712efc453\n",
      "2016-02-16 09:36\n",
      "I don't know this implementation seems intuitive and is supported by Ding and Simonoff's paper but apparently none of the R packages use this... A lot use multiple techniques of imputation and rpart alone uses a surrogate split method...\n",
      "\n",
      "5\n",
      "2055\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-16 09:38\n",
      "is this the paper you are using http://people.stern.nyu.edu/jsimonof/jmlr10.pdf ?\n",
      "\n",
      "2\n",
      "2056\n",
      "53135b495e986b0712efc453\n",
      "2016-02-16 09:41\n",
      "@jmschrei Your thoughts on #5974 ?\n",
      "\n",
      "1\n",
      "2057\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-16 09:41\n",
      "@rvraghav93  too much to read! http://link.springer.com/article/10.1007/s10115-011-0424-2 :)\n",
      "@rvraghav93  I think what you are doing is awesome :)\n",
      "and I really love the way things are done by the devs at scikit learn\n",
      "\n",
      "6\n",
      "2058\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-16 09:42\n",
      "what I particularly love is the way that methods are rejected if they can't be shown to actually work on publicly available data!\n",
      "if only everything was so evidence based\n",
      "and also the aim to automate everything :)  I am excited by the PR to automate the choice of the number of clusters for example when clustering\n",
      "@rvraghav93  can you access that paper I linked to?\n",
      "@rvraghav93  do you think of any those \"missing values\" data sets could be useful?\n",
      "\n",
      "5\n",
      "2059\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-16 10:08\n",
      "@rvraghav93  the paper I linked to also has links public data sets it uses to test its missing value imputation .  They are all fro http://archive.ics.uci.edu/ml/ I think\n",
      "@rvraghav93  there is even a \"missing values?\" field I see :) E.g. https://archive.ics.uci.edu/ml/datasets/Horse+Colic\n",
      "@rvraghav93 sounds good.\n",
      "\n",
      "3\n",
      "2060\n",
      "53135b495e986b0712efc453\n",
      "2016-02-16 16:36\n",
      "Oops @lesshaste Thanks for correcting me... Its MNAR not NMAR :( I've been using it wrongly all along\n",
      "\n",
      "1\n",
      "2061\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-16 20:45\n",
      "I just asked this on the hyper-parameter optimization PR but maybe it was better for here. I don't know how relevant this is but have generic optimization methods such as basin hopping been considered and rejected for hyper-parameter optimization?\n",
      "\n",
      "1\n",
      "2062\n",
      "53135b495e986b0712efc453\n",
      "2016-02-16 21:50\n",
      "Yea thanks the adult dataset is reasonably big and has missing values... I'm gonna try that out by encoding all the categorical values as we don't have categorical value support yet...\n",
      "\n",
      "1\n",
      "2063\n",
      "5624bbb016b6c7089cb77b2a\n",
      "2016-02-16 22:42\n",
      "Any answer to [this](https://github.com/scikit-learn/scikit-learn/pull/2805)?\n",
      "\n",
      "1\n",
      "2064\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-17 09:42\n",
      "@rvraghav93  actually it might be worth saying what I do in practice. I run dictvectorizer with categorical variables and then missing values just become another category. It would be great to compare to that approach as it works quite well, at least when I use a decision tree based classifier/regressor\n",
      "maybe it wouldn't work so well with other classifiers but that would also be good to know\n",
      "\n",
      "2\n",
      "2065\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-17 14:24\n",
      "Is there any point to train_test_split when I could be simply using GridSearchCV?\n",
      "\n",
      "1\n",
      "2066\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-17 16:10\n",
      "@Fredilly  speed?\n",
      "\n",
      "1\n",
      "2067\n",
      "56c4c780e610378809c1f19a\n",
      "2016-02-17 19:20\n",
      "Hello, please review my pull request: https://github.com/scikit-learn/scikit-learn/pull/5900\n",
      "\n",
      "1\n",
      "2068\n",
      "5363a92c048862e761fa03c3\n",
      "2016-02-18 06:31\n",
      "I am trying to update my local scikit-learn folder by using `git pull upstream master` but then I am getting this: ``` (devscikit)kaichogami@kaichogami:~/codes/development_scikit-learn/scikit-learn$ git pull upstream master From https://github.com/scikit-learn/scikit-learn  * branch            master     -> FETCH_HEAD Updating 1aa0ec2..1b27536 error: The following untracked working tree files would be overwritten by merge: \tcontinuous_integration/circle/build_doc.sh \tcontinuous_integration/circle/check_build_doc.py \tcontinuous_integration/circle/push_doc.sh \tdoc/tutorial/statistical_inference/unsupervised_learning_fixture.py \texamples/cluster/plot_face_compress.py \texamples/cluster/plot_face_segmentation.py \texamples/cluster/plot_face_ward_segmentation.py \texamples/mixture/plot_gmm_covariances.py Please move or remove them before you can merge. Aborting ``` I messed up some merge conflicts although that I was not in `master` branch.  What would be the best approach to resolve this?\n",
      "\n",
      "1\n",
      "2069\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-18 06:45\n",
      "hmm\n",
      "\n",
      "1\n",
      "2070\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-18 06:45\n",
      "im presuming you want to preserve the untracked working tree files?\n",
      "\n",
      "1\n",
      "2071\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-18 14:03\n",
      "Assuming you don't care about any local changes to your master branch or uncommit changes:  ```bash git checkout master git reset --hard upstream/master ```\n",
      "BTW, you should never commit anything to your local master. Always use branches.\n",
      "I thought your French lessons would only start next month ;)\n",
      "\n",
      "3\n",
      "2072\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-18 15:08\n",
      "@amueller I am building the wheels for osx and windows for 0.17.1, how did you sync with conda people?\n",
      "\n",
      "1\n",
      "2073\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-18 15:27\n",
      "@ogrisel I emailed them. give me a minute\n",
      "\n",
      "1\n",
      "2074\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-18 15:28\n",
      "The 0.17.1 tag is already public\n",
      "I am ready to upload :)\n",
      "\n",
      "7\n",
      "2075\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-18 15:29\n",
      "I cc'ed peter wang, which might have helped the process lol\n",
      "thanks for working on the release again, and sorry I'm not more help\n",
      "I'm doing a company visit today and also I'm dead sick. hurray ^^\n",
      "\n",
      "3\n",
      "2076\n",
      "53135b495e986b0712efc453\n",
      "2016-02-18 15:35\n",
      "@ogrisel Ah caught you on gitter - Now could you please review and merge this - #6254 ? :P\n",
      "Merci ;)\n",
      "\n",
      "2\n",
      "2077\n",
      "53135b495e986b0712efc453\n",
      "2016-02-18 15:40\n",
      "Next month I'll progress to full sentences in French ^_^\n",
      "\n",
      "1\n",
      "2078\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-18 15:41\n",
      "I sent an email to continuum.\n",
      "\n",
      "1\n",
      "2079\n",
      "5363a92c048862e761fa03c3\n",
      "2016-02-18 17:07\n",
      "@nelson-liu\n",
      "\n",
      "1\n",
      "2080\n",
      "5363a92c048862e761fa03c3\n",
      "2016-02-18 17:13\n",
      "@nelson-liu I think I accidentally committed in `master` which resulted in that.  @ogrisel Thank you, that helped me. I do use different branches. Will be extra careful next time! :)\n",
      "\n",
      "1\n",
      "2081\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-18 18:19\n",
      "scikit-learn 0.17.1 is online!\n",
      ":beers:\n",
      "\n",
      "3\n",
      "2082\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-18 18:31\n",
      "@kaichogami got it, glad you got it fixed!\n",
      "\n",
      "1\n",
      "2083\n",
      "56c625c3e610378809c22760\n",
      "2016-02-18 20:14\n",
      "Never knew that the gitter channel was this active! @nelson-liu please add this to the doc!\n",
      "\n",
      "1\n",
      "2084\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-18 20:17\n",
      "yeah I find it a bit odd that it isnt in there already...searching gitter in the repos issue history even shows many people referencing collaborating / talking on it. at least its much more active than irc :P\n",
      "\n",
      "2\n",
      "2085\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-18 21:24\n",
      "\n",
      "1\n",
      "2086\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 04:52\n",
      "Why does predict_proba give better accuracy than the predict function? Whats the difference?\n",
      "What are the best parameters for param_grid when performing GridSearchCV on  svm?\n",
      "@ogrisel wohoo\n",
      "\n",
      "3\n",
      "2087\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-19 05:11\n",
      "I'll try to answer your question in a few minutes when I return to a computer.\n",
      "\n",
      "1\n",
      "2088\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-19 05:22\n",
      "@Fredilly as for your second question, it really depends on your dataset. Could you describe it?\n",
      "for predict_proba vs predict, theyre two completely different methods. predict_proba predicts the probability of your input being a certain class. Predict returns what class the model predicts your input to be (e.g. by taking the class with the highest probability from predict_proba)\n",
      "does that make sense?\n",
      "(if anyone else wants to clarify / correct / validate my explanation, please do)\n",
      "\n",
      "4\n",
      "2089\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 09:18\n",
      "@nelson-liu That makes sense. From my understanding, predict_proba does the same thing as predict but it requires a threshhold value to activate in case of binary outputs. It still doesnt explain why it gets better results\n",
      "\n",
      "2\n",
      "2090\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 09:21\n",
      "Yes...I get .55 score with predict and .88 score with predict_proba\n",
      "\n",
      "1\n",
      "2091\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-19 09:23\n",
      "how are you getting a score with predict_proba? shouldnt it output an array with 2 elements?\n",
      "\n",
      "7\n",
      "2092\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 09:27\n",
      "Logistic Regression and Gradient Boosting Classifier\n",
      "I tried both separately just to see which produced a better result and noticed that predict_proba gave a much higher accuracy in both cases.\n",
      "Should that always be the case? Im always discovering new techniques that completely invalidate everything else Ive learned.\n",
      "\n",
      "3\n",
      "2093\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 09:39\n",
      "``` encoder = preprocessing.OneHotEncoder() encoder.fit(np.vstack((X, X_test))) X = encoder.transform(X) X_test = encoder.transform(X_test)  # LogisticRegression logreg = LogisticRegression(C=3) logreg.fit(X, y) y_pred = logreg.predict_proba(X_test)[:, 1] df = pd.DataFrame({'id': test.id, 'Action': y_pred}) df.tail() df.to_csv('kagglesubmission.csv') #scores about .88  ```\n",
      "\n",
      "1\n",
      "2094\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-19 09:44\n",
      "why do you slice [:, 1]?\n",
      "Maybe theres some pandas magic going on that Im not familiar with, but if you slice [:,1] arent you always getting the probabilities of the second element of model.classes_? Is there some way that you check whether this is greater than or less than the probability of the other class, and then put the correctly labeled prediction into the df?\n",
      "\n",
      "7\n",
      "2095\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 09:49\n",
      "One quick question: Is it possible that gridsearchcv give worse score accuracy? I get .946 off the bat on an svm implementation but I get about .92 with the best parameters from gridsearchcv.\n",
      "\n",
      "1\n",
      "2096\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 09:55\n",
      "I see what you mean but choosing the column with the highest true positives means you can immediately compare with null accuracy and decide whether to improve sensitivity of specificity.\n",
      "\n",
      "1\n",
      "2097\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-19 09:56\n",
      "what do you mean highest true positives?\n",
      "\n",
      "1\n",
      "2098\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 09:58\n",
      "Predicting a correct value of true. Eg predicting >0.5 when the real value is 1\n",
      "In my classification problem, if I just predicted 1 everytime, I would be correct 94% of the time.\n",
      "One of the two columns, [X, y] would have more 1s. When I slice, I use that column for predictions.\n",
      "They represent whether a user is granted access permission or not\n",
      "\n",
      "4\n",
      "2099\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-19 10:00\n",
      "Oh. is that just an innate feature of the dataset? So basically you want to predict based on some sort of confidence ratio? e.g. you know its most likely 1, so predict 1 every time unless theres a very high confidence for 0?\n",
      "what do the two columns [X,y] represent?\n",
      "no sorry i mean is [x,y] the input, the result of predict_proba(), etc?\n",
      "\n",
      "12\n",
      "2100\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 10:10\n",
      "I get 0.10~0.99\n",
      "\n",
      "1\n",
      "2101\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-19 10:12\n",
      "then why even bother using ml if you could just always guess `1` and be right 95% of the time?\n",
      "\n",
      "5\n",
      "2102\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-19 10:15\n",
      "hmm if you want to get your feet wet, id suggest titanic on kaggle. This is a pretty good tutorial https://github.com/savarin/pyconuk-introtutorial\n",
      "but regardless, predict_proba() doesnt work the way you think it does. Taking one column and encoding everything as that only happens to work because a large portion of your dataset is one label.\n",
      "did you use predict_proba() in titanic?\n",
      "its probably best to just stick to using predict() and further tuning your model / performing feature selection\n",
      "Its just a coincidence in this case that predict_proba() seems to work so much better than predict()\n",
      "Beyond that, feature engineering in terms of normalization and other transformations on the data can also be quite useful to do things like remove outliers, etc.\n",
      "\n",
      "6\n",
      "2103\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 10:17\n",
      "I did that one....was pretty helpful. The employee challenge and vowpal rabbit challenge prepare you more thoroughly for real world ML problems\n",
      "\n",
      "1\n",
      "2104\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 10:20\n",
      "From what Ive gathered so far, tuning features is tedious and not usually worth the hassle unless you have like a 100 features.\n",
      "\n",
      "5\n",
      "2105\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 10:24\n",
      "Youre probably right. Thats where <unconvertable> domain expertise <unconvertable> or just plain old common sense comes into play.\n",
      "\n",
      "1\n",
      "2106\n",
      "561d08d0d33f749381a937bf\n",
      "2016-02-19 10:29\n",
      "Youre absolutely right. Have you tried gbm? Its awesome....I wanna learn more about XGboost. They produce high accuracies right off the bat.\n",
      "\n",
      "1\n",
      "2107\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-02-19 10:50\n",
      "In https://github.com/scikit-learn/scikit-learn/pull/5491 I am confused by which classifier is having its hyper parameters optimized in the examples with graphs. Does anyone know?\n",
      "@MechCoder If you happen to be about I think this question is aimed at you :)\n",
      "\n",
      "2\n",
      "2108\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-19 14:33\n",
      "if your possible target classes are consecutive integers like `[0, 1, 2]` the `predict(X_test)` should return the same as `predict_proba(X_test).argmax(axis=1)`. `predict_proba` is just a way to ask for the confidence levels of the model when it's making a prediction. The final classification decision should be exactly the same.\n",
      "\n",
      "1\n",
      "2109\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-20 06:58\n",
      "@rvraghav93 and @jnothman I just uploaded a first draft of the issue / pr template we were discussing, please let me know what you think :) its at #6411.\n",
      "\n",
      "1\n",
      "2110\n",
      "544906e2db8155e6700cdd16\n",
      "2016-02-20 15:52\n",
      "@Fredilly Here's my two cents... I think that the difference that you saw in the results is related to the metric used by Kaggle to measure the performance: Area Under the (ROC) Curve. According to this issue (https://github.com/scikit-learn/scikit-learn/issues/1393) the auc score will give better scores if you feed it with probabilities instead of binary decisions.\n",
      "\n",
      "1\n",
      "2111\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-22 09:20\n",
      "hey @ogrisel not sure if youre around, but Im having some issues rebasing commits in https://github.com/scikit-learn/scikit-learn/pull/6417. I took the original contributors commits and rebased master on top of it. However, I am trying to squash it now and am unable to. Do you have any advice as to how to fix this?\n",
      "\n",
      "1\n",
      "2112\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-22 10:03\n",
      "@nelson-liu done\n",
      "\n",
      "1\n",
      "2113\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-22 10:04\n",
      "I meant I gave it a review. It seems squashed enough to me. What problems do you have?\n",
      "\n",
      "1\n",
      "2114\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-22 10:05\n",
      "Ah, thanks! well, commit https://github.com/nelson-liu/scikit-learn/commit/6209098c38cb7aa4e7aad381407da6f42fe7b464 has quite a long diff, and Id rather not pollute the commit history with that\n",
      "additionally, itd be nice to squash the commits into 1 instead of having 4 separate commits\n",
      "\n",
      "2\n",
      "2115\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-22 10:14\n",
      "when I currently try to squash the commits, i get a bunch of merge conflicts and such. running `git rebase -i HEAD~3` (to squash my last 3 commits) ends up for some reason pulling up interactive rebase for ~211 commits onto https://github.com/nelson-liu/scikit-learn/commit/09672f516d8592fb82f42e5da3ee0f29210d7366, even though the head is at https://github.com/nelson-liu/scikit-learn/commit/8bea87efc7f6db484f583c15d36a775d82381ef3\n",
      "\n",
      "1\n",
      "2116\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-22 10:17\n",
      "Right I did no see that from the github diff view of the PR. Indeed this comment should be removed.\n",
      "I would start over again from the original contributor's PR. Squash the top commits first, then rebase on master and fix the conflicts. Then add your changes on top and squash again.\n",
      "nelson-liu/scikit-learn@6209098 has many changes with conflict markers that should not be part of this PR.\n",
      "The commits in https://github.com/scikit-learn/scikit-learn/pull/5968 are clean though.\n",
      "\n",
      "5\n",
      "2117\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-22 10:29\n",
      "how do you generally resolve merge conflicts? im using `git mergetool`, which is opening opendiff on my osx machine\n",
      "wondering if this might be why conflict markers are there?\n",
      "\n",
      "2\n",
      "2118\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-22 10:40\n",
      "anyway, I just pushed a new version at https://github.com/scikit-learn/scikit-learn/pull/6419. can you let me know if there are any issues?\n",
      "ah ok, i wasnt aware that you could just do it through the text editor. I think ive properly done it in the last PR, could you let me know?\n",
      "\n",
      "2\n",
      "2119\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-22 10:41\n",
      "edit the files with the conflicts,  look for section with conflict markers, edit the code to replace the segments in conflict with the expected code segment and remove the markers\n",
      "check that there are no other files with conflict markers and then `git add` the files where you resolved the conflicts and `git rebase --continue`\n",
      "\n",
      "2\n",
      "2120\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-22 18:43\n",
      "does anyone have a good example of when l2 normalizer is useful? I only ever really use l1\n",
      "hm... good point, but slightly to advanced ^^\n",
      "\n",
      "2\n",
      "2121\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-22 18:43\n",
      "if you want to compute cosine similarities: preprocess the data once and then use np.dot.\n",
      "\n",
      "1\n",
      "2122\n",
      "56b80528e610378809c05a48\n",
      "2016-02-23 09:20\n",
      "Hello @amueller , sorry for disrupting you. Could you please answer my question in #6322 ?\n",
      "\n",
      "1\n",
      "2123\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-23 09:20\n",
      "For TF-IDF vectorization, normalizing by l2 norm makes BoW representation more invariant to doc length (similar idea but rephrased :).\n",
      "\n",
      "1\n",
      "2124\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-23 09:35\n",
      "Maybe a bit off topic, but should the topic in the IRC channel be changed to reflect the release of 0.17.1? (Does anyone ever use the IRC anymore? Ive been on it for the past week and havent seen a single message in the channel)\n",
      "\n",
      "1\n",
      "2125\n",
      "541a528b163965c9bc2053de\n",
      "2016-02-23 09:37\n",
      "I don't remember who has op rights. I think I do but I don't remember the password of my NickServ account.\n",
      "\n",
      "1\n",
      "2126\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-23 09:40\n",
      "seems like only @amueller does, or thats what `/msg chanserv access #scikit-learn LIST` is reporting.\n",
      "\n",
      "1\n",
      "2127\n",
      "53135b495e986b0712efc453\n",
      "2016-02-23 12:34\n",
      "@ogrisel could you close this - https://github.com/scikit-learn/scikit-learn/issues/5622\n",
      "\n",
      "1\n",
      "2128\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-02-24 22:06\n",
      "any reason why `GradientBoostingClassifier`'s `decision_function` does not allow sparse data? ([relevant line](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L1469))\n",
      "basically you can fit on sparse, but can't predict\n",
      "Hmm, got it, it's because [`predict_stages`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_gradient_boosting.pyx#L101) doesn't support it.\n",
      "\n",
      "3\n",
      "2129\n",
      "56aa7e09e610378809beb481\n",
      "2016-02-25 09:48\n",
      "hi\n",
      "\n",
      "1\n",
      "2130\n",
      "53135b495e986b0712efc453\n",
      "2016-02-25 14:14\n",
      "Hello @unautre\n",
      "\n",
      "1\n",
      "2131\n",
      "53135b495e986b0712efc453\n",
      "2016-02-25 14:15\n",
      "BTW Can anyone give a final review on https://github.com/scikit-learn/scikit-learn/pull/5568 (@amueller @ogrisel @vighneshbirodkar?)\n",
      "\n",
      "1\n",
      "2132\n",
      "56b80528e610378809c05a48\n",
      "2016-02-25 15:54\n",
      "Sorry to disturb. Can anyone please help review on #6371  and #6395 ? A merge can urge me to face my midterm tomorrow :pray:\n",
      "\n",
      "1\n",
      "2133\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-25 16:12\n",
      "@rvraghav93 I'll try to review #5568 today\n",
      "\n",
      "1\n",
      "2134\n",
      "56c625c3e610378809c22760\n",
      "2016-02-25 16:38\n",
      "Just had a doubt regarding issue #6443. How is it that this failure is not happening on travis but is just happening locally? Shouldn't both be synced atleast for the same python version?\n",
      "\n",
      "1\n",
      "2135\n",
      "56c625c3e610378809c22760\n",
      "2016-02-25 17:04\n",
      "Is anybody else getting this error on their system or is it just me :/\n",
      "\n",
      "1\n",
      "2136\n",
      "53135b495e986b0712efc453\n",
      "2016-02-25 17:22\n",
      "@amueller Thanks a tonnnnnne for the review!\n",
      "@yenchenlin1994 I can have a look...\n",
      "\n",
      "2\n",
      "2137\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-25 17:33\n",
      "@rvraghav93 sorry for being kinda offline at the moment. I really wanna get going with the book ;)\n",
      "btw, do you know if anyone started working on the multiple metrics? I think @MechCoder asked me about it.\n",
      "\n",
      "2\n",
      "2138\n",
      "56b80528e610378809c05a48\n",
      "2016-02-25 18:14\n",
      "@rvraghav93 thanks a lot!\n",
      "\n",
      "1\n",
      "2139\n",
      "56c625c3e610378809c22760\n",
      "2016-02-25 18:37\n",
      "could someone also help review #6176 and #6173 if time permits :P\n",
      "\n",
      "1\n",
      "2140\n",
      "56d19aa5e610378809c3dde8\n",
      "2016-02-29 16:29\n",
      "Hello! Can anyone tell me how can I make this python script ignore DS store files... very silly issue. I have this function, I've googled but I don't know where to put the code in my current function. Thank you!  def list_files(dir): \tr = [] \tsubdirs = [x[0] for x in os.walk(dir)]  \tfor subdir in subdirs: \t\tfiles = os.walk(subdir).__next__()[2] \t\tif (len(files) > 0): \t\t\tfor file in files: \t\t\t\tr.append(subdir + \"/\" + file) \treturn r      the_list = list_files(file_rep)\n",
      "\n",
      "1\n",
      "2141\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-29 16:39\n",
      "I dont think this is necessarily the proper venue for your question. That being said, your code doesnt work properly on my mac. Theoretically though, you would wrap your `r.append()` statement in an if-block that checks if the filename is .DS_Store.\n",
      "\n",
      "1\n",
      "2142\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-29 16:40\n",
      "@thejivester you can go to stackoverflow\n",
      "\n",
      "1\n",
      "2143\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-29 19:05\n",
      "Did scikit-learn apply to Google summer of code?\n",
      "\n",
      "1\n",
      "2144\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-29 21:15\n",
      "@nelson-liu it is under the umbrella of the PSF\n",
      "7th of march\n",
      "\n",
      "2\n",
      "2145\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-02-29 21:16\n",
      "Ah ok. I talked to Terri and she said that scikit-learn hasn't submitted a proposal yet, are there plans to do so?\n",
      "\n",
      "4\n",
      "2146\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-02-29 21:31\n",
      "@rvraghav93 do you have anything for the multiple metrics? I think it might be easier to work on that before trying to clean up\n",
      "If you're busy with your other PRs, I can look into it.\n",
      "\n",
      "2\n",
      "2147\n",
      "53135b495e986b0712efc453\n",
      "2016-02-29 22:38\n",
      "Okay please go ahead. I am just fighting with the tree code. I am sorry :/ I will take up something else instead of it later....\n",
      "\n",
      "1\n",
      "2148\n",
      "53810862048862e761fa2887\n",
      "2016-03-01 00:25\n",
      "Guys, take a look at http://contrib.scikit-learn.org/project-template/\n",
      "\n",
      "1\n",
      "2149\n",
      "53810862048862e761fa2887\n",
      "2016-03-01 00:26\n",
      "Right now the because of the code and website being hosted in the same root folder, the source files are available on that url\n",
      "For example,  http://contrib.scikit-learn.org/project-template/setup.py. Is that a problem ?\n",
      "\n",
      "2\n",
      "2150\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-01 00:45\n",
      "i registered scikit-learn.ml and redirected it to the main site haha\n",
      "[http://scikit-learn.ml](http://scikit-learn.ml)\n",
      "\n",
      "2\n",
      "2151\n",
      "53135b495e986b0712efc453\n",
      "2016-03-01 07:27\n",
      "Thats sweet! You should ping @ogrisel\n",
      "\n",
      "1\n",
      "2152\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-01 07:28\n",
      "on another note, does anyone know whether its possible to do multi-line links in markdown? If not, maybe we can get a scikit-learn link shortener to make the issue/pr template more digestible.\n",
      " short things like [http://sklearn.ml/contributing](http://sklearn.ml/contributing)\n",
      "\n",
      "7\n",
      "2153\n",
      "56b80528e610378809c05a48\n",
      "2016-03-01 07:31\n",
      "smart move haha\n",
      "\n",
      "1\n",
      "2154\n",
      "53135b495e986b0712efc453\n",
      "2016-03-01 07:39\n",
      "Really? I am getting one then ;)\n",
      "\n",
      "1\n",
      "2155\n",
      "53135b495e986b0712efc453\n",
      "2016-03-01 07:55\n",
      "@nelson-liu Thanks for the nice idea :D\n",
      "\n",
      "2\n",
      "2156\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-01 08:57\n",
      "aw, just got an email that for some reason [sklearn.ml](sklearn.ml) was already registered and that i dont have the rights to it. oh well, [scikit-learn.ml](scikit-learn.ml) will do for link shortening.\n",
      "in that vein, should we move the discussion about the issue / pr template to https://github.com/scikit-learn/scikit-learn/issues/6394 or just create a new issue?\n",
      "\n",
      "2\n",
      "2157\n",
      "53135b495e986b0712efc453\n",
      "2016-03-01 13:20\n",
      "Arrrgh some advertising company purchased sklearn.ml. It would have been cool to have it redirect to our page too!\n",
      "In the end I'm guessing github will realize they just need to improve their interface rather than providing these hacky workarounds\n",
      "\n",
      "2\n",
      "2158\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-01 16:59\n",
      "why is it important to have different domains?\n",
      "\n",
      "1\n",
      "2159\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-01 17:30\n",
      "Have you taken a look at the pr / issue template?\n",
      "A lot of the links in there are very long, and since you can't create multi-line links in markdown it greatly diminishes readability\n",
      "So I was thinking maybe use the  .ml domain for link shortening internally\n",
      "Basically: it's not important, just an idea haha\n",
      "haha that would be ideal. made a pr at https://github.com/scikit-learn/scikit-learn/pull/6470\n",
      "\n",
      "5\n",
      "2160\n",
      "53135b495e986b0712efc453\n",
      "2016-03-01 19:21\n",
      "@vene I thought it would be cool to have a .ml domain :p BTW @nelson-liu there was a thread claiming those free .ml providers as scam. Be careful!\n",
      "As for the link shortener we should implement it at our main site.\n",
      "Sounds great!\n",
      "\n",
      "3\n",
      "2161\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-01 19:22\n",
      "Im drafting up a new, more template-y version of the templates right now\n",
      "\n",
      "1\n",
      "2162\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-03 18:50\n",
      "we reached 10,000 stars today :beers:\n",
      "\n",
      "1\n",
      "2163\n",
      "56b80528e610378809c05a48\n",
      "2016-03-03 22:40\n",
      ":+1:\n",
      "\n",
      "1\n",
      "2164\n",
      "56d918bde610378809c4f013\n",
      "2016-03-04 05:10\n",
      "Hi\n",
      "\n",
      "1\n",
      "2165\n",
      "56d918bde610378809c4f013\n",
      "2016-03-04 05:13\n",
      "I have an issue with affinity propagation\n",
      "\n",
      "1\n",
      "2166\n",
      "53135b495e986b0712efc453\n",
      "2016-03-04 11:51\n",
      "10k stars!! :beers:\n",
      "\n",
      "1\n",
      "2167\n",
      "541a528b163965c9bc2053de\n",
      "2016-03-05 18:17\n",
      ":beers:\n",
      "\n",
      "1\n",
      "2168\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-03-07 19:11\n",
      "@ogrisel someone just told me that doing conda upgrade --all breaks scikit-learn. I think that's related to the CI failure\n",
      "\n",
      "1\n",
      "2169\n",
      "560313510fc9f982beb1a331\n",
      "2016-03-07 20:43\n",
      "Yeah, they started bundling mkl with everything. To fix this, you should try to install `nomkl` first and then install everything else that is BLAS dependent.\n",
      "\n",
      "1\n",
      "2170\n",
      "541a528b163965c9bc2053de\n",
      "2016-03-08 17:21\n",
      ">  conda upgrade --all breaks scikit-learn  scikit-learn installed from conda or built from the source prior to the upgrade? What does break mean? A specific test fails? or a segfault at `import sklearn`?\n",
      "ok great\n",
      "\n",
      "2\n",
      "2171\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-03-08 18:10\n",
      "@ogrisel running conda upgrade twice actually makes it work again\n",
      "it was a linking issue\n",
      "there is a specific version of sklearn on conda that is not properly linked, but I think they fixed it now\n",
      "(and I meant installed from conda)\n",
      "\n",
      "4\n",
      "2172\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-03-08 22:01\n",
      "@vighneshbirodkar maybe also compare against https://github.com/jkarnows/rpcaADMM\n",
      "and possibly https://github.com/dganguli/robust-pca\n",
      "\n",
      "2\n",
      "2173\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-03-08 22:33\n",
      "oh @vighneshbirodkar you should definitely try https://gist.github.com/bmcfee/a378bfe31a75769c583e first\n",
      "\n",
      "1\n",
      "2174\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-03-08 23:11\n",
      "@vighneshbirodkar brian says to read this book ^^ http://www.web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf\n",
      "(the variable names are taken from there, in particular 3.11 and 3.12)\n",
      "\n",
      "2\n",
      "2175\n",
      "5478876cdb8155e6700d907b\n",
      "2016-03-09 13:09\n",
      "hello guys, who do I badger about strange travis failures: https://github.com/scikit-learn/scikit-learn/pull/6166#issuecomment-194246460\n",
      "in one it fails to find libgfortran and in the other some tests related to scorers and memmapping fail :-/\n",
      "\n",
      "2\n",
      "2176\n",
      "52a485b6ed5ab0b3bf04fab6\n",
      "2016-03-10 03:44\n",
      "I opened an issue here https://github.com/scikit-learn/scikit-learn/issues/6513\n",
      "\n",
      "1\n",
      "2177\n",
      "53810862048862e761fa2887\n",
      "2016-03-10 04:47\n",
      "@amueller I fixed a bug in Brian's code that was causing the low rank output to be scaled differently. It optimizes the objective better than the last PCP implementation.\n",
      "\n",
      "1\n",
      "2178\n",
      "56cad8a7e610378809c2bef5\n",
      "2016-03-10 15:24\n",
      "roles of data structure and algorithm in open source?\n",
      "\n",
      "1\n",
      "2179\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-10 20:54\n",
      "I was reading https://github.com/scikit-learn/scikit-learn/pull/5974#issuecomment-194840168\n",
      "can anyone explain what MV means?\n",
      "\n",
      "2\n",
      "2180\n",
      "53135b495e986b0712efc453\n",
      "2016-03-10 22:31\n",
      "Missing Value ;)\n",
      "And yes thanks please share your thoughts on it.\n",
      "\n",
      "2\n",
      "2181\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-11 07:17\n",
      "@rvraghav93  ah thanks... I do have some small thoughts in fact\n",
      "\n",
      "1\n",
      "2182\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-11 07:33\n",
      "@rvraghav93  first.. what are the dotted lines?\n",
      "@rvraghav93  and what is the bootstrap you refer to? (sorry these are naive questions)\n",
      "also, one standard way to handle missing values when using a random forest is just to treat them as categorical values. That is make a new feature \"X is missing\" and set it to 1 if it is missing.\n",
      "would it be worth comparing to that approach?\n",
      ":)\n",
      "@rvraghav93  isn't that a lot simpler in that case?\n",
      "what I do in practice is make the feature categorical if it is missing and then just run dictvectorizer\n",
      "which handles it all for me\n",
      "it creates one new feature per feature at most\n",
      "so at most doubles\n",
      "I don't have any code here sorry\n",
      "@rvraghav93 thanks.\n",
      "\n",
      "12\n",
      "2183\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-11 08:23\n",
      "I found the bootstrap option so please cancel that part of my question :)\n",
      "\n",
      "1\n",
      "2184\n",
      "53135b495e986b0712efc453\n",
      "2016-03-11 11:31\n",
      "Yes! You are right. This approach does exactly the same thing. It tries to send the missing values to the best partition as if it were a categorical variable.\n",
      "BTW hurraayyy we have github reactions to comments and PR comments...\n",
      "Sorry. I don't get you. Simpler in which case?\n",
      "Oh wait you mean make a new feature for \"X is missing\"... Hmm no this approach does not do that...\n",
      "But how will you do that? What will you do with the missing values in the features which are not \"This feature is missing\"?\n",
      "\n",
      "5\n",
      "2185\n",
      "53135b495e986b0712efc453\n",
      "2016-03-11 11:42\n",
      "That just explodes your feature space no? Also could you give me a minimal code example so I can be sure to follow what you mean.\n",
      "\n",
      "1\n",
      "2186\n",
      "53135b495e986b0712efc453\n",
      "2016-03-11 11:45\n",
      "No my question is lets say I have a data `X = [[1.2,], [2.2,], [np.nan,]]` How does your new data (after your preprocessing for missing values) look like?\n",
      "\n",
      "2\n",
      "2187\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-11 11:52\n",
      "I think it looks like [[1.2,0], [2.2,0], [0,1]]\n",
      "assuming I am parsing this correctly\n",
      "you just add one more feature for each feature that can have a missing value\n",
      "\n",
      "3\n",
      "2188\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-11 12:09\n",
      "@rvraghav93  does this make sense?\n",
      "@rvraghav93 I am not sure what you mean by \"What will you do with the missing values in the features which are not \"This feature is missing\"\"\n",
      "\n",
      "2\n",
      "2189\n",
      "53135b495e986b0712efc453\n",
      "2016-03-11 12:51\n",
      "Say we have 10 features and the 10th feature has missing values. We now have 11 features right? Will that mean we amplify the importance of the 10th feature and not the other features? Anyway this is an interesting case for comparison. I will compare that and let you know how it performs in comparison with the implemented method.\n",
      "My intuition is that, at a higher level, both these methods are similar...\n",
      "both as in the one that you propose and my implementation at #5974\n",
      "\n",
      "3\n",
      "2190\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-11 13:13\n",
      "@rvraghav93  \"Will that mean we amplify the importance of the 10th feature and not the other features?\" That hadn't occurred to me as a possibility as the 11th feature is only ever 1 or 0\n",
      "I would love to see the results of your testing on this\n",
      "@rvraghav93  \"We now have 11 features right? \" yes\n",
      "\n",
      "3\n",
      "2191\n",
      "53135b495e986b0712efc453\n",
      "2016-03-11 13:20\n",
      "\"as the 11th feature is only ever 1 or 0\" - Correct! But I'm not sure if the feature importance will now be shared between the 10th and 11th feature or will be independently assigned... Have to look into it. Nevertheless this is a good comparison for my method. Another thing that I tried was replacing the missing values by the 10*maximum across all the features... This seems to not perform as well as the implementation. Thanks for your inputs! Please feel free to share more thoughts!\n",
      "\n",
      "1\n",
      "2192\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-11 13:22\n",
      "@rvraghav93  thanks. I should say that I am particularly interested in categorical variables so things like replacing missing values by huge values never occurs to me :)\n",
      "@rvraghav93  were the dotted lines the timings? I mean in the graphs\n",
      "@rvraghav93  you make a very interesting point about feature importance.\n",
      "I think we need a smarter imputation for categorical values\n",
      "\n",
      "7\n",
      "2193\n",
      "53135b495e986b0712efc453\n",
      "2016-03-11 13:49\n",
      "Yes! the dotted lines are time taken for `cross_val_score` . I'm now trying to plot the time taken for a single fit.\n",
      "\n",
      "1\n",
      "2194\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-11 14:25\n",
      "@rvraghav93  actually categorical values in general make the imputation strategies for missing values tricky, or at least different\n",
      "I think this is an important use case\n",
      "@rvraghav93  mode could work but I am not sure what median would mean as there is no natural ordering\n",
      "@rvraghav93  I really hope https://github.com/scikit-learn/scikit-learn/issues/4899 makes progress\n",
      "although mode is a little worrying too.. imagine lots of categories which occur 7,8,9 or 10 times. It's not clear the missing ones should be given to the category that occurs 10 times\n",
      "@rvraghav93 Great and a huge thank you.\n",
      "\n",
      "13\n",
      "2195\n",
      "53135b495e986b0712efc453\n",
      "2016-03-11 14:25\n",
      "I think the imputation strategy for categorical value should be 'median' or 'mode' instead of 'mean' no?\n",
      "And if the categorical support is introduced (in https://github.com/scikit-learn/scikit-learn/pull/4899), handling missing values in categorical features is not difficult. The missing simply becomes an additional category.\n",
      "yes correct. Median is not appropriate.\n",
      "#4899 is next on my list, (as #5974 is brought to a reviewable state).\n",
      "\n",
      "4\n",
      "2196\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-12 13:59\n",
      "what's the default scoring scheme when using  cross_val_score with RandomForestRegressor?\n",
      "If the docs say, I haven't seen it\n",
      "\n",
      "2\n",
      "2197\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-13 07:04\n",
      "I found it ... I think it should be renamed :)\n",
      "\n",
      "1\n",
      "2198\n",
      "53135b495e986b0712efc453\n",
      "2016-03-13 09:18\n",
      "Which one should be renamed?\n",
      "\n",
      "1\n",
      "2199\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-13 09:55\n",
      "\"score(X, y[, sample_weight]) \tReturns the coefficient of determination R^2 of the prediction.\" This is confusing I think. It's really a correlation coefficient. Why is it called R^2?\n",
      "it is a value between -1 and 1\n",
      "as far as I can tell\n",
      "@rvraghav93  ^^\n",
      "\n",
      "4\n",
      "2200\n",
      "5537027215522ed4b3df56ab\n",
      "2016-03-13 15:16\n",
      "Hey guys, would it make sense to contribute confidence intervals for linear models?\n",
      "because without standard errors for each coefficient estimated from test data, it's hard to interpret coefficients\n",
      "\n",
      "2\n",
      "2201\n",
      "5537027215522ed4b3df56ab\n",
      "2016-03-13 15:22\n",
      "i work with data that changes over time, so in my case I need confidence intervals on output probabilities for new samples. When using the standard output probabilities,  they are sometimes wrong and confidence intervals would help figure out a) how wrong you are and b) which coefficients have lower standard errors over time for more robust feature selection\n",
      "\n",
      "1\n",
      "2202\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-13 15:46\n",
      "@lesshaste not really. Training R^2 is between 0 and 1, but on unseen data it can become negative.  And \"coefficient of determination\" is an established term in statistics.\n",
      "\n",
      "1\n",
      "2203\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-13 16:50\n",
      "@vene Hi. If it's an established term then that is what we should use. Maybe a very brief explanation of the range it can take on test data and why could be added?\n",
      "@vene  I managed to get these scores [-0.38971809 -1.32178009 -0.20038367]  . What is the range? It clearly isn't -1 to 1\n",
      " @vene  that's interesting. It would be great if something definitive and clear could be added to the docs about this. Please :)\n",
      "@vene that's a very interesting observation! I have found them to be great when I have a mix of categorical and numerical values and lots of data\n",
      "@vene  maybe I should send you my data to see what you can make of it :)\n",
      "because I am completely failing currently\n",
      "\n",
      "6\n",
      "2204\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-13 16:59\n",
      "It's -INF to 1, I think.\n",
      "\n",
      "1\n",
      "2205\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-13 18:38\n",
      "@lesshaste have you read this? http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score\n",
      "the intuition is: a model can never do worse than predicting the mean *on training data*. (at least a linear model that can set all its coefs to 0) but it can do much worse on test data if it overfits.\n",
      "@lesshaste did you try feature selection?\n",
      "@lqdc confidence intervals are cool, but it seems more in the domain of statsmodels. And I think it's actually already implemented in statsmodels.\n",
      "can you assign 127.0 to a char normally?\n",
      "\n",
      "15\n",
      "2206\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-13 18:46\n",
      "on \"hard\" regression problems (few samples, many irrelevant features) MSE/MAE can lead you to believe you're doing well,  if you don't compare against a dummy baseline that predicts the mean, or something simple like that. I've fallen in this trap.\n",
      "\n",
      "1\n",
      "2207\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-13 18:59\n",
      "@vene Oops! I hadn't read that. Sorry that's my bad\n",
      "@vene  I seem to have a hard regression problem currently :(\n",
      "\n",
      "2\n",
      "2208\n",
      "56e5af1285d51f252ab894f7\n",
      "2016-03-13 19:05\n",
      "hey, if anyone has any remote remote , designer,  DevOps  or Sysadmin jobs they can post them at http://webwork.io\n",
      "\n",
      "1\n",
      "2209\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-13 19:15\n",
      "[for OLS at least](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLSResults.conf_int.html#statsmodels.regression.linear_model.OLSResults.conf_int)\n",
      "\n",
      "1\n",
      "2210\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-13 19:20\n",
      "seems like it's there for [GLM](http://statsmodels.sourceforge.net/devel/generated/statsmodels.genmod.generalized_linear_model.GLMResults.conf_int.html#statsmodels.genmod.generalized_linear_model.GLMResults.conf_int) too\n",
      "\n",
      "1\n",
      "2211\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-13 19:24\n",
      "@vene  I didn't.. I just assumed that randomforestregressor doesn't really benefit from that. Is that wrong?\n",
      "\n",
      "1\n",
      "2212\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-13 19:31\n",
      "I haven't used random forests much, dunno.\n",
      "\n",
      "1\n",
      "2213\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-13 19:39\n",
      "@vene  I think problem is that I have 140 samples where I am used to 100s of thousands\n",
      "so my intuition for what works is wrong\n",
      "\n",
      "2\n",
      "2214\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-13 19:45\n",
      "I never actually managed to get random forests to outperform linear models on the datasets I worked with\n",
      "which are usually <<10k samples\n",
      "\n",
      "2\n",
      "2215\n",
      "56b80528e610378809c05a48\n",
      "2016-03-13 19:52\n",
      "Hello guys, Is there anyone familiar with cythons fused type?  ```cython cimport cython  ctypedef fused char_or_float:     cython.char     cython.float  def show_me():     cdef char_or_float  cython.char a = 127  show_me() ```\n",
      "Oh theres a typo\n",
      "Following script dosnt work ...  ```cython cimport cython  ctypedef fused char_or_float:     cython.char     cython.float  def show_me():     cdef char_or_float a = 127.0  show_me() ```\n",
      "Can somebody tell me whats the problem?\n",
      "\n",
      "4\n",
      "2216\n",
      "56b80528e610378809c05a48\n",
      "2016-03-13 20:11\n",
      "oh yeah I simplify the code alot\n",
      "\n",
      "1\n",
      "2217\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-13 20:13\n",
      "it's not that; I don't think it makes sense to assign a literal to a fused type\n",
      "this works, for example:\n",
      "``` %%cython  cimport cython  ctypedef fused char_or_float:     cython.char     cython.float  cdef char_or_float add_1(char_or_float x):     return x + 1  def show_me():     cdef cython.char a = 1     print(add_1(a))  show_me()```\n",
      "note i'm declaring and assigning to a char, not to a char_or_float. But I can pass it to a function that takes char_or_float\n",
      "\n",
      "8\n",
      "2218\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-03-13 21:06\n",
      "@vighneshbirodkar thanks. Sorry, I'm still travelling. How does it compare in terms of runtime?\n",
      "\n",
      "1\n",
      "2219\n",
      "5537027215522ed4b3df56ab\n",
      "2016-03-14 02:21\n",
      "@vene  That makes sense but stats models doesn't deal with a million features\n",
      "basically the whole lib doesn't work for omre than like 20 features\n",
      "@lesshaste sounds good\n",
      "\n",
      "3\n",
      "2220\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-14 08:56\n",
      "Also statsmodels in general is not developed very quickly or actively. I used to follow it but gave up.\n",
      "\n",
      "1\n",
      "2221\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-14 11:16\n",
      "Maybe there is a way to initialize the statsmodels result object with our coefs.\n",
      "And use their post processing\n",
      "\n",
      "2\n",
      "2222\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-14 16:42\n",
      "@vene  that's a nice idea.\n",
      "\n",
      "1\n",
      "2223\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-14 16:47\n",
      "@lqdc  Do you want to open an issue with an example large enough that  statsmodels can't cope?\n",
      "and paste in @vene's thought maybe\n",
      "\n",
      "2\n",
      "2224\n",
      "56c625c3e610378809c22760\n",
      "2016-03-15 05:33\n",
      "just had a small doubt; how to compile a .cpp file into a python extension? I generate the cpp file by using --cplus extension but how can I compile after this? Is there an easier way to do the whole process?\n",
      "\n",
      "1\n",
      "2225\n",
      "56c625c3e610378809c22760\n",
      "2016-03-15 05:57\n",
      "no probs. running `python setup.py build_ext --inplace` from source works plus you can see the command for compiling a c++ file into a python extension\n",
      "\n",
      "1\n",
      "2226\n",
      "54c630d6db8155e6700f168d\n",
      "2016-03-15 15:43\n",
      "Hey everybody, I'm struggling a little bit with understanding how I'm going to deploy a scikit learn algorithm which has been implemented using scaled feature values\n",
      "You see, I'm creating a calculator which'll do a logistic classification based upon a few values given by the user.\n",
      "And I don't get how  I can scale the values I get from the user using the same scaler as I used in the algorithm itself.\n",
      "Would love to get help if anybody's interested !\n",
      "\n",
      "4\n",
      "2227\n",
      "56cc7481e610378809c304aa\n",
      "2016-03-15 19:15\n",
      "@perborgen  I'm not sure if this is what you're looking for but, if you're using preprocessing.StandardScaler, you could set scale_, mean_ and variance_ explicitly once you've gotten those values from fitting the scalar with your training set. You'd only need to persist those somehow.\n",
      "\n",
      "1\n",
      "2228\n",
      "56b80528e610378809c05a48\n",
      "2016-03-16 06:09\n",
      "Hey guys, can anyone tell me why the CI failed at [this line](https://travis-ci.org/scikit-learn/scikit-learn/jobs/116304478#L2329)? According to the [doc](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.rand.html), scipy.sparse.rand() should accept `random_state`as an argument.\n",
      "\n",
      "1\n",
      "2229\n",
      "56b80528e610378809c05a48\n",
      "2016-03-16 06:10\n",
      "And there is no erro if I run `nosetests` on my own computer.\n",
      "Oh I got it ... CIs scipy version = 0.9\n",
      "\n",
      "2\n",
      "2230\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-18 08:45\n",
      "hi @rvraghav93\n",
      "\n",
      "1\n",
      "2231\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-18 14:34\n",
      "@rvraghav93  In relation to the email thread \"Class Weight Random Forest Classifier \" I should say that I remember it making no difference for me either when I tried it some time last year\n",
      "\n",
      "1\n",
      "2232\n",
      "56c625c3e610378809c22760\n",
      "2016-03-20 05:40\n",
      "Hey everyone, could some please review #6221? Just need a second affirmative on that one. Thanks!\n",
      "\n",
      "1\n",
      "2233\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-20 05:40\n",
      "sure ill take a look\n",
      "\n",
      "1\n",
      "2234\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-20 06:00\n",
      "done @dsquareindia :)\n",
      "\n",
      "1\n",
      "2235\n",
      "56c625c3e610378809c22760\n",
      "2016-03-20 06:02\n",
      "Thanks a lot @nelson-liu !\n",
      "\n",
      "2\n",
      "2236\n",
      "53135b495e986b0712efc453\n",
      "2016-03-21 19:44\n",
      "@dsquareindia @maniteja123 @nelson-liu @yenchenlin1994 Please make sure you submit your proposals soon into the withgoogle website. I believe the deadline is in less than 3 days... Thanks and good luck :)\n",
      "\n",
      "1\n",
      "2237\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-21 19:45\n",
      "Will do, thanks for the reminder!\n",
      "\n",
      "1\n",
      "2238\n",
      "53135b495e986b0712efc453\n",
      "2016-03-21 19:47\n",
      "I'm not sure who else is interested. If there is any other interested person, please make sure you submit it within the next 3 days as you won't be able to do so after the deadline... (don't worry about it being perfect. **Just make sure that you clearly outline, at a high level what you wish to achieve within the timeslot**...)\n",
      "\n",
      "1\n",
      "2239\n",
      "55a36f535e0d51bd787b3400\n",
      "2016-03-21 20:34\n",
      "Hi everyone, (I'm not sure this is the best place to ask; else let me know.).  I'm trying to cross-val a KMeans clustering and retrieve the most likely cluster:  ``` kmeans = list() for x in X:     dist = pairwise_distances(x)     kmeans.append(KMeans().fit_predict(dist)) ```  Although the clusters are very similar across iterations, the cluster labels are (obviously) random.  Do you know how I can aggregate these labels to find the most robust clusters across iterations: e.g.  `cluster_idx = scipy.stats.mode([sample for sample in kmeans])`\n",
      "\n",
      "1\n",
      "2240\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-21 21:24\n",
      "Hmm, you should x-post to stackoverflow. Theyd probably be more responsive\n",
      "\n",
      "1\n",
      "2241\n",
      "55a36f535e0d51bd787b3400\n",
      "2016-03-21 21:24\n",
      "@nelson-liu ok thanks!\n",
      "http://stats.stackexchange.com/questions/202883/how-to-combine-the-results-of-several-clustering-with-scikit-learn\n",
      "\n",
      "2\n",
      "2242\n",
      "56b80528e610378809c05a48\n",
      "2016-03-22 00:09\n",
      "Great thanks for your reminder!\n",
      "\n",
      "1\n",
      "2243\n",
      "56c625c3e610378809c22760\n",
      "2016-03-22 06:13\n",
      "I had some doubts regarding the project which I have listed in my [proposal](https://github.com/scikit-learn/scikit-learn/wiki/%5BDevashish%5D-GSoC-2016-project-proposal:-Adding-fused-types-to-Cython-files) itself.  It would be wonderful if anyone could give their opinions on them. Thanks!\n",
      "\n",
      "2\n",
      "2244\n",
      "56c625c3e610378809c22760\n",
      "2016-03-22 06:17\n",
      "yeah there wouldn't be a huge difference by adding fused types there right? I could work on that later after crucial modules have already been worked on. wdyt?\n",
      "\n",
      "2\n",
      "2245\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-22 06:19\n",
      "I agree with yen\n",
      "\n",
      "1\n",
      "2246\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-22 11:39\n",
      "hi @rvraghav93\n",
      "\n",
      "1\n",
      "2247\n",
      "53135b495e986b0712efc453\n",
      "2016-03-22 13:43\n",
      "Hi!\n",
      "\n",
      "1\n",
      "2248\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-22 14:50\n",
      "@rvraghav93  Hi. Have you a moment to discuss the categorical features/random forest/benchmark issue?\n",
      "\n",
      "1\n",
      "2249\n",
      "53135b495e986b0712efc453\n",
      "2016-03-22 15:07\n",
      "@lesshaste Yup!\n",
      "\n",
      "1\n",
      "2250\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-22 15:09\n",
      "@rvraghav93  great!  So... a) what is going on? :) What I mean is, do xgboost and H20 actually support categorical variables at all?\n",
      "\n",
      "1\n",
      "2251\n",
      "53135b495e986b0712efc453\n",
      "2016-03-22 15:19\n",
      "Yes apparently they do... :/ We are working on that and we'll become awesome in a few more months B)\n",
      "\n",
      "1\n",
      "2252\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-22 15:20\n",
      "@rvraghav93  ok but the comment on the PR was that it would actually not help\n",
      "which is what confused me\n",
      "\n",
      "2\n",
      "2253\n",
      "53135b495e986b0712efc453\n",
      "2016-03-22 15:23\n",
      "No I definitely do think introducing native support for categorical variables would indeed speed up our rf\n",
      "Hmmm I didn't see that.. Give me a moment!\n",
      "\n",
      "2\n",
      "2254\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-22 15:25\n",
      "\"It looks like they are using decision tree-based classifiers (i.e., RandomForestClassifier and GradientBoostingClassifier) rather than extra-random tree-based classifiers. And it looks like their dataset's categorical features (airlines, origin & destination airports) probably have cardinality > 64. These two factors together mean NOCATS can't be used.\"\n",
      "did you see that?\n",
      "thanks\n",
      "\n",
      "3\n",
      "2255\n",
      "53135b495e986b0712efc453\n",
      "2016-03-22 15:34\n",
      "Okay so I think before benchmarking against H2O and xgb. We need to make the splitting of categories locally optimal (we should decide what way the categories go at each split) and not just globally optimal. Then if the cardinality is > 64, we need to investigate why they support such high cardinality and whether or not we could do the same...\n",
      "I think even R has restrictions on the cardinality of the categorical features...\n",
      "How about rpart? I hear good things about it...\n",
      "\n",
      "3\n",
      "2256\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-22 15:35\n",
      "R does but the R RF code is bad\n",
      "well.. the default version is.. there are better versions and people also use xgboost with R\n",
      "rpart maybe be better .. hmm which version do they use in their benchmark?\n",
      "http://www.wise.io/tech/benchmarking-random-forest-part-1 is another example that shows how bad R randomForest is though\n",
      "ok thanks.  There is also H20 but I don't know how well their implementation us\n",
      "in any case.. it would be great to have somewhere where concrete improvements relevant to that benchmark could be discussed. It all seems slightly confusing at the moment\n",
      "\n",
      "6\n",
      "2257\n",
      "53135b495e986b0712efc453\n",
      "2016-03-22 15:39\n",
      "Also one another thing to note is that xgboost works somewhat differently compared to sklearn's rf as they seem to use approximate splitting and a second order objective as described in the paper that got recently published by Tianqi Chen...  You should look into that paper... I think there is a section which briefly explains why they are faster than us... I haven't had time to take a good look into that paper. But if you do, please share your insights...\n",
      "I think our top priority, as far as the tree based modules are concerned, is to merge the missing value support and the categorical variable support soon into scikit-learn... Once that is done we can think of making it better comparing it with xgboost...\n",
      "Maybe if these two are done, I'll see if I can make a blog post with readable code that compares the rf implementations... ;)\n",
      "yupp!\n",
      "No that one is a condensed version... wait\n",
      "http://arxiv.org/pdf/1603.02754v1.pdf\n",
      "\n",
      "6\n",
      "2258\n",
      "56b80528e610378809c05a48\n",
      "2016-03-22 15:42\n",
      "you mean [this paper ](http://learningsys.org/papers/LearningSys_2015_paper_32.pdf)\n",
      "?\n",
      "\n",
      "2\n",
      "2259\n",
      "56b80528e610378809c05a48\n",
      "2016-03-22 15:45\n",
      "Oh I am really interest in gradient boosting since Microsoft use it to [learn how to play Minecraft](http://research.microsoft.com/en-us/um/people/alekha/arxiv_geql.pdf)\n",
      "Thanks for the link and sorry to interrupt :worried:\n",
      ":smile:\n",
      "\n",
      "3\n",
      "2260\n",
      "53135b495e986b0712efc453\n",
      "2016-03-22 15:45\n",
      "Or maybe both are same... I'm not sure... The last link is the one that I have on my table accumulating dust... Have to read it soon :@\n",
      "\n",
      "3\n",
      "2261\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-22 15:47\n",
      "@yenchenlin1994  interruptions welcome :)\n",
      "\n",
      "1\n",
      "2262\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-22 15:48\n",
      "@rvraghav93  one more dim question.. :) I see in the NOCATS PR (RandomForestClassifier, full, One-hot) AUC: 0.712132537822 and  (RandomForestClassifier, truncated(8), NOCATS) AUC: 0.668807372591\n",
      "why is the second so much worse than the first?\n",
      "is it the truncated part?\n",
      "I assume so.. so is there no example that shows NOCATS doing better?\n",
      "I am not sure I understood \"the full dataset with NOCATS categorical splitting (actually no random forest in this case),\"\n",
      "\n",
      "5\n",
      "2263\n",
      "53a5cf04a9176b500d1ced1a\n",
      "2016-03-23 10:32\n",
      "Hi guys, how are you able to make imports like \"from sklearn.some_module import some_func\" from any python file. Have you changed sys.path from anywhere? Thanks\n",
      "Actually I was just browsing code.\n",
      "For example in this file\n",
      "https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/covariance/tests/test_covariance.py\n",
      "We are able to do `from sklearn.utils.testing import something`\n",
      "I was wondering how were you able to do that? Is there any configuration for this?\n",
      "\n",
      "6\n",
      "2264\n",
      "56b80528e610378809c05a48\n",
      "2016-03-23 10:33\n",
      "Can you `import sklearn`?\n",
      "\n",
      "1\n",
      "2265\n",
      "53a5cf04a9176b500d1ced1a\n",
      "2016-03-23 10:38\n",
      "@yenchenlin1994 there?\n",
      "\n",
      "1\n",
      "2266\n",
      "5525b91815522ed4b3deb7d6\n",
      "2016-03-23 10:53\n",
      "@SaurabhJha No, there isn't any. This is how the import system of python works.\n",
      "\n",
      "1\n",
      "2267\n",
      "56b80528e610378809c05a48\n",
      "2016-03-23 10:53\n",
      "a [reference](http://stackoverflow.com/questions/448271/what-is-init-py-for)\n",
      "\n",
      "1\n",
      "2268\n",
      "56b80528e610378809c05a48\n",
      "2016-03-23 10:59\n",
      "it shows how `__init.py__` works!\n",
      "\n",
      "2\n",
      "2269\n",
      "56b80528e610378809c05a48\n",
      "2016-03-23 11:04\n",
      "Hope this answers your question :smile:\n",
      "\n",
      "2\n",
      "2270\n",
      "56f2b66385d51f252aba60dd\n",
      "2016-03-23 15:30\n",
      "Hi. Trying to use LogisticRegression with multi_class='multinomial'. Ending up with this error:  __init__() got an unexpected keyword argument 'multi_class'\n",
      "sklearn version is '0.15.2'\n",
      "Can anybody please help?\n",
      "\n",
      "3\n",
      "2271\n",
      "56f2dfde85d51f252aba689b\n",
      "2016-03-24 05:53\n",
      "Hi @VarunKShetty , it seems that such parameter doesnt exist. Check the documentation to see which parameters can be used.\n",
      "\n",
      "1\n",
      "2272\n",
      "54c630d6db8155e6700f168d\n",
      "2016-03-24 09:16\n",
      "@ksafford Thanks a lot, that helped!\n",
      "\n",
      "1\n",
      "2273\n",
      "5552313315522ed4b3e0482a\n",
      "2016-03-24 13:29\n",
      "@VarunKShetty  LogisticRegression in 0.15.2 doesn't support multi_class, if you update to the 0.17.1 version it should work.\n",
      "\n",
      "1\n",
      "2274\n",
      "56f2b66385d51f252aba60dd\n",
      "2016-03-24 19:03\n",
      "Thanks @dvdnglnd\n",
      "I'll now go figure out how to update it\n",
      "\n",
      "2\n",
      "2275\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-24 22:43\n",
      "Hi everyone. I have a featureunion of several pipelines, is there any way I can turn the featureunion into a numpy array for use in other applications?\n",
      "e.g. in this case, Im using scikit-learn to do the preprocessing, and keras for the learning\n",
      "\n",
      "2\n",
      "2276\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-24 23:32\n",
      "You should be able to call transform to obtain an array\n",
      "As long as your pipelines have only transformers and no predictors\n",
      "\n",
      "2\n",
      "2277\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-03-24 23:38\n",
      "It doesn't really make sense to me to turn the \"feature union\" into an array, but to apply it on data to obtain an array. Or am I misunderstanding?\n",
      "\n",
      "3\n",
      "2278\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-03-25 20:29\n",
      " @ogrisel do you know what happened to the pdf docs? They are gone :-/\n",
      "\n",
      "1\n",
      "2279\n",
      "53810862048862e761fa2887\n",
      "2016-03-27 04:23\n",
      "@amueller Isn't ADMM guaranteed to produce an optimal solution irrespective of the value of `mu` ? I am referring to equation 5.1 [here](http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf)\n",
      "`mu` is multiplied by a factor which should be 0 for all feasible solutions.\n",
      "\n",
      "2\n",
      "2280\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-03-28 13:45\n",
      "@vighneshbirodkar well the solutions are not feasible\n",
      "and with very small mu they are very far from being feasible\n",
      "\n",
      "2\n",
      "2281\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-29 10:44\n",
      "hi @rvraghav93\n",
      "\n",
      "1\n",
      "2282\n",
      "551061f615522ed4b3ddb1c0\n",
      "2016-03-29 17:32\n",
      "Anyone can help on https://github.com/scikit-learn/scikit-learn/issues/6574?\n",
      "\n",
      "1\n",
      "2283\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-03-29 18:10\n",
      "@nelson-liu   I love your GSOC proposal by the way\n",
      "\n",
      "1\n",
      "2284\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-29 18:13\n",
      "thanks @lesshaste :)\n",
      "\n",
      "1\n",
      "2285\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-03-29 19:40\n",
      "The GMM documentation says that all components are initialized to mean 0, identity covariance.\n",
      "How is it possible to update the components if each component is identical?\n",
      "\n",
      "2\n",
      "2286\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-03-29 19:45\n",
      "I vaguely recall something about sklearn using kmeans++ to initialize the components somewhere. Maybe I'm imagining things.\n",
      "\n",
      "1\n",
      "2287\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-03-29 22:22\n",
      "The code seems to imply it uses a round of kmeans to initialize the components, but the documentation doesn't say that.\n",
      "\n",
      "1\n",
      "2288\n",
      "53135b495e986b0712efc453\n",
      "2016-03-29 22:50\n",
      "@lesshaste Hello! Sorry I've been a bit busy lately. ;(\n",
      "\n",
      "1\n",
      "2289\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-03-31 14:20\n",
      "I feel like I should know that, but does scikit-learn have a doi?\n",
      "\n",
      "1\n",
      "2290\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-03-31 21:58\n",
      "@amueller I dont think so? I just searched the docs and couldnt find it. It also isnt in the original publication. I also looked at a few papers that cited scikit-learn, but they omitted the DOI\n",
      "\n",
      "1\n",
      "2291\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-01 04:03\n",
      "I don't recall us having one.\n",
      "\n",
      "1\n",
      "2292\n",
      "56c3065ce610378809c1ab7d\n",
      "2016-04-01 15:08\n",
      "hi, pls i will need some explanation. if i have a honeypot logfile, and i want to apply neural network to the logfile, so i can get analysis type of the logfile, which i want to use to generate intrusion signature.\n",
      "\n",
      "1\n",
      "2293\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-04-01 15:11\n",
      "@ikennarene what is your question?\n",
      "\n",
      "1\n",
      "2294\n",
      "56c3065ce610378809c1ab7d\n",
      "2016-04-01 15:15\n",
      " i am thinking of applying classification and clustering algorithm on a honeypot logfile, after which i want to generate intrusion signature based on the algorithm results, pls can i get more explanation on this, and approach i can use if possible. thanks\n",
      "\n",
      "1\n",
      "2295\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-04-01 18:13\n",
      "so you want to detect intrusions? Do you know which ones were intrusions or not?\n",
      "\n",
      "1\n",
      "2296\n",
      "56c3065ce610378809c1ab7d\n",
      "2016-04-01 19:30\n",
      "@amueller unknown\n",
      "\n",
      "1\n",
      "2297\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-04-01 19:44\n",
      "try outlier detections methods like isolationforest\n",
      "\n",
      "1\n",
      "2298\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-01 21:50\n",
      "hi everyone. Im working with countvectorizer, and I already have a corpus that has been pretokenized and everything. How would i extract ngrams from it? setting `analyzer=str.split()` breaks the ngram_range argument. Would i have to write my own analyzer? setting the default (`analyzer=word`) does not work for me because that strips punctuation and I want to keep punctuation.\n",
      "\n",
      "1\n",
      "2299\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-04-02 08:41\n",
      "@rvraghav93  no problem at all!\n",
      "\n",
      "1\n",
      "2300\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-04-02 09:44\n",
      "http://scikit-learn.org/dev/auto_examples/ensemble/plot_isolation_forest.html has a nice looking example but it would be great if it had a little more explanation about the decision boundaries? Why are there 4 areas that are non-anomalous and not just two for example?\n",
      "\n",
      "1\n",
      "2301\n",
      "5592d95215522ed4b3e31c79\n",
      "2016-04-03 09:27\n",
      "hello everyone\n",
      "\n",
      "1\n",
      "2302\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-03 12:02\n",
      "Nrlson-liu it should be str.split rather than str.split()\n",
      "\n",
      "1\n",
      "2303\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-03 19:01\n",
      "ah, yeah sorry that was a typo on my part. i ended up just making a custom analyzer.\n",
      "\n",
      "2\n",
      "2304\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-03 19:13\n",
      "using str.split seems to break ngram_range, though?\n",
      "\n",
      "1\n",
      "2305\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-03 19:15\n",
      "e.g.  ``` >>> from sklearn.feature_extraction.text import CountVectorizer >>> ngram_vectorizer = CountVectorizer(analyzer=str.split, ngram_range=(1, 2)) >>> ngram_vectorizer.fit_transform(['The quick brown fox jumped over the lazy dog .']) <1x10 sparse matrix of type '<type 'numpy.int64'>' \twith 10 stored elements in Compressed Sparse Row format> >>> print ngram_vectorizer.get_feature_names() ['.', 'The', 'brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the] ```\n",
      "looking at the code for `CountVectorizer`, it seems like the analyzer argument is also responsible for making ngrams. So `str.split` would only make unigrams?\n",
      "\n",
      "2\n",
      "2306\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-04-05 01:37\n",
      "@jnothman I am here now :)\n",
      "\n",
      "1\n",
      "2307\n",
      "54b2524adb8155e6700e8a8e\n",
      "2016-04-05 02:12\n",
      "Okay. I think you're misinterpreting the numpy error's relevance to this situation.\n",
      "But np.where(mask_matrix.max(axis=0))[0] might not quite work, because `mask_matrix.max(axis=0)` returns a 2d matrix.\n",
      "`np.flatnonzero(mask_matrix.max(axis=0))` should fix that issue, though\n",
      "I don't get at all how you could be getting an error on a *print* statement!\n",
      "I hope you got that @maniteja123\n",
      "\n",
      "7\n",
      "2308\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-04-05 02:22\n",
      "Also ``np.flatnonzero`` gives an error ``AttributeError: ravel not found``\n",
      "\n",
      "1\n",
      "2309\n",
      "54b2524adb8155e6700e8a8e\n",
      "2016-04-05 02:22\n",
      "Oh. Okay. I'm not in the right frame of mind. This should be easy for me!\n",
      "np.where(masked_matrix.max(axis=0).toarray().ravel())[0] should work!\n",
      "or equivalently: `np.flatnonzero(masked_matrix.max(axis=0))`\n",
      "or just\n",
      "masked_matrix.nonzero()[1]\n",
      "sorry,\n",
      "`masked_matrix.max(axis=0).nonzero()[1]`\n",
      "\n",
      "7\n",
      "2310\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-04-05 02:30\n",
      "Thanks I got the idea now.  Will implement and get back to you.\n",
      "\n",
      "1\n",
      "2311\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-04-05 02:37\n",
      "``` (0, 0)\t4.0   (1, 0)\t6.0   (2, 0)\t7.0   (0, 1)\t2.0   (1, 1)\t3.66666666667   (2, 1)\t6.0   (0, 2)\t1.0   (1, 3)\t1.0 ``` This is the transformed X for X = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]]). I think it is working now as you expected.  Thanks for all the help.\n",
      "\n",
      "5\n",
      "2312\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-04-05 02:48\n",
      "Oh okay got it.. Building the matrix using the ``_with_data`` logic still populates the zeroes and needs to be manually removed to make it sparse.\n",
      "\n",
      "1\n",
      "2313\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-04-05 02:54\n",
      "I will add some tests for sparse matrices too. Is there anything else you want me to look into here ? I need to go to my college now. Won't have access to the internet. Sorry. Will reply by evening. Thanks again for all the help.\n",
      "\n",
      "1\n",
      "2314\n",
      "56a12a1de610378809bd831b\n",
      "2016-04-05 14:00\n",
      "Hi all.  Is scikit-learn 0.17.1 the recommended version to use for new projects?  Thanks.\n",
      "\n",
      "1\n",
      "2315\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-05 15:23\n",
      "@staffhorn generally, yes\n",
      "\n",
      "1\n",
      "2316\n",
      "56b80528e610378809c05a48\n",
      "2016-04-05 15:40\n",
      "Anyone can help me review [this PR]? (https://github.com/scikit-learn/scikit-learn/pull/6593) :pray:\n",
      "\n",
      "1\n",
      "2317\n",
      "56b80528e610378809c05a48\n",
      "2016-04-07 16:18\n",
      "Hey guys\n",
      "\n",
      "1\n",
      "2318\n",
      "56b80528e610378809c05a48\n",
      "2016-04-07 16:19\n",
      "Whats the recommended way to recompile a single .pyx file after I modify it?\n",
      "Yeah @nelson-liu you are right, only compile is not what I want haha\n",
      "\n",
      "2\n",
      "2319\n",
      "53135b495e986b0712efc453\n",
      "2016-04-07 16:20\n",
      "You just do the `python setup.py build_ext` or `python setup.py build_ext -i` (for inplace building) again. It generates c and recompiles for the changed cython sources only.\n",
      ";)\n",
      "\n",
      "2\n",
      "2320\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-07 16:20\n",
      "if you just want to compile, you could do `cython primes.pyx`. but you probably want to build in place again for it all to work together.\n",
      "yup, what @rvraghav93 said\n",
      "\n",
      "2\n",
      "2321\n",
      "56b80528e610378809c05a48\n",
      "2016-04-07 16:21\n",
      "oh okay thanks for your quick help :smile:\n",
      "\n",
      "1\n",
      "2322\n",
      "53135b495e986b0712efc453\n",
      "2016-04-07 16:29\n",
      "> Anyone can help me review [this PR]?  ![](https://i.imgur.com/FNqNVVw.png) ;P\n",
      "\n",
      "5\n",
      "2323\n",
      "56b80528e610378809c05a48\n",
      "2016-04-07 16:44\n",
      ":satisfied:\n",
      "\n",
      "1\n",
      "2324\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-07 17:04\n",
      "Do we have a make target to cythonize only what's needed?\n",
      "\n",
      "1\n",
      "2325\n",
      "53135b495e986b0712efc453\n",
      "2016-04-07 17:13\n",
      "By only what's needed, you mean to say - Only specific modules?\n",
      "\n",
      "1\n",
      "2326\n",
      "55866cb115522ed4b3e23aa4\n",
      "2016-04-07 17:33\n",
      "is python 3.x fully supported by scikit\n",
      "\n",
      "4\n",
      "2327\n",
      "55866cb115522ed4b3e23aa4\n",
      "2016-04-07 17:35\n",
      "in that case if i want to get into contributing to scikit, using python 3 syntax for `print` should not be a problem in pull requests\n",
      "\n",
      "4\n",
      "2328\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-07 19:28\n",
      "@rvraghav93 / others working with the tree module - if youve read breimans <unconvertable> Classification and Regression Trees, would you recommend it as a resource to get familiar with the theory?\n",
      "might as well tag @glouppe as well\n",
      "\n",
      "2\n",
      "2329\n",
      "56c63d67e610378809c22b14\n",
      "2016-04-07 20:42\n",
      "i'd like to do classification/regression with multiple outputs, but each example only has the observed labels for one of the outputs. Although the DataSet object has entires for masks, which seems like it should fit the bill, I'm getting the impression that the masks only works for rnns and not for feedfoward classification/regression nets. Is that correct?\n",
      "\n",
      "1\n",
      "2330\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-07 20:44\n",
      "can you link the example you were looking at?\n",
      "\n",
      "1\n",
      "2331\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-04-08 11:05\n",
      "Hi guys\n",
      "\n",
      "1\n",
      "2332\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-04-08 11:30\n",
      "I have images with different objects in them such as trees, grass,river etc and want to classify which object is present in each image. Can anyone help me do the following a) load all the images one time from a folder b)Extract features b) Concatenate all features features into a matrix or a vector. This is my link to the working files https://github.com/Ben-Kobby/My-Project.git. Thank you.\n",
      "\n",
      "1\n",
      "2333\n",
      "56c625c3e610378809c22760\n",
      "2016-04-08 11:34\n",
      "Hey everyone, just had a small doubt. I'm trying to implement a feature selection algorithm which uses a term-category matrix. I'm trying to implement it on the 20NG dataset. Currently I'm using `CountVectorizer` to produce the term-doc matrix (by  transposing the output of `fit_transform`). However I want to create a term-category matrix from this by mapping the count in these docs to another matrix with 20 columns. What would be the most efficient way to do this mapping? Thanks in advance!\n",
      "\n",
      "1\n",
      "2334\n",
      "53135b495e986b0712efc453\n",
      "2016-04-08 13:11\n",
      "@nelson-liu Yes its a good resource, but is pretty huge. You could start with some simple youtube videos/blog posts explaining the concept of CART and learn more as you go (atleast that is what I did)... Please feel free to ping myself or Jacob (here or in e-mail) if you need any help.\n",
      "\n",
      "1\n",
      "2335\n",
      "570beab4187bb6f0eadeef70\n",
      "2016-04-14 20:56\n",
      "hey all, is there anyone experienced using LSTM-RNN for labeling each frame of a video data\n",
      "\n",
      "1\n",
      "2336\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-04-14 22:11\n",
      "No,  but I can sound confident when I tell you stuff.\n",
      "\n",
      "1\n",
      "2337\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-14 22:12\n",
      "haha :smile:\n",
      "\n",
      "1\n",
      "2338\n",
      "557fdd5f15522ed4b3e1f9bd\n",
      "2016-04-14 22:14\n",
      "@dsquareindia I don't think your question is answerable as currently stated. How do you intend to map counts to categories?\n",
      "\n",
      "1\n",
      "2339\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-14 22:15\n",
      "@dsquareindia it sounds like you want to use [LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to get a categorical one-hot Y, then just do Y.T * X\n",
      "where X is the output of CountVectorizer. This should give you word counts per category.\n",
      "(I'm using * instead of dot because X is sparse)\n",
      "\n",
      "5\n",
      "2340\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-14 22:49\n",
      "You can find reason to count word-category occurrences without making that strong assumption. Maybe he's using the count matrix to do some other calculations afterwards.\n",
      "\n",
      "1\n",
      "2341\n",
      "557fdd5f15522ed4b3e1f9bd\n",
      "2016-04-15 01:14\n",
      "@vene makes sense, thanks.\n",
      "\n",
      "1\n",
      "2342\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-15 01:20\n",
      "I think our chi2 feature selector is asking these lines\n",
      "*along these lines\n",
      "\n",
      "2\n",
      "2343\n",
      "56c625c3e610378809c22760\n",
      "2016-04-15 09:04\n",
      "Thanks a lot @Zintinio and @vene for the help! Yes I want to use that count matrix for some further calculations. Also, yes I'm making the assumption that if a doc is labeled X then the term belonging to that doc also belongs to the category X. Basically if a particular term occurs in a document categorized as X, then in the term-cat matrix the entry correspoding to that term and cat (X in this case) is updated by one. I then assign certain weights for each term for each category and see how much that term contributes to each category. This is in turn used in performing feature selection.\n",
      "\n",
      "1\n",
      "2344\n",
      "570beab4187bb6f0eadeef70\n",
      "2016-04-15 11:16\n",
      "hey all, is there anyone experienced using LSTM-RNN for labeling each frame of a video data\n",
      "\n",
      "1\n",
      "2345\n",
      "53135b495e986b0712efc453\n",
      "2016-04-15 11:49\n",
      "@oakkas re-posting the same question will not get you a response ;) If you have a particular question that is not suitable for a quick discussion, I would suggest that you post it as a thread to our mailing list or stackoverflow. If people know about it and your question interests them, they will respond in detail.\n",
      "That being said, LSTM-RNN is deep learning stuff that is not a part of scikit-learn. You would be better off contacting the Mailing List of some deep learning library like theano, tensorflow or caffe... Or even better like I said before stackoverflow.\n",
      "\n",
      "2\n",
      "2346\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-04-15 15:24\n",
      "reddit seems to be decent for deep learning discussions these days\n",
      "\n",
      "1\n",
      "2347\n",
      "53810862048862e761fa2887\n",
      "2016-04-15 20:03\n",
      "Any idea whats happening here ? Due to some reason numpy does not print strings with quotes\n",
      "https://travis-ci.org/scikit-learn/scikit-learn/jobs/123430159\n",
      "\n",
      "2\n",
      "2348\n",
      "544906e2db8155e6700cdd16\n",
      "2016-04-18 21:01\n",
      "hi everyone, I'd like to compare different scorers for univariate feature selection. Currently scikit-learn provides chi2 and f_classif but I'd like to use others like document frequency, infogain and bi-normal separation too (here is a comparative study http://www.jmlr.org/papers/volume3/forman03a/forman03a.pdf).  I've already written a vectorized implementation of infogain but according to the chi2 documentation (http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2) it seems that it should return the infogain scores along with an array containing the p_values of each feature. How should I calculate/interpret that p_values?\n",
      "\n",
      "1\n",
      "2349\n",
      "530c03e25e986b0712efafb8\n",
      "2016-04-19 00:24\n",
      "Howdy folks, I've been playing with sklearn for an example using [dask.distributed](http://github.com/dask/distributed) `Futures`.  I suspect that I'm making poor choices regarding machine learning and would appreciate feedback.  https://gist.github.com/mrocklin/80b0d6f57dedc1628954ced5ef5500b0\n",
      "\n",
      "1\n",
      "2350\n",
      "530c03e25e986b0712efafb8\n",
      "2016-04-20 15:43\n",
      "http://matthewrocklin.com/blog/work/2016/04/20/dask-distributed-part-5\n",
      "\n",
      "1\n",
      "2351\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-20 16:43\n",
      "I read this blog post recently describing how the Atom editor developers manage github issues, and i think lots of the advice given could apply to  scikit-learn as well! http://blog.atom.io/2016/04/19/managing-the-deluge-of-atom-issues.html\n",
      "\n",
      "1\n",
      "2352\n",
      "541a528b163965c9bc2053de\n",
      "2016-04-21 08:01\n",
      "Sorry too late to reply: I commented on your post. I think besides the randomized parameter search or exhaustive grid search use case, the other common machine learning use case that really benefit from distributed training is gradient boosted trees. The implementation of boosted trees in scikit-learn is not really amenable to cluster-wise distribution in its current form. However xgboost is really mature in that regard and already provides hadoop yarn integration. I think it would run great on top of dask.distributed. They also plan better integration with pandas dataframe but this is still on the roadmap.\n",
      "\n",
      "1\n",
      "2353\n",
      "569ebe44e610378809bd2db4\n",
      "2016-04-21 15:18\n",
      "I am a little concerned about example http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html#example-feature-selection-plot-feature-selection-py\n",
      "there is not blue bar for the second feature\n",
      "and smaller bar for the forth one\n",
      "thus I am not sure how the example proves that feature selection improves SVM..\n",
      "\n",
      "4\n",
      "2354\n",
      "5693aff116b6c7089cc207c3\n",
      "2016-04-24 06:09\n",
      "Hi all,\n",
      "\n",
      "1\n",
      "2355\n",
      "5693aff116b6c7089cc207c3\n",
      "2016-04-24 06:12\n",
      "I am new to sklearn, and I really like this stuff. I want to contribute, but I think, I must use sklearn to a good level. And then start diving into methods. Can you suggest me a way to start? At end I want to be a one of the core contributor of sklearn, no matter how much time or years it takes.\n",
      "And I can start using sklearn from basics here?\n",
      "\n",
      "2\n",
      "2356\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-24 06:13\n",
      "A good place to start is to try kaggle competitions\n",
      "\n",
      "1\n",
      "2357\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-04-24 06:16\n",
      "are you trying to learn sklearn or are yout rying to learn data science?\n",
      "\n",
      "5\n",
      "2358\n",
      "5693aff116b6c7089cc207c3\n",
      "2016-04-24 06:19\n",
      "Yes sklearn. I am a pythonist, and I want to dwelve into sklearn functions and contribute. Right way is to strengthen my basics. I think I can go with what @nelson-liu  suggested.\n",
      "\n",
      "4\n",
      "2359\n",
      "56b80528e610378809c05a48\n",
      "2016-04-24 06:25\n",
      "If the goal is to contribute, then [here](http://scikit-learn.org/stable/developers/contributing.html#easy-issues) has already answered your question\n",
      "\n",
      "1\n",
      "2360\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-04-24 20:34\n",
      "@harshul1610 To answer your question:  Implementing a machine-learning algorithm is hard.  Its not enough to be able to program.  You have to really understand the *math*, and debugging the math is challenging.  And you have to also understand how data scientists actually use these algorithms.\n",
      "\n",
      "1\n",
      "2361\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-04-28 10:44\n",
      "Hi.. I have a binary classification task with 300 positive examples 300 million negative\n",
      "Is there a sensible way to handle this?\n",
      "well I would like to use the knowledge about the  300 million negative examples to learn what \"normal\" looks like\n",
      "@hmha  I could just throw it at a random forest and ignore the massive skew. Is that a sensible thing to do?\n",
      "\n",
      "7\n",
      "2362\n",
      "569ebe44e610378809bd2db4\n",
      "2016-04-28 10:47\n",
      "what do you mean by \"to handle\" ?\n",
      "\n",
      "1\n",
      "2363\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-04-28 10:53\n",
      "@hmha  no problem at all\n",
      "it would be nice if some of these were in scikit-learn https://github.com/fmfn/UnbalancedDataset\n",
      "\n",
      "2\n",
      "2364\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-28 11:42\n",
      "If you think a linear model could work, you could optimize for AUC by training on pairwise differences of positive and negative examples.\n",
      "\n",
      "1\n",
      "2365\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-28 12:51\n",
      "there are a lot indeed, but you could stochastically subsample the pairs and do partial_fit sgd iterations\n",
      "it's the idea used in sofia-ml, which is pretty great\n",
      "\n",
      "2\n",
      "2366\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-04-28 13:47\n",
      "now I need to look up sofia-ml! :)\n",
      "\n",
      "1\n",
      "2367\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-04-28 13:58\n",
      "@vene  but for my problem I can't see that linear models would work\n",
      "the \"numerical data\" has special values that seem to indicate particular things. So 1,56,123 have some meaning from 200-10000 don't for example. Except I don't get told what those are\n",
      "random forests are good at picking these out\n",
      "\n",
      "3\n",
      "2368\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-28 15:05\n",
      "true. You could discretize the data, I guess. In random forests you can just use sample weights to deal with class imbalance.\n",
      "actually it seems now you can actually use `class_weight=\"balanced\"`\n",
      "@lesshaste I'm not saying there is no bug there, I am not familiar with the code\n",
      "but I'm saying that, even with linear models, `class_weight=\"balanced` does not necessarily lead to better generalization\n",
      "\n",
      "4\n",
      "2369\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-04-28 15:06\n",
      "the problem is its not clear a priori how to bin the numerical data, if that is what you mean\n",
      "I am not 100% convinced that actually does anything :)\n",
      "there is an issue about that I think\n",
      "but I will certainly try that\n",
      "\n",
      "4\n",
      "2370\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-28 15:45\n",
      "it should reweigh the samples accordingly. The problem is, that isn't guaranteed to be better.\n",
      "\n",
      "3\n",
      "2371\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-04-28 23:50\n",
      "If you're into deep learning stuff, you can use a similar sampling strategy + pairwise training there. I think they call it \"contrastive loss\" in that world :)\n",
      "\n",
      "1\n",
      "2372\n",
      "5629b22416b6c7089cb7f6f7\n",
      "2016-04-29 04:04\n",
      "@lesshaste other options for imbalanced data: 1) when it is that skewed, try anomaly detection. 2) I found this downsampling+bagging from Wallace et. al. to be principled approach (https://scholar.google.com/scholar?cluster=225520837537786880&hl=en&as_sdt=0,5&as_vis=1)\n",
      "\n",
      "1\n",
      "2373\n",
      "53810862048862e761fa2887\n",
      "2016-04-29 06:07\n",
      "@ogrisel @amueller  You think it's necessary to upload the wheels of the template project on Rackspace ? Granted the wheels won't be of much use, but it could serve as an example for projects which clone it.\n",
      "\n",
      "1\n",
      "2374\n",
      "54bd1809db8155e6700ed1e4\n",
      "2016-04-29 08:57\n",
      "does anyone know when the argument `loss_func` was removed from GridSearchCV? Cannot find it in the changelog\n",
      "\n",
      "1\n",
      "2375\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-29 09:44\n",
      "@HolgerPeters https://github.com/scikit-learn/scikit-learn/pull/3411\n",
      "Seems like it was planned to be removed in 0.15, but might have been removed later. I couldnt find any mention of it in the changelog as well.\n",
      "\n",
      "2\n",
      "2376\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-04-29 09:49\n",
      "date on that is july 17, 2014. 0.15.0 was released on July 15 2014, and 0.15.1 was released on Aug 1 2014. Im presuming it was thus gone by 0.15.1?\n",
      "\n",
      "1\n",
      "2377\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-04-29 13:15\n",
      "@cfperez  thanks very much\n",
      "@vene I will look up \"contrastive loss\" thanks\n",
      "\n",
      "2\n",
      "2378\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-04-29 13:17\n",
      "@cfperez  in my case I am wondering whether the extreme nature of the class imbalance (300 versus 300 millions) means that black box methods might not be appropriate\n",
      "that is i may have to do something other than try to infer the model from the data\n",
      "\n",
      "2\n",
      "2379\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-04-29 14:29\n",
      "@vighneshbirodkar Sounds like a good idea\n",
      "\n",
      "1\n",
      "2380\n",
      "57231db2659847a7aff51acf\n",
      "2016-04-30 12:18\n",
      "@cfperez  downsampling and probably boosting?\n",
      "\n",
      "1\n",
      "2381\n",
      "53135b495e986b0712efc453\n",
      "2016-05-03 15:17\n",
      "@amueller @ogrisel when is the 0.18 release?\n",
      "\n",
      "1\n",
      "2382\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-05-05 05:30\n",
      "@vene How do you do a random forest embedding?\n",
      "Oh...reading about it now.\n",
      "\n",
      "2\n",
      "2383\n",
      "56b80528e610378809c05a48\n",
      "2016-05-05 07:11\n",
      "hello @rvraghav93   Really sorry for the late reply, just survived from my midterm :smile:  Yeah Ill send it to the mailing list today!\n",
      "haha you too\n",
      "\n",
      "2\n",
      "2384\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-05-05 07:11\n",
      " yay, congrats!\n",
      "\n",
      "1\n",
      "2385\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-05-05 08:35\n",
      "when was random forest embedding added to scikit-learn?\n",
      "@vene  thanks I will try that idea. Do you happen to know when was random forest embedding added to scikit-learn?\n",
      "oh.. 0.13! Not sure how I missed it\n",
      "do you have a view about https://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering.htm ?\n",
      "I can't quite tell if it would provide something significantly new to what scikit learn already has\n",
      "if you follow the citations it seems isolation forests come from a similar idea\n",
      "I could open an issue I suppose but I feel a little ignorant on this topic\n",
      "@amueller  As the author of the random forest embedding, does this provide anything extra?\n",
      "@amueller  where \"this\" is https://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering.htm\n",
      "\n",
      "9\n",
      "2386\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-05-05 16:18\n",
      "I think at least 2ish releases ago\n",
      "Oh\n",
      "\n",
      "2\n",
      "2387\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-05-05 16:26\n",
      "That tech report is really hard to skim\n",
      "@lesshaste, the random forest embedding uses totally randomized trees, ie there is no learning\n",
      "\n",
      "2\n",
      "2388\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-05-05 16:28\n",
      "yes... I did try myself.   http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolationThis performs a comparison with this method apparently\n",
      "\" In the first experiment we compare iForest with ORCA [3], LOF [6] and Random Forests (RF) [12]\" where [12] is https://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering.htm\n",
      "\n",
      "2\n",
      "2389\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-05-05 16:30\n",
      "I don't know anything about how isolation forests work. But it seems that the RFclustering approach trains a discriminative RF between the real data and randomly sampled data.\n",
      "\n",
      "9\n",
      "2390\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-05-05 16:33\n",
      "what methods currently in scikit learn are suitable when the distance between features is highly non-linear? That is not at all Euclidean\n",
      "\n",
      "11\n",
      "2391\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-05-05 16:41\n",
      "Well, discriminative methods, even linear ones, could capture such a threshold I think\n",
      "\n",
      "1\n",
      "2392\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-05-05 17:16\n",
      "interesting.. I did try linear regression on some labelled data and it was a disaster\n",
      "where a random forest worked really well\n",
      "I mean the linear regression essentially failed completely\n",
      "\n",
      "3\n",
      "2393\n",
      "572e2e2bc43b8c601971aa31\n",
      "2016-05-07 18:08\n",
      "hey guys i'm new here\n",
      "needed some help  regarding machine learning\n",
      "i've just started studying about machine learning\n",
      "and everywhere i read that machine learning course on Coursera by Andrew Ng\n",
      "is good to begin with\n",
      "but it is not focused on python\n",
      "and i want to use machine learning in python\n",
      "so should i consider learning from somewhere else?\n",
      "help me out\n",
      "and what would you recommend on some good resources\n",
      "\n",
      "15\n",
      "2394\n",
      "572d31b4c43b8c6019718fca\n",
      "2016-05-07 18:08\n",
      "Whats up? :)\n",
      "\n",
      "1\n",
      "2395\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-05-07 18:11\n",
      "You can do the assignments in Python if you want to\n",
      "\n",
      "2\n",
      "2396\n",
      "549fecafdb8155e6700e3675\n",
      "2016-05-10 10:09\n",
      "Has autoencoder not been implemented in scikit ?\n",
      "\n",
      "1\n",
      "2397\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-05-10 10:43\n",
      "I suppose #2099 is a WIP for sparse auto encoder.\n",
      "\n",
      "1\n",
      "2398\n",
      "56b0803ee610378809bf7535\n",
      "2016-05-10 11:18\n",
      "Hello all, every time I get the error: \"ERROR:py4j.java_gateway:Error while sending or receiving. Traceback (most recent call last):   File \"/data/analytics/Spark1.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 746, in send_command     raise Py4JError(\"Answer from Java side is empty\") Py4JError: Answer from Java side is empty ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server Traceback (most recent call last):   File \"/data/analytics/Spark1.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 690, in start     self.socket.connect((self.address, self.port))   File \"/usr/local/anaconda/lib/python2.7/socket.py\", line 228, in meth     return getattr(self._sock,name)(*args) error: [Errno 111] Connection refused ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server Traceback (most recent call last):   File \"/data/analytics/Spark1.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 690, in start     self.socket.connect((self.address, self.port))   File \"/usr/local/anaconda/lib/python2.7/socket.py\", line 228, in meth     return getattr(self._sock,name)(*args) error: [Errno 111] Connection refused\". My conf-file: \"spark.serializer org.apache.spark.serializer.KryoSerializer  spark.kryoserializer.buffer.max 1500mb spark.driver.memory 65g #spark.driver.extraJavaOptions -Djava.io.tmpdir=/data/spark-tmp  spark.driver.extraJavaOptions -XX:-PrintGCDetails -XX:-PrintGCTimeStamps -XX:-PrintTenuringDistribution #XX:PermSize=20480m  spark.python.worker.memory 65g spark.local.dir /data/spark-tmp\" The amount of data is about 5Gb.\n",
      "Does anybody know the answer?\n",
      "\n",
      "2\n",
      "2399\n",
      "541a528b163965c9bc2053de\n",
      "2016-05-10 11:24\n",
      "@AlexanderModestov please do not paste large (unquoted) error messages in the conversation but instead use a link to some gist or pastebin. Furthermore this seems to be specific to spark and not related to scikit-learn (the `sklearn` package does not even occur in the traceback) so I don't think this is the right place to ask such a question. You might want to ask this question on stackoverflow with the spark tag.\n",
      "\n",
      "1\n",
      "2400\n",
      "56b0803ee610378809bf7535\n",
      "2016-05-10 11:41\n",
      "@ogrisel I'm sorry. It's not about sklearn ...\n",
      "\n",
      "1\n",
      "2401\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-05-10 14:13\n",
      "@vene  Hi. Sorry I meant logistic regression\n",
      "\n",
      "1\n",
      "2402\n",
      "53135b495e986b0712efc453\n",
      "2016-05-11 02:29\n",
      "@amueller By making GridSearchCV work well with EstimatorCV did you mean implementing generalized cross-validation? (https://github.com/scikit-learn/scikit-learn/issues/1626)\n",
      "\n",
      "1\n",
      "2403\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-11 02:30\n",
      "is anyone here very competitive about the relative capabilities of scikit-learn and R?  @amueller ?\n",
      "... because with the R package I just pushed to my git, I would really enhjoy some trash talking\n",
      "\n",
      "2\n",
      "2404\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-05-11 14:47\n",
      "@elbamos well I'd like to be scikit-learn as good as possible, and if you have an implementation that is much better than ours in some respect, I'd love to have your input ;)\n",
      "@elbamos what did you push?\n",
      "@rvraghav93 well part of that. I want to be able to use an EstimatorCV in GridSearchCV.\n",
      "@rvraghav93 like using a CV object in a pipeline.\n",
      "if you find a way to enable ``make_pipeline(StandardScaler(), LogisticRegressionCV())`` grid-searchable, that would be a start\n",
      "or ``make_pipeline(SelectPercentile(...), LogisticRegressionCV())``\n",
      "\n",
      "6\n",
      "2405\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-11 16:24\n",
      "@amueller largevis. It's like tsne but efficient on ultra large datasets because it scales in O(n). The algo is only two weeks old, they haven't presented their reference code yet. You have a few hours to catch up though - I found a bug in my C++ code that's gonna take me a while to fix\n",
      "\n",
      "1\n",
      "2406\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-05-11 17:43\n",
      "^^ I don't think this is a race.\n",
      "you could also add python wrappers to your R package\n",
      "barnes-hut t-sne is also O(n) right?\n",
      "@elbamos  I hope it's BSD licensed ;)\n",
      "hm makes sense\n",
      "\n",
      "5\n",
      "2407\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-05-11 17:46\n",
      "barnes hut t-sne is O(N log N)\n",
      "http://arxiv.org/abs/1301.3342\n",
      "\n",
      "2\n",
      "2408\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-05-11 17:47\n",
      "@elbamos let me know if you have an mnist picture ;)\n",
      "\n",
      "1\n",
      "2409\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-11 20:35\n",
      "they say (n log n) i actually think its a little worse, at least as implemented.  whether largeVis really scales remains to be seen -- in theory its O(n) holding everything else constant, but when you grow n, you add nearest neighbors and sgd iterations too.\n",
      "@amueller Will do ;) Should be tonight actually -- the algo is working now on small data sets (does iris beautifully) but something is making it segfault when i scale up to mnist.  working on it now\n",
      "oh, and if I were in your shoes, I would totally agree its not a race\n",
      "\n",
      "3\n",
      "2410\n",
      "561a58f7d33f749381a8ff2f\n",
      "2016-05-11 21:06\n",
      "hey guys, about decision tree\n",
      "how do we visualize it?\n",
      "\n",
      "2\n",
      "2411\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-05-11 21:07\n",
      "you can export it to graphviz\n",
      "see http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
      "\n",
      "13\n",
      "2412\n",
      "561a58f7d33f749381a8ff2f\n",
      "2016-05-11 21:12\n",
      "I have the idea that the categorical variables as binary decisions in a sparse matrix would be awesome\n",
      "have RF as a preprocessing\n",
      "and then do sparse Ridge on top of it\n",
      "or NN\n",
      "rf.tree_.feature and .tree_.threshold are the ones huh :)\n",
      "`tree_.feature`, `tree_.threshold`\n",
      "yea it works\n",
      "\n",
      "7\n",
      "2413\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-05-11 21:19\n",
      "hmm wheres `tree.feature_`?\n",
      "it seems like it would be what you want though\n",
      "\n",
      "2\n",
      "2414\n",
      "561a58f7d33f749381a8ff2f\n",
      "2016-05-11 21:22\n",
      "it could basically transform a non-linear problem to a linear one, well.. if it can be captured by a decision tree\n",
      "\n",
      "1\n",
      "2415\n",
      "557c765a15522ed4b3e1de4f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-05-11 21:25\n",
      "@nelson-liu If your plan is to treat each tree's prediction as a a distinct categorical variable and then make your actual prediction by applying ridge regression or some other mechanism to those variables, I will bet you 10:1 odds that (a) you never implement this algorithm because of the dimensionality of what you'd have to feed into the ridge regression, or (b) if you do implement it, it overfits\n",
      "\n",
      "3\n",
      "2416\n",
      "561a58f7d33f749381a8ff2f\n",
      "2016-05-11 21:27\n",
      "or at least to remove too strongly correlating kind of duplicates\n",
      "@elbamos  I thought it more of a preprocessing step\n",
      "indeed it'd be too intense\n",
      "\n",
      "3\n",
      "2417\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-11 21:29\n",
      "oops sorry nelson, i mixed your two chats up\n",
      "\n",
      "1\n",
      "2418\n",
      "561a58f7d33f749381a8ff2f\n",
      "2016-05-11 21:31\n",
      "what about from a single decision tree?\n",
      "is there a situation in which you could imagine that using the leaf node's id as a binary var for a next model useful?\n",
      "as kind of feature engineering step\n",
      "that sounds very interesting elbamos\n",
      "googled suppressor: one variable that increases the effect of another var\n",
      "lol\n",
      "strange naming\n",
      "I'm interested for this particular challenge: http://blackboxchallenge.com/\n",
      "super nasty one\n",
      "\n",
      "9\n",
      "2419\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-11 21:32\n",
      "wanna try something funky for preprocessing that would probably work?  take your data and cut it into square cunks, like if your data is 1000 dimensional, chop it into one 31 * 31 square and one 7 * 7 square.  The run a convolution over both squares using a kernel initialized to have mean 0 and unit norm.  you can control the amount of dimensional reduction by controlling the size of the kernel.\n",
      "@lqdc that's just the effect of a suppressor variable on overfitting.\n",
      "\n",
      "2\n",
      "2420\n",
      "5537027215522ed4b3df56ab\n",
      "2016-05-11 21:32\n",
      "I think I've seen people use node ids as IVs before for the next estimator\n",
      "and they got \"better\" results\n",
      "Some people on kaggle at least improved their model by a fraction of a percent. It's very empirical, but was useful in their case.\n",
      "i.e. less overfitting because the number of splits is regularized?\n",
      "\n",
      "4\n",
      "2421\n",
      "5537027215522ed4b3df56ab\n",
      "2016-05-11 21:38\n",
      "Hey guys, I have a question: How do you deal with unrealistic estimates for probabilities for some estimators?  I tried using CIs (#6773), which were fine in my case, but apparently this is not the preferred approach.\n",
      "Is that competition for spam filtering?\n",
      "i can imagine mail.ru dealing with lots of that\n",
      "\n",
      "5\n",
      "2422\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-11 21:40\n",
      "@lqdc no, any time you add a variable to your training data, performance on the training data will improve slightly, regardless of whether there's a genuine relationship.  That's just the math.  Its called a \"suppressor\" variable because it suppresses the true error.  But the result is just increased overfitting.\n",
      "\n",
      "7\n",
      "2423\n",
      "561a58f7d33f749381a8ff2f\n",
      "2016-05-11 21:41\n",
      "the 36th var goes from -1.1 to 1.1\n",
      "every round you also get the reward\n",
      "reward can be super delayed\n",
      "rules unclear\n",
      "35 vars are roughly between -18 and 12 or something, mostly around -1 to 1\n",
      "it is very non-linear\n",
      "\n",
      "6\n",
      "2424\n",
      "5537027215522ed4b3df56ab\n",
      "2016-05-11 21:44\n",
      "But let's say the original random forest made an incorrect estimate for a split on some variable for which there was little data. Then just getting to that variable in some of the trees would be additional information\n",
      "i.e. trees too deep and too few of them\n",
      "sure, but an incorrect decision based on original information by an upstream estimator can be worse than just the original information in a downstream estimator. IE you are recovering information that was lost\n",
      "I'll try to construct a synthetic test for this\n",
      "\n",
      "4\n",
      "2425\n",
      "53135b495e986b0712efc453\n",
      "2016-05-12 12:19\n",
      "@amueller okay thanks!\n",
      "\n",
      "1\n",
      "2426\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-05-12 16:27\n",
      "When using EM, perplexity in LDA should be monotonic, right?\n",
      "\n",
      "1\n",
      "2427\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-05-16 18:02\n",
      "is there ever a case when the requirements.txt is parsed automatically?\n",
      "\n",
      "1\n",
      "2428\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-05-16 18:03\n",
      " what do you mean? i occasionally run `pip install -r requirements.txt <unconvertable> upgrade`?\n",
      "assuming versions are not pinned, of course\n",
      "\n",
      "2\n",
      "2429\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-18 06:29\n",
      "can anyone suggest a good \"standard\" dataset for evaluating the performance of a clustering/visualization package?  I'm looking for something with a few hundred thousand rows.  (I've tested on mnist, and my machine doesn't have the RAM to handle 1M+ row datasets.)\n",
      "\n",
      "1\n",
      "2430\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-05-18 16:27\n",
      "I don' think there are many standard datasets\n",
      "\n",
      "1\n",
      "2431\n",
      "53f1f08a107e137846bab1c2\n",
      "2016-05-19 11:16\n",
      "This is regarding Scikit-Learn Day, Paris. Will the talks be in french or english?\n",
      "\n",
      "1\n",
      "2432\n",
      "5586719a15522ed4b3e23add\n",
      "2016-05-22 20:15\n",
      "Hi everyone,  I am new to this room, found it through gitter listing. I am a usual scikit-learn user. Would this be a place to know more about projects using the tool? Thanks!\n",
      "\n",
      "1\n",
      "2433\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-05-22 20:16\n",
      "Hmm not quite, thus room is mainly used for developer chat.\n",
      "\n",
      "1\n",
      "2434\n",
      "5586719a15522ed4b3e23add\n",
      "2016-05-22 20:27\n",
      "@nelson-liu Hi, I guess you are referring to me? > Hmm not quite, thus room is mainly used for developer chat.  I see... I guess that it is the desired use. A quick scroll-up seems to show a plethora of different interesting topics... :) I will respect the main interest of the founders though. Thanks for the info! I hope there is no issue is I stay as observer meanwhile?\n",
      "\n",
      "3\n",
      "2435\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-05-22 20:31\n",
      "Np! BTW, does anyone have a 32 bit machine (ideally linux) and would be willing to spare a few minutes to pull down some code from one of my branches and run a nosetests?\n",
      "Yeah I mean running a 32 bit OS  I mean haha\n",
      "Ah got it. I have some tests failing on appveyor, only when running on windows 32 bit though.  I'm considering setting up a vm to debug, but I'm also curious if it breaks on Linux 32 bit bc it's a lot easier to get Linux set up and running on a vm vs windows haha\n",
      "\n",
      "3\n",
      "2436\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-22 20:31\n",
      "a 32-bit linux machine?\n",
      "\n",
      "1\n",
      "2437\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-22 20:33\n",
      "yeah i caught what you meant, its just my time machine is in the shop\n",
      "\n",
      "1\n",
      "2438\n",
      "557c765a15522ed4b3e1de4f\n",
      "2016-05-22 20:49\n",
      "hah yeah i don't even try to support windows\n",
      "\n",
      "1\n",
      "2439\n",
      "53135b495e986b0712efc453\n",
      "2016-05-24 11:34\n",
      "@amueller if you can spare a few minutes, review for https://github.com/scikit-learn/scikit-learn/pull/6697 please? ;)\n",
      "\n",
      "1\n",
      "2440\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-05-24 19:24\n",
      "@rvraghav93 a few minutes, right :P thanks for the ping, though. I'll try to take a look.\n",
      "\n",
      "1\n",
      "2441\n",
      "53135b495e986b0712efc453\n",
      "2016-05-24 19:25\n",
      "My hope is you'll get fully nerdsniped in those few minutes ;)\n",
      "And with that PR merged multiple metric is just a few lines of code away B)\n",
      "\n",
      "2\n",
      "2442\n",
      "53165e195e986b0712efc93c\n",
      "2016-05-25 21:13\n",
      "Given an estimator `est`, is there a standard way to determine if the estimator has previously been fit? Check if any `*_` attributes are present (e.g. `coef_`)?\n",
      "Hmmm, if there is it doesn't seem to be present on all estimators.\n",
      "\n",
      "2\n",
      "2443\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-05-25 21:13\n",
      "i do believe there is a fitted attribute\n",
      "\n",
      "1\n",
      "2444\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-05-25 21:15\n",
      "maybe this might help? https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/estimator_checks.py#L1035\n",
      "oops wrong link\n",
      "although that might be helpful\n",
      "i was meaning to send this: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py#L650\n",
      "is there any estimator you have in mind?\n",
      "or is this one youve built yourself\n",
      "or are you just looking for a more general solution\n",
      "that makes sense, hmm yeah im not too sure how to do that sorry :(\n",
      "\n",
      "8\n",
      "2445\n",
      "53165e195e986b0712efc93c\n",
      "2016-05-25 21:16\n",
      "Yeah, that's useful if you know what attributes to check for. Oh well.\n",
      "Thanks for the links.\n",
      "General solution preferably.\n",
      "I'm working with scikit-learn and dask (which does things lazily). It'd be nice to be able to catch not-fit errors at graph build time instead of at execution time.\n",
      "Not necessary though, just nice :)\n",
      "\n",
      "5\n",
      "2446\n",
      "541a528b163965c9bc2053de\n",
      "2016-05-26 17:31\n",
      "you can check if there is any attribute that ends in \"_\". If not it's not fitted.\n",
      "Unless it's stateless, like the HashingVectorizer that does not require any fit to start transforming :P\n",
      "\n",
      "2\n",
      "2447\n",
      "572bc25ac43b8c60197160c9\n",
      "2016-05-26 23:59\n",
      "Hi nice to know this group exists\n",
      "\n",
      "1\n",
      "2448\n",
      "572bc25ac43b8c60197160c9\n",
      "2016-05-27 00:00\n",
      "##hello\n",
      "#hello\n",
      "\n",
      "2\n",
      "2449\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-05-27 00:14\n",
      "lol\n",
      "\n",
      "1\n",
      "2450\n",
      "54b2524adb8155e6700e8a8e\n",
      "2016-05-28 10:32\n",
      "Seeing this a little late, @jcrist, you might try performing predict/transform, a `NotFittedError` should be indicative, but other errors may occur uninformatively.\n",
      "\n",
      "1\n",
      "2451\n",
      "5573124c15522ed4b3e184c2\n",
      "2016-06-02 15:11\n",
      "Hi, all. Sorry if this is the wrong place to ask my question - but I'd like to use Naive Bayes, training by a list of sentences with their own labels (i.e. multiple different sentences per label). However, each sentence carries a certain importance weight that I'd like for them to play a role in while classifying - some sentences I want to have more of an impact than others. I was considering just resampling these sentences and creating duplicates or whatever depending on importance, but that seems horribly inefficient (especially because importance values/weights are essentially continuous)  Is there any way to weight certain samples to be more important?  ***TL;DR:** In other words, my training data is of the form ```[label, sentence, weight]```, where I might have multiple different weights and sentences for the same label, of course. I'm not actually sure what the best way might be to go about classifying this using scikit-learn. Any ideas?\n",
      "\n",
      "1\n",
      "2452\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-02 15:29\n",
      "Can't you specify the weight as a parameter at fit time?\n",
      "See: http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.fit\n",
      "Not sure if that answered your question\n",
      "\n",
      "3\n",
      "2453\n",
      "5573124c15522ed4b3e184c2\n",
      "2016-06-02 15:33\n",
      "That should do the trick! Thanks\n",
      "\n",
      "1\n",
      "2454\n",
      "57504263c43b8c6019765b0e\n",
      "2016-06-03 11:14\n",
      " is there any chinese develper ? I have an translate project ,hope volunteers join in it.\n",
      "here is the repo    https://github.com/lzjqsdd/scikit-learn-doc-cn\n",
      "\n",
      "2\n",
      "2455\n",
      "53135b495e986b0712efc453\n",
      "2016-06-06 23:15\n",
      "Andy, could you close this one - https://github.com/scikit-learn/scikit-learn/issues/5669?\n",
      "@amueller\n",
      "\n",
      "2\n",
      "2456\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-06-06 23:15\n",
      "you summoned me\n",
      "ah\n",
      "done\n",
      "\n",
      "3\n",
      "2457\n",
      "53135b495e986b0712efc453\n",
      "2016-06-06 23:21\n",
      "lol... How's your writing going? :)\n",
      "\n",
      "1\n",
      "2458\n",
      "5581814615522ed4b3e20c6a\n",
      "2016-06-07 03:44\n",
      "What is the best way to save the trained models other than Pickle?\n",
      "\n",
      "1\n",
      "2459\n",
      "55a487245e0d51bd787b4e45\n",
      "2016-06-07 03:53\n",
      "@BastinRobin Quite unfortunately, there isn't an easy path to that in sklearn.\n",
      "@BastinRobin You're looking for a format to store some arrays and some random parameters? What are your constraints?\n",
      "\n",
      "2\n",
      "2460\n",
      "575664f0c43b8c601978412c\n",
      "2016-06-07 06:11\n",
      "hello ,everyone\n",
      "\n",
      "1\n",
      "2461\n",
      "5581814615522ed4b3e20c6a\n",
      "2016-06-07 09:46\n",
      "@mikegraham I just want to know if pickle is the best or not for storing\n",
      "\n",
      "1\n",
      "2462\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-07 10:01\n",
      "@BastinRobin  It's OK ... imho sklearn is crying out for someone to volunteer to implement a better solution\n",
      "\n",
      "1\n",
      "2463\n",
      "57565ad2c43b8c6019783fbb\n",
      "2016-06-07 14:02\n",
      "Where would I have to go if I want to learn about A.Is\n",
      "\n",
      "1\n",
      "2464\n",
      "56e9685085d51f252ab91b6d\n",
      "2016-06-07 14:17\n",
      "Hello everyone\n",
      "I've made a pipeline for a classification problem\n",
      "learning goes fast thanks to parallel with n_jobs=-1\n",
      "but when i use the predict function, it goes very slow, and only one process is used\n",
      "is there a way to make prediction in parallel?\n",
      "\n",
      "5\n",
      "2465\n",
      "55a487245e0d51bd787b4e45\n",
      "2016-06-07 17:17\n",
      "@BastinRobin pickle is a very problematic solution: it is fragile, not safely transferable, not explicitly specified, etc. It is all that sklearn is geared to and it will be a lot of work to use something else.\n",
      "@BastinRobin For a narrow use case, you can hack something up. There isn't a more general solution\n",
      "@kmehl Probably not with plain sklearn, but you can probably make a specialized evaluator for the slow parts. Have you profiled? Where is the time being spent?\n",
      "\n",
      "3\n",
      "2466\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-07 17:27\n",
      "@mikegraham  would making a better solution make a good discrete project? There are quite a lot of people who are keen to work on scikit learn it seems so if it was clearly advertised it might get someone to bite\n",
      "I think a good solution would be very popular\n",
      "\n",
      "2\n",
      "2467\n",
      "55a487245e0d51bd787b4e45\n",
      "2016-06-07 18:09\n",
      "@lesshaste I don't know how welcome it would be -- it would be a lot of work and a huge maintenance burden.\n",
      "\n",
      "1\n",
      "2468\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-08 07:14\n",
      "@mikegraham  Ah. I am not 100% clear why it would be a huge maintenance problem. Doesn't that depend on whatever elegant solution someone comes up with? Or to put it another way, if the problem is stated in parts with \"part 1) Devise a solution that minimises the maintenance needed\" would this not be plausible?\n",
      "has someone done a survey to see what other solutions exist out there?\n",
      "For example in R or weka\n",
      "From a very quick look, the standard solution in R just seems to be saveRDS https://stat.ethz.ch/R-manual/R-devel/library/base/html/readRDS.html\n",
      "\n",
      "5\n",
      "2469\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-08 07:19\n",
      "spark has this https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html\n",
      "weka has https://weka.wikispaces.com/Saving+and+loading+models\n",
      "\n",
      "2\n",
      "2470\n",
      "5571fe1015522ed4b3e17d90\n",
      "2016-06-08 08:32\n",
      "@lesshaste  @BastinRobin  @mikegraham you can use joblib.dump (based on pickle with some optimization on numpy arrays) too, see http://scikit-learn.org/stable/modules/model_persistence.html#model-persistence for more details. There were some discussion on the mailing list IIRC about this for example http://thread.gmane.org/gmane.comp.python.scikit-learn/14905/focus=14909.\n",
      "\n",
      "1\n",
      "2471\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-08 09:09\n",
      "@lesteve  Thanks. That mailing list thread is somehow slightly negative. I would love to see an objective and technical analysis of the situation.\n",
      "For example, what is wrong with developing the PMML idea?\n",
      "iirc  joblib.dump makes a large number of small files. One simple improvement would be to reduce the number of files to 1 or 2\n",
      "ah.. I see the other problems are mentioned\n",
      "\n",
      "4\n",
      "2472\n",
      "5571fe1015522ed4b3e17d90\n",
      "2016-06-08 09:13\n",
      "> For example, what is wrong with developing the PMML idea?\n",
      "I don't think there is anything wrong per se. It's just that it is quite some work and it probably won't happen inside scikit-learn. Not an expert though. There may have been other discussions on the mailing list on this serialization issues. It does come up from time to time.\n",
      "\n",
      "2\n",
      "2473\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-08 09:15\n",
      "OK thanks. I am just a big fan of having clearly stated tasks for keen volunteers to pick from. I feel lots of people want to contribute to scikit learn as it is so great :)\n",
      "@ogrisel  Oh that sounds very interesting.\n",
      "no I did :) I just sent a lot of links\n",
      "to be clear, the interesting part is that an expert (that's you) thinks it might be relevant\n",
      "my knowledge is very shallow in this area\n",
      "\n",
      "5\n",
      "2474\n",
      "5571fe1015522ed4b3e17d90\n",
      "2016-06-08 09:15\n",
      "> iirc joblib.dump makes a large number of small files. One simple improvement would be to reduce the number of files to 1 or 2  For the record, joblib master creates only a single pickle file (and not one per numpy array as previously)\n",
      "\n",
      "1\n",
      "2475\n",
      "541a528b163965c9bc2053de\n",
      "2016-06-08 09:15\n",
      "PMML is a very verbose XML-based format. The new spark mllib lightweight format  would probably be better much more efficient.\n",
      "\n",
      "1\n",
      "2476\n",
      "541a528b163965c9bc2053de\n",
      "2016-06-08 09:16\n",
      "@lesshaste it's from the link you just sent\n",
      "\n",
      "3\n",
      "2477\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-08 09:22\n",
      "@lesteve  I didn't know  joblib master creates only a single pickle file. Thank you\n",
      "\n",
      "5\n",
      "2478\n",
      "57565ad2c43b8c6019783fbb\n",
      "2016-06-08 13:34\n",
      "any of you guys work with c++\n",
      "\n",
      "1\n",
      "2479\n",
      "57565ad2c43b8c6019783fbb\n",
      "2016-06-08 14:13\n",
      "Im trying to make an AI using python\n",
      "\n",
      "1\n",
      "2480\n",
      "55a487245e0d51bd787b4e45\n",
      "2016-06-08 15:13\n",
      "@lesshaste To avoid the problems of pickle in full, you need to be explicit. To be explicit, you need to have your data model change every time anything applicable changes.\n",
      "\n",
      "1\n",
      "2481\n",
      "55495eb515522ed4b3dffb00\n",
      "2016-06-08 19:46\n",
      "hello, I am seeing many feature scaling methods, but I cannot find things like RobustScaler and etc on the internet.\n",
      "what are they called in academic society?\n",
      "\n",
      "2\n",
      "2482\n",
      "55495eb515522ed4b3dffb00\n",
      "2016-06-08 21:06\n",
      "and what is difference between scale and standardscaler?\n",
      "is standardscaler just a class implementation of scale?\n",
      "\n",
      "2\n",
      "2483\n",
      "55a487245e0d51bd787b4e45\n",
      "2016-06-08 21:10\n",
      "@keonkim Yes, scale is a function that returns the result, StandardScaler can go in a pipeline or whatever.\n",
      "@keonkim I don't know that RobustScaler has a consistent academic name. To indicate what they did, someone might describe it. I've seen it contrasted with naive scaling by calling it \"IQR\", but that's not a formal name for the scaling technique, which merely uses the actual IQR to do its job. A lot of people might know what you mean if you just said \"IQR scaling\" or \"Scaled to the IQR\" though.\n",
      "\n",
      "2\n",
      "2484\n",
      "53135b495e986b0712efc453\n",
      "2016-06-09 00:35\n",
      "@ogrisel @amueller Could you cancel all my appveyor builds. I forgot to use the CI skip and it seems to block other PRs...\n",
      "\n",
      "1\n",
      "2485\n",
      "55495eb515522ed4b3dffb00\n",
      "2016-06-09 00:59\n",
      "@mikegraham Thanks!\n",
      "\n",
      "1\n",
      "2486\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-09 01:40\n",
      "@rvraghav93 what is this ci skip you speak of?\n",
      "\n",
      "1\n",
      "2487\n",
      "54c630d6db8155e6700f168d\n",
      "2016-06-09 06:55\n",
      "Hey all! I have a question regarding CountVectorizer. Is is possible to 'reverse engineer' the original text from the vectors?\n",
      "I'm wondering as I just wrote an article on our methods, and I want to share our vectorized data, but cannot share if it's possible for people to figure out the original input text based on this (as the original text contains sensitive info).\n",
      "For reference, here is the article: https://medium.com/xeneta/boosting-sales-with-machine-learning-fbcf2e618be3\n",
      "\n",
      "3\n",
      "2488\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-09 07:03\n",
      "haha I saw this on hacker news earlier\n",
      "I dont think its possible to reverse engineer countvectorizer\n",
      "\n",
      "6\n",
      "2489\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-09 07:06\n",
      "the countvectorizer code is here, https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py it doesnt store the raw documents\n",
      "although its possible ofc to get the vocabulary, so if individual words are sensitive then that might pose a problem.\n",
      "\n",
      "2\n",
      "2490\n",
      "54c630d6db8155e6700f168d\n",
      "2016-06-09 07:29\n",
      "I see. And if you have the vocabulary, and the vectors, you can kind of recreate keyword-based descriptions.\n",
      "I think I'll rather just share the vectorized descriptions, and not the joblib vectorizer.\n",
      "That'll make it impossible to guess the words, right?\n",
      "Really appreciate your help @nelson-liu :)\n",
      "\n",
      "4\n",
      "2491\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-09 07:48\n",
      "Yup that is correct @perborgen\n",
      "\n",
      "2\n",
      "2492\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-09 07:49\n",
      "What do you mean vectorized descriptions actually?\n",
      "\n",
      "1\n",
      "2493\n",
      "54c630d6db8155e6700f168d\n",
      "2016-06-09 08:45\n",
      "Sorry for my late reply.  I mean after using the CountVectorizer to turn text into vectors.\n",
      "That's what I refer to aws 'vectorized descriptions' (as my input text is company descriptions).\n",
      "\n",
      "2\n",
      "2494\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-09 12:21\n",
      "@mikegraham What's your view on the spark 2 serialization format ?\n",
      "\n",
      "1\n",
      "2495\n",
      "53135b495e986b0712efc453\n",
      "2016-06-09 12:37\n",
      "@nelson-liu If you add `[ci skip]` to a commit message, Appveyor/Travis will skip the tests for that commit... This could be useful if you just push your unfinished work (for a review or to switch computers)...\n",
      "\n",
      "1\n",
      "2496\n",
      "55a487245e0d51bd787b4e45\n",
      "2016-06-09 19:20\n",
      "@lesshaste As in kyro or something else?\n",
      "@lesshaste In any event, I'm probably not fit to weigh in :)\n",
      "\n",
      "2\n",
      "2497\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-10 23:55\n",
      "hello, I had a quick question.\n",
      "I wanted to take up the issue https://github.com/scikit-learn/scikit-learn/issues/6867\n",
      "I could some one please share the link to new gaussian process.\n",
      "So the user guide is http://scikit-learn.org/stable/modules/gaussian_process.html\n",
      "\n",
      "4\n",
      "2498\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-11 00:01\n",
      "2nd question (https://github.com/scikit-learn/scikit-learn/issues/6857) I cannot find the file references.rst in  the scikit-learn/doc folder. Where is this file present?.\n",
      "\n",
      "1\n",
      "2499\n",
      "5582b5a615522ed4b3e21903\n",
      "2016-06-11 02:21\n",
      "@krishnakalyan3 someone else who commented on that issue couldn't find it either. And neither could I from a quick look.\n",
      "\n",
      "1\n",
      "2500\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-11 09:39\n",
      "@pdurbin  thank anyway :)\n",
      "\n",
      "1\n",
      "2501\n",
      "53135b495e986b0712efc453\n",
      "2016-06-13 00:37\n",
      "@krishnakalyan3 I think @amueller meant that the `GPRegressor` class does not have a reference to that user guide.\n",
      "\n",
      "1\n",
      "2502\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-13 08:51\n",
      "@rvraghav93 just created a pull request could you let me know if things are okay?\n",
      "\n",
      "1\n",
      "2503\n",
      "53135b495e986b0712efc453\n",
      "2016-06-13 14:32\n",
      "Thanks for the PR! I'll look into that :)\n",
      "\n",
      "1\n",
      "2504\n",
      "53135b495e986b0712efc453\n",
      "2016-06-13 14:41\n",
      "@amueller Could you share your suggestions about https://github.com/scikit-learn/scikit-learn/pull/6380#issuecomment-185699518 please?\n",
      "\n",
      "1\n",
      "2505\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-13 16:55\n",
      "it would be great if one could visualise boosted trees (e.g. xgboost) too\n",
      "\n",
      "1\n",
      "2506\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-06-13 17:34\n",
      "@lesshaste in scikit-learn you can\n",
      "xgboost no idea\n",
      "\n",
      "2\n",
      "2507\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-13 18:11\n",
      "if you look at http://www.r-bloggers.com/an-introduction-to-xgboost-r-package/ and scroll down to https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/xgb.plot.multi.png can you do that sort of thing in scikit-learn?\n",
      "@amueller  Sorry I meant that for you\n",
      "oh I must have missed this completely.  Would you mind pointing me to the docs for ensembled tree visualization?\n",
      "I take your xgboost point of course :) I only ever use it through scikit-learn so sometimes forget it isn't part of it\n",
      "\n",
      "4\n",
      "2508\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-06-13 18:13\n",
      "well for scikit-learn trees yes, for xgboost trees obviously not, because that has nothing to do with xgboost?\n",
      "that would go into the xgboost package...\n",
      "err has nothing to do with scikit-learn\n",
      "\n",
      "3\n",
      "2509\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-06-13 18:40\n",
      "well you can plot each tree in the ``estimators_``\n",
      "\n",
      "1\n",
      "2510\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-13 18:40\n",
      "true.. they seem to have a nice trick for plotting one merged tree\n",
      "I am still intrigued if one can derive a single decision tree from an ensemble of trees which is easier to interpret and almost as good as a classifier/regressor\n",
      "this would seem potentially useful\n",
      "\n",
      "3\n",
      "2511\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-06-15 09:33\n",
      "@amueller I think the issue #6857 is solved by #6886. Can you confirm it was what you wanted ? Thx ;)\n",
      "\n",
      "1\n",
      "2512\n",
      "5761a537c2f0db084a1e0fe7\n",
      "2016-06-15 18:58\n",
      "Hi all. I just wanted to ask, is there anything else I should do in https://github.com/scikit-learn/scikit-learn/pull/6874 or just wait? :)\n",
      "\n",
      "1\n",
      "2513\n",
      "53135b495e986b0712efc453\n",
      "2016-06-15 20:43\n",
      "@yenchenlin @nelson-liu The GSoC midterms are approaching. Hope you guys are ready with your blogs posts? :)\n",
      "\n",
      "1\n",
      "2514\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-15 21:32\n",
      "soon :shipit: :)\n",
      "\n",
      "1\n",
      "2515\n",
      "53135b495e986b0712efc453\n",
      "2016-06-16 09:16\n",
      "Good to know!!\n",
      "\n",
      "1\n",
      "2516\n",
      "53135b495e986b0712efc453\n",
      "2016-06-16 11:30\n",
      "@amueller @ogrisel Is #6897 the correct fix for the Circle CI build failure on master?\n",
      "\n",
      "1\n",
      "2517\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-06-16 20:13\n",
      "@raghavrv sorry haven't looked\n",
      "\n",
      "1\n",
      "2518\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-06-16 20:14\n",
      "But my book is now in beta if anyone wants to check it out ;) http://shop.oreilly.com/product/0636920030515.do\n",
      "\n",
      "1\n",
      "2519\n",
      "5763118fc2f0db084a1e46cf\n",
      "2016-06-17 01:39\n",
      "just in case you need machine learning engine in java/scala, checkout [smile](https://github.com/haifengl/smile), which includes many algorithms.\n",
      "\n",
      "1\n",
      "2520\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-17 03:40\n",
      "hi..sorry if I missed it but does scikit learn support regularized greedy forests? https://arxiv.org/pdf/1109.0887.pdf\n",
      "or is there a PR for it?\n",
      "\n",
      "2\n",
      "2521\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-17 07:29\n",
      "@amueller I was looking into this issue https://github.com/scikit-learn/scikit-learn/issues/6120. Could you let me know if the images need to be updated should I just fix the print message?.\n",
      "\n",
      "1\n",
      "2522\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-17 08:55\n",
      "@amueller I am looking forward to buying your book!\n",
      "@amueller  that's interesting.  I was going by the usage in kaggle competitions. But I really like your quality control system and support it entirely :)\n",
      "\n",
      "6\n",
      "2523\n",
      "55d842e50fc9f982beae3dcf\n",
      "2016-06-17 12:14\n",
      "\n",
      "1\n",
      "2524\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-06-17 14:19\n",
      "@lesshaste regularized greedy forests has only 18 cites. so no to both ;)\n",
      "\n",
      "1\n",
      "2525\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-17 14:45\n",
      "\"This is the code for my second place finish in Kaggle's HiggsML challenge. It is a blend of a large number of boosted decision tree ensembles constructed using Regularized Greedy Forest.\"\n",
      "as an example\n",
      "\n",
      "2\n",
      "2526\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-06-17 15:04\n",
      "@amueller  were you thinking of https://arxiv.org/abs/1603.02754 ?\n",
      "hmm.. https://www.kaggle.com/c/higgs-boson/forums/t/10053/did-anyone-try-rgf-regularized-greedy-forest ... maybe can be ignored now that xgboost rules everything :)\n",
      "\n",
      "2\n",
      "2527\n",
      "567d7eca16b6c7089cc02a05\n",
      "2016-06-18 09:42\n",
      "Hi, I am new to scikit-learn , I want to contribute, can i get some guidance. Frankly I am having problems in understanding the easy issues. Sorry for being naive\n",
      "\n",
      "1\n",
      "2528\n",
      "56b80528e610378809c05a48\n",
      "2016-06-18 09:58\n",
      "Hi :smile: , which issues are you solving?\n",
      "or trying to understand\n",
      "\n",
      "2\n",
      "2529\n",
      "567d7eca16b6c7089cc02a05\n",
      "2016-06-18 10:01\n",
      "I tried to understand more than one... but I am unable to narrow down the scope of the problem\n",
      "\n",
      "6\n",
      "2530\n",
      "56b80528e610378809c05a48\n",
      "2016-06-18 10:04\n",
      "I see, maybe you should leave a comments there and tag the issue opener for more specific elaboration.\n",
      "Doing this can help you make sure you are on the right path!\n",
      "No haha, Im not a pro here.\n",
      "\n",
      "5\n",
      "2531\n",
      "56b80528e610378809c05a48\n",
      "2016-06-18 10:21\n",
      "You can leave a comment about how you gonna solve this, and ask the issue opener whether you are correct.  It looks like to implement a function based on [this script](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) which can conveniently plot the confusion matrix, you can find doc of confusion matrix [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)\n",
      "\n",
      "2\n",
      "2532\n",
      "5766ec81c2f0db084a1ec669\n",
      "2016-06-20 11:23\n",
      "@amueller hey man, glad to see you here. Is your book out already? Got a potential buyer here ;)\n",
      "\n",
      "1\n",
      "2533\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-20 16:48\n",
      "@c4ndym4n not andy, but its in beta rn http://shop.oreilly.com/product/0636920030515.do\n",
      "\n",
      "1\n",
      "2534\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-21 01:31\n",
      "I have a question\n",
      "after doing a git pull of sklearn repository\n",
      "say I am woking on a bug\n",
      "how do I test my changes on the code?\n",
      "\n",
      "7\n",
      "2535\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-21 01:32\n",
      "hi\n",
      "\n",
      "1\n",
      "2536\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-21 01:33\n",
      "or run random python code that might use scikit-learn on the version that you have modified\n",
      "\n",
      "6\n",
      "2537\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-21 01:35\n",
      "so im not sure how you have it installed currently, but I uninstalled the pip version i had. then, I installed it again from source with `python setup.py develop`\n",
      "\n",
      "5\n",
      "2538\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-21 01:36\n",
      "``` In [1]: import sklearn  In [2]: print sklearn.__file__ /Users/nelsonliu/Documents/Github/scikit-learn/sklearn/__init__.pyc ```\n",
      "that should point to wherever youve cloned the sklearn repo\n",
      "\n",
      "4\n",
      "2539\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-21 01:40\n",
      "so every time a git pull is done\n",
      "you recompile\n",
      "\n",
      "2\n",
      "2540\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-21 01:41\n",
      "with python setup.py build_ext --inplace\n",
      "?\n",
      "ok\n",
      "\n",
      "3\n",
      "2541\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-21 01:41\n",
      "uh you technically dont have to recompile if none of the .pyx files are changed, but i generally do so anyway because I dont want to bother looking at what was pulled\n",
      "its quick anyway\n",
      "\n",
      "2\n",
      "2542\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-21 02:49\n",
      "thanks @nelson-liu  it worked\n",
      "\n",
      "2\n",
      "2543\n",
      "54f7c23815522ed4b3dcd290\n",
      "2016-06-21 21:26\n",
      "I have limited data science experience, and reasonable programming experience. What are the best ways to get started??\n",
      "\n",
      "1\n",
      "2544\n",
      "55aee1ab0fc9f982beaa80a5\n",
      "2016-06-22 00:11\n",
      "@crimsonsoccer55 what do you want to do?\n",
      "\n",
      "1\n",
      "2545\n",
      "574454a0c43b8c601974a563\n",
      "2016-06-22 21:27\n",
      "Hi all :smile:\n",
      "\n",
      "1\n",
      "2546\n",
      "574454a0c43b8c601974a563\n",
      "2016-06-22 21:28\n",
      "I just want mention that Im working on a module which ports trained (sklearn) decision tree models to Java and C. Have a look if you are interested: https://github.com/nok/sklearn-decision-tree-porting\n",
      "\n",
      "2\n",
      "2547\n",
      "55aee1ab0fc9f982beaa80a5\n",
      "2016-06-22 21:40\n",
      "Does anyone know if there are plans to expand the MLP/RBM modules to include more hierarchical learning techniques\n",
      "\n",
      "1\n",
      "2548\n",
      "576c83e3c2f0db084a1fa0c2\n",
      "2016-06-24 01:36\n",
      "@nok  404 error on your link\n",
      "\n",
      "1\n",
      "2549\n",
      "574454a0c43b8c601974a563\n",
      "2016-06-24 08:47\n",
      "@alayassir https://github.com/nok/scikit-learn-model-porting (under active development)\n",
      "\n",
      "1\n",
      "2550\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-06-24 10:19\n",
      "@amueller could you please let me know how to proceed with https://github.com/scikit-learn/scikit-learn/issues/6120\n",
      "\n",
      "1\n",
      "2551\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-06-24 17:32\n",
      "It seems impossible to pass check_estimator for a sparse classifier that does not do multi-class out of the box.\n",
      "Because of [this test](https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/utils/estimator_checks.py#L301)\n",
      "@neale not sure what you mean by hierarchical learning, but in general any framework expansion of the neural network components is out of scope for scikit-learn\n",
      "\n",
      "3\n",
      "2552\n",
      "55aee1ab0fc9f982beaa80a5\n",
      "2016-06-24 21:03\n",
      "@vene why is that out of scope, there is a beta MLP module that works pretty well\n",
      "Giving users the ability to build networks layer by layer should be feasible\n",
      "\n",
      "2\n",
      "2553\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-06-24 21:07\n",
      "@neale I didn't say it's infeasable, it just doesn't fit within the simple API that scikit-learn strives for. There are great libraries that allow modular composition of deep nets, like Lasagne and Keras. Check out the scikit-learn [faq](http://scikit-learn.org/stable/faq.html) for more about this \"vision\".\n",
      "\n",
      "2\n",
      "2554\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-06-24 21:09\n",
      "Currently scikit-learn pipelines and feature unions are extremely simple objects, and steps in a pipeline cannot and do not communicate with each other, but instead are trained independently.\n",
      "\n",
      "1\n",
      "2555\n",
      "55aee1ab0fc9f982beaa80a5\n",
      "2016-06-24 21:10\n",
      "Yeah I always assumed the contributions weren't there, not that scikit actively stayed away from those kinds of models\n",
      "\n",
      "1\n",
      "2556\n",
      "5739265bc43b8c6019731a58\n",
      "2016-06-25 20:30\n",
      "GSOC admins need to resolve the evaluation situation immediately I'm on irc .. Python Gsoc Admin\n",
      "\n",
      "1\n",
      "2557\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-06-26 04:41\n",
      "@meflin what is the issue?\n",
      "\n",
      "1\n",
      "2558\n",
      "55a487245e0d51bd787b4e45\n",
      "2016-06-26 15:31\n",
      "@jmschrei It seemed that the student evals were overdue\n",
      "@jmschrei (At this point, I think there is a risk of sklearn being blacklisted from GSOC)\n",
      "\n",
      "2\n",
      "2559\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-06-26 16:23\n",
      "@mikegraham I submitted my student eval days ago\n",
      "\n",
      "1\n",
      "2560\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-06-26 16:31\n",
      "okay seems like it has been resolved\n",
      "\n",
      "1\n",
      "2561\n",
      "5739265bc43b8c6019731a58\n",
      "2016-06-26 22:30\n",
      "everything is resolved and good thanks for all your hard work\n",
      "\n",
      "1\n",
      "2562\n",
      "53135b495e986b0712efc453\n",
      "2016-06-26 22:30\n",
      "Thanks for the update :)\n",
      "\n",
      "1\n",
      "2563\n",
      "53135b495e986b0712efc453\n",
      "2016-06-29 23:27\n",
      "@amueller Would you be interested in doing the honor of giving a -1 and closing this? :P (https://github.com/scikit-learn/scikit-learn/pull/5883)\n",
      "\n",
      "1\n",
      "2564\n",
      "576bb437c2f0db084a1f7ead\n",
      "2016-06-30 02:11\n",
      "Any node wrapper for Scikitlearn?\n",
      "\n",
      "1\n",
      "2565\n",
      "55d054b70fc9f982bead8af7\n",
      "2016-06-30 03:45\n",
      "Hi everyone , I am Khanh and newbie in Scikit-learn :D\n",
      "\n",
      "1\n",
      "2566\n",
      "56ee4f3185d51f252ab9c4a1\n",
      "2016-06-30 11:39\n",
      "Hi everyone, I have this: ```rf = RandomForestClassifier(n_estimators = 1000, n_jobs = -1)     clf = Pipeline([('preproc', StandardScaler()),('classifier', rf)])```\n",
      "fit the classifier with all of the training set     ```data = clf.fit(features, labels)``` where ```features``` & ```labels``` are of size 4\n",
      "And I save it as a Pickle object.\n",
      "Now, How can I extract those array of features and labels again??\n",
      "Any idea any one?\n",
      ":(\n",
      "\n",
      "6\n",
      "2567\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-30 11:48\n",
      "what is it? What specifically are you saving as a Pickle object\n",
      "\n",
      "1\n",
      "2568\n",
      "56ee4f3185d51f252ab9c4a1\n",
      "2016-06-30 12:08\n",
      "the ```data``` variable. Which is a pipeline class object.\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "\n",
      "2\n",
      "2569\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-06-30 12:21\n",
      "hmm as far as i know there is no way to get the input features and labels from a fitted classifier...\n",
      "\n",
      "1\n",
      "2570\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-06-30 16:03\n",
      "@girisagar46 what would be the point of storing the dataset in the model?\n",
      "\n",
      "1\n",
      "2571\n",
      "56818d2d16b6c7089cc06972\n",
      "2016-06-30 19:14\n",
      "Hello. I am using sklearn to try to predict lux values from a iOS camera data. I have created some training data for multiple models of iPad/iPods, and have it working to some degree, but still get something like 20% error.\n",
      "I am relatively new to sklearn, and am wondering where I can get feedback on my iPython notebook to improve my results. Is this a good place? If so, what is the best way to share a notebook and relevant external files?\n",
      "\n",
      "2\n",
      "2572\n",
      "54317aec163965c9bc208ec9\n",
      "2016-07-02 12:24\n",
      "@nspaeth without looking at your approach I know that cameras automatically adjust themselves to keep the brightness of an image in a \"useful range\". I don't think this is a good project for someone starting in machine learning. The only way I see this working is that the algorithm needs to understand about certain camera \"artefacts\" and then use these to interpret the brightness. I can't see this working short of a deep convolutional network.\n",
      "\n",
      "1\n",
      "2573\n",
      "5581814615522ed4b3e20c6a\n",
      "2016-07-05 05:08\n",
      "hi guys i m having a doubt If i train a classifier and pickle it. How to make it relearn everytime when i introduce a new test example. Is it like everytime i want to predict something i need to retrain with updated training set or will the updating can happen on the go.?\n",
      "@akloster  thank you :)\n",
      "\n",
      "2\n",
      "2574\n",
      "54317aec163965c9bc208ec9\n",
      "2016-07-05 05:31\n",
      "@BastinRobin In my opinion retraining with every new test example is a bad idea, mainly because it will change the accuracy characteristics of your model over time. Not necessarily always towards the better.  Some Bayesian methods support very natural \"updating\". Otherwise you can look into \"Reinforcement learning\" which may be better suited to your task if you need to learn \"online\".\n",
      "It may also be a good Idea to keep the new examples in a special \"out of sample\" test set, and evaluate ongoing accuracy on that test set.\n",
      "\n",
      "2\n",
      "2575\n",
      "5757de1ec43b8c6019787b6c\n",
      "2016-07-05 09:31\n",
      "Hi  I have a txt file containing words and corresponding word vectors . I want to plot this using tSNE\n",
      "Can anyone point me to good example ?\n",
      "\n",
      "2\n",
      "2576\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-05 09:44\n",
      "Hi, I have read a few articles on word embeddings and tsne but am new to these topics. Some of the websites were\n",
      "https://lvdmaaten.github.io/tsne/\n",
      "http://blog.christianperone.com/2016/01/voynich-manuscript-word-vectors-and-t-sne-visualization-of-some-patterns/\n",
      "And I came across this just now.. https://www.quora.com/How-do-I-visualise-word2vec-word-vectors/answer/Vered-Shwartz hope it is helpful.\n",
      "\n",
      "4\n",
      "2577\n",
      "54b4f2d1db8155e6700e99c0\n",
      "2016-07-05 19:29\n",
      "Hey Folks, Id like to understand the implementation of the Random Forests. Is there a specific reason why _parallel_build_trees() is a function and not a method?\n",
      "\n",
      "1\n",
      "2578\n",
      "53135b495e986b0712efc453\n",
      "2016-07-05 23:42\n",
      "Because it is a helper which is called inside the `fit` method.\n",
      "The `n_jobs` param that you set at the initialization of `RandomForestClassifier` controls the number of processes used by `fit`...\n",
      "\n",
      "2\n",
      "2579\n",
      "53135b495e986b0712efc453\n",
      "2016-07-05 23:47\n",
      "The `fit`  uses `joblib`'s `delayed` to run the `_parallel_build_trees` helper in batches to build `n_jobs` trees at a time.\n",
      "\n",
      "1\n",
      "2580\n",
      "53135b495e986b0712efc453\n",
      "2016-07-08 16:40\n",
      "Is there a way to install sklearn to python package namespace under a different name (say `skmaster`) it would be nice to have two versions of scikit-learn installed under different names to compare against master...\n",
      "\n",
      "1\n",
      "2581\n",
      "53810862048862e761fa2887\n",
      "2016-07-08 18:09\n",
      "@raghavrv You can open 2 different shells and activate 2 different environments in them ?\n",
      "\n",
      "1\n",
      "2582\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-08 18:11\n",
      "it'd be useful to be able to run both in the same script for easy comparison / benchmarking\n",
      "\n",
      "1\n",
      "2583\n",
      "53135b495e986b0712efc453\n",
      "2016-07-09 12:33\n",
      "@vighneshbirodkar yes indeed but like @nelson-liu says it would be nice to use it both in a single script... I tried messing around with the setup.py file but looks like there are quite a lot of imports inside sources which do not use relative imports (hence explicitly importing from `sklearn.module`...)... I'd have to change all of that ;(\n",
      "\n",
      "1\n",
      "2584\n",
      "53135b495e986b0712efc453\n",
      "2016-07-12 11:04\n",
      "@ogrisel could you reset travis cache at #5974 please?\n",
      "\n",
      "1\n",
      "2585\n",
      "574454a0c43b8c601974a563\n",
      "2016-07-14 21:09\n",
      "Now you can port a learned AdaBoost classifier based on pruned DecisionTree estimators to Java:\n",
      "https://github.com/nok/scikit-learn-model-porting/blob/master/examples/classification/adaboost_predict.py\n",
      "But note that the project is still under active development. :-)\n",
      "\n",
      "3\n",
      "2586\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-15 04:06\n",
      "is https://github.com/mblondel/svmlight-loader still the recommended way to read in very large libsvm format files? It is quote old now and scikit-learn has been through many versions in the last 3 years\n",
      "@amueller True\n",
      "\n",
      "2\n",
      "2587\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-15 14:33\n",
      "@lesshaste might still be faster\n",
      "\n",
      "1\n",
      "2588\n",
      "53135b495e986b0712efc453\n",
      "2016-07-15 17:10\n",
      "@vighneshbirodkar Are you planning to continue work on the GBCV PR? Or would it be okay if I gave a hand?  (Either by push access to your repo or by cherry-picking the commits?)\n",
      "\n",
      "1\n",
      "2589\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-15 18:41\n",
      "I have a general ML classification question.. I hope this isn't the wrong place. I have data which is labelled into A, B, C, D, E and Other. In the end I just want to tell if new data is in A-E or other. That is perform a binary classification. A,B,C, D and E are very different from each other however. Should I build 5 binary classifiers and combine them somehow or just go straight for a binary A-E versus Other classifier? I am using a random forest currently.\n",
      "I could try to build a multiclass classifiers but the class sizes are very imbalanced and I am not sure how well RF copes with that\n",
      "\n",
      "2\n",
      "2590\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-15 19:29\n",
      "is there any tool to lint cython? through working on the tree .pyx files, ive noticed there are lots of style discrepancies...\n",
      "hmm is there any method that doesnt rely on pycharm? e.g. pep8 flake8 or a similar standard\n",
      "\n",
      "2\n",
      "2591\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-15 19:29\n",
      "@nelson-liu https://www.jetbrains.com/help/pycharm/2016.1/cython-support.html\n",
      "not that I am aware of sadly\n",
      "thanks\n",
      "\n",
      "3\n",
      "2592\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-15 19:46\n",
      "@nelson-liu https://udiboy1209.github.io/2016-06-18-cython-needs-a-flake-and-lint-tool/\n",
      "that's a pretty up to date complaint\n",
      "my pleasure.. any idea about my general classification query above by any chance?\n",
      "not really sure where the best place is for question like that so sorry it is a tiny bit OT for this gitter\n",
      "\n",
      "4\n",
      "2593\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-15 19:47\n",
      "haha ok, so seems like one doesnt exist sadly. thanks for the help @lesshaste :)\n",
      "\n",
      "1\n",
      "2594\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-15 19:50\n",
      "ive seen general ML questions here so it should (?) be fine? I dont think RF copes very well with unbalanced multiclass data...do you have ample data for each of the classes?\n",
      "\n",
      "1\n",
      "2595\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-15 19:50\n",
      "well I have millions of Other records but only thousands of the A-E cases\n",
      "is there anything better suited to the task?\n",
      "do you mean just sampling the same record repeatedly? I never saw the point in that to be honest\n",
      "\n",
      "3\n",
      "2596\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-15 19:53\n",
      "hmm have you looked into resampling methods?\n",
      "\n",
      "1\n",
      "2597\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-15 19:57\n",
      "hmm i mean you can try taking a random sample of your data, then fit your model on one set and use a held out set to find good probability cutoffs with an ROC curve or something...\n",
      "and you dont have to sample the same record repeatedly, but take a random sample of your other classes to generate a synthetic balanced dataset\n",
      "this answer on crossvalidated looks good, could be a starting point for more reading? http://stats.stackexchange.com/questions/131255/class-imbalance-in-supervised-machine-learning\n",
      "\n",
      "3\n",
      "2598\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-15 20:36\n",
      "\"Some methods, like random forests, don't need any modifications.\" I wonder how true that is\n",
      "\n",
      "1\n",
      "2599\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-15 21:06\n",
      "yeah, same. please let me know what ends up working best for you?\n",
      "\n",
      "1\n",
      "2600\n",
      "54e07d6515522ed4b3dc0858\n",
      "2016-07-15 21:10\n",
      "There's also https://github.com/fmfn/imbalanced-learn, which will soon become part of scikit-learn-contrib\n",
      "\n",
      "1\n",
      "2601\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-15 21:12\n",
      "this looks great, thanks for the link @vene !\n",
      "\n",
      "1\n",
      "2602\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-16 02:32\n",
      " weve reached 7000 PRs / issues :octocat:  not sure if its a cause for celebration, though :)\n",
      "\n",
      "1\n",
      "2603\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-16 07:27\n",
      "@vene oh cool!\n",
      "\n",
      "1\n",
      "2604\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-16 07:39\n",
      "Another basic question.. I have some data which is mixed categorical and numerical. There are various different ways I could preprocess it before trying to train my classifier. At the moment I just have a python script which I have to edit and rerun. Is there some neat way, using pipelines maybe(?) to make this all more systematic?  I would like to do the equivalent of gridsearch really but with the different ways of preprocessing the data\n",
      "\n",
      "1\n",
      "2605\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-16 12:05\n",
      "Well depends what you want to do? You could write your own transformers and that would work\n",
      "\n",
      "1\n",
      "2606\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-16 14:23\n",
      "hey sprinters!\n",
      "\n",
      "2\n",
      "2607\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-16 14:23\n",
      "I tagged some issues with \"sprint\" that might be good entry points: https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+is%3Aissue+label%3ASprint\n",
      "other good tags are \"easy\" and \"needs contributor\"\n",
      "You can find the contributor guide here: http://scikit-learn.org/dev/developers/index.html\n",
      "Please start with something very simple, happy to talk about more complicated issues. You can also start reviewing other pull requests, or see if there are pull requests that have been stalled for a long time.\n",
      "\n",
      "4\n",
      "2608\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-16 14:43\n",
      "@lesshaste thinking more about it, transformers are probably what you want. there are a bunch of preprocessing tools within the library natively but if youre writing your own scripts, its pretty trivial to wrap them in a transformer and put them into a pipeline\n",
      "\n",
      "2\n",
      "2609\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-07-16 14:46\n",
      "Hey sprinters. Thanks for helping us. Tell us if you need help for something. Have a good sprints.\n",
      "\n",
      "1\n",
      "2610\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-16 14:48\n",
      "if anyone needs help on things or setting up dev environment or finding things to start, id be happy to help\n",
      "\n",
      "1\n",
      "2611\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-16 15:00\n",
      "Btw, if any issue says \"change X\", if this is an API change or if it changes behavior of the code, instead of actually changing it, you need to deprecate the old behavior http://scikit-learn.org/dev/developers/contributing.html#deprecation\n",
      "btw, if you find bugs or stuff is not working properly, also feel free to open issues or talk to me or the other developers\n",
      "so if you want to create a new conda environment for the sprint, you should create one with all the dependencies, fork and clone the repo, and then do pip install -e . (while in the scikit-learn folder that you cloned)\n",
      "sure, thanks :)\n",
      "FYI, there are issues that need contributors that don't have any of the tags. the tags are a very rough approximation to the state of the issues\n",
      "\n",
      "5\n",
      "2612\n",
      "53135b495e986b0712efc453\n",
      "2016-07-16 15:33\n",
      "Hi to sprinters from Paris! Have fun! :)\n",
      "\n",
      "2\n",
      "2613\n",
      "53135b495e986b0712efc453\n",
      "2016-07-16 16:05\n",
      "I got bored at home and came to the lab. The Internet speed is very good here ;) And I will be glad to help if there is any PR to review. :)\n",
      "\n",
      "1\n",
      "2614\n",
      "56d577ffe610378809c46670\n",
      "2016-07-16 16:17\n",
      "@nelson-liu Hey, I'm a beginner in ML. I was looking for some good first time issues which don't involve completing documention. I went through the list of issues on GitHub but would really appreciate if you or anyone could point to any specific issue for beginners. Thanks.\n",
      "\n",
      "1\n",
      "2615\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-16 16:21\n",
      "this PR stalled and might be a good place to pick back up? https://github.com/scikit-learn/scikit-learn/issues/6670\n",
      "\n",
      "2\n",
      "2616\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-16 16:26\n",
      "I tried to make sure all the issues that are still available are tagged as \"need contributors\"\n",
      "\n",
      "1\n",
      "2617\n",
      "578a5f01c2f0db084a23472d\n",
      "2016-07-16 16:33\n",
      "Hi @amueller , I have some time and since there is a scikit learn sprint going on now, I was thinking of contributing remotely from Tuebingen. Is this possible?  I will be glad to work on some documentation stuff.\n",
      "\n",
      "5\n",
      "2618\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-16 16:44\n",
      "For anyone joining recently important links and advice is here: https://github.com/scikit-learn/scikit-learn/wiki/Scipy-2016-Sprint-instructions\n",
      "does anyone want to pick this up? https://github.com/scikit-learn/scikit-learn/pull/5551\n",
      "I'll be in 103 for a bit\n",
      "\n",
      "3\n",
      "2619\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-07-16 23:22\n",
      "If you need a review of your PR just add @tguillemot in the PR and I will have a look\n",
      "\n",
      "1\n",
      "2620\n",
      "54ac29f9db8155e6700e6980\n",
      "2016-07-17 20:45\n",
      "@amueller , I just sent a requirement about the book. thanks!\n",
      "\n",
      "1\n",
      "2621\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-18 10:00\n",
      "I am trying to follow the example at http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html but using a random forest. It seems decision_function doesn't exist for the random forest classifier\n",
      "what should I use instead (sorry for the simple question)?\n",
      "it is this line that is causing the problem y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
      "ok cancel that..silly me\n",
      "how can you mix calibratedclassifier and onevsrestclassifier? I just want to do classifier = OneVsRestClassifier(RandomForestClassifier()) but with the classifier correctly calibrated\n",
      "apologies if this is slightly OT but.. my really simple xgboost code for the MNIST data set is way slower than ExtraTreesClassifer http://paste.ubuntu.com/19943014/ . What am I doing wrong?\n",
      "\n",
      "6\n",
      "2622\n",
      "55d6ea0d0fc9f982beae242d\n",
      "2016-07-18 14:59\n",
      "Using KNN in sklearn, is there anyway to have the weights be based on the labels of datapoints? More specifically, I have some unbalanced data and would like to combat this by weighting samples of less common labels higher. I could of course just add copies of the under represented classes but this adds some non determinism if I choose the extra copies randomly so I thought just weighting them would be better.  Is this possible?\n",
      "\n",
      "1\n",
      "2623\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-18 15:03\n",
      "Hmm I don't think so. You could just set a random seed for sampling up if you're worried about reproducibility.\n",
      "\n",
      "1\n",
      "2624\n",
      "55d6ea0d0fc9f982beae242d\n",
      "2016-07-18 15:21\n",
      "yep sure, but should I add a feature request for this?\n",
      "\n",
      "1\n",
      "2625\n",
      "56d577ffe610378809c46670\n",
      "2016-07-19 10:23\n",
      "Hey, Can someone explain what's the difference between [MRG] and [MRG+1] in case of PR's?\n",
      "\n",
      "1\n",
      "2626\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 10:30\n",
      "@manu-chroma  What does MRG stand for to start with?\n",
      "\n",
      "1\n",
      "2627\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 10:39\n",
      "MRG is for the PR when it is ready for review, MRG + X is when X number of core developers are okay with PR. In general it is around 2 or 3.\n",
      "\n",
      "7\n",
      "2628\n",
      "56d577ffe610378809c46670\n",
      "2016-07-19 10:40\n",
      "@lesshaste https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist\n",
      "\n",
      "1\n",
      "2629\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 10:42\n",
      "I have just tried for the first time to use xgboost with sklearn and am getting poor results and it is very slow.  I realise xgboost is not core scikit-learn but would any very kind person be able to give a newbie a hand please?\n",
      "my very simple sample script is just running on the MNIST digits data and it is at http://paste.ubuntu.com/20022811/  . What am I doing wrong?\n",
      "\n",
      "2\n",
      "2630\n",
      "5508681715522ed4b3dd6630\n",
      "2016-07-19 10:54\n",
      "Hello, I am trying to do MNIST task with data from Kaggle and not data in scikitlearn. I am getting accuracies ~90% using LinearSVC, SGDClassifier and KNeighborsClassifier.  But every algorithm just gives a label for a test image as output. That is fine.  I just wanted to have log probabilities of each class. For example if a test image is given- I want an array of length of classes with probabilities that test image is from this class.\n",
      "Is there any way to do that using sklearn?\n",
      "No.\n",
      "I will go through that.\n",
      "\n",
      "4\n",
      "2631\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 10:56\n",
      "@SnShine  Are you using predict_proba ?\n",
      "\n",
      "1\n",
      "2632\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 11:14\n",
      "@SnShine As @lesshaste said, this might probably be what you are looking for http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba\n",
      "@lesshaste Even I am a novice and haven't used XGBoost before but maybe the loss function needs to be changed for multiclass classification ?\n",
      "\n",
      "2\n",
      "2633\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 11:27\n",
      "@maniteja123 Let me try that. There must be at least one xgboost here soon :)\n",
      "\n",
      "1\n",
      "2634\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 11:36\n",
      "@maniteja123  According to https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py it should use multi:softprob if the number of classes > 2 I think\n",
      "\n",
      "1\n",
      "2635\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 11:42\n",
      "Yup I suppose you are right, it should set the objective automatically as in https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py#L405.  Sorry I have no idea about the performance then. Just to know, what is the cv score you are getting using XGBoost ?\n",
      "\n",
      "1\n",
      "2636\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 11:43\n",
      "I get [ 0.9323022 0.93157206 0.92796762 0.93271406 0.93854216] from 5-fold cv of xgboost and it takes a really really long time\n",
      "I get [ 0.96216538 0.96465548 0.96428146 0.96558295 0.9670081 ] from ExtraTreesClassifier and it is really quick\n",
      "I am clearly doing something wrong\n",
      "\n",
      "4\n",
      "2637\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 11:48\n",
      "can you reproduce my problem?\n",
      "\n",
      "1\n",
      "2638\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 11:48\n",
      "Oh thanks for the results. I too will try running the script on my machine and tune various parameters for the XgBoost model.\n",
      "My machine doesn't have a good RAM. But will try to reproduce in VM and get back to you.\n",
      "\n",
      "5\n",
      "2639\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 11:51\n",
      "http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn Have you looked at the ``evals_result`` dict mentioned in the docs\n",
      "\n",
      "3\n",
      "2640\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 11:53\n",
      "Also this page http://xgboost.readthedocs.io/en/latest//parameter.html has ``eval_metric`` which has ``merror`` for multi class error\n",
      "I should thank you for getting me started to look into this :)\n",
      "I am not sure if it might help but it seemed relevant to multi class but was mentioned it was specific to the objective.``eval_metric [ default according to objective ]``. So do have a look and let us know if you get any insight. Thanks!\n",
      "Oh I see. I don't know any other forum. Maybe stack overflow ? (Sorry for my naive suggestions)\n",
      "\n",
      "4\n",
      "2641\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 12:42\n",
      "Interesting.. I am currently also confused why it is so slow when everyone says how fast xgboost is!\n",
      "@maniteja123  just changing to objective=\"multi:softmax\" increased the CV scores!\n",
      "for reasons I am 100% unclear about\n",
      "\n",
      "3\n",
      "2642\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 15:29\n",
      "Hi, sorry for the slow reply. That's great to know. Is the algorithm running faster now ?\n",
      "\n",
      "9\n",
      "2643\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 15:36\n",
      "Okay so I too am waiting for the rra\n",
      "*reply\n",
      ":-)\n",
      "\n",
      "3\n",
      "2644\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 15:37\n",
      "one problem is that I don't understsand the parameters despite the huge number of \"guides\" online\n",
      "which make xgboost run slower or faster? Can you find the answer to that?\n",
      "\n",
      "2\n",
      "2645\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 15:40\n",
      "Maybe having a look at this paper https://arxiv.org/pdf/1603.02754v3 can help  us in understanding the algorithm better ? I too shall try reading it once\n",
      "\n",
      "5\n",
      "2646\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-19 15:51\n",
      "@lesshaste have you tried with different number of threads ? I saw some issue about multi threading here https://github.com/dmlc/xgboost/issues/523\n",
      "\n",
      "4\n",
      "2647\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-19 16:19\n",
      "@maniteja123  I have a much simpler question now :)  I simply want to modify http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html for the digits dataset we are using\n",
      "have you managed to get that to work?\n",
      "@maniteja123  thanks.. that was indeed the  answer\n",
      "\n",
      "3\n",
      "2648\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-21 20:44\n",
      "hi.. I am trying to find the most important features in a large random forest and it takes about 1 second per feature and there are 10s of thousands of features. The random forest only took ten minutes to build in total.\n",
      "am I doing something wrong or is this expected?\n",
      "ah.. I think I might realise my mistake!\n",
      "for i in xrange(len(clf.feature_importances_)):     print clf.feature_importances_[i]\n",
      "\n",
      "4\n",
      "2649\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-21 21:24\n",
      "does this recompute the whole feature_importances_ array in each step of the for loop?\n",
      "\n",
      "1\n",
      "2650\n",
      "56b80528e610378809c05a48\n",
      "2016-07-23 08:53\n",
      "Hello can anyone help me on this [test-failing](https://github.com/scikit-learn/scikit-learn/pull/6913#issuecomment-234704911) issue?\n",
      "My test pass on 64-bit computer but fail on 32-bit computer, and the occurs at [this line](https://github.com/yenchenlin/scikit-learn/blob/cd-fused-types/sklearn/src/cblas/ATL_srefaxpy.c#L131).\n",
      "I wonder what is the difference to call [ATL_srefaxpy.c](https://github.com/yenchenlin/scikit-learn/blob/cd-fused-types/sklearn/src/cblas/ATL_srefaxpy.c) on 32-bit and 64-bit computer when fitting `np.float32` data?\n",
      "And the link you pointed out seems obviously wrong :smile:\n",
      "\n",
      "4\n",
      "2651\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-23 10:09\n",
      "Hi everyone, just a small question. Is the link to ``nose`` package [here](http://scikit-learn.org/stable/developers/advanced_installation.html#testing) the expected one ?\n",
      "@yenchenlin1994 the tests are failing on AppVeyor , so it is Windows environment right ? Is it failing on 32 bit machines working on ubuntu or mac too ?\n",
      "\n",
      "4\n",
      "2652\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-23 10:15\n",
      "Yeah the other day someone was asking about the testing instructions and then I accidentally stumbled upon this. I just thought it should be clarified once !\n",
      "\n",
      "1\n",
      "2653\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-23 10:24\n",
      "And in case you don't have access to one, I can checkout your branch and check on my machine. I am running Ubuntu 14.04 on 32 bit VM machine.\n",
      "\n",
      "1\n",
      "2654\n",
      "56b80528e610378809c05a48\n",
      "2016-07-23 10:25\n",
      "I only try to reproduce it using 32-bit Python but run on an 64-bit mac\n",
      "thanks a lot!\n",
      "It looks good on 32-bit Python with 64-bit mac\n",
      "\n",
      "3\n",
      "2655\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-07-23 10:32\n",
      "Oh okay. I have no other idea about the possibility for the error. Sorry.\n",
      "\n",
      "1\n",
      "2656\n",
      "56b80528e610378809c05a48\n",
      "2016-07-24 02:03\n",
      "Hello @maniteja123 , can you tell me the results on Ubuntu 14.04, 32 bit VM machine?\n",
      "thanks!\n",
      "\n",
      "2\n",
      "2657\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-24 08:09\n",
      "hi... has there been any work in incorporating minhash to help cluster large sets of documents?\n",
      "\n",
      "1\n",
      "2658\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-24 08:15\n",
      "also.. in this very highly cited paper \"Clustering Large Graphs via the Singular Value Decomposition\" http://www.cc.gatech.edu/fac/vempala/papers/dfkvv.pdf the main algorithm is to take a random submatrix (suitably chosen) to make the SVD computation feasible. Has this been proposed at all for sklearn?\n",
      "\n",
      "1\n",
      "2659\n",
      "572c72eec43b8c60197173f7\n",
      "2016-07-24 11:31\n",
      "<unconvertable> , .,... .....   <unconvertable> .,,\n",
      "<unconvertable>\n",
      "\n",
      "2\n",
      "2660\n",
      "53135b495e986b0712efc453\n",
      "2016-07-25 14:32\n",
      "@amueller I spotted you online ;) Could you take a look at https://github.com/scikit-learn/scikit-learn/pull/7071 please? ;)\n",
      "\n",
      "1\n",
      "2661\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-25 14:32\n",
      "@raghavrv is it still wip? ;)\n",
      "\n",
      "3\n",
      "2662\n",
      "565c21ed16b6c7089cbcae65\n",
      "2016-07-26 13:45\n",
      "Hi guys i'm new to the room i'm working on a subject and i'm looking for someone that have experience with this subject to advice me and hopefully point me to the right direction, my project is as following a document classification i have a big data base with texts for all kind off categories holidays politics sport etc i already managed for now if if i enter a text to my code that it detects which category it belongs using SVM classifier\n",
      "what i want to do next is to predict a category for a costumer but until now i don't know yet how to start this\n",
      "\n",
      "2\n",
      "2663\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-26 13:46\n",
      "do you have any training data?\n",
      "you want a recommendation system it seems\n",
      "have you looked up that general term?\n",
      "but it's completely offtopic here\n",
      "\n",
      "4\n",
      "2664\n",
      "565c21ed16b6c7089cbcae65\n",
      "2016-07-26 13:47\n",
      "yes i do have a training data\n",
      "yes i wnat to have a recommendation system\n",
      "to explain more my situation  i'm doing in an internship and my company has a data base with tones of messages each text is tagged to a catogery\n",
      "\n",
      "3\n",
      "2665\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-26 13:51\n",
      "ok sure.. so the first thing to do is to look up recommendation systems really\n",
      "are you stuck somewhere?\n",
      "\n",
      "2\n",
      "2666\n",
      "565c21ed16b6c7089cbcae65\n",
      "2016-07-26 13:52\n",
      "until now what i managed to do is classify this data and when i enter a text as an input my system manages to tell me to which catogery it belongs\n",
      "now my next step is prediction i have to predict what a user wants i looked up some terms like recommendation system but didn't find something really helpfull what i can start with\n",
      "\n",
      "2\n",
      "2667\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-07-26 13:57\n",
      "try http://muricoca.github.io/crab/\n",
      "\n",
      "1\n",
      "2668\n",
      "565c21ed16b6c7089cbcae65\n",
      "2016-07-26 15:26\n",
      "what do you think about pyspark\n",
      "\n",
      "2\n",
      "2669\n",
      "565c21ed16b6c7089cbcae65\n",
      "2016-07-26 15:38\n",
      "im on a linux debian trying to install it\n",
      "there some preinstalls to do\n",
      "java 8 scala etc...\n",
      "\n",
      "3\n",
      "2670\n",
      "565c21ed16b6c7089cbcae65\n",
      "2016-07-26 16:36\n",
      "i used scilearn for my code so i thought ill get some from info here but thank you ill stop bothering you guys\n",
      "\n",
      "1\n",
      "2671\n",
      "561a58f7d33f749381a8ff2f\n",
      "2016-07-26 19:55\n",
      "guys, what do you think about this post :D http://stats.stackexchange.com/questions/225773/sparsity-as-missing-data-mle\n",
      "\n",
      "1\n",
      "2672\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-26 21:10\n",
      "does anyone know anything about the sample_without_replacement  function... it's kinda weird\n",
      "\n",
      "1\n",
      "2673\n",
      "57908b05c2f0db084a23fc9a\n",
      "2016-07-28 08:42\n",
      "Has anyone used Amazon Machine Learning... I am sorry if I have asked question in wrong chat room\n",
      "\n",
      "1\n",
      "2674\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-07-28 14:33\n",
      "Is there any problems with travis, appveyor, ... ?\n",
      "The new PR are not tested\n",
      "\n",
      "2\n",
      "2675\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-28 14:46\n",
      "@87sanchavan_twitter yeah this is not a good place to ask. Go to stackoverflow\n",
      "@tguillemot I noticed that yesterday. Can you point me towards a PR?\n",
      "\n",
      "2\n",
      "2676\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-07-28 14:48\n",
      "@amueller #6651 #7101\n",
      "That's strange because I saw that the travis works on it on the activity (on the right)\n",
      "\n",
      "2\n",
      "2677\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-28 15:13\n",
      "fuck that was me\n",
      "\n",
      "1\n",
      "2678\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-07-28 15:14\n",
      "@amueller No pb ;)\n",
      "\n",
      "4\n",
      "2679\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-07-28 15:19\n",
      "@amueller Travis was launched on #6651\n",
      "but not circle or appveyor\n",
      "Just to know what was the problem ?\n",
      "ok for appveyor and travis\n",
      "\n",
      "4\n",
      "2680\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-28 15:20\n",
      "now I only need to find out how access to appveyor and coveralls and circleci was granted\n",
      "\n",
      "2\n",
      "2681\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-28 15:52\n",
      "and again\n",
      "\n",
      "1\n",
      "2682\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-28 17:46\n",
      "hm looks like I took away my ability to restart travis or clear the cache\n",
      "great\n",
      "ah, fixed\n",
      "\n",
      "3\n",
      "2683\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-28 19:50\n",
      "@raghavrv you around?\n",
      "@raghavrv Where is the warm start used in the gradient boosting CV?\n",
      "\n",
      "2\n",
      "2684\n",
      "56c9d685e610378809c29d5d\n",
      "2016-07-29 16:24\n",
      "I don't want to derail this chat so if there's a more appropriate place please redirect me.  I'm working on an ML library which draws a lot of inspiration from sklearn. I'd love to talk with some of the devs on sklearn who may be able to share some insights into things they'd like to be able to change/do differently. Or things that they think have worked really well that I might not appreciate.\n",
      "\n",
      "1\n",
      "2685\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-29 18:01\n",
      "@raghavrv you around now?\n",
      "@AtheMathmo which language?\n",
      "\n",
      "2\n",
      "2686\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-29 18:02\n",
      "@AtheMathmo have you read the API paper? And there are talks by me and by Gael about the api design\n",
      "\n",
      "1\n",
      "2687\n",
      "56c9d685e610378809c29d5d\n",
      "2016-07-29 23:12\n",
      "@amueller It's written in rust. Happy to share here if you want. I also gave a talk on it yesterday that I would be happy to share.  I haven't read the API paper or seen the talks. Would you be able to provide me links to those? Thanks!\n",
      "\n",
      "1\n",
      "2688\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-29 23:14\n",
      "@AtheMathmo heres the paper: https://arxiv.org/abs/1309.0238\n",
      "\n",
      "1\n",
      "2689\n",
      "56c9d685e610378809c29d5d\n",
      "2016-07-29 23:15\n",
      "@nelson-liu thanks!\n",
      "\n",
      "1\n",
      "2690\n",
      "56c9d685e610378809c29d5d\n",
      "2016-07-29 23:48\n",
      "I really like how the transformer interface is described. It's a problem I haven't found a nice way to tackle yet in my own work.\n",
      "\n",
      "1\n",
      "2691\n",
      "53135b495e986b0712efc453\n",
      "2016-07-30 00:08\n",
      "@amueller Sorry I totally missed the chat... :( BTW the warm start is being set at [this line](https://github.com/scikit-learn/scikit-learn/pull/7071/files#diff-439a21b751083bf0e4a535e8f9075520R794)\n",
      "\n",
      "1\n",
      "2692\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-07-30 13:53\n",
      "yeah found it finally ;)\n",
      "\n",
      "1\n",
      "2693\n",
      "578a4f87c2f0db084a23455f\n",
      "2016-07-30 17:11\n",
      "Does \"LGTM\" mean +1, or it's kind of uncertain +1? Never seen this in other projects.\n",
      "\n",
      "1\n",
      "2694\n",
      "56b80528e610378809c05a48\n",
      "2016-07-30 17:13\n",
      "Generally LGTM from a core contributor means +1\n",
      "\n",
      "3\n",
      "2695\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-07-31 23:26\n",
      "is there anyway to run gridsearchcv and save _all_ all of the models you train to disk?\n",
      "or is it only possible to get the `best_estimator_` and then just save that\n",
      "\n",
      "2\n",
      "2696\n",
      "53135b495e986b0712efc453\n",
      "2016-08-01 13:35\n",
      "That would be extremely space costly for some models. For instance knn basically stores your training data. If you decide to store all the models that would explode the memory. This is why it is not implemented. Do you have any specific use case where you need this? Also the `results_`give you more statistics than before. Finally you could always loop through all the parameters and retrain the models if you need.\n",
      "\n",
      "1\n",
      "2697\n",
      "57a0575540f3a6eec05d8bc7\n",
      "2016-08-02 08:22\n",
      "can anyone pleaes tell me where should I start for data science\n",
      "\n",
      "1\n",
      "2698\n",
      "56b80528e610378809c05a48\n",
      "2016-08-02 08:23\n",
      "Quora is your friend :)  https://www.quora.com/How-can-I-become-a-data-scientist-1\n",
      "Its better to ask these kind of questions on quora (and youll get more responses), this place is more specific for scikit-learn development\n",
      "\n",
      "2\n",
      "2699\n",
      "57a0575540f3a6eec05d8bc7\n",
      "2016-08-02 08:41\n",
      "@yenchenlin Thanks for this But I have concern with language whether I should go with python or R\n",
      "\n",
      "2\n",
      "2700\n",
      "57a0575540f3a6eec05d8bc7\n",
      "2016-08-02 08:46\n",
      "@yenchenlin Currently I am working with python  and I am new to machine learning and for that I was asking\n",
      "\n",
      "1\n",
      "2701\n",
      "5644994d16b6c7089cba759b\n",
      "2016-08-02 10:02\n",
      "correct choice.\n",
      "\n",
      "1\n",
      "2702\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-08-02 18:26\n",
      "@leem2714 it depends a lot on what you want to do. Are you more interested in prediction or inference? What kind of methods would you like to use?\n",
      "\n",
      "1\n",
      "2703\n",
      "56e7a3c885d51f252ab8d6ce\n",
      "2016-08-04 08:02\n",
      "hello guysss i m new here and what t learn data science can anyone suggest good stuff for learning data science\n",
      "\n",
      "1\n",
      "2704\n",
      "5729ef37c43b8c60197119b1\n",
      "2016-08-04 09:56\n",
      " @phalodi You will probably get more info on Quora or even just doing a search on Google.  This area is more suited to specific scikit-learn development.\n",
      "\n",
      "1\n",
      "2705\n",
      "56b0a775e610378809bf7a7c\n",
      "2016-08-04 11:55\n",
      "@amueller @ogrisel coveralls is still down, can you reset it ?\n",
      "\n",
      "1\n",
      "2706\n",
      "57a39e9340f3a6eec05df999\n",
      "2016-08-04 20:05\n",
      "hey guys, i'm new here. i just want to confirm if anyone has successfully installed cuda in ubuntu 16.04 for theano gpu usage?\n",
      "\n",
      "1\n",
      "2707\n",
      "56d577ffe610378809c46670\n",
      "2016-08-05 11:20\n",
      "Hey, I'm not able to locate the ``load_breast_cancer`` in http://scikit-learn.org/dev/modules/classes.html#module-sklearn.datasets. The only reference I can find in docs is here http://scikit-learn.org/dev/datasets/index.html#breast-cancer-wisconsin-diagnostic-database\n",
      "There is no reference to it's method details. Why is so ?\n",
      "\n",
      "2\n",
      "2708\n",
      "53135b495e986b0712efc453\n",
      "2016-08-05 15:40\n",
      "@amueller Andy are you around and have a few minutes to talk about the gbcv?\n",
      "\n",
      "1\n",
      "2709\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-08-09 20:21\n",
      "@raghavrv sorry not today\n",
      "have you talked with @pprett ?\n",
      "\n",
      "2\n",
      "2710\n",
      "53135b495e986b0712efc453\n",
      "2016-08-10 11:52\n",
      "Np. I'll try sending him an e-mail and let you know...\n",
      "\n",
      "1\n",
      "2711\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-08-12 14:58\n",
      "Hello, is there an easy way to get leaf node count from decision tree? I can get total node count, but is there a way to get also leaf node count?\n",
      "Ok, thanks. I'll do that\n",
      "\n",
      "2\n",
      "2712\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-12 16:35\n",
      "@mkoske first get `tree._tree.left_children`and right_children I think it's called. Then count the number of elements where the value is - 1, those are leaves\n",
      "\n",
      "1\n",
      "2713\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-12 16:42\n",
      "``` In [1]: from sklearn.datasets import load_boston In [2]: from sklearn.tree import DecisionTreeRegressor In [3]: boston = load_boston() In [4]: tree = DecisionTreeRegressor() In [5]: tree.fit(boston.data, boston.target) In [6]: internal_tree = tree.tree_ In [7]: left_children = internal_tree.children_left In [8]: right_children = internal_tree.children_right In [9]: leaf_count = 0 In [10]: for i in left_children:     ...:     if i == -1:     ...:         leaf_count += 1     ...: In [11]: for i in right_children:     ...:     if i == -1:     ...:         leaf_count += 1     ...: In [12]: leaf_count Out[12]: 940 ```\n",
      "\n",
      "1\n",
      "2714\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-12 17:04\n",
      "oops, sorry you only have to go through it once.....\n",
      "going through it twice is double counting\n",
      "\n",
      "2\n",
      "2715\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-12 17:13\n",
      "since left_children and right_children are -1 if they dont have a left or right child, (thus leaf) and a node cant have only a left or a right child.\n",
      "\n",
      "1\n",
      "2716\n",
      "55f3a7830fc9f982beb071a9\n",
      "2016-08-14 14:41\n",
      "Does anyone know a good reference for how the verbose arguement acts?\n",
      "\n",
      "1\n",
      "2717\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-08-14 18:56\n",
      "how does decision tree find the cut point?\n",
      "\n",
      "1\n",
      "2718\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-14 18:57\n",
      "haha youre getting yourself into a pretty deep rabbit hole here ;) do you mean decision trees in general or specifically scikit-learns implementation\n",
      "\n",
      "4\n",
      "2719\n",
      "565b64bd16b6c7089cbc9de9\n",
      "2016-08-15 11:27\n",
      "Hi, is  `skl.svm.LinearSVC` handling unknown/NaN values? I couldn't find that information in the class documentation page\n",
      "\n",
      "1\n",
      "2720\n",
      "53135b495e986b0712efc453\n",
      "2016-08-16 14:05\n",
      "@nirizr You need to impute those missing values (using `sklearn.preprocessing.Imputer`) first before passing on to the `LinearSVC`.\n",
      "\n",
      "1\n",
      "2721\n",
      "56b80528e610378809c05a48\n",
      "2016-08-16 14:07\n",
      "@raghavrv I got you! Can you give #7187 another quick review?\n",
      "\n",
      "1\n",
      "2722\n",
      "565b64bd16b6c7089cbc9de9\n",
      "2016-08-16 14:08\n",
      "@raghavrv Thanks a lot! Are there easy ways to get more advanced strategies? regression, nearest neighbors?\n",
      "\n",
      "1\n",
      "2723\n",
      "53135b495e986b0712efc453\n",
      "2016-08-16 14:34\n",
      "@yenchenlin Thanks for the ping. Done... Some minor comments and +1 :)\n",
      "\n",
      "1\n",
      "2724\n",
      "53135b495e986b0712efc453\n",
      "2016-08-16 15:52\n",
      "@ogrisel you around? ;)\n",
      "\n",
      "1\n",
      "2725\n",
      "56212f4e16b6c7089cb74321\n",
      "2016-08-16 16:02\n",
      "hey everyone, i have a _curiosity_ for you!\n",
      "``` def pipeline_to_weights_and_bias(clf):    <unconvertable> \"\"\" Turns a SKLearn StandardScaler() --> LogisticRegression()  <unconvertable> <unconvertable> pipeline into single weights and bias. \"\"\"  <unconvertable> <unconvertable> assert set(clf.named_steps.keys()) == {'logisticregression', 'standardscaler'}  <unconvertable> <unconvertable> lr = clf.named_steps['logisticregression']  <unconvertable> <unconvertable> sc = clf.named_steps['standardscaler']  <unconvertable> <unconvertable> W = (lr.coef_ / sc.scale_)  <unconvertable> <unconvertable> B = lr.intercept_ - (lr.coef_ / sc.scale_).dot(sc.mean_.T)  <unconvertable> <unconvertable> return (W, B) ```\n",
      "for those doing logistic regression, this turns a StandardScaler + LR pipeline into a single weight+bias matrix\n",
      "you use it like this: ``` X = np.random.randn(10000, 512) W,B = pipeline_to_weights_and_bias(clf) assert np.allclose( clf.decision_function(X), W.dot(X.T) + B )  %timeit clf.decision_function(X) #=> 10 loops, best of 3: 122 ms per loop  %timeit W.dot(X.T) + B #=> The slowest run took 7.54 times longer than the fastest. This could mean that an intermediate result is being cached. #=> 1000 loops, best of 3: 265 us per loop ```\n",
      "the speedup is significant\n",
      "\n",
      "5\n",
      "2726\n",
      "56212f4e16b6c7089cb74321\n",
      "2016-08-16 16:03\n",
      "the real funny part is that the resulting calculation runs only on a single core, even though it's hundreds of times faster than sklearn vanilla pipeline\n",
      "now, i have 1800 of these classifiers to run. it's much faster to do a (1800,512) * (512, N) matrix operation than to call 1800 pipelines in sequence\n",
      "this makes me wonder if sklearn couldn't benefit from some pipeline optimization tricks? i know i'd love like a `FastAssortmentOfScaledLogisticRegressionClassifiers` class\n",
      "but of course there could be other possible ways to hand-tune other combinations of linear transformations (e.g. PCA and random projections are the same thing)\n",
      "\n",
      "4\n",
      "2727\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-16 16:04\n",
      "hmm, thats pretty interesting\n",
      "\n",
      "1\n",
      "2728\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-16 16:07\n",
      "for that question, I suggest you raise an issue; not everyone is on / checks gitter :)\n",
      "\n",
      "1\n",
      "2729\n",
      "564a00d016b6c7089cbae908\n",
      "2016-08-16 22:20\n",
      "I want to build a recommendation system for movies. What are all things I should learn. I am presently doing undergraduate course with basics in python, web dev and java.\n",
      "\n",
      "1\n",
      "2730\n",
      "564a00d016b6c7089cbae908\n",
      "2016-08-16 22:33\n",
      "I had completed ML course by Andrew ng and ML through case study by Carlos on coursera\n",
      "\n",
      "2\n",
      "2731\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-08-17 06:51\n",
      "just out of curiousity, am I supposed to pour the whole dataset to `gridsearchcv.fit`?\n",
      "my dataset has roughly 15M (* 500 features) in total, and I am testing with just 2M of them, I wanted to throw them all into `.fit` but kept getting out-of-memory warning (obviously)\n",
      "\n",
      "2\n",
      "2732\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-17 20:04\n",
      "Can you pass .5mil at a time 4 times?\n",
      "I mean if you are being limited by the memory.. probably make data set smaller? :)\n",
      "@shivakrishna9 depends on how you are trying to relate those 2.\n",
      "\n",
      "3\n",
      "2733\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-17 20:24\n",
      "@Jeffrey04 a common thing people do to get around this is to just get more ram ;) might be worth looking into using an AWS machine or something for a little bit. Passing .5mil 4 times isnt really theoretically sound, because then its difficult to discern which model is actually the best because the results might be affected by the fact that the model only sees part of the dataset.\n",
      "\n",
      "1\n",
      "2734\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-17 20:49\n",
      "@nelson-liu ahh i see, but is a model with .5mil better then a model with .5mil * 4 model? There might be a chance for certain training set, the second is better?\n",
      "I mean, i think i see what you mean (error from each .5mil being fed adds up (maybe compounds)).\n",
      "\n",
      "2\n",
      "2735\n",
      "565b64bd16b6c7089cbc9de9\n",
      "2016-08-17 20:51\n",
      "@Jeffrey04 It might be worth seeing how diverse is your data, perhaps you can neatly drop a certain percentage of it without impacting the model too much.\n",
      "\n",
      "1\n",
      "2736\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-17 20:52\n",
      "sure, but the point of using GridSearchCV is to pick the best model for an unseen test set, is it not? <unconvertable> more data is better <unconvertable> is a common adage that is generally true. but lets say you have 1.5 million mislabeled samples (for some reason); if you were to  luckily select just the .5 million samples that were clean and train on them, youd do well. By selecting a subset of the data, youre inherently biasing the model a bit.\n",
      "training on 4 partitions of a set and picking the one that does best on the most out of 4 != training on the whole set and picking the best one\n",
      "\n",
      "10\n",
      "2737\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-17 21:03\n",
      "Is there any other keyword i can search for? \"warm start scikit\" isn't giving me anything that explains concepts. :s\n",
      "\n",
      "1\n",
      "2738\n",
      "565b64bd16b6c7089cbc9de9\n",
      "2016-08-17 21:04\n",
      "Can gridsearchcv implement partial_fit for estimators that support it?\n",
      "\n",
      "1\n",
      "2739\n",
      "565b64bd16b6c7089cbc9de9\n",
      "2016-08-17 21:14\n",
      "It's definitely not possible now. I'm not familiar with sklearn's code at all, but looks like a call to `fit` may be replaced with a call to `partial_fit`, when it is supported.\n",
      "\n",
      "1\n",
      "2740\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-17 21:15\n",
      "Thats good to know! partial_fit is something like a warm start then?\n",
      "This is a bit offtopic but : http://webhostingegg.com/vps/top-alternative-amazon-aws-ec2/\n",
      "Wondering if there were non aws recommendation.\n",
      "\n",
      "3\n",
      "2741\n",
      "565b64bd16b6c7089cbc9de9\n",
      "2016-08-17 21:19\n",
      "`partial_fit` feeds the estimator with partial data iteratively, having it only process portions of all available data at a time. There are some considerations there (how to split the partitions, sizes, the order in which data is fed) and sklearn doesn't support that for gridsearchcv as far as I can tell.\n",
      "\n",
      "2\n",
      "2742\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-08-17 21:36\n",
      "@nelson-liu Hello! You gave me earlier (few days ago) short code snippet to count leaf nodes. But It seems not to work and I don't know why. The value for `leaf_count` variable seems to be even one more than `tree.node_count`.\n",
      "So, my tree has ~2300 leaf nodes :) That's large tree\n",
      "\n",
      "2\n",
      "2743\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-17 21:36\n",
      "i mentioned after the snippet to remove one of the loops\n",
      "so if you take out one of the for loops, that should do the trick\n",
      "\n",
      "2\n",
      "2744\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-17 21:37\n",
      "``` In [1]: from sklearn.datasets import load_boston In [2]: from sklearn.tree import DecisionTreeRegressor In [3]: boston = load_boston() In [4]: tree = DecisionTreeRegressor() In [5]: tree.fit(boston.data, boston.target) In [6]: internal_tree = tree.tree_ In [7]: left_children = internal_tree.children_left In [8]: right_children = internal_tree.children_right In [9]: leaf_count = 0 In [10]: for i in left_children:     ...:     if i == -1:     ...:         leaf_count += 1     ...: ```\n",
      "because left_children is an array that maps each node to another node that is its left child\n",
      "but if it has no left child, its marked as -1\n",
      "a node cannot have a left child and no right child or no right child and a left chlid, so iterating through one of the children arrays is enough\n",
      "\n",
      "5\n",
      "2745\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-17 22:50\n",
      "http://scikit-learn.org/stable/auto_examples/decomposition/plot_image_denoising.html  Didn't realize you can do crazy stuff like this.. :o\n",
      "\n",
      "1\n",
      "2746\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-08-18 04:55\n",
      "@ItchyJunk OOOoo? gridsearchcv allows multiple calls to `.fit`?\n",
      "\n",
      "1\n",
      "2747\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-08-18 05:03\n",
      "OOOOOOOOH, yea, I am using sgdclassifier, lemme try (: > It's definitely not possible now. I'm not familiar with sklearn's code at all, but looks like a call to `fit` may be replaced with a call to `partial_fit`, when it is supported.\n",
      "\n",
      "1\n",
      "2748\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-08-18 05:21\n",
      "well after a quick test i am able to do multiple `.fit` calls, but not sure whether the classifier uses all the training set tho\n",
      "i misread for some reason\n",
      "*facepalm*\n",
      "argh... markdown\n",
      "\n",
      "4\n",
      "2749\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-08-18 05:55\n",
      "I really need to get some sleep, i misread and thought this is already implemented #facepalm > It's definitely not possible now. I'm not familiar with sklearn's code at all, but looks like a call to `fit` may be replaced with a call to `partial_fit`, when it is supported.\n",
      "\n",
      "1\n",
      "2750\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-18 08:30\n",
      "@Jeffrey04 multiple .fit is not the same as calling 1 .fir for the 2M data apparent.y\n",
      "\n",
      "6\n",
      "2751\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-08-18 09:12\n",
      "@ItchyJunk that's what I did previously, i was wondering if that's the proper way to do it\n",
      "if i fetch fair enough random sample for gridsearchcv, I probably should get parameters that should be quite close to the \"ideal\" ones\n",
      "then i can proceed and retrain the proper model with the not-so-ideal parameters\n",
      "^ my previous workaround\n",
      "\n",
      "4\n",
      "2752\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-08-18 09:15\n",
      "also I am trying to break into multiple smaller classifiers, so each classifier should be built with much smaller dataset\n",
      "so whenever i try to classify some data, i would run through all of them, and pick the best result with highest probability or something\n",
      "\n",
      "2\n",
      "2753\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-18 09:18\n",
      "Yeah I suppose if the difference is small enough, approximation should be fine.\n",
      "\n",
      "1\n",
      "2754\n",
      "5730c2dcc43b8c601971eca1\n",
      "2016-08-18 15:49\n",
      "Quick question:  After having downloaded scikit-learn on my machine, How can I compile the modified code package to iPython (ie import sklearn2 as sk ?) . Where should I put the repository scikit-learn so Python can recognize it ?\n",
      "\n",
      "1\n",
      "2755\n",
      "565b64bd16b6c7089cbc9de9\n",
      "2016-08-18 16:24\n",
      "@Jeffrey04 I do think it is quite easy to add, as far as I can understand sklearn's code.\n",
      "\n",
      "1\n",
      "2756\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-19 02:37\n",
      "@arita37 Not sure what you are asking? are you asking how to import the scikit in your python code? In which case, you just import it like any module. I might be misunderstanding though.\n",
      "\n",
      "1\n",
      "2757\n",
      "57b71cc040f3a6eec0605326\n",
      "2016-08-19 14:52\n",
      "Hey. Is it me or the current python version does not have neural networks ? I followed that : http://scikit-learn.org/dev/modules/neural_networks_supervised.html and got an import error with \"from sklearn.neural_network import MLPClassifier\"\n",
      "Ok thanks a lo\n",
      "t\n",
      "\n",
      "3\n",
      "2758\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-08-19 14:52\n",
      "neural nets are in 0.18, not 0.17.1 (which is the stable version)\n",
      "theres a release coming soon, but you can get it in the meanwhle by downloading and installing the master branch\n",
      "\n",
      "2\n",
      "2759\n",
      "56f2390385d51f252aba4de9\n",
      "2016-08-21 00:12\n",
      "Does anyone know how the images are generated in the \"example gallery\" documentation?Im trying to contribute an example but the plot legend and title are cut off on the html page in my local build.\n",
      "\n",
      "1\n",
      "2760\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-08-21 09:53\n",
      "@mlliou112  I don't think dropout has been implemented yet which afaik is crucial for getting good out of sample prediction performance\n",
      "I once saw a PR for it I think but I am not sure what happened\n",
      "\n",
      "2\n",
      "2761\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-08-21 09:56\n",
      "https://github.com/scikit-learn/scikit-learn/issues/6175 and https://github.com/glennq/scikit-learn/tree/mlp_dropout_new . Maybe these are stalled?\n",
      "on a similar note... has https://github.com/scikit-learn/scikit-learn/pull/4899 stalled? I am hugely looking forward it.\n",
      "\n",
      "2\n",
      "2762\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-21 15:24\n",
      "http://www.telegraph.co.uk/technology/2016/01/28/first-driverless-buses-travel-public-roads-in-the-netherlands/\n",
      "\n",
      "1\n",
      "2763\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-08-21 15:32\n",
      "@ItchyJunk  yes but what's the point :) Bus drivers take a lot of passengers so I can't believe it saves much money\n",
      "\n",
      "1\n",
      "2764\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-21 19:43\n",
      "@_@ what the point of automating vehicles?\n",
      "\n",
      "1\n",
      "2765\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-08-23 08:16\n",
      "if i am getting mostly score=0.2 in my gridsearchcv for sgdclassifier, what can I do to increase the score\n",
      "```    classifier = GridSearchCV(SGDClassifier(),                               {                                   'loss': ['hinge', 'modified_huber', 'log'],                                   'penalty': ['elasticnet', 'l2', 'l1'],                                   'alpha': [0.000004,                                             0.0000045,                                             0.000005,                                             0.0000055,                                             0.000006],                                   'warm_start': [True],                                   'l1_ratio': [0.12, 0.13, 0.14, 0.15]                               },                               iid=False,                               n_jobs=8,                               verbose=10,                               scoring='f1_weighted',                               refit=False) ```\n",
      "\n",
      "2\n",
      "2766\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-23 10:19\n",
      "http://scikit-learn.org/stable/modules/grid_search.html#gridsearch-scoring\n",
      "\n",
      "1\n",
      "2767\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-24 02:15\n",
      "https://plus.google.com/+VincentVanhoucke/posts/8T7DSJhGY3u\n",
      "\n",
      "1\n",
      "2768\n",
      "572748b5c43b8c601970c18f\n",
      "2016-08-24 15:19\n",
      "did in room speak about opencv???\n",
      "i want to make real-time  object tracking with open-cv in python and i want to sent cordinate of object to tracking module??? who can help me. Thank a lot .;-)\n",
      "\n",
      "2\n",
      "2769\n",
      "574c0070c43b8c601975aee3\n",
      "2016-08-25 22:42\n",
      "\n",
      "1\n",
      "2770\n",
      "55889fee15522ed4b3e27273\n",
      "2016-08-31 08:04\n",
      "got it. thanks! i understand markov chain mechanics, but was wondering if there are other better alternatives\n",
      "plus you mentioned tensorflow using syntaxnet, i think that is awesome\n",
      "\n",
      "2\n",
      "2771\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-08-31 08:08\n",
      "@kennetham ahh there are different algos each with their ups and downs i think.\n",
      "\n",
      "1\n",
      "2772\n",
      "55889fee15522ed4b3e27273\n",
      "2016-08-31 09:16\n",
      "@ItchyJunk is it possible to use for example NLTK to do NLP parsing, then I use tf-idf and Bag of words to classify the tokens, and possibly attempt to make a prediction? does that even work?\n",
      "\n",
      "1\n",
      "2773\n",
      "54e09a6d15522ed4b3dc08d0\n",
      "2016-08-31 14:31\n",
      "Hello everyone! Is anyone enrolling in Hinton's ML Coursera course https://www.coursera.org/learn/neural-networks ?\n",
      "\n",
      "1\n",
      "2774\n",
      "579e492240f3a6eec05d5245\n",
      "2016-09-01 06:26\n",
      "Hi, how do you guys do dl experiments (in terms of infrastructure). Will Amazon ec2 instances suffice for research purposes? Thanks for your help.\n",
      "\n",
      "1\n",
      "2775\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-09-01 10:26\n",
      "@kennetham I know very little about language processing, but in theory, it sounds like it would work? pretty sure  you'll find out why it doesn't when you code it out :P\n",
      "\n",
      "1\n",
      "2776\n",
      "562a7da216b6c7089cb80965\n",
      "2016-09-01 22:59\n",
      "@vyraun it realls depends on the scale of the analysis that you're doing. I'm a neuroscientist who works mostly with fMRI data, which can be pretty big, so our labs tend to have dedicated computers with many cores and a good deal of RAM, but I could certainly replicate that environment on ec2 if I wanted to pay as I went\n",
      "\n",
      "1\n",
      "2777\n",
      "579e492240f3a6eec05d5245\n",
      "2016-09-02 03:03\n",
      "@dankessler thanks.\n",
      "\n",
      "1\n",
      "2778\n",
      "53fdba59163965c9bc200ba2\n",
      "2016-09-03 21:03\n",
      "Hi guys. I'd like to learn how to use machine learn as a black box. Do you recommend a book or course? Thanks\n",
      "\n",
      "1\n",
      "2779\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-09-03 22:44\n",
      "Not sure what you mean by \"black box\" here.\n",
      "https://gist.github.com/off99555/b6190df237562aa6e8c922c485dc7ad0\n",
      "Here is some machine learning resources in general, if that helps.\n",
      "Sorry for that link expanding\n",
      "now sure how to make it not do that\n",
      "\n",
      "5\n",
      "2780\n",
      "56a34c16e610378809bdc988\n",
      "2016-09-04 03:53\n",
      "Hello @amueller @nelson-liu @GaelVaroquaux I had a look at #7319.\n",
      "Well I agree, nose has some deprecated code now\n",
      "Shifting to pytest can be a really good choice.\n",
      "I recently completed GSoC 16 with _TARDIS_, I am staying there as a regular contributor, although I always wanted to get involved in an ML centric community.\n",
      "Throughout my project I have worked a lot with pytest, and read a lot of its docs - I can take up this issue to solve on a regular basis.\n",
      "While I have used a little of scikit, I am planning to get comfortable with the codebase for sometime, and then move ahead with a series of PRs. Would you accept my PRs a few weeks down the line ?\n",
      "( Because I see you are choosing it for next GSoC )\n",
      "\n",
      "7\n",
      "2781\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-04 03:59\n",
      "hi @karandesai-96, thanks for inquiring! Id suggest you copy and paste your message onto the issue, since not everyone checks gitter.\n",
      "that would help it get more visibility + there might be nuances in the switch to consider that would require input from other developers, so best to see if you can get a solid green light there first\n",
      "\n",
      "3\n",
      "2782\n",
      "56b80528e610378809c05a48\n",
      "2016-09-04 03:59\n",
      "+1 on what @nelson-liu said\n",
      "\n",
      "2\n",
      "2783\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-04 04:06\n",
      "great, id also be happy to pitch in :) i dont know too much about py.tests and this seems like a good opportunity to learn\n",
      "\n",
      "1\n",
      "2784\n",
      "53fdba59163965c9bc200ba2\n",
      "2016-09-04 04:57\n",
      "Thanks @ItchyJunk\n",
      "\n",
      "1\n",
      "2785\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-09-05 07:16\n",
      "just out of curiousity, for `RadiusNeighborsClassifier` I am trying multiple `.fit()` calls,  and the code didnt throw any exceptions, can I assume it is possible to fit my whole collection into it with multiple `.fit()` calls?\n",
      "\n",
      "1\n",
      "2786\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-09-05 07:17\n",
      "Probably no?\n",
      "\n",
      "1\n",
      "2787\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-09-05 07:51\n",
      "oh?\n",
      "\n",
      "1\n",
      "2788\n",
      "56f4e1b785d51f252abab7d7\n",
      "2016-09-05 08:06\n",
      "so subsequent `.fit()` calls overwrite the previous call?\n",
      "ok, apparently that's the case S:\n",
      "\n",
      "2\n",
      "2789\n",
      "565b64bd16b6c7089cbc9de9\n",
      "2016-09-05 10:00\n",
      "@Jeffrey04 Yes. You're correct. each `fit` call will overwrite previous calls. Some estimators have `partial_fit` for what you're looking for, which isn't easily achievable for each classifier.\n",
      "\n",
      "1\n",
      "2790\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-09-05 12:34\n",
      "Nice, I guessed correctly = )\n",
      "\n",
      "1\n",
      "2791\n",
      "57be017740f3a6eec0612914\n",
      "2016-09-06 04:08\n",
      "@vyraun i suggest you use AWS EMR directly, which supports hadoop and spark 1.x.\n",
      "you can quickly try it w/o spending hours on setting up the cluster.\n",
      "who is taking the Udacity \"Intro to ML\" class here?\n",
      "\n",
      "3\n",
      "2792\n",
      "57008ddd187bb6f0eadd989b\n",
      "2016-09-06 04:14\n",
      "I'm takin Stanford (coursera) and MIT (OCW) ML classes\n",
      "\n",
      "2\n",
      "2793\n",
      "57008ddd187bb6f0eadd989b\n",
      "2016-09-06 04:19\n",
      "Haha I usually have to watch them 2-3 times over. They start off head first in the math and I haven't taken calc yet so it's been pretty slow learning. But MIT posts a lot of their courses on YouTube, so I'm going through the Artificial Intelligence course, much easier. How's the Udemy one?\n",
      "\n",
      "2\n",
      "2794\n",
      "579e492240f3a6eec05d5245\n",
      "2016-09-06 04:28\n",
      "\n",
      "1\n",
      "2795\n",
      "579e492240f3a6eec05d5245\n",
      "2016-09-06 04:31\n",
      "@txie thanks for the suggestion. Will try <unconvertable>\n",
      "\n",
      "1\n",
      "2796\n",
      "579e492240f3a6eec05d5245\n",
      "2016-09-06 04:42\n",
      "\n",
      "1\n",
      "2797\n",
      "564a0e2916b6c7089cbaead6\n",
      "2016-09-06 19:41\n",
      "Are there plans for a codesprint at #ODSC this year?\n",
      "\n",
      "1\n",
      "2798\n",
      "564a0e2916b6c7089cbaead6\n",
      "2016-09-06 19:43\n",
      "@txie I am just about completed with the Udacity ML Nanodegree.\n",
      "\n",
      "1\n",
      "2799\n",
      "578a4f87c2f0db084a23455f\n",
      "2016-09-07 03:48\n",
      "Guys from scikit-learn team, could someone check and add +1 to this https://github.com/scikit-learn/scikit-learn/pull/6116 ?\n",
      "\n",
      "1\n",
      "2800\n",
      "57c97e4040f3a6eec062f82d\n",
      "2016-09-07 04:20\n",
      "hello, i am begin learn deep learning\n",
      "i am reading http://www.deeplearningbook.org/\n",
      "\n",
      "2\n",
      "2801\n",
      "57430fe4c43b8c60197476d9\n",
      "2016-09-07 12:37\n",
      "Does scikit-learn already have or intend to have metric-learning algorithms included?\n",
      "\n",
      "1\n",
      "2802\n",
      "53fdba59163965c9bc200ba2\n",
      "2016-09-07 17:34\n",
      "Hi  guys. Id to `train_test_split` but on `stratify` I need of a multi-label data. Any idea how can I do, it?\n",
      "\n",
      "1\n",
      "2803\n",
      "53fdba59163965c9bc200ba2\n",
      "2016-09-07 17:39\n",
      "On my Pandas a had 4 columns. `[X, y, area, stratify]`. On `stratify` I populated with a string concatenation of `area` and `y` values. But this hack does not working.\n",
      "`train_test_split(df[X], df[y], test_size=0.3, stratify=df[stratify])`I got the error: `ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of labels for any class cannot be less than 2.`\n",
      "\n",
      "2\n",
      "2804\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-09-07 20:47\n",
      "Why is there exit() in the middle of the code at http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html ?\n",
      "the code has nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf) exit() print(\"done in %0.3fs.\" % (time() - t0))\n",
      "\n",
      "2\n",
      "2805\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-09-07 20:54\n",
      "idk maybe NMF does something that requires you to exit() to go back into your code?\n",
      "\n",
      "7\n",
      "2806\n",
      "5571fe1015522ed4b3e17d90\n",
      "2016-09-08 08:17\n",
      "@lesshaste the exit() is not in the dev doc so it has been fixed in master. See http://scikit-learn.org/dev/auto_examples/applications/topics_extraction_with_nmf_lda.html.\n",
      "\n",
      "1\n",
      "2807\n",
      "57430fe4c43b8c60197476d9\n",
      "2016-09-08 15:02\n",
      "@amueller , is there any interest in merging the Metric Learning algorithm implemented [#4789](https://github.com/scikit-learn/scikit-learn/pull/4789) ? And whether any Metric Learning algorithms are in the pipeline?\n",
      "\n",
      "1\n",
      "2808\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-08 15:02\n",
      "@bhargavvader Yes there is interest. but currently we are mostly working on a release\n",
      "only the things in PRs are in the pipeline AFAIK\n",
      "@raghavrv can you maybe work at making the new grid-search docs render correctly?\n",
      "\n",
      "3\n",
      "2809\n",
      "57430fe4c43b8c60197476d9\n",
      "2016-09-08 15:20\n",
      "@amueller , cool, I'll start by giving it a shot. It can be reviewed after the release is done and you guys get some time :)\n",
      "\n",
      "1\n",
      "2810\n",
      "56c625c3e610378809c22760\n",
      "2016-09-09 06:11\n",
      "Hey is anyone coming for PyCon Rennes from here?\n",
      "\n",
      "1\n",
      "2811\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-09 14:33\n",
      "only india ;)\n",
      "\n",
      "1\n",
      "2812\n",
      "55a36f535e0d51bd787b3400\n",
      "2016-09-09 17:38\n",
      "Hi guys, I recently re-installed the dev version of sklearn, and I get an mkl error   ```  from .murmurhash import murmurhash3_32 .Intel MKL FATAL ERROR: Cannot load libmkl_avx.so or libmkl_def.so. ``` If I don't first call a sklearn function (e.g. call `linear_model.Ridge()` first prevents the error, but calling `linear_model.LogisticRegression()` doesn't)\n",
      "Do you have an idea where I should look to track this error down?\n",
      "\n",
      "2\n",
      "2813\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-09 19:03\n",
      "update conda?\n",
      "\n",
      "1\n",
      "2814\n",
      "56c625c3e610378809c22760\n",
      "2016-09-09 19:21\n",
      "@amueller yep looking forward to your talk there!\n",
      "\n",
      "1\n",
      "2815\n",
      "56f0af1085d51f252aba1606\n",
      "2016-09-12 08:35\n",
      "Hi guys,  I meet a MemoryError problem when using minibatch k-means for clustering a data set with 1,700,000 rows and 5 cols. My desktop has 8G RAM and the scikit-learn version is 0.17.1. I searched google and found a same issue fixed by updating sklearn (https://github.com/scikit-learn/scikit-learn/issues/3048). Due to the algorithm of minibatch kmeans, I think 8G RAM should be enough. Anyone has ideas about this situation? Thanks!\n",
      "\n",
      "1\n",
      "2816\n",
      "53135b495e986b0712efc453\n",
      "2016-09-12 13:35\n",
      "> @raghavrv can you maybe work at making the new grid-search docs render correctly?  @amueller Sorry I missed the chat. Where does it not render correctly?\n",
      "\n",
      "1\n",
      "2817\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-12 17:06\n",
      "@raghavrv there are a bunch of errors when generating the sphinx doc\n",
      "@CasiaFan have you tried ``init=\"random\"``\n",
      "\n",
      "2\n",
      "2818\n",
      "57d73afd40f3a6eec064e7a0\n",
      "2016-09-12 23:37\n",
      "@amueller  dude tell me what are some cool things i can do with sci-kit learn and i promise i'll do them.\n",
      "\n",
      "1\n",
      "2819\n",
      "56f0af1085d51f252aba1606\n",
      "2016-09-13 01:07\n",
      "I tried it. Still running out all memory @amueller\n",
      "\n",
      "1\n",
      "2820\n",
      "56f0af1085d51f252aba1606\n",
      "2016-09-13 01:46\n",
      "Sorry, I check the script again and find this error is caused by plotting cluster patterns after clustering using matplotlib. Thanks anyway! @amueller\n",
      "\n",
      "1\n",
      "2821\n",
      "5770330bc2f0db084a2008c6\n",
      "2016-09-13 05:07\n",
      "hi\n",
      "i have a question\n",
      "\n",
      "2\n",
      "2822\n",
      "57d4603540f3a6eec06494c4\n",
      "2016-09-13 05:13\n",
      "So... you can ask :)\n",
      "\n",
      "1\n",
      "2823\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 17:29\n",
      "@amueller I will push the 0.18rc tag soon\n",
      "\n",
      "1\n",
      "2824\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-13 18:11\n",
      "@ogrisel thanks :)\n",
      "\n",
      "1\n",
      "2825\n",
      "57d83c7940f3a6eec0650f6e\n",
      "2016-09-13 18:12\n",
      "If i am python programmer and no knowledge about Ml but i want to go in ML so how can i start with python ?\n",
      "\n",
      "1\n",
      "2826\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 18:15\n",
      "you can follow the tutorials on http://scikit-learn.org then read a book such as @amueller's and work the examples, then try a kaggle.com challenge then follow a class online.\n",
      "basically alternate between theory and practice\n",
      "to get the theory you need basic linear algebra and stats / proba\n",
      "also AI is wider than ML\n",
      "but ML is a very very rich subfield of AI\n",
      "what I said:  linear algebra and stats / proba + basic differential multivariate calculus (what is a continuous function, a differentiable function, a gradient...)\n",
      "@amueller I pushed the 0.18rc tag. I also triggered the wheel builder.\n",
      "I tried with Python 3.6.0b1 and we have broken tests because of int / str comparisons: need to look in details\n",
      "\n",
      "11\n",
      "2827\n",
      "57d83c7940f3a6eec0650f6e\n",
      "2016-09-13 18:17\n",
      "I know AI theory and reading Modern approach but i am lacking to understand equations and expression in ML , for understanding those euations and expression what should i learn first??\n",
      "\n",
      "5\n",
      "2828\n",
      "57d83c7940f3a6eec0650f6e\n",
      "2016-09-13 18:19\n",
      "ok i see you mentioned Linear algebra ! Any good tutorial or blog or book for linear algebra ?\n",
      "\n",
      "1\n",
      "2829\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 18:21\n",
      "regression is a stats / machine learning concept, not a linear algebra concept. But generally it is presented in terms of vector space with a euclidean metric so you need to know about vector spaces and norms and distances first\n",
      "\n",
      "2\n",
      "2830\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 18:23\n",
      "to apply ML you don't necessarily need to understand the underlying math in details though. It's good to start with a bit of ml practice (e.g. sklearn tutorials) then learn a bit about the underlying maths and then iterate\n",
      "\n",
      "8\n",
      "2831\n",
      "57d83c7940f3a6eec0650f6e\n",
      "2016-09-13 18:25\n",
      "I am also undergraduate student and this is last year of my graduation !\n",
      "Ok , thanks :)\n",
      "Yet now but i will self learn no prob.\n",
      "\n",
      "3\n",
      "2832\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-13 18:36\n",
      "sweet!\n",
      "\n",
      "1\n",
      "2833\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-13 18:38\n",
      "@ogrisel err did you merge #7414 ?\n",
      "\n",
      "1\n",
      "2834\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-13 18:40\n",
      "shouldn't the version be 0.18rc1?\n",
      "\n",
      "1\n",
      "2835\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 18:41\n",
      "no I did not merge #7414, let me review it quickly, we can merge it and then tag 0.18rc1 will be after 0.18rc and nobody will ever know ;)\n",
      "\n",
      "2\n",
      "2836\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 18:44\n",
      "actually I am not sure about updating the website.\n",
      "I think we should wait for 0.18 final to update the nav\n",
      "BTW I think 0.18rc is equivalent to 0.18rc0\n",
      "\n",
      "4\n",
      "2837\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-13 18:48\n",
      "it's not entirely impossible that we do manual builts of the stable website, which would be silly\n",
      "also https://circleci.com/gh/scikit-learn/scikit-learn-feedstock\n",
      "\n",
      "2\n",
      "2838\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-13 18:49\n",
      "ugh test failures in the pickle test\n",
      "\n",
      "1\n",
      "2839\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 18:49\n",
      "this is looking good: https://travis-ci.org/MacPython/scikit-learn-wheels / https://ci.appveyor.com/project/sklearn-wheels/scikit-learn-wheels (the Python 3.3 failure is known but we don't care for the RC)\n",
      "\n",
      "7\n",
      "2840\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-13 18:54\n",
      "turns out that running a \"replace\" on a pickled string is not a good idea\n",
      "who'd thought?\n",
      "gimme a sec\n",
      "@ogrisel https://github.com/scikit-learn/scikit-learn/pull/7415\n",
      "let's wait for CI, then backport\n",
      "\n",
      "5\n",
      "2841\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 19:02\n",
      "yeah\n",
      "\n",
      "1\n",
      "2842\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 19:25\n",
      "travis is slow...\n",
      "\n",
      "1\n",
      "2843\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-13 19:26\n",
      "hm I'm having trouble with the feedstock\n",
      "https://circleci.com/gh/scikit-learn/scikit-learn-feedstock/4\n",
      "\n",
      "11\n",
      "2844\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-13 19:31\n",
      "I opened an issue here: https://github.com/conda-forge/conda-smithy/issues/304\n",
      "I tried setting conda-forge as source channel\n",
      "not sure if that does anything\n",
      "it'll take like half an hour to complete anyhow :-/\n",
      "\n",
      "4\n",
      "2845\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 19:42\n",
      "travis must be completely overloaded by the californians...\n",
      "the wheels tests break because of the pickle issue under Python 3\n",
      "I cancelled them to wait for #7415\n",
      "The Python 2.7 wheels did work\n",
      ":)\n",
      "...\n",
      "I going AFK, see you tomorrow. Good luck with conda-forge.\n",
      "\n",
      "7\n",
      "2846\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 20:39\n",
      "@amueller I merged, backported and pushed 0.18rc1\n",
      "the wheel builder is triggered\n",
      "\n",
      "2\n",
      "2847\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 21:11\n",
      "@amueller the wheels are on their way. How are you doing with the conda-forge feedstock?\n",
      "coool\n",
      "\n",
      "2\n",
      "2848\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 21:22\n",
      "wheel builder was going fine but no more travis workers at the moment unfortunately: https://travis-ci.org/MacPython/scikit-learn-wheels\n",
      "the windows wheels are almost all ready: https://ci.appveyor.com/project/sklearn-wheels/scikit-learn-wheels\n",
      "OSX travis workers are overloaded\n",
      "I have to wake up early tomorrow, I won't have time to wait for them, feel free to push everything to PyPI while I'm sleeping :)\n",
      "\n",
      "4\n",
      "2849\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-13 21:25\n",
      "``` git checkout 0.18rc1 pip install wheelhouse-uploader python setup.py sdist fetch_artifacts ```\n",
      "then `twine upload dist/`\n",
      "\n",
      "2\n",
      "2850\n",
      "57d8575a40f3a6eec0651444\n",
      "2016-09-13 21:31\n",
      "hi\n",
      "\n",
      "1\n",
      "2851\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-14 12:30\n",
      "@amueller how do you test the feedstock without issue PRs?\n",
      "\n",
      "1\n",
      "2852\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-14 19:13\n",
      "there is no real way to test the feedstock\n",
      "as far as I can tell\n",
      "\n",
      "2\n",
      "2853\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-14 19:14\n",
      "@ogrisel have you see this https://circleci.com/gh/scikit-learn/scikit-learn/5456?utm_campaign=build-failed&utm_medium=email&utm_source=notification ?\n",
      "\n",
      "1\n",
      "2854\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-14 19:15\n",
      "Indeed, I'm on it.\n",
      "done\n",
      "\n",
      "2\n",
      "2855\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-14 19:19\n",
      "If travis is green after this fix, I think we can tag 0.18rc2 and trigger the wheel builders. How is the feedstock going?\n",
      "\n",
      "1\n",
      "2856\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-14 19:40\n",
      "@amueller shall I tag 0.18rc2?\n",
      "\n",
      "1\n",
      "2857\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-14 19:41\n",
      "@ogrisel sure\n",
      "@ogrisel feedstock is been running for like an hour to build the 0.18rc1\n",
      "\n",
      "2\n",
      "2858\n",
      "541a528b163965c9bc2053de\n",
      "2016-09-14 19:46\n",
      "0.18rc2 is on the launch pad\n",
      "I believe the wheels will be ready by tomorrow ;)\n",
      "\n",
      "5\n",
      "2859\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-14 19:48\n",
      "Hopefully the conda packages too\n",
      "I expect they arrive here: https://anaconda.org/conda-forge/repo?label=rc\n",
      "I'll update the feedstock for 0.18.2\n",
      "\n",
      "4\n",
      "2860\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-14 19:52\n",
      "I think the conda package takes about 3h to build\n",
      "a single entry of the matrix takes 30 minutes, there are 6\n",
      "and they all run in the same job\n",
      "\n",
      "3\n",
      "2861\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-14 21:30\n",
      "@ogrisel do you want to fix the lbgfs thing or should I?\n",
      "rc3? ;)\n",
      "\n",
      "2\n",
      "2862\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-14 23:07\n",
      "YES!\n",
      "https://anaconda.org/conda-forge/scikit-learn/labels\n",
      "\n",
      "2\n",
      "2863\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-14 23:08\n",
      ":clap:\n",
      "\n",
      "1\n",
      "2864\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-14 23:27\n",
      "ugh so conda users can't update\n",
      "unless they use conda-forge\n",
      "eh\n",
      "\n",
      "3\n",
      "2865\n",
      "55a36f535e0d51bd787b3400\n",
      "2016-09-16 13:19\n",
      "@amueller yes I do, at least I'm a contributor of the package; I'm a postdoc at nyu.\n",
      "@dankessler thanks for taking a look. So to clarify: `X` is 3D, e.g. `(n_samples, n_features, n_dimensions)` and we want to fit an estimator across all features for each dimension separately, such that we end up with `n_dimensions` estimators, and therefore `n_dimensions` scores.\n",
      "@amueller briefly at a meetup\n",
      "yes #7388 seems indeed relevant. If you allow `cross_val_score` to have scoring metrics that are not float but custom (e.g. numpy arrays), then our problem is solved at the sklearn level: we would directly do `cross_val_score(SearchLight())` where by default `SearchLight().score(X, y)`with `X` shape being `(n_samples, n_features, n_dimensions)` returns an array of `n_dimensions`\n",
      "\n",
      "4\n",
      "2866\n",
      "55a36f535e0d51bd787b3400\n",
      "2016-09-16 13:29\n",
      "Our `SearchLight` is indeed similar to nilearn's, but nilearn's does everything at once (parametrize how to move across dimensions of an MRI scan, fit/predict estimators, do the whole thing within a cv etc); by contrast we  implemented a single-purpose object: i.e. fit different classifiers over different dimensions of the data, but don't summarize or combine these classifiers. We can thus pipeline the search light : e.g. `make_pipeline(PrepareData(), SearchLight(Regressiont()))` and cross_val_predict this estimator\n",
      "However, we can't apply `cross_val_score` because this functions requires that the score is a float, not an array.\n",
      "\n",
      "2\n",
      "2867\n",
      "55a36f535e0d51bd787b3400\n",
      "2016-09-16 13:37\n",
      "But perhaps there s a way in sklearn to get cross_val_score compatible with arrays? i.e. if one wants to retrieve the cross-validated confusion matrix , instead of the average score?\n",
      "\n",
      "1\n",
      "2868\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-16 16:00\n",
      "@kingjr have we met? Sorry If I forgot :(\n",
      "@kingjr also check out https://github.com/scikit-learn/scikit-learn/pull/7388#issuecomment-247565362\n",
      "\n",
      "2\n",
      "2869\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-16 21:02\n",
      "@kingjr I think that would be the best. We already want that for f1_scores without averaging for example, where you get one for each class.\n",
      "\n",
      "1\n",
      "2870\n",
      "57dfed0240f3a6eec0660e8d\n",
      "2016-09-19 16:24\n",
      "Hello, is this the right place to ask beginners ML questions?\n",
      "\n",
      "1\n",
      "2871\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-19 17:05\n",
      "I think crossvalidated or stackoverflow would be better\n",
      "thank you for asking\n",
      "\n",
      "2\n",
      "2872\n",
      "564a0e2916b6c7089cbaead6\n",
      "2016-09-19 18:00\n",
      "As an intermediate ML practitioner still stinging from being a noob, I found both of those communities very off putting in getting assistance. On cross validated, I was literally told that the answers to a question I asked were \"in any basic econometrics text\". They are invaluable resources to be sure, but not the place to learn as a beginner. I would advise trying to find a community around one of the MOOCs - udacity, coursera, ... Perhaps even Reddit\n",
      "I am biased in which I would choose as I got a lot out of the Udacity ML nanodegree and have found their slack community inviting and informative.\n",
      "\n",
      "2\n",
      "2873\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-09-19 18:30\n",
      "Hello, how would one extract decision paths from a decision tree classifier? I mean I would like to get something like, \"if x < 1 and ... then class 1\"\n",
      "\n",
      "1\n",
      "2874\n",
      "57dfed0240f3a6eec0660e8d\n",
      "2016-09-19 18:41\n",
      "Thank you @joshuacook and @nelson-liu\n",
      "\n",
      "1\n",
      "2875\n",
      "53fdba59163965c9bc200ba2\n",
      "2016-09-20 12:42\n",
      "Hi. Reading the SVC doc(http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) has it `... may not work properly in a multithreaded context `  Is possible to execute SVC using all threads? how ?\n",
      "\n",
      "1\n",
      "2876\n",
      "56c625c3e610378809c22760\n",
      "2016-09-20 14:27\n",
      "Hi, I'm trying to build an outlier detection system on textual data using the `EllipticEnvelope`. I was wondering if there's a better way to choose the optimal number of features out of a TFIDF vectorizer so as to not cause a memory error or a singular covariance matrix apart from repeatedly checking on the training or the cv set. Thanks a lot for the help and I apologise if this is a very noob-ish doubt!\n",
      "\n",
      "1\n",
      "2877\n",
      "53fdba59163965c9bc200ba2\n",
      "2016-09-20 19:07\n",
      "How can I see the most frequent word using a TfidfVectorizer instance? Is possible?\n",
      "\n",
      "1\n",
      "2878\n",
      "55a361b55e0d51bd787b3315\n",
      "2016-09-24 15:41\n",
      "Could anyone peep in to see how can I correct this issue? https://travis-ci.org/scikit-learn/scikit-learn/builds/162438563\n",
      "\n",
      "1\n",
      "2879\n",
      "57e4ea1140f3a6eec066d9b4\n",
      "2016-09-28 08:09\n",
      "hello! I am Satya Prakash wanting to work with scikit package and as it involves ML stuff I am intrested in it so any one please give me location to start and also looking forward to contribute in GSOC2017 any guidance will be really helpful\n",
      "\n",
      "1\n",
      "2880\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-09-28 11:44\n",
      "Hello! I am new with this stuff, I intend extracting features from gray-scale images and then further do analysis using Decision Tree algorithm  but it doesn't seem working.I will appreciate any guidance. Below is a screenshot of my work. Thank you\n",
      "%matplotlib inline from matplotlib import pyplot as plt import numpy as np from skimage import io  count = 0  images = io.imread_collection('/home/data/Desktop/IMAGES/GRAY-SCALE/*.jpg')        x_images = np.vstack(images)   while count < len(images):      new_x = (x_images[count])     new_y =(images.files[count])             print(new_y)     print(new_x)      count = count + 1\n",
      " %matplotlib inline from matplotlib import pyplot as plt import numpy as np from skimage import io  count = 0  images = io.imread_collection('/home/data/Desktop/IMAGES/GRAY-SCALE/*.jpg')        x_images = np.vstack(images)   while count < len(images):      new_x = (x_images[count])     new_y =(images.files[count])             print(new_y)     print(new_x)      count = count + 1  from sklearn.tree import DecisionTreeClassifier from sklearn.cross_validation import train_test_split from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score   features = x_images  targetVariable = new_y  featureTrain,featureTest,TargetTrain,TargerTest = train_test_split(features, targetVariable, test_size = 0.2)  model = DecisionTreeClassifier() fittedModel = fit.model(featureTrain,targetTrain) predictions = fittedModel.predict()  print(predictions) #print(TargetTest,predictions) #print(accuracy_score(TargetTest,predictions))\n",
      "Thanks a lot\n",
      "\n",
      "4\n",
      "2881\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-09-28 11:48\n",
      "Doesn't seem to be working as in you get errors? if so what? or do you mean the code words but it's not doing wwhat you want it to do?\n",
      "\n",
      "1\n",
      "2882\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-09-28 11:50\n",
      "I get an error when i run the algorithm...\n",
      "\n",
      "2\n",
      "2883\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-09-28 11:50\n",
      "\n",
      "0\n",
      "2884\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-09-28 11:50\n",
      "\n",
      "0\n",
      "2885\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-09-28 12:12\n",
      "Is there any line of code that can help extract features from these images(gray-scale) so I can further do analysis using decision Tree algorithm...@ItchyJunk...??\n",
      "\n",
      "1\n",
      "2886\n",
      "57b3fd8640f3a6eec05fe0e8\n",
      "2016-09-28 12:44\n",
      "Hmm, not sure. I'll let someone who know better answer you. but looks like it's just x_image variable being used without being declared? is x_image supposed to be an array or string or something? x_image = [] or x_image = ''  would be the needed step.\n",
      "\n",
      "1\n",
      "2887\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-28 20:56\n",
      "stable docs down, is this intended?\n",
      "@amueller @ogrisel\n",
      "\n",
      "2\n",
      "2888\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-28 20:58\n",
      "@nelson-liu typo, sorry about that!\n",
      "\n",
      "4\n",
      "2889\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-09-28 21:06\n",
      "should now be up as 0.18 :)\n",
      "\n",
      "1\n",
      "2890\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-28 21:13\n",
      ":fireworks: yay!\n",
      "\n",
      "1\n",
      "2891\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-09-28 21:49\n",
      "Hello! is there any simple way of extracting features from fifty  gray scale images for further analysis? few lines of code would be much appreciated.Thanks\n",
      " I tried using numerical matrix of each  Grayscale image as a feature for further analysis but it doesn't working. Any guidance would help.Thanks\n",
      "\n",
      "2\n",
      "2892\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-28 22:03\n",
      "have you looked at something like [the mnist example](http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)\n",
      "\n",
      "1\n",
      "2893\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-28 22:03\n",
      "\n",
      "0\n",
      "2894\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-28 22:03\n",
      "\n",
      "0\n",
      "2895\n",
      "57ed4d1c40f3a6eec0680da9\n",
      "2016-09-29 17:27\n",
      "Hello everyone.\n",
      "\n",
      "1\n",
      "2896\n",
      "57decc9840f3a6eec065e9e2\n",
      "2016-09-29 17:28\n",
      "Hi :smile:\n",
      "\n",
      "1\n",
      "2897\n",
      "57a5a74c40f3a6eec05e31c1\n",
      "2016-09-29 17:28\n",
      "hello\n",
      "\n",
      "1\n",
      "2898\n",
      "57ed4d1c40f3a6eec0680da9\n",
      "2016-09-29 17:30\n",
      "I want some help on project based learning in Recommendation systems... I started with movielens dataset but now I'm stuck. can anyone help me please.\n",
      "\n",
      "1\n",
      "2899\n",
      "562a7da216b6c7089cb80965\n",
      "2016-09-29 17:34\n",
      "@Ben-Kobby are you sure you want to be reshaping `D_images` and not `x_images`?\n",
      "\n",
      "1\n",
      "2900\n",
      "561a58f7d33f749381a8ff2f\n",
      "2016-09-29 19:31\n",
      "Guys... I'm super confused about `DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.`\n",
      "How can we make our modules compliant with older and newer versions?\n",
      "\n",
      "2\n",
      "2901\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-29 20:37\n",
      "@kootenpv by modules do you mean modules that interface with scikit-learn?\n",
      "\n",
      "1\n",
      "2902\n",
      "56d587f0e610378809c46913\n",
      "2016-09-30 03:15\n",
      "Hello everyone, I want to contribute to scikit-learn. Can someone please direct me towards the documentation to get started? Thanks.\n",
      "\n",
      "1\n",
      "2903\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-09-30 03:18\n",
      "have you read the contributing guide?\n",
      "thats a good start.\n",
      "\n",
      "2\n",
      "2904\n",
      "56d587f0e610378809c46913\n",
      "2016-09-30 03:33\n",
      "Yes sir, I did give it a read. However, not having worked with such a huge library before, the code base seems a bit intimidating. Can you please tell me as to how long would it take to understand the code before I can make any contribution?\n",
      "\n",
      "3\n",
      "2905\n",
      "56d587f0e610378809c46913\n",
      "2016-09-30 04:06\n",
      "Okay sir, thanks for letting me know. I'll try to solve any issue marked 'easy' to start-off as mentioned in the guide then.\n",
      "\n",
      "1\n",
      "2906\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-09-30 04:19\n",
      "@dankessler  thanks a lot for the correction,  Am suppose to reshape x_images instead...\n",
      "\n",
      "1\n",
      "2907\n",
      "56c625c3e610378809c22760\n",
      "2016-09-30 10:55\n",
      "Hi, this is one error that I always seem to encounter when I pull from upstream (using 0.19.dev0) even after executing `sudo make`: ``` ImportError: /home/devashish/EXPERIMENTATION/scikit-learn/sklearn/utils/sparsefuncs_fast.so: undefined symbol: PyFPE_jbuf ``` Any solutions to this?\n",
      "I'm trying to import LogisticRegression here\n",
      "\n",
      "2\n",
      "2908\n",
      "56c625c3e610378809c22760\n",
      "2016-09-30 10:55\n",
      "\n",
      "0\n",
      "2909\n",
      "56c625c3e610378809c22760\n",
      "2016-09-30 10:55\n",
      "\n",
      "0\n",
      "2910\n",
      "56c625c3e610378809c22760\n",
      "2016-09-30 11:22\n",
      "I'm getting pretty much the same error as reported [here](http://stackoverflow.com/questions/37577210/build-sklearn-error-cythonize-failed).\n",
      "I have cython installed however I still get this error :disappointed:\n",
      "\n",
      "2\n",
      "2911\n",
      "56c625c3e610378809c22760\n",
      "2016-09-30 12:05\n",
      "reported it here: https://github.com/scikit-learn/scikit-learn/issues/7542\n",
      "\n",
      "1\n",
      "2912\n",
      "57f33dd7d73408ce4f2b9adf\n",
      "2016-10-04 05:28\n",
      "Hello everyone\n",
      "\n",
      "1\n",
      "2913\n",
      "57f33dd7d73408ce4f2b9adf\n",
      "2016-10-04 05:29\n",
      "I just started using scikit-learn few weeks back and I am loving every bit of it.\n",
      "I want to contribute to scikit-learn but not able to understand where to start\n",
      "Can anyone guide me through\n",
      "\n",
      "3\n",
      "2914\n",
      "5770c02dc2f0db084a2017bd\n",
      "2016-10-04 12:46\n",
      "@vibrantabhi19 their website has some contribution guidelines : http://scikit-learn.org/stable/developers/index.html\n",
      "\n",
      "1\n",
      "2915\n",
      "57f33dd7d73408ce4f2b9adf\n",
      "2016-10-04 14:18\n",
      "Thanks @RohanVB. I went through these guidlines. Hope I can get over some issues and make my first PR.\n",
      "\n",
      "1\n",
      "2916\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-04 15:08\n",
      "theres a low hanging fruit @ https://github.com/scikit-learn/scikit-learn/issues/7521\n",
      "\n",
      "1\n",
      "2917\n",
      "57f33dd7d73408ce4f2b9adf\n",
      "2016-10-04 16:13\n",
      "Thank you @nelson-liu  I am up for it :)\n",
      "\n",
      "1\n",
      "2918\n",
      "54ec8cb715522ed4b3dc693b\n",
      "2016-10-04 16:45\n",
      "Scikit learn is amazing! I just had a question regarding what is the plan of scikitlearn as tensorflow has written something called sci flow, which is intended to be replacement for scikit-learn. Please be advised that I still do not totally understand tensorflow!\n",
      "\n",
      "1\n",
      "2919\n",
      "57f33dd7d73408ce4f2b9adf\n",
      "2016-10-04 17:22\n",
      "@nelson-liu The file testimonial.html is available only after we build the html docs.  And then in testimonials.html, even after fixing the issue we get 'working directory clean' So kindly help me with the file/code structure\n",
      "\n",
      "4\n",
      "2920\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-04 17:23\n",
      "i bet you could grep rangespan and find wher eyou want it\n",
      "@thewhitetulip if youre referring to skflow, I dont think its a replacement for scikit-learn\n",
      "and i dont think its their intention to be a replacement\n",
      "skflow is useful for providing a scikit-learn interface to the deep learning capabilities of tensorflow, because scikit-learn does not do deep learning\n",
      "they occupy different roles\n",
      "\n",
      "5\n",
      "2921\n",
      "57f33dd7d73408ce4f2b9adf\n",
      "2016-10-04 18:19\n",
      "@nelson-liu Done, Kindly review my PR\n",
      "\n",
      "1\n",
      "2922\n",
      "57f33dd7d73408ce4f2b9adf\n",
      "2016-10-05 04:17\n",
      "I got through my first contribution, can anyone suggest few more issues that are easy to solve. I wanted to get some confidence before going to the core progamming structure. I also reviewed some fo  the 'Easy' labeled issues.  @nelson-liu any more low hanging fruit?\n",
      "\n",
      "1\n",
      "2923\n",
      "54ec8cb715522ed4b3dc693b\n",
      "2016-10-05 14:56\n",
      "@nelson-liu good to know!\n",
      "\n",
      "1\n",
      "2924\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-05 16:18\n",
      "@vibrantabhi19 @thewhitetulip one thing that's also very appreciated is going through old issues and seeing if they are still relevant.\n",
      "\n",
      "1\n",
      "2925\n",
      "57f33dd7d73408ce4f2b9adf\n",
      "2016-10-05 16:19\n",
      "@amueller Will surely look into it\n",
      "\n",
      "1\n",
      "2926\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-05 16:43\n",
      "@raghavrv you know that the ping on numpy was Travis, the creator of numpy and scipy and CEO of continuum? ;)\n",
      "\n",
      "1\n",
      "2927\n",
      "53135b495e986b0712efc453\n",
      "2016-10-05 18:00\n",
      "Wow. I didn't know that o.O\n",
      "\n",
      "1\n",
      "2928\n",
      "57f553ebd73408ce4f2c1766\n",
      "2016-10-05 19:30\n",
      "hello, may I ask a question about sk-learn here?\n",
      "It's related to the deprecation warning.\n",
      "DeprecationWarning: Function residues_ is deprecated; ``residues_`` is deprecated and will be removed in 0.19\n",
      "I'm wondering what should I use instead of 'residues_' to get residue values from LinearRegression (fitted) model.\n",
      "\n",
      "4\n",
      "2929\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-07 23:48\n",
      "wow this makes me shiver: https://github.com/scikit-learn/scikit-learn/pull/6348/files\n",
      "@naokishibuya yes you should\n",
      "\n",
      "2\n",
      "2930\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-07 23:51\n",
      "err sorry use ``_residues`` or  just compute them from the predictions?\n",
      "np.sum((y_train - lr.predict(X_train)) ** 2)\n",
      "\n",
      "2\n",
      "2931\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-08 03:50\n",
      "hm is there a way to see how many issues I closed today?\n",
      "\n",
      "1\n",
      "2932\n",
      "57f553ebd73408ce4f2c1766\n",
      "2016-10-08 04:10\n",
      "@amueller understood, thx!\n",
      "\n",
      "1\n",
      "2933\n",
      "57f4f049d73408ce4f2bfae9\n",
      "2016-10-08 15:31\n",
      "Is there anything to do discretization on scikilearn?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2934\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-08 16:45\n",
      "@Piperod__twitter not really right now. You can just use numpy though\n",
      "numpy.digitize is the thing you want to look at iirc\n",
      "\n",
      "2\n",
      "2935\n",
      "56a34c16e610378809bdc988\n",
      "2016-10-09 05:32\n",
      "@amueller not sure, bu pulse can give really good insights\n",
      "https://github.com/scikit-learn/scikit-learn/pulse\n",
      "Hi, I had a feature request\n",
      "can we have a `mean_squared_log_error` or a `root_mean_squared_log_error` in `sklearn.metrics` ?\n",
      "former is preferred looking at the current set of metrics\n",
      "\n",
      "5\n",
      "2936\n",
      "56a34c16e610378809bdc988\n",
      "2016-10-09 05:37\n",
      "Frequent competitions on kaggle have this evaluation metric\n",
      "https://www.kaggle.com/competitions?sortBy=prize&group=all&page=1&site=main&eval=rmsle\n",
      "for now, i make a new column in my dataframe by taking `np.log(1 + x)` transformation, and then calculate `mean_squared_error` on it. Then while making a submission I reverse it as `np.exp(x) - 1`.\n",
      "Thoughts ?\n",
      "[![blob](https://files.gitter.im/scikit-learn/scikit-learn/VhqO/thumb/blob.png)](https://files.gitter.im/scikit-learn/scikit-learn/VhqO/blob)\n",
      "\n",
      "5\n",
      "2937\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2016-10-10 10:24\n",
      "hello, I was working on https://github.com/scikit-learn/scikit-learn/issues/6120. I am not sure I understand what needs to be exactly needs to be changed.\n",
      "what do I need to do in order to use only 20 components?\n",
      "\n",
      "2\n",
      "2938\n",
      "555687ea15522ed4b3e079c7\n",
      "2016-10-10 15:53\n",
      "How would one submit a feature request on the documentation. With the addition of float elements to some hyper parameters it might be assumed that values can range from (0-1] , but in fact ranges often have a minimum i.e.( (0-1]*n_elements  must be greater than 2 , (0-.5] )\n",
      "\n",
      "1\n",
      "2939\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-11 18:20\n",
      "@dylanbstorey on the issue tracker\n",
      "\n",
      "1\n",
      "2940\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-10-12 12:38\n",
      "Does mean_score_time in GridSearchCV.cv\\_results_ mean time that it took to predict classes (in classification case) and calcuclate the relevant evaluation scores?\n",
      "\n",
      "1\n",
      "2941\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-10-12 17:47\n",
      "if you fit randomforestclassifier with verbosity  =1, say and then pickle it\n",
      "it seems you are stuck with this verbosity level forever. Is that correct/wanted?\n",
      "unless you can secretly set the verbosity level when doing .predict_proba?\n",
      "\n",
      "3\n",
      "2942\n",
      "56a34c16e610378809bdc988\n",
      "2016-10-12 20:23\n",
      "> Hi, I had a feature request  #7655 :)\n",
      "I'll be waiting eagerly for the reviews\n",
      "\n",
      "2\n",
      "2943\n",
      "55d213e50fc9f982beadaa87\n",
      "2016-10-12 21:50\n",
      "has anyone successfully added sklearn to a requirements.txt with a specific version?\n",
      "\n",
      "1\n",
      "2944\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-13 00:17\n",
      "yes i have\n",
      "by version do you mean release #? @alexbednarczyk\n",
      "whats the error you are getting right now?\n",
      "\n",
      "3\n",
      "2945\n",
      "53135b495e986b0712efc453\n",
      "2016-10-13 02:23\n",
      "> Does mean_score_time in GridSearchCV.cv\\_results_ mean time that it took to predict classes (in classification case) and calcuclate the relevant evaluation scores?  @mkoske Yes. `mean_fit_time` corresponds to the training time...\n",
      "\n",
      "1\n",
      "2946\n",
      "55d213e50fc9f982beadaa87\n",
      "2016-10-13 09:16\n",
      "@nelson-liu what does your requirements.txt line for sklearn look like? I've tried scikit-learn==0.17 and sklearn==0.17 both give me errors\n",
      "\n",
      "1\n",
      "2947\n",
      "55d213e50fc9f982beadaa87\n",
      "2016-10-13 10:31\n",
      "it doesn't seem like sklearn interacts with pip3 freeze. when i use pip3 freeze it displays the version as 0.0 when im using 0.18\n",
      "Collecting sklearn==0.17 (from -r requirements.txt (line 3))  Could not find a version that satisfies the requirement sklearn==0.17 (from -r requirements.txt (line 3)) (from versions: 0.0) No matching distribution found for sklearn==0.17 (from -r requirements.txt (line 3))\n",
      "\n",
      "2\n",
      "2948\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-13 13:38\n",
      "hmm thats odd. my pip outputs `scikit-learn==0.18` perfectly fine\n",
      "although i am on python 2, so ill try with 3\n",
      "\n",
      "2\n",
      "2949\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-13 13:43\n",
      "``` conda create --name test python=3 source activate test pip install numpy scipy cython scikit-learn  $ pip --version pip 8.1.2 from /Users/nfliu/miniconda2/envs/test/lib/python3.5/site-packages (python 3.5)  $ pip freeze Cython==0.24.1 numpy==1.11.2 scikit-learn==0.18 scipy==0.18.1 ```  seemed to work for me\n",
      "@alexbednarczyk\n",
      "\n",
      "2\n",
      "2950\n",
      "55d213e50fc9f982beadaa87\n",
      "2016-10-13 14:03\n",
      "alright. im using pip 8.1.1, maybe i need to update\n",
      "did it work with 0.17?\n",
      "\n",
      "2\n",
      "2951\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-13 14:08\n",
      "yeah it did\n",
      "\n",
      "1\n",
      "2952\n",
      "55d213e50fc9f982beadaa87\n",
      "2016-10-13 14:17\n",
      "ok ill try updating pip, thanks\n",
      "\n",
      "1\n",
      "2953\n",
      "56a34c16e610378809bdc988\n",
      "2016-10-14 04:47\n",
      "Hi, https://github.com/scikit-learn/scikit-learn/pull/7655 is ready. Can any core team member help me improve it ? :D\n",
      "\n",
      "1\n",
      "2954\n",
      "55a36f535e0d51bd787b3400\n",
      "2016-10-15 03:00\n",
      "Hi Sklearn community. Is there a way to do clustering with a constrain such that there is an equal number of training samples in each cluster?\n",
      "\n",
      "1\n",
      "2955\n",
      "529c6e38ed5ab0b3bf04dde0\n",
      "2016-10-17 14:23\n",
      "Hey Guys, I was going through the issues on github, so have a few  questions regarding  Neural Network modules(MLPclassifier/MLPregressor):\n",
      "1. I think Dropout has already been implemented, is the PR already merged with the 'master' branch\n",
      "2. What are the other things in the pipeline related to neural networks\n",
      "Also, are there any plans for implementing Word Vectors in scikit-learn?\n",
      "@amueller ?\n",
      "\n",
      "5\n",
      "2956\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-17 14:36\n",
      "iirc there was a proposal to add word2vec awhile ago but it got dismantled pretty quickly since gensim is already a thing\n",
      " also i dont think dropout is implemented / merged, see https://github.com/scikit-learn/scikit-learn/pull/7407\n",
      "heres the word2vec one https://github.com/scikit-learn/scikit-learn/issues/6247\n",
      "\n",
      "3\n",
      "2957\n",
      "529c6e38ed5ab0b3bf04dde0\n",
      "2016-10-17 14:39\n",
      "@nelson-liu true that. and although gensim is pretty straight forward to use, do you think that, if we had the word2vec module in scikit-learn itself, then one doesn't have to rely on two different libraries, besides, we already have couple of existing vectorization methods for text, in that way you can simply try word vectors instead of tfidf/count vectors etc.\n",
      "\n",
      "4\n",
      "2958\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-17 15:27\n",
      "yup or perhaps https://github.com/scikit-learn-contrib\n",
      "\n",
      "1\n",
      "2959\n",
      "57f4f049d73408ce4f2bfae9\n",
      "2016-10-18 16:07\n",
      "Decision trees support categorical data or do I have to convert categorical data into numbers?\n",
      "\n",
      "1\n",
      "2960\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-18 17:22\n",
      "you have to convert categorical data into numbers\n",
      "\n",
      "1\n",
      "2961\n",
      "58011513d73408ce4f2e472f\n",
      "2016-10-19 01:11\n",
      "Hello -- does anyone know what's the best way to serialize scikit learn models to string/bytes etc, which can be stored in a database?\n",
      "I have tried pickle -- but many people seem to advice against it.\n",
      "joblib is good for writing to a file, but does not provide support to converting to string.\n",
      "\n",
      "3\n",
      "2962\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-10-19 16:21\n",
      "Hello, are the neighbors returned by NearestNeighbors.kneighbors() always sorted by distance?\n",
      "\n",
      "1\n",
      "2963\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-10-19 16:28\n",
      "Hi, I suppose it is sorted by distance - nearest( least distance) being the first element.\n",
      "\n",
      "1\n",
      "2964\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-10-19 21:08\n",
      "If I use same estimator in a loop but with different subset of data, should I always call .fit(X, y) with the new subset?\n",
      "\n",
      "1\n",
      "2965\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-10-20 02:03\n",
      "Is partial_fit suitable for your purpose ?\n",
      "Though not all estimators support it. [This](http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning) might be relevant. Hope I am understanding your use case right.\n",
      "\n",
      "2\n",
      "2966\n",
      "561be13fd33f749381a91e6e\n",
      "2016-10-21 01:07\n",
      "@rajhans  I'm curious, if you were to store it in a db, what would you want to do with the record\n",
      "\n",
      "1\n",
      "2967\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-21 01:30\n",
      "@rajhans you could write the joblib file to disk, get a hash, and then store the hash? That might be a bit too much space for your application, though.\n",
      "\n",
      "1\n",
      "2968\n",
      "561be13fd33f749381a91e6e\n",
      "2016-10-21 01:31\n",
      "@nelson-liu is that what you do?\n",
      "\n",
      "1\n",
      "2969\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-21 04:28\n",
      "no, I dont work with scikit-learn and databases. Im just postulating a possible solution to his answer\n",
      "\n",
      "1\n",
      "2970\n",
      "56d8cf14e610378809c4e7e9\n",
      "2016-10-21 16:59\n",
      "Having issues with akka netty\n",
      "I have two services in akka communicating over netty tcp. How can I dockerize them. I am as of now getting an error\n",
      "Can anyone help me on this?\n",
      "\n",
      "3\n",
      "2971\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-21 17:02\n",
      "i dont think this is the proper place to ask a question about akka, this gitter room is for scikit-learn\n",
      "\n",
      "1\n",
      "2972\n",
      "5730c2dcc43b8c601971eca1\n",
      "2016-10-22 06:42\n",
      "Hello, Just wondering, how can we setup path dependant input states for Random Forest ? This is similar to Recurrent Network,   where input Xt=(x0,...,xi,.. yt-1, yt-2) depends on past output states Yt.  If we could put the exact output states Yt, it creates a bias in the training. So, wondering how we can put  recurrent states in Random Forest training. Thanks\n",
      "\n",
      "1\n",
      "2973\n",
      "580cad1dd73408ce4f303424\n",
      "2016-10-24 12:18\n",
      "hey everyone\n",
      "i am new here and want to learn machine learning\n",
      "\n",
      "2\n",
      "2974\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-25 14:32\n",
      "@neerajwadhwa take andrew Ng's coursera course and by my book ;) https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413/ref=sr_1_1?ie=UTF8&qid=1477405931&sr=8-1&keywords=introduction+to+machine+learning+with+python\n",
      "\n",
      "1\n",
      "2975\n",
      "53135b495e986b0712efc453\n",
      "2016-10-25 16:27\n",
      "@amueller Do you have any issue that needs fixing or review for `0.18.1`?\n",
      "\n",
      "1\n",
      "2976\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-25 16:28\n",
      "@raghavrv  https://github.com/scikit-learn/scikit-learn/pulls?q=is%3Aopen+is%3Apr+milestone%3A0.18.1 ;)\n",
      "and https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+is%3Aissue+milestone%3A0.18.1+label%3ABlocker\n",
      "so two things that would be great are fixing this: https://github.com/scikit-learn/scikit-learn/issues/7563 (I'm investigating but getting no-where)\n",
      "and a non-regression test for this: https://github.com/scikit-learn/scikit-learn/pull/7724\n",
      "\n",
      "4\n",
      "2977\n",
      "53135b495e986b0712efc453\n",
      "2016-10-25 16:32\n",
      "@amueller Okay! BTW for #7672, you can try `curl -Ls https://goo.gl/jhGwkV | git apply -v --index; git commit -m \"NOMERGE recythonize all\";` to clear up the cache and make tests pass... (Resetting the cache at travis seems to not work...)\n",
      "Have addressed your comments at #5874\n",
      "@amueller For #7724 do you want me to cherry-pick his commits and add a test or do we wait for him to add it?\n",
      "\n",
      "4\n",
      "2978\n",
      "562a7da216b6c7089cb80965\n",
      "2016-10-25 17:29\n",
      "For `sklearn.svm.SVC` with a [callable kernel](http://scikit-learn.org/dev/modules/svm.html#using-python-functions-as-kernels), is there any way the callee knows if the outer estimator is in the midst of a `fit` or `predict` (or similar)?\n",
      "\n",
      "19\n",
      "2979\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-25 17:31\n",
      "I think I did implement mkl with sklearn at some point...\n",
      "hm\n",
      "Well you should implement your own class that calls the kernels, I would say\n",
      "you can always just nystroem it and use a liner model on the combined embedding ;)\n",
      "compute the nystroem feature map (see Nystroem class), which makes the SVM problem a linear problem in that feature space. MKL is just doing a linear model on concatenated features\n",
      "I guess it's a group lasso, though\n",
      "\n",
      "6\n",
      "2980\n",
      "562a7da216b6c7089cb80965\n",
      "2016-10-25 17:49\n",
      "ah, but that callable isn't going to have access to `Y`, even if estimator is doing a fit, so that kind of sinks learning weights based on labels\n",
      "Seems like feature xform is the way to go (if I'm going to fit this into sklearn)\n",
      "\n",
      "2\n",
      "2981\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-10-25 22:15\n",
      "Hello, I am using NearestNeighbors.kneighbors. If there's some cases that are at equal distance from the query point, the number of neighbors might be larger than n_neighbors. Is there a way to find how many neighbors there actually were, i.e. counting the neighbors with equal distance.\n",
      "\n",
      "1\n",
      "2982\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-10-26 01:17\n",
      " Hi guys can anyone help me with some lines of code on how to extract features from 400,000 images dateset using color histogram...in a vector form?? Thank you.\n",
      "\n",
      "1\n",
      "2983\n",
      "561be13fd33f749381a91e6e\n",
      "2016-10-26 04:21\n",
      "although joblib exists, I'm curious if people have experience using dill to pickle both arrays and models.\n",
      "as well as experience with cloudpickle\n",
      "\n",
      "2\n",
      "2984\n",
      "58109daad73408ce4f30f654\n",
      "2016-10-26 12:14\n",
      "Is there any algorithm to make understand the sentence in english.. Actually the meaning of sentence\n",
      "\n",
      "1\n",
      "2985\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-10-26 14:03\n",
      "My code takes about 4seconds to run when using just NearestNeighbors(), but if I use n_jobs=-1 argument, it takes 8.5 minutes. Why it does this?\n",
      "\n",
      "1\n",
      "2986\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-26 16:23\n",
      "@mkoske depending on how much data you have, parallelization might make it slower. Starting the jobs has some overhead. Try n_jobs=2\n",
      "\n",
      "1\n",
      "2987\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-10-26 16:53\n",
      "@amueller I think it is 2, since I have 2 cores, right? I'm using the digits dataset for testing\n",
      "\n",
      "1\n",
      "2988\n",
      "561be13fd33f749381a91e6e\n",
      "2016-10-26 17:02\n",
      "@nilkanthshirodkar apologies, my native tongue is gibberish.  ^_^  Here's the meaning:  Lately, I've been using [dill](https://github.com/uqfoundation/dill) instead of `joblib.dump` to serialize models.  While dill can pickle the object itself,  it can also recursively pickle the object definition, which means you don't need to import the object's class(es) ahead of time.   Unfortunately, I get a variety of file handle errors when dumping some* scikit models using dill, and I'm not sure why.  I know there have been [thoughts to integrate dill into joblib itself](https://github.com/scikit-learn/scikit-learn/issues/5623).  And I'm curious to know if others have had similar experiences with dill or [cloudpickle](https://github.com/cloudpipe/cloudpickle).\n",
      "\n",
      "1\n",
      "2989\n",
      "581195aed73408ce4f3124fe\n",
      "2016-10-27 06:44\n",
      "@saket1192 can anyone help me out with NLTK python i got hindi POS tagger and Hindi parser ,but I am not able to excess it. m new to this so i need some help for my in-house project\n",
      "\n",
      "1\n",
      "2990\n",
      "53135b495e986b0712efc453\n",
      "2016-10-27 15:28\n",
      "Andy, for #7724, do you want me to send a PR to his branch to speed up the process?\n",
      "@amueller\n",
      "\n",
      "2\n",
      "2991\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-27 15:29\n",
      "@raghavrv hm not sure. I mean I'd like to finish up this week, but there is no super rush\n",
      "\n",
      "1\n",
      "2992\n",
      "53135b495e986b0712efc453\n",
      "2016-10-27 15:51\n",
      "Okie so I'll clean up that as a test, comment there and wait for him to make the changes. If it still remains at the end of the week unaddressed, ping me so I can carry forward the work...\n",
      "\n",
      "2\n",
      "2993\n",
      "53135b495e986b0712efc453\n",
      "2016-10-27 22:24\n",
      "Anyone here wants to pick up an easy doc issue? - #7772\n",
      "\n",
      "1\n",
      "2994\n",
      "56a34c16e610378809bdc988\n",
      "2016-10-28 05:01\n",
      "@raghavrv Please have a look at #7776 :)\n",
      "\n",
      "1\n",
      "2995\n",
      "56a34c16e610378809bdc988\n",
      "2016-10-28 05:03\n",
      "Oops, #7775 was submitted just seven minutes before mine and it solves the same issue :sweat_smile: Feel free to close mine if it works out well !\n",
      "\n",
      "1\n",
      "2996\n",
      "53135b495e986b0712efc453\n",
      "2016-10-28 11:58\n",
      "@karandesai-96 Thanks for the PR :)\n",
      "\n",
      "1\n",
      "2997\n",
      "56a34c16e610378809bdc988\n",
      "2016-10-28 13:15\n",
      "@raghavrv my pleasure :) on a similar note, i have one more PR which needs review ( #7655 )\n",
      "\n",
      "1\n",
      "2998\n",
      "53135b495e986b0712efc453\n",
      "2016-10-30 13:35\n",
      "Have done some review. Let me know once you address them!\n",
      "\n",
      "1\n",
      "2999\n",
      "56a34c16e610378809bdc988\n",
      "2016-10-30 17:35\n",
      "@raghavrv thanks, also I just checked out - #7775 was closed for some reason. Let me know if #7776 needs to be reopened again :smile:\n",
      "\n",
      "1\n",
      "3000\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-31 04:48\n",
      "hi everyone\n",
      "\n",
      "1\n",
      "3001\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-31 04:49\n",
      "I want to apply the elastic net to do sparse feature selection on my data, but I only want a subset of the features to be regularized (and the rest of the features to be unregularized)\n",
      "theres probably nothing that supports this right now in scikit-learn, right?\n",
      "not too keen on re-implementing the algorithms myself haha, so Id figure Id ask.\n",
      "@waterponey you should comment that on the original PRs, gitter is fairly low-visibility\n",
      "\n",
      "4\n",
      "3002\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-10-31 04:55\n",
      "I also find it interesting that the elastic net is not mentioned anywhere on the [feature selection page](http://scikit-learn.org/stable/modules/feature_selection.html), since it tends to avoid the problem that the lasso has of selecting an individual variable out of a group of highly correlated features...\n",
      "\n",
      "1\n",
      "3003\n",
      "5626239616b6c7089cb79f22\n",
      "2016-10-31 09:39\n",
      "#7659 and #7671 sould be closed\n",
      "\n",
      "1\n",
      "3004\n",
      "55d21ee30fc9f982beadabb8\n",
      "2016-10-31 15:37\n",
      "@amueller I wonder if you could give a look to #6509\n",
      "ok np\n",
      "\n",
      "2\n",
      "3005\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-31 15:38\n",
      "@glemaitre I'm crazy busy today and tomorrow (ending a job today, starting a new tomorrow). I'll try to catch up with the issue tracker soon but feel free to ping me again.\n",
      "\n",
      "1\n",
      "3006\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2016-10-31 15:39\n",
      "@amueller what's the convention when two PR are opened for the same bug?\n",
      "It's a minor one... very very very minor documentation issue.\n",
      "\n",
      "2\n",
      "3007\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-31 15:40\n",
      "@NelleV you tell them both and maybe give a recommendation on which offers a better solution?\n",
      "@NelleV if they are both ready to merge, merge one and explain / apologize to the other?\n",
      "\n",
      "4\n",
      "3008\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2016-10-31 15:41\n",
      "well, there is a second reviewer needed, if you'd like to review two 2-liners documentation change\n",
      "\n",
      "2\n",
      "3009\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2016-10-31 15:42\n",
      "#7783 and #7784\n",
      "The first one is better.\n",
      "\n",
      "2\n",
      "3010\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-31 15:45\n",
      "the first one is also first.  this can be merged with one review as minor docs.\n",
      "I think we try a mixture of nice and pragmatic\n",
      "\n",
      "3\n",
      "3011\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-10-31 19:43\n",
      "are the scipy docs down or is it just me? https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html\n",
      "\n",
      "2\n",
      "3012\n",
      "57ca533e40f3a6eec0631e0c\n",
      "2016-11-01 00:19\n",
      "what is the best  external  PCIe Box where i can use titan X. I have mac book pro 2009 model.\n",
      "\n",
      "1\n",
      "3013\n",
      "574454a0c43b8c601974a563\n",
      "2016-11-01 15:41\n",
      "@amueller Today the docs are back online.\n",
      "\n",
      "1\n",
      "3014\n",
      "574454a0c43b8c601974a563\n",
      "2016-11-01 15:56\n",
      "<unconvertable> Transpile trained scikit-learn models to a low-level programming language like C, Java, JavaScript or Go with https://github.com/nok/sklearn-porter :smile:\n",
      "\n",
      "1\n",
      "3015\n",
      "54dea33315522ed4b3dc0092\n",
      "2016-11-01 18:07\n",
      "Hello, Need some help with this:  I'm working on a project where I'm creating a spam classifier (instagram photos) I have features from the user and features from the content, would you combine all data knowing that there are multiple photos from the same users. Thank you!\n",
      "\n",
      "1\n",
      "3016\n",
      "53135b495e986b0712efc453\n",
      "2016-11-02 12:21\n",
      "@amueller Could we just have the `return self` fix refactored out of #6141 for 0.18.1?\n",
      "\n",
      "1\n",
      "3017\n",
      "5761eefbc2f0db084a1e1a53\n",
      "2016-11-02 17:54\n",
      "@nok  something I've been looking for!\n",
      "\n",
      "1\n",
      "3018\n",
      "574454a0c43b8c601974a563\n",
      "2016-11-02 22:14\n",
      "@tonnamb Ty!\n",
      "\n",
      "1\n",
      "3019\n",
      "57fed465d73408ce4f2dd5f6\n",
      "2016-11-02 22:59\n",
      "Hey everyone, with sklearn.metrics.roc_curve, is there a way to manually specify the thresholds/cutoffs I want to test at?\n",
      "\n",
      "1\n",
      "3020\n",
      "5770c02dc2f0db084a2017bd\n",
      "2016-11-03 12:22\n",
      "Hey guys. Can someone help with this error I get\n",
      " when I do : `>>> dataset = fetch_mldata('MNIST original'` any idea why I get this error: OSError: could not read bytes the .mat file downloaded, it has 1498112 lines\n",
      "The error itself is : `OSError: could not read bytes`\n",
      "I tried : ```dataset = fetch_mldata('MNIST original', data_home='/Users/myname/Virtualenv/virt1/lib/python3.5/site-packages/sklearn/datasets/')``` aswell\n",
      "\n",
      "4\n",
      "3021\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-11-03 17:16\n",
      "the cached data might be corrupted\n",
      "try removing downloaded files and trying again\n",
      "\n",
      "2\n",
      "3022\n",
      "57475b90c43b8c601975254f\n",
      "2016-11-03 19:03\n",
      "Are Pandas dataframes supported for use in scikit-neuralnetwork?  It appears so, via the Lasagne implementation according to https://recordnotfound.com/scikit-neuralnetwork-aigamedev-8422   but I keep getting errors when using a Pandas dataframe in sknn.\n",
      "\n",
      "1\n",
      "3023\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-11-03 23:05\n",
      "@cpoptic this room is for scikit-learn, and the developers of scikit-learn arent involved in scikit-neuralnetwork. youd probably be better off asking your question in the scikit-neuralnetwork gitter (if there is one) or raising an issue on their github repo / posting to stackoverflow\n",
      "\n",
      "1\n",
      "3024\n",
      "57475b90c43b8c601975254f\n",
      "2016-11-04 23:29\n",
      "@nelson-liu  Thanks, I checked and there is no scikit-neuralnetwork gitter channel, and haven't gotten an answer to this issue when I asked on the github, but I'll post to StackOverflow.  Thanks\n",
      "\n",
      "1\n",
      "3025\n",
      "53135b495e986b0712efc453\n",
      "2016-11-07 13:13\n",
      "@amueller Do you want #7812 into 0.18.1?\n",
      "\n",
      "1\n",
      "3026\n",
      "53135b495e986b0712efc453\n",
      "2016-11-07 14:26\n",
      "Also should we move #7627 out of 0.18.1 into 0.19?\n",
      "Also #7544 and #7173?\n",
      "\n",
      "2\n",
      "3027\n",
      "55e7596f0fc9f982beaf75b6\n",
      "2016-11-09 07:41\n",
      "Hello, I've proposed a project for chromosomes segmentation : https://github.com/jeanpat/DeepFISH . Up to now there are datasets (82146 images+ground-truth labels) and python notebooks (way to generate datasets, scikit-image based). The notebooks could be better:  the size of the datasets are limited by the amount of RAM. The datasets are waiting to train a classifier.\n",
      "\n",
      "1\n",
      "3028\n",
      "56b38569e610378809bfe274\n",
      "2016-11-10 08:55\n",
      "Completely new to contributing to sklearn. Is there a recommended contributions guide? Considering fixing the memory issues with bh-tsne.\n",
      "\n",
      "1\n",
      "3029\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-11-10 18:12\n",
      "@hvaara http://scikit-learn.org/dev/developers/contributing.html#contributing\n",
      "\n",
      "1\n",
      "3030\n",
      "5824bf65d73408ce4f350625\n",
      "2016-11-10 19:17\n",
      "Hi, I'm new with machine learning, do you can help me for information?\n",
      "\n",
      "1\n",
      "3031\n",
      "55d054b70fc9f982bead8af7\n",
      "2016-11-11 11:16\n",
      "Hi forks, I am trying import ```from sklearn.model_selection import cross_val_score``` but it had an error ```ImportError: No module named 'sklearn.model_selection'``` and I installed scikit-learn package but I don't know what matter for me , I cannot import this module to my project. I am using python 3.5. Can you help me ? Many thanks :D\n",
      "\n",
      "1\n",
      "3032\n",
      "576e76e2c2f0db084a1fdb14\n",
      "2016-11-12 16:50\n",
      "@khanhnguyenneka you have to update your `scikit-learn` installation to 0.18.x version\n",
      "\n",
      "1\n",
      "3033\n",
      "5827d866d73408ce4f3575e6\n",
      "2016-11-14 03:07\n",
      "@VictorArias24 Hi, Victor! For some basics you can refer to: http://scikit-learn.org/stable/tutorial/basic/tutorial.html hope it helps!\n",
      "\n",
      "1\n",
      "3034\n",
      "5730c2dcc43b8c601971eca1\n",
      "2016-11-14 10:16\n",
      "Just wondering if anybody in Scikit Community has already worked on Recurrent Decision Tree implementation ?\n",
      "\n",
      "1\n",
      "3035\n",
      "564789be16b6c7089cbab8b7\n",
      "2016-11-17 11:12\n",
      "Does https://github.com/scikit-learn/scikit-learn/milestone/22 need updating?\n",
      "\n",
      "1\n",
      "3036\n",
      "54c80b5ddb8155e6700f2708\n",
      "2016-11-17 11:46\n",
      "Hi! Does anyone know how to show percentage of similarity of predicted data to one of train data?\n",
      "And how to group train data by labels?\n",
      "thanks\n",
      "\n",
      "3\n",
      "3037\n",
      "580500d6d73408ce4f2ee055\n",
      "2016-11-18 05:49\n",
      "Hi\n",
      "\n",
      "1\n",
      "3038\n",
      "55a36aae5e0d51bd787b33cd\n",
      "2016-11-18 16:11\n",
      "Hello, I'm not sure what metric my cross_val_score uses. The estimator is a RF regressor, and I leave the scorer for cross_val_score at None. Does that mean I get R^2? Because I somehow have an R^2 of minus 300 then ...\n",
      "\n",
      "1\n",
      "3039\n",
      "543d7fd1db8155e6700cb700\n",
      "2016-11-21 20:01\n",
      "I'm also running into an issue downloading datasets with `datasets.fetch_mldata()` (cc @RohanVB)\n",
      "urlopen() fails with a \"Connection reset by peer\", though downloading the same URL with curl works (but curl --verbose shows a warning)\n",
      "Did mldata.org change their server, and it now does something weird with HTTP that Python's urllib2 can't handle?\n",
      "\n",
      "3\n",
      "3040\n",
      "543d7fd1db8155e6700cb700\n",
      "2016-11-21 20:04\n",
      "``` $ curl -LI -X GET http://mldata.org/repository/data/download/matlab/mnist-original HTTP/1.0 301 MOVED PERMANENTLY Date: Mon, 21 Nov 2016 20:03:05 GMT Content-Type: text/html; charset=utf-8 Location: http://mldata.org/repository/data/download/matlab/mnist-original/ Server: fapws3/0.9  HTTP/1.0 200 OK Content-Length: 55440440 Content-Disposition: attachment; filename=mnist-original.mat Vary: Cookie Server: fapws3/0.9 Date: Mon, 21 Nov 2016 20:03:05 GMT Content-Type: application/x-matlab  $ python -c 'import sklearn.datasets; sklearn.datasets.fetch_mldata(\"MNIST original\")' Traceback (most recent call last):   File \"<string>\", line 1, in <module>   File \"/home/vagrant/.virtualenvs/digits-sklearn-opencv/local/lib/python2.7/site-packages/sklearn/datasets/mldata.py\", line 143, in fetch_mldata     mldata_url = urlopen(urlname)   File \"/usr/lib/python2.7/urllib2.py\", line 154, in urlopen     return opener.open(url, data, timeout)   File \"/usr/lib/python2.7/urllib2.py\", line 435, in open     response = meth(req, response)   File \"/usr/lib/python2.7/urllib2.py\", line 548, in http_response     'http', request, response, code, msg, hdrs)   File \"/usr/lib/python2.7/urllib2.py\", line 467, in error     result = self._call_chain(*args)   File \"/usr/lib/python2.7/urllib2.py\", line 407, in _call_chain     result = func(*args)   File \"/usr/lib/python2.7/urllib2.py\", line 651, in http_error_302     fp.read()   File \"/usr/lib/python2.7/socket.py\", line 355, in read     data = self._sock.recv(rbufsize)   File \"/usr/lib/python2.7/httplib.py\", line 612, in read     s = self.fp.read(amt)   File \"/usr/lib/python2.7/socket.py\", line 384, in read     data = self._sock.recv(left) socket.error: [Errno 104] Connection reset by peer ```\n",
      "\n",
      "1\n",
      "3041\n",
      "543d7fd1db8155e6700cb700\n",
      "2016-11-21 20:14\n",
      "Wow now I'm seeing \"Connection reset by peer\" from curl as well. I'll just try and self-host that somewhere, if I can ever grab it\n",
      "\n",
      "1\n",
      "3042\n",
      "543d7fd1db8155e6700cb700\n",
      "2016-11-21 20:22\n",
      "There is no way to load a MATLAB mldata file from a different URL though, is there?\n",
      "\n",
      "1\n",
      "3043\n",
      "574454a0c43b8c601974a563\n",
      "2016-11-21 21:25\n",
      "Today I found https://github.com/scikit-learn-contrib/scikit-learn-contrib , is it possible to add https://github.com/nok/sklearn-porter ? But it isn't an estimator like one of the listed examples. Who can help me?\n",
      "\n",
      "1\n",
      "3044\n",
      "574c6042c43b8c601975bcdb\n",
      "2016-11-23 00:14\n",
      "Hi guys. I'm a beginning undergrad researcher in data science and trying to run scikits svm model on 500k models.\n",
      "rows*\n",
      "It's taking 5 terms of life to complete. Can someone help point me in the right direction for how I should apporoach this?\n",
      "\n",
      "3\n",
      "3045\n",
      "54dea33315522ed4b3dc0092\n",
      "2016-11-23 01:58\n",
      "@Schachte Try using linearSVM instead of SVC, you can see in the documentation the following  The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.\n",
      "\n",
      "1\n",
      "3046\n",
      "574c6042c43b8c601975bcdb\n",
      "2016-11-23 03:20\n",
      "hi @firetix , thanks a lot. I ran this and it completes very quickly now. Can you explain how the testing works with the clf-score?\n",
      "@firetix does it automatically just split the input data into test/train for me?\n",
      "\n",
      "2\n",
      "3047\n",
      "54dea33315522ed4b3dc0092\n",
      "2016-11-23 18:41\n",
      "@Schachte it works the same way as an SVC you will have to do the split yourself and then call .score or run f1 evaluation\n",
      "\n",
      "1\n",
      "3048\n",
      "583a6e71d73408ce4f38e30d\n",
      "2016-11-27 05:27\n",
      "I'm a newbie, so please pardon what might be a very simple / silly question.  I'm wondering whether the example code for various sample problems posted on the scikit-learn website are done in Python 2 or Python 3?\n",
      "\n",
      "1\n",
      "3049\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-11-27 20:00\n",
      "@FLalani should be compatible with both\n",
      "\n",
      "1\n",
      "3050\n",
      "56c4f19ae610378809c1f8ae\n",
      "2016-11-27 20:02\n",
      "@nok contact the scikit-learn contrib folks about that. I think the proper procedure is to raise an issue on their repo, but I'm not sure.\n",
      "\n",
      "1\n",
      "3051\n",
      "574454a0c43b8c601974a563\n",
      "2016-11-27 20:04\n",
      "Thanks @nelson-liu\n",
      "\n",
      "1\n",
      "3052\n",
      "583a6e71d73408ce4f38e30d\n",
      "2016-11-27 23:07\n",
      "@nelson-liu:  Thanks!  It did indeed run with no exceptions on Python 2.7 after my comment so I can at least verify Python 2.  Thanks again.\n",
      "\n",
      "1\n",
      "3053\n",
      "583d3402d73408ce4f3962b6\n",
      "2016-11-29 07:59\n",
      "Hi, i am running into some really wierd behavior on gridsearchCV on an ESXI host. Although number of jobs is set to a fixed number, the spawned pythonw.exe will claim all available cores after the 2nd-3th task iteration and run into some sort of infinite loop\n",
      "\n",
      "1\n",
      "3054\n",
      "56ed0b8885d51f252ab9a1b8\n",
      "2016-11-30 16:35\n",
      "Hi everyone,please is it possible to join two image feature vectors as a vector and use it to train a model for classification (example is raw pixel feature vector and color histogram feature vector of an image).I would appreciate your help, thank you.\n",
      "\n",
      "1\n",
      "3055\n",
      "55901c1b15522ed4b3e2f949\n",
      "2016-12-02 20:26\n",
      "@Ben-Kobby can't you just concatenate the two vectors together?\n",
      "\n",
      "1\n",
      "3056\n",
      "5770c02dc2f0db084a2017bd\n",
      "2016-12-04 05:10\n",
      "@remram44 I attained a fix if its still required.\n",
      "I'll have to look into my code to see exactly what I did, ping me if you need it.\n",
      "\n",
      "2\n",
      "3057\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-06 14:34\n",
      "Is there any post-pruning implementation in scikit for decision trees?\n",
      "\n",
      "1\n",
      "3058\n",
      "57781008c2f0db084a211eda\n",
      "2016-12-06 16:20\n",
      "Any suggestions for how to implement handwriitten signature verification in Python ?\n",
      "\n",
      "1\n",
      "3059\n",
      "530c03e25e986b0712efafb8\n",
      "2016-12-06 17:13\n",
      "General question, why haven't tensor factorizations (like Parafac, Tucker) become more widely adopted?  (or, are they widely adopted and I'm not aware?)\n",
      "\n",
      "1\n",
      "3060\n",
      "530c03e25e986b0712efafb8\n",
      "2016-12-06 17:43\n",
      "(I apologize for being somewhat off topic here)\n",
      "\n",
      "1\n",
      "3061\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-07 17:23\n",
      "@mrocklin they will be. For what application were you thinking?\n",
      "wow I haven't been here in a while.. whoops\n",
      "\n",
      "2\n",
      "3062\n",
      "530c03e25e986b0712efafb8\n",
      "2016-12-07 17:42\n",
      "They've been around for a long while, why the delay?  And more importantly, why the optimism?\n",
      "\n",
      "1\n",
      "3063\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-07 17:43\n",
      "in the context of machine learning they haven't been around that long, and I think for topic modeling for example they are only now becoming popular\n",
      "they've been around for CRFs for a while\n",
      "which applications / algorithms did you have in mind?\n",
      "\n",
      "3\n",
      "3064\n",
      "530c03e25e986b0712efafb8\n",
      "2016-12-07 17:44\n",
      "I've had a request to build implementations on top of dask.array.  I'm polling to see how much value there is to the general community.\n",
      "If you happen to have strong opinions about the value of various algorithms I'd enjoy getting your thoughts.\n",
      "\n",
      "2\n",
      "3065\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-08 11:25\n",
      "What does it mean, when I do grid search and in ther cv_results_ there's param_n_estimators, for example, and it's type is masked_array and then next to it, there's mask which is full of boolean false values?\n",
      "\n",
      "1\n",
      "3066\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-09 16:04\n",
      "@mrocklin can you share your application domain?\n",
      "@mkoske it means none of the values is masked out\n",
      "i.e. it's a normal full array\n",
      "huh alright\n",
      "well in machine learning, tensor decomposition applications are somewhat \"new\"\n",
      "\n",
      "5\n",
      "3067\n",
      "530c03e25e986b0712efafb8\n",
      "2016-12-09 17:10\n",
      "@amueller I genuinely don't know the application domain.  Government types are asking for a general library for tensor decompositions in Python.\n",
      "Which, while that ignorance is somewhat suboptimal, also means that we get to build software for the common case.\n",
      "(this is somewhat typical in my world)\n",
      "\n",
      "3\n",
      "3068\n",
      "530c03e25e986b0712efafb8\n",
      "2016-12-09 17:14\n",
      "Given the wiggle room, how would you direct my time to best benefit the community if other people were paying for it?\n",
      "\n",
      "1\n",
      "3069\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-10 13:50\n",
      "In RandomForestClassifier, if the max_features is sqrt or log2, does it round it to nearest int or what?\n",
      "\n",
      "1\n",
      "3070\n",
      "55d21ee30fc9f982beadabb8\n",
      "2016-12-11 13:48\n",
      "@mkoske This is casted to an integer. Refer to [here](https://github.com/scikit-learn/scikit-learn/blob/b7e370244795c291fc8ac10686ce5867adde988e/sklearn/tree/tree.py#L211) for details.\n",
      "\n",
      "1\n",
      "3071\n",
      "584d9ad9d73408ce4f3c4b46\n",
      "2016-12-11 18:35\n",
      " am asked to only stimulate a smart device in c++ that could belong to the IOT. Although, I am not entirely sure what to choose. I have been given an example such as a weather device, also it is to produced data by 3 (simulated) sensors. For example the weather device, may register data from temperature, air pressure and wind speed sensors. Any ideas on a device i could choose?\n",
      "\n",
      "1\n",
      "3072\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-11 18:58\n",
      "@glemaitre, thanks\n",
      "\n",
      "1\n",
      "3073\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-12 16:37\n",
      "@mrocklin I talked to the person working on tensor factorization for LDA. That woudl be nice but I think they said it's not really ready yet. So I'm not sure there is a killer application. Maybe check out http://www.cs.columbia.edu/~djhsu/papers/power-jmlr.pdf\n",
      "\n",
      "1\n",
      "3074\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-13 13:16\n",
      "Hello, what's the difference between mean_test_score and mean_train_score in GridSearchCV.cv\\_results_?\n",
      "\n",
      "1\n",
      "3075\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-13 15:32\n",
      "they are the mean of the score on the training folds vs the hold-out folds\n",
      "@mkoske\n",
      "only the hold-out test score is used to select the model\n",
      "the training score is good monitor overfitting / underfitting\n",
      "\n",
      "4\n",
      "3076\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-13 15:46\n",
      "@amueller So hold-out folds means, that if I use e.g. 10-fold cross validation, there's 10 hold-out folds, i.e. dataset is split into 10 and then the model is trained on 9 of those and one is used for testing . And this is repeated until every one of those 10 is used as testing. Right?\n",
      "\n",
      "5\n",
      "3077\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-13 15:49\n",
      "scores are computed for each of the 10 iterations\n",
      "and each iteration has a 9 folds that the model is trained on and one that it is tested on\n",
      "the training score is the score on the training set, i.e. the 9 folds\n",
      "\n",
      "3\n",
      "3078\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-13 15:54\n",
      "yes. training score meaning the data it was trained on\n",
      "\n",
      "1\n",
      "3079\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-13 16:01\n",
      "Ok, I'll recap so I make sure I understood correctly :) In my example, one iteration consists of building the model on 9 folds and testing it with both those 9 folds and hold-out data. Is this correct?\n",
      "\n",
      "4\n",
      "3080\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-13 16:04\n",
      "well if there is a large gap then you might want to regularize more. if there is a very small gap you might want to try a more complex model\n",
      "\n",
      "1\n",
      "3081\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-13 16:14\n",
      "One more question. When I run my script, Parallel reported that it took almost 3hours to complete. But when I sum all the fit and score times from GridSearchCV.cv\\_results\\_, the sum is significantly lower. Why? Is it that Parallel reports the total time my script was running and GridSearchCV exact times that scoring and fitting took?\n",
      "\n",
      "2\n",
      "3082\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-13 17:13\n",
      "@amueller nothing special, something like this: https://gist.github.com/mkoske/1f9b92b3f214260278ea115507cea72d\n",
      "I use Jupyter Notebook and it shows some timing information on red background\n",
      "@amueller ok, thanks anyway\n",
      "\n",
      "3\n",
      "3083\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-13 17:19\n",
      "I updated that gist to contain example similar to my use case.\n",
      "I also updated my comment on that gist to explain\n",
      "\n",
      "2\n",
      "3084\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-13 18:33\n",
      "sorry not sure\n",
      "I don't have time to look into it right now\n",
      "\n",
      "2\n",
      "3085\n",
      "544906e2db8155e6700cdd16\n",
      "2016-12-16 18:35\n",
      "Hi everyone! I'm doing some research on hyperparameter optimization and I felt very impressed by the technique shown in this tutorial by @ogrisel  https://www.youtube.com/watch?v=iFkRt3BCctg Do you recommend any similar package or any library that implement that technique? I was playing with GridSearchCV and RandomizedSearchCV optimizers but those seem to be less interactive versions of the one in the tutorial (more \"blackbox\")\n",
      "\n",
      "1\n",
      "3086\n",
      "5634e15116b6c7089cb8f9f2\n",
      "2016-12-20 05:08\n",
      "Hi all, how would I do knowledge representation in SKL?\n",
      "\n",
      "1\n",
      "3087\n",
      "57714f9ec2f0db084a203010\n",
      "2016-12-20 05:11\n",
      "Can someone explain what hyperparameter optimization?\n",
      "\n",
      "1\n",
      "3088\n",
      "584ae429d73408ce4f3bed37\n",
      "2016-12-20 11:17\n",
      "Hello everyone I wanted to contribute. Can anybody help me how to do so????\n",
      "\n",
      "1\n",
      "3089\n",
      "584ae429d73408ce4f3bed37\n",
      "2016-12-20 11:28\n",
      "I have knowledge of ML and R but am completely new to scikit-learn\n",
      "\n",
      "1\n",
      "3090\n",
      "54d4a1d6db8155e6700f853b\n",
      "2016-12-20 17:42\n",
      "Abinash check out the contributor docs on the website\n",
      "\n",
      "1\n",
      "3091\n",
      "562a7da216b6c7089cb80965\n",
      "2016-12-21 02:10\n",
      "I apologize for a rather naive python question, but I've forked sklearn, made some local changes introducing new functinoality, committed them, and now want to use my modified sklearn in a test project. Should I manipulate sys.path to import my modified sklearn? Do I need to run `make` in it first? Do I need to re-run `make` if I edit sklearn again? Or should I be running `make` as in [here](http://scikit-learn.org/stable/developers/advanced_installation.html#testing-scikit-learn-from-within-the-source-folder)?\n",
      "\n",
      "1\n",
      "3092\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-12-21 02:16\n",
      "Hi, I suppose the last link would be better in case you are okay to use dev version for all purposes. I\n",
      "Sorry sent message early. I use ``python setup.py develop``. In case you want both stable and dev versions, it would be better to use virtualenv to create a virtual environment for working on dev. hope it helps.\n",
      "\n",
      "2\n",
      "3093\n",
      "562a7da216b6c7089cb80965\n",
      "2016-12-21 02:24\n",
      "Ah, great idea RE virtualenv; thanks!\n",
      "\n",
      "2\n",
      "3094\n",
      "584ae429d73408ce4f3bed37\n",
      "2016-12-21 02:45\n",
      "Thanks  @amueller  :smile:\n",
      "\n",
      "1\n",
      "3095\n",
      "578b5ed1c2f0db084a235992\n",
      "2016-12-21 04:58\n",
      "I am having some trouble getting an MLPRegressor sample to work. Any hints or pointers appreciated. Thank you.\n",
      "P\n",
      "http://stackoverflow.com/questions/41069905/trouble-fitting-simple-data-with-mlpregressor\n",
      "\n",
      "3\n",
      "3096\n",
      "5757de1ec43b8c6019787b6c\n",
      "2016-12-23 11:09\n",
      "Is it possible to write wrapper on python code to use it in Java , for NER Purpose ?\n",
      "\n",
      "1\n",
      "3097\n",
      "585e5cecd73408ce4f3f162e\n",
      "2016-12-24 11:34\n",
      "I am using a decision tree to classify my data. Using scikit-learn, how do i load my own dataset? Unable to understand it in scikit-learn.org where they talk about loading from external datasets. Any help would be appreciated\n",
      "\n",
      "1\n",
      "3098\n",
      "585e5cecd73408ce4f3f162e\n",
      "2016-12-24 12:32\n",
      "Anyoone?? My dataset is this format:  Day Outlook Temp Humidity Wind Play 1 Sunny Hot High Weak No 2 Sunny Hot High Strong No 3 Overcast Hot High Weak Yes 4 Rain Mild High Weak Yes 5 Rain Cool Normal Weak Yes 6 Rain Cool Normal Strong No 7 Overcast Cool Normal Strong Yes 8 Sunny Mild High Weak No 9 Sunny Cool Normal Weak Yes 10 Rain Mild Normal Weak Yes 11 Sunny Mild Normal Strong Yes 12 Overcast Mild High Strong Yes 13 Overcast Hot Normal Weak Yes 14 Rain Mild High Strong No\n",
      "\n",
      "1\n",
      "3099\n",
      "553d32d715522ed4b3df8b92\n",
      "2016-12-24 12:56\n",
      "Hi, in case your data is in a CSV or test file, you can use [`numpy.load_txt`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) and [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) and then use the scikit learn DecisionTreeClassifier for classification. Hope it helps.\n",
      "\n",
      "1\n",
      "3100\n",
      "5808a180d73408ce4f2f919f\n",
      "2016-12-27 13:34\n",
      "Hey guys, I hope you can help me out\n",
      "\n",
      "1\n",
      "3101\n",
      "5808a180d73408ce4f2f919f\n",
      "2016-12-27 13:34\n",
      "I want to train a random forest regressor, and my features are mainly categorical\n",
      "The thing is, the amount of possible labels per example is variable, so I can't easily dump it in a CSV\n",
      "I tried creating a 1-hot encoded CSV but that would result in a ~50GB file\n",
      "So what I want is saving my label-encoded values in some data format (maybe JSON, which supports arrays), and one-hot encoding on the fly during training\n",
      "I'm not sure where to start though, any clues?\n",
      "\n",
      "5\n",
      "3102\n",
      "5808a180d73408ce4f2f919f\n",
      "2016-12-27 14:09\n",
      "Hmm I guess scipy sparse matrices could help me here, then I just need to find a good file format\n",
      "\n",
      "1\n",
      "3103\n",
      "572cc82bc43b8c6019718138\n",
      "2016-12-27 15:50\n",
      "Hello everyone! My name is Samriddhi Sinha. I am interested in contributing to this project. I have been working with various Machine Learning algorithms an have participated in a few Data Analytics competitions. Can some one help me getting started?\n",
      "\n",
      "1\n",
      "3104\n",
      "55d21ee30fc9f982beadabb8\n",
      "2016-12-29 11:22\n",
      "@djokester You can check out the [contributors guide](http://scikit-learn.org/stable/developers/contributing.html)\n",
      "\n",
      "1\n",
      "3105\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-30 17:25\n",
      "Hello, what's the feature importance implemented in trees / ensembles? Is it Gini-importance or what?\n",
      "\n",
      "1\n",
      "3106\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2016-12-30 17:38\n",
      "At least DecisionTreeClassifier documentation says Gini importance\n",
      "\n",
      "1\n",
      "3107\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-01-03 18:31\n",
      "@mkoske depends on the criterion ;)\n",
      "hm I can't build master because of some C++ linking issues :-/\n",
      "ah right, I had that issue before on another box https://github.com/scikit-learn/scikit-learn/issues/7869\n",
      "\n",
      "3\n",
      "3108\n",
      "58183b57d73408ce4f323e33\n",
      "2017-01-04 03:32\n",
      "Does anyone know what is the precision used in metrics calculation in sklearn. I'm asking because I see significant differences in calculated metrics between sklearn and xgboost which might be occuring due to precision issues.\n",
      "\n",
      "1\n",
      "3109\n",
      "56a34c16e610378809bdc988\n",
      "2017-01-05 04:32\n",
      "Hi, is #7319 (Move to py.test) still under consideration as gsoc '17 project ? I pinged there earlier to work on it but realised that it is not that small to do within a week or two. I migrated joblib's testing framework from nose to py.test and am willing to take up the same here :smile:\n",
      "\n",
      "1\n",
      "3110\n",
      "55a36f535e0d51bd787b3400\n",
      "2017-01-06 15:05\n",
      "Hi all. Is there a plan for making `cross_val_score` accept multidimensional scoring metrics (i.e. a scorer that returns an array rather than a float)? I see that #7388 may be related, but IIUC it mainly to compare different metrics.\n",
      "\n",
      "1\n",
      "3111\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-01-10 20:14\n",
      "@kingjr There is an intend. I'm not sure I'd say there is a plan. I haven't caught up with #7388 and I'm not sure whether it will include returning arrays, probably not\n",
      "@karandesai-96 yes, it's still a possible project. I'm not sure who would be mentoring, though.\n",
      "@nareshshah139 which metric?\n",
      "\n",
      "3\n",
      "3112\n",
      "57ab24e040f3a6eec05ec701\n",
      "2017-01-11 20:45\n",
      "I posted a question on Stack Overflow related to preprocessing-scaling my features taking the logarithm but column-based as with the `MaxAbsScaler` -- Question: http://stackoverflow.com/questions/41600349/scale-apply-function-sparse-matrix-logarithmically\n",
      "Any help is much appreciated <unconvertable>\n",
      "\n",
      "2\n",
      "3113\n",
      "56a34c16e610378809bdc988\n",
      "2017-01-12 03:54\n",
      "@amueller Loic Esteve reviewed 20 of my PRs on joblib. :) We would probably complete by this weekend.\n",
      "\n",
      "1\n",
      "3114\n",
      "57ab24e040f3a6eec05ec701\n",
      "2017-01-12 10:01\n",
      "Thank you to scikit-learn contributors -- It seems to be a great community\n",
      "\n",
      "1\n",
      "3115\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-01-16 15:09\n",
      "@karandesai-96 sweet! Good you two!\n",
      "\n",
      "1\n",
      "3116\n",
      "55a36f535e0d51bd787b3400\n",
      "2017-01-16 16:47\n",
      "@amueller ok thanks\n",
      "\n",
      "1\n",
      "3117\n",
      "5874edafd73408ce4f428857\n",
      "2017-01-16 16:48\n",
      "Anybody recommend any starter guides for scikit-learn?\n",
      "Whats the best way to get into this?\n",
      "lol. What book?\n",
      "Ok awesome thanks.\n",
      "CS\n",
      "\n",
      "5\n",
      "3118\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-01-16 16:48\n",
      "@aquan9 my book ;)\n",
      "or sebastians book?\n",
      "https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413/ref=sr_1_1?ie=UTF8&qid=1479485017&sr=8-1&keywords=introduction+to+machine+learning+with+python\n",
      "https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130/ref=pd_bxgy_14_img_2?_encoding=UTF8&psc=1&refRID=HA8X4W6W4KAK3RCZ7G85\n",
      "(if you get mine, make sure it's the second print that just came out)\n",
      "There's also a bunch of free tutorials linked on the website:\n",
      "http://scikit-learn.org/stable/presentations.html\n",
      "what's your background?\n",
      "my book is \"no math\". sebastians is \"some math\"\n",
      "I recommend reading mine alongside bishops book or elements of statistical learning if your math-minded\n",
      "thanks. it's mostly all the worst typos.\n",
      "And obviously someone pointed out something real bad just after it got finished lol\n",
      "\n",
      "13\n",
      "3119\n",
      "57ec309f40f3a6eec067e511\n",
      "2017-01-16 16:50\n",
      "Just saw your update on the book. Congrats  @amueller\n",
      "\n",
      "1\n",
      "3120\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-01-16 16:52\n",
      "@aquan9 there's also a bunch of free tutorials on my website: amueller.github.io\n",
      "\n",
      "1\n",
      "3121\n",
      "5874edafd73408ce4f428857\n",
      "2017-01-16 16:58\n",
      "@amueller Thanks for the advice\n",
      "\n",
      "1\n",
      "3122\n",
      "57ec309f40f3a6eec067e511\n",
      "2017-01-16 17:02\n",
      "In the past I have also found the following book to be very helpful \"Machine Learning- The Art and Science of Algorithms that Make Sense of Data\". @amueller . Whats your opinion about the same @amueller ?\n",
      "\n",
      "2\n",
      "3123\n",
      "55b8f4510fc9f982beab69f3\n",
      "2017-01-17 06:24\n",
      "**\\[shubham4060\\]** Hi,\n",
      "\n",
      "1\n",
      "3124\n",
      "55b8f4510fc9f982beab69f3\n",
      "2017-01-17 06:27\n",
      "**\\[shubham4060\\]** I am a 3rd year engineering currently pursuing Computer Science and Engineering department, IIT Kharagpur. i am really interested in this project and i really want to contribute. it would be very helpful if i could get some guidance on how to start. really looking forward to hearing from you.\n",
      "\n",
      "1\n",
      "3125\n",
      "587de5b8d73408ce4f441152\n",
      "2017-01-17 09:37\n",
      "hello, everybody, i'm coming\n",
      "\n",
      "1\n",
      "3126\n",
      "55b8f4510fc9f982beab69f3\n",
      "2017-01-18 05:53\n",
      "**\\[madan96\\]** Can anyone please explain me the reason for this error? Ref: https://travis-ci.org/sympy/sympy/jobs/192867768\n",
      "**\\[madan96\\]** @jksuom I think the `elif` condition might resolve the issue.\n",
      "\n",
      "2\n",
      "3127\n",
      "539f11a8a9176b500d1ce23e\n",
      "2017-01-18 06:31\n",
      "@yhaddad, why are you echoing chats in the sympy room here?\n",
      "\n",
      "1\n",
      "3128\n",
      "58033d49d73408ce4f2e92ba\n",
      "2017-01-26 10:04\n",
      "Hi all ! I have some trouble with the MinMaxScaler, I m using it together with a keras model with input with 3 features and 1 output . my xdate is of shape (XX, 3) and output (XX,1) , I dont have any issue when calling scaler.fit_transform on both input and output training data\n",
      "but when I try to call inverse_transform on my predictions I get errors like this : ValueError: non-broadcastable output operand with shape (18,1) doesn't match the broadcast shape (18,3) or ValueError: operands could not be broadcast together with shapes (18,) (3,) (18,) or ValueError: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,3)  (I tried so many reshape but nothing works as excepted :( )\n",
      "Using a different scaler for input and output fixes the issue but is it the only solution?\n",
      "\n",
      "3\n",
      "3129\n",
      "588b6fe9d73408ce4f467366\n",
      "2017-01-27 16:11\n",
      "@amueller is there  a reason that the new gaussian process classifier doesn't provide an error estimate on the fitted probability, similar to the gaussian process regressor?  I believe this can be done, similar as to how one would use the covariance matrix/Fisher information of logistic regression model to confidence bounds upon the fitted probabilities?  Thx in advance.\n",
      "\n",
      "1\n",
      "3130\n",
      "5729ef37c43b8c60197119b1\n",
      "2017-01-28 08:10\n",
      "@amueller Are there any immediate plans on inproving clustering performance?  Thanks!\n",
      "\n",
      "1\n",
      "3131\n",
      "580cd140d73408ce4f3039f6\n",
      "2017-01-29 13:14\n",
      "Hi, is there any one with experience with Gaussian processes. Are 20 data points enough to fit a GP? Is there any way to overfit a GP?\n",
      "\n",
      "1\n",
      "3132\n",
      "588b76ccd73408ce4f4674e7\n",
      "2017-01-29 15:49\n",
      "iirc, I thought the \"magic minimum\" for gaussian  distributions was a sample of 30\n",
      "\n",
      "1\n",
      "3133\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-01-31 11:16\n",
      "How fast or slow is SpasePCA? I have a dataset of size about 15000 x 500 and it seems to take quite a while\n",
      "\n",
      "1\n",
      "3134\n",
      "54b4f2d1db8155e6700e99c0\n",
      "2017-01-31 13:44\n",
      "Hey Folks, [Core question], is there a specific reason why `RandomForests` dont inherit from `BaseBagging`?\n",
      "\n",
      "1\n",
      "3135\n",
      "5891ba54d73408ce4f476005\n",
      "2017-02-01 11:40\n",
      "Does scikit have any ideas page for gsoc 2017?\n",
      "\n",
      "1\n",
      "3136\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-02-01 18:33\n",
      "not yet\n",
      "\n",
      "1\n",
      "3137\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-02-01 18:34\n",
      "@Djabbz things inherit when there's shared functionality. if there's not, there's no reason to inherit\n",
      "\n",
      "1\n",
      "3138\n",
      "54b4f2d1db8155e6700e99c0\n",
      "2017-02-01 19:31\n",
      "Thanks @amueller. But if you compare the code, there are a lot of similarities (not surprising given that RF are bagged random trees). My question is: is it done on purpose or is it a consequence of different persons working on different algorithms?\n",
      "\n",
      "1\n",
      "3139\n",
      "579618a040f3a6eec05c5e42\n",
      "2017-02-02 14:45\n",
      "Hey guys, I have a question regarding Gaussian process. It seems to me that is taking for ever to converge even with the n_jobs=-1 option. The data is very high dimensional. Any suggestions to speed up the convergence? Any tricks I should be aware of? Last but not least I've noticed that the option of `n_jobs=-1`is included only in a number of algorithms. Why was that the case? What is the logic behind it to include it only in a subset of the algorithms that scikit offers?\n",
      "\n",
      "1\n",
      "3140\n",
      "5891ba54d73408ce4f476005\n",
      "2017-02-02 20:21\n",
      "are there no newcomer issues to solve at https://github.com/scikit-learn/scikit-learn/issues\n",
      "i want to start contributing here\n",
      "\n",
      "2\n",
      "3141\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-02-07 22:26\n",
      "check the \"easy\" tag\n",
      "\n",
      "1\n",
      "3142\n",
      "589dc0e0d73408ce4f4968ce\n",
      "2017-02-10 14:07\n",
      "Hi guys\n",
      "is there anyone know C# ?\n",
      "\n",
      "2\n",
      "3143\n",
      "5869f3ded73408ce4f4098f2\n",
      "2017-02-12 10:02\n",
      "Are there any 13 year olds on\n",
      "\n",
      "1\n",
      "3144\n",
      "56bb7a56e610378809c0cb2c\n",
      "2017-02-13 16:51\n",
      "`@ThatGeoGuy:matrix.org` Hey all, was wondering if anyone had any thoughts regarding https://github.com/scikit-learn/scikit-learn/issues/4682. I've been thinking about  the issue more and the more I think about it, it seems the appropriate choice would be to change the covariance scaling in `MinCovDet` from `n` to `n-1`\n",
      "`@ThatGeoGuy:matrix.org` Or, barring that, rename `mcd.covariance_` to `mcd.scatter_`, since covariance usually implies that you did `n-1` scaling\n",
      "\n",
      "2\n",
      "3145\n",
      "570d67cb187bb6f0eadf268b\n",
      "2017-02-18 18:58\n",
      "hey! quick question about LSTMs if someone has a minute\n",
      "\n",
      "1\n",
      "3146\n",
      "58a9a221d73408ce4f4b6740\n",
      "2017-02-19 13:52\n",
      "Hello friends, first time on gitter so introduction: I am Shubham Bhardwaj 2nd year computer science undergrad from VIT Vellore. I mostly contributed on back end tasks using python in Google Developers Group-VIT. But I made a transition to Machine Learning this Winter after taking a course on UDACITY. Looking forward to working and learning from all. Thanks.\n",
      "\n",
      "1\n",
      "3147\n",
      "58ab8421d73408ce4f4bbbb2\n",
      "2017-02-21 13:13\n",
      "Hello everybody, I am new here! It is nice to be here ! Cheers\n",
      "\n",
      "1\n",
      "3148\n",
      "58ab8421d73408ce4f4bbbb2\n",
      "2017-02-21 13:16\n",
      "I have a question regarding applying the standardaization of the training set to a test set , for classification procedures. Do we repeat this 10 times for 10CV ?\n",
      "@shubham0704 Thank you, do we apply the mean/std of the train set to the test for each fold ?\n",
      "Sure, I read that a prefered practise in pre processing is to standardize the train set and apply the configuration used for standardization to the test set before we test with the classifier. I wonder if we need to do this for each fold in 10CV.\n",
      "\n",
      "3\n",
      "3149\n",
      "58a9a221d73408ce4f4b6740\n",
      "2017-02-21 15:08\n",
      "@faprz  5CV is also fine but 10CV feels like standard. You can claim your accuracy better. Lets dive in: If you have k=5. that is 5 folds what you do is to divide the data set into 5 sets of equal length say s1,s2.....s5. Now you take s1 as test s2...s5 as training set and check accuracy. Each of the set gets a chance to become a test set.Now you would get 5 accuracies- average it and get the mean accuracy. Usage: sometimes using a particular split you can get better results but in real world you model doesn't performs good. So try some splits and get the average it helps you see the truth.\n",
      "Yes 10 times.\n",
      "\n",
      "2\n",
      "3150\n",
      "58a9a221d73408ce4f4b6740\n",
      "2017-02-21 17:41\n",
      "cannot get what you are trying to ask @faprz  can you describe what you are trying to say a little more elaborately.\n",
      "\n",
      "1\n",
      "3151\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2017-02-22 18:01\n",
      "@amueller I was reading the docstring of the f-regression and I am wondering if what is written is not very misleading\n",
      "@amueller I would have to read the code to check, but I was very confused with the docstring, so I googled and I arrived on stack exchange: http://stats.stackexchange.com/questions/204141/difference-between-selecting-features-based-on-f-regression-and-based-on-r2\n",
      "@amueller To me, the docstring suggests that the code actually does p linear regressions, and looks for the significance of the parameter (usually, this is done by converting the  t-score/t-test), but it mentions and f-score and f-test which is usually used to compare nested models\n",
      "disclaimer: I have no clue what I talking about :D\n",
      "\n",
      "4\n",
      "3152\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2017-02-22 18:25\n",
      "So I think the stack exchange link is misleading as well.\n",
      "In the specific case of scikit-learn's implementation, the f-score and t-score are identical (as we are comparing one model to the null), so it indeed boils down to selecting the features that are significantly correlated with the target. If the K-top features are selected, it is exactly what sure independance screening is about\n",
      "\n",
      "2\n",
      "3153\n",
      "54bd5965db8155e6700ed583\n",
      "2017-02-22 23:01\n",
      "Hi everyone. I have a problem that I would love to solve with scikit-learn, but cant seem to figure out how to crack it. Im hoping someone who knows a bit more about the package might have a quick answer.  I have a matrix $$X$$ that I want to factorize with sparse NMF as $$X = WH$$. I would like to use cross-validation to tune the hyperparameters (to wit, level of sparsity and # of components). Since the only place that $$X$$ shows up in the objective function for NMF is $$||X-WH||_F$$, I would imagine doing the cross validation in random folds, each time leaving out a random subset of the elements of  $$X$$ from the Forbenius norm, fitting the model, and then computing the reconstruction error on those elements. However I cant seem to find any way or think of a trick to do the fit on just a subset of the matrix (scikit-learns NMF algorithm doesnt seem to like it when I try to <unconvertable> leave out <unconvertable> some of the elements by setting them to `nan` :-P ). Is there any other route to accomplishing this within the package?\n",
      "\n",
      "1\n",
      "3154\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-02-24 21:16\n",
      "@NelleV I'm probably not the right person to ask this either.  It's just an F-test, right? https://en.wikipedia.org/wiki/F-test\n",
      "@jwittenbach have you checked out GridSearchCV? that does that automatically\n",
      "you can use ShuffleSplit if you want random splits of the data\n",
      "What's the objective that you want to use for selection?\n",
      "If you want to do it manually, you shouldn't leave them out by setting them to NaN but by just subsetting the data and throwing out those rows.\n",
      "\n",
      "5\n",
      "3155\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2017-02-24 21:23\n",
      "@amueller I am definitely not the right person to ask this, but f-tests are used to compare different models, while t-tests used for significance testing\n",
      "@amueller I *think* that in that specific case, the code compares 1 model (univariate regression model) versus the null (mean == 0), and thus it is identical\n",
      "@amueller now, as f-tests are often use to compare models, they can be use for feature selection by comparing linear models with covariate X1 and linear models with covariate X1 and X2.\n",
      "@amueller in practice, our f-regression does not do this, and thus I think the Stack exchange answer is wrong (though I would have to look at the code): our f-regression just fit univariate linear models, and rank them with the significance of the regression parameter (with is computed with a t-test)\n",
      "am I making any sense?\n",
      "now, I have recently realized that sure independance screening and our f-regression is the same. I think that might be worth mentioning somewhere, considering how widely used SIS is.\n",
      "\n",
      "13\n",
      "3156\n",
      "54bd5965db8155e6700ed583\n",
      "2017-02-24 21:30\n",
      "@amueller I guess my issue  is that I dont want to throw out entire rows, because that will change the shape of the factors ($$X = WH$$). I just want to hold out random elements during the fit (analogous to k-fold CV for regression) and then use the reconstruction error, i.e. $$\\sum_{i, j \\in S} (X_{ij} - (WH)_{ij})^2$$ (where $$S$$ is the subset of held-out elements), for the cross-validation\n",
      "\n",
      "1\n",
      "3157\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-02-25 16:06\n",
      "That's matrix completion with is not really easily supported by sklearn, because the algorithm doesn't deal with missing values. You could try fancyimpute or any from this list: https://www.quora.com/What-is-the-best-open-source-package-to-build-a-recommender-system-in-Python/answer/Xavier-Amatriain?srid=cgo\n",
      "\n",
      "1\n",
      "3158\n",
      "54bd5965db8155e6700ed583\n",
      "2017-02-25 18:47\n",
      "@amueller ok, thanks; that answers my question. I was just curious as to whether it  might be straightforward to do this in `scikit-learn` but I was just missing something obvious :)\n",
      "\n",
      "1\n",
      "3159\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-02-27 11:10\n",
      "I noticed scikit-learn is taking part in GSoC. I was wondering if https://github.com/scikit-learn/scikit-learn/issues/5736 would make a nice projecet?\n",
      "it has the advantage not being too technical I think\n",
      "@glemaitre  OK thanks\n",
      "\n",
      "3\n",
      "3160\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-02-27 12:49\n",
      "@lesshaste The projects should be in line with the proposals specified there: https://github.com/scikit-learn/scikit-learn/wiki/Google-summer-of-code-(GSOC)-2017\n",
      "\n",
      "1\n",
      "3161\n",
      "58a9a221d73408ce4f4b6740\n",
      "2017-02-28 01:36\n",
      "@glemaitre  I have implemented a DecisionTree version in order to gain some insight regarding your fix #8458 .My approach during split is to remove the feature we split on and each child node doesn't contain that feature. But again we are going to have to look at say n-1 features .How did you overcome that. Your help can go a long way into helping me. Also I can make the code available to you. its actually from a book. Thanks.\n",
      "thanks @glemaitre .\n",
      "\n",
      "2\n",
      "3162\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-02-28 16:06\n",
      "@shubham0704 The idea is to keep a list of splitter and when scanning a feature, each sample is distributed to the given splitter to evaluate if this is a best split. Once the feature is scanned, all the potential splits have been evaluated with a single scan.\n",
      "\n",
      "1\n",
      "3163\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-03-01 12:55\n",
      "how much memory I need with PolynomialFeatures?\n",
      "I have data with shape (10374, 500) and I use interaction_only=True, but it gives me memory error\n",
      "I  think I have 8G limit for memory on that machine where I'm running it\n",
      "if I use 32 bit floats, would that be half of it? like ~5GB?\n",
      "\n",
      "4\n",
      "3164\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2017-03-01 15:24\n",
      "@mkoske Well, `PolynomialFeatures(degree=2, interaction_only=True).fit_transform(np.ones((1, 500)))`  generates an array with 125251 features, so if you do that for all of your 10374 rows, it would produce a ~10.4 GB array (64 bit floats) and 8 GB RAM wont be enough..\n",
      "\n",
      "3\n",
      "3165\n",
      "5634e15116b6c7089cb8f9f2\n",
      "2017-03-02 14:37\n",
      "Hi all, I am having issues installing scipy for scikit-learn. Is winPython a good workaround?\n",
      "*thumbsUp\n",
      "Thanks a million\n",
      "\n",
      "3\n",
      "3166\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-03-02 15:55\n",
      "@Ij888 I personally opted for conda.\n",
      "\n",
      "1\n",
      "3167\n",
      "5634e15116b6c7089cb8f9f2\n",
      "2017-03-02 15:57\n",
      "Okay thanks @glemaitre! Can I safely use pip and conda side by side?\n",
      "\n",
      "3\n",
      "3168\n",
      "5633b77d16b6c7089cb8e50e\n",
      "2017-03-02 20:07\n",
      "I'm persisting a random forest classifier using joblib, and in another process loading it. If I use sample_weights in fit, when I load the model it produces all 0 predictions. If I turn off sample_weights and train, everything is fine. During kfold validation, both settings produce expected results. Any ideas?\n",
      "\n",
      "1\n",
      "3169\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-03-02 20:11\n",
      "@ccarter-cs If this is something that you can reproduce with a simple snippet\n",
      "you can open an issue\n",
      "\n",
      "6\n",
      "3170\n",
      "5633b77d16b6c7089cb8e50e\n",
      "2017-03-02 20:14\n",
      "ty for your help\n",
      "\n",
      "1\n",
      "3171\n",
      "5633b77d16b6c7089cb8e50e\n",
      "2017-03-03 00:31\n",
      "@glemaitre It was a problem with my predict code. I was not imputing values for a feature that doesn't matter in the non sample weighted case, but apparently does with the weighted samples. I've learned my lesson on using the same processing on both sides. =)\n",
      "\n",
      "1\n",
      "3172\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-03-03 08:47\n",
      "@ccarter-cs :+1:\n",
      "\n",
      "1\n",
      "3173\n",
      "57e4ea1140f3a6eec066d9b4\n",
      "2017-03-03 15:03\n",
      "Hey is sklearn not in GSOC 2017?\n",
      "\n",
      "1\n",
      "3174\n",
      "5634e15116b6c7089cb8f9f2\n",
      "2017-03-03 15:17\n",
      "Please all, what DIY level apps can I build with scikit-learn? So far I feel as if I have answers but no problems!\n",
      "\n",
      "1\n",
      "3175\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-03 21:52\n",
      "@SatyaPrakashDwibedi it is if we find good students ;)\n",
      "@NelleV have you opened an issue on the f_regression issue we discussed? Could be good for the sprint tomorrow.\n",
      "\n",
      "2\n",
      "3176\n",
      "57e4ea1140f3a6eec066d9b4\n",
      "2017-03-04 00:14\n",
      "@amueller  after attending your talk I was really looking forward to it.\n",
      "\n",
      "1\n",
      "3177\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-04 00:14\n",
      "Which one? Yesterday?\n",
      "Or PyCon India?\n",
      "If you don't mind and don't want to do it yourself next week, it would be great if you could open one.\n",
      "\n",
      "4\n",
      "3178\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-04 00:15\n",
      "We expect students to having contributed before applying for doing a GSoC with us. I haven't been around much the last two month, have you contributed so far?\n",
      "\n",
      "1\n",
      "3179\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2017-03-04 00:16\n",
      "@amueller I have not yet\n",
      "\n",
      "1\n",
      "3180\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2017-03-04 00:19\n",
      "if it is not done by next week, I'll try to tackle this during the docathon\n",
      "\n",
      "2\n",
      "3181\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2017-03-04 00:58\n",
      "I'll try to review some of the pull requests of the sprint btw\n",
      "so don't hesitate to ping me on PR that you think I could help review\n",
      "\n",
      "7\n",
      "3182\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-04 01:01\n",
      "FYI thanks for being in the top 100 scikit-learn contributors and not making us look like the worst possible ;) For an article about the sprint tomorrow I was asked about the state of women contributing to sklearn. Since counting contributors in a meaningful way is hard, I settled for the top 100 reported by github..... which has a 99:1 ration men:women unless I miscounted :-/ well let's hope we can do something about that tomorrow\n",
      "\n",
      "4\n",
      "3183\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2017-03-04 01:07\n",
      "I'm hoping to contribute more, but matplotlib is still taking a lot of my time\n",
      "\n",
      "3\n",
      "3184\n",
      "55e7596f0fc9f982beaf75b6\n",
      "2017-03-04 16:00\n",
      "Hi, If interested, there's a semantic segmentation problem waiting to be solved here : https://github.com/chromosome-seg/DeepFISH\n",
      "\n",
      "1\n",
      "3185\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-04 16:29\n",
      "Started the sprint :) as you might have notices ;)\n",
      "\n",
      "1\n",
      "3186\n",
      "58bd932cd73408ce4f4ec329\n",
      "2017-03-06 17:47\n",
      "Hi, I am new to the community.. and still a beginner. I would appreciate if you could share with me any documentation on the development process and ways to involve.. that information would be very helpful\n",
      "thanks Andreas :)\n",
      "\n",
      "2\n",
      "3187\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-06 17:47\n",
      "http://scikit-learn.org/dev/developers/contributing.html\n",
      "\n",
      "1\n",
      "3188\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-07 16:39\n",
      "is codecov still commenting?\n",
      "\n",
      "1\n",
      "3189\n",
      "58beb8b7d73408ce4f4ef904\n",
      "2017-03-08 15:03\n",
      "hello\n",
      "\n",
      "1\n",
      "3190\n",
      "58beb8b7d73408ce4f4ef904\n",
      "2017-03-08 15:05\n",
      "while I am trying to fix an issue using the the inspect.signature method with python3, I am getting this error: /sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8\n",
      "Have I built something wrong , or do you know how I can build again so as to be python3 compatible?\n",
      "https://github.com/scikit-learn/scikit-learn/issues/8194\n",
      "I am running the rcheck.py to scan the modules\n",
      "I managed to build successfully with make using python3. Everything looks good now, I 'll get on with the fix\n",
      "Thank you Andreas!\n",
      "\n",
      "6\n",
      "3191\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-08 15:13\n",
      "do you want to use the development version? Otherwise I'd suggest you just use anaconda or the wheels provided by pip\n",
      "What's the error you're trying to fix?\n",
      "how did you build scikit-learn?\n",
      "and how are you running rcheck.py?\n",
      "Make the installation procedure matches the python environment you are running\n",
      "\n",
      "5\n",
      "3192\n",
      "530c03e25e986b0712efafb8\n",
      "2017-03-10 16:52\n",
      "Question: how large are typical parameter sets to `GridSearchCV`?\n",
      "And how large are larger-than-typical parameter sets?\n",
      "@jcrist and I are trying to decide how much we should care about overheads in Dask+sklearn work\n",
      "\n",
      "3\n",
      "3193\n",
      "58a26882d73408ce4f4a2f0d\n",
      "2017-03-11 18:11\n",
      "@amueller Any idea on when this pull request can be accepted:  https://github.com/ja9harper/scikit-learn/pull/1/files\n",
      "\n",
      "1\n",
      "3194\n",
      "53135b495e986b0712efc453\n",
      "2017-03-12 12:34\n",
      "@reshama You have made a pull request to your fork... You should instead [raise one at the scikit-learn/scikit-learn](https://github.com/scikit-learn/scikit-learn/compare/master...ja9harper:crossdecompostionmethods)...\n",
      "\n",
      "1\n",
      "3195\n",
      "5891ba54d73408ce4f476005\n",
      "2017-03-16 05:26\n",
      "Hello! how many slots does scikit-learn usually gets every year?\n",
      "\n",
      "1\n",
      "3196\n",
      "589af8c1d73408ce4f48e81f\n",
      "2017-03-16 10:14\n",
      "Reinforcement learning\n",
      "\n",
      "1\n",
      "3197\n",
      "56ef431a85d51f252ab9e16d\n",
      "2017-03-17 15:00\n",
      "hi would this room fall under wanting to learn about neural networks?\n",
      "\n",
      "1\n",
      "3198\n",
      "58ccbc04d73408ce4f51be85\n",
      "2017-03-18 05:01\n",
      "Hello I am new to machine learning\n",
      "\n",
      "1\n",
      "3199\n",
      "58ccd9ead73408ce4f51c219\n",
      "2017-03-18 06:58\n",
      "hello world\n",
      "\n",
      "1\n",
      "3200\n",
      "5841ea1ed73408ce4f3a543e\n",
      "2017-03-20 16:31\n",
      "Hi guys i'm new to scikit learning\n",
      "i want to learn more about data science\n",
      "i hope you can help me with that.\n",
      "\n",
      "3\n",
      "3201\n",
      "5657989e16b6c7089cbc5309\n",
      "2017-03-21 04:57\n",
      "@El-moro hi there! A first step is to take a look at the [tutorial](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) for scikit-learn. [This GitHub repo](https://github.com/hangtwenty/dive-into-machine-learning) might also be a good start for places to get your hands on data science and machine learning.\n",
      "\n",
      "1\n",
      "3202\n",
      "58cea98cd73408ce4f5208c2\n",
      "2017-03-21 12:10\n",
      "hello guys\n",
      "\n",
      "1\n",
      "3203\n",
      "57f68cf5d73408ce4f2c567d\n",
      "2017-03-21 18:18\n",
      "Hey!!  Would implementing dropout in current Neural Network module count as proposal for GSOC?   \"https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\"\n",
      "\n",
      "1\n",
      "3204\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-03-22 12:58\n",
      "@geekSiddharth You should probably address your proposal to the mailing list instead of the gitter. You have more chance to get the attention of the core developer.\n",
      "Check the wiki page also regarding the topic which have an higher chance for the GSOC\n",
      "\n",
      "2\n",
      "3205\n",
      "571e7c86659847a7aff47954\n",
      "2017-03-24 18:57\n",
      "\n",
      "1\n",
      "3206\n",
      "5717ab2f659847a7aff3b583\n",
      "2017-03-24 22:36\n",
      "Hi\n",
      "I have answered it\n",
      "\n",
      "2\n",
      "3207\n",
      "58d6965fd73408ce4f539cff\n",
      "2017-03-25 16:17\n",
      "Hello guys ! I'm a beginner in Machine Learning, do you recommend me a tutorial for  working on a dataset with sikit-learn ?\n",
      "\n",
      "1\n",
      "3208\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-25 16:18\n",
      "@aniked we have two tutorials on our website, there is some on kaggle.com and there's videos on my website\n",
      "amueller.io\n",
      "\n",
      "2\n",
      "3209\n",
      "57f68cf5d73408ce4f2c567d\n",
      "2017-03-25 20:22\n",
      "@aniked Try intro to ML by udacity.\n",
      "\n",
      "1\n",
      "3210\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-03-26 07:09\n",
      "@aniked You can also got with the [book](http://shop.oreilly.com/product/0636920030515.do) of @amueller\n",
      "\n",
      "1\n",
      "3211\n",
      "58d7c559d73408ce4f53c7d3\n",
      "2017-03-26 13:46\n",
      "Do you guys have any online courses/ bootcamp recommendation for someone that have been learning the basics of machine learning?\n",
      "I just finished the Machine Learning Coursera course and I would like to learn more deeply about the more advanced topics in machine learning.\n",
      "\n",
      "2\n",
      "3212\n",
      "58d6965fd73408ce4f539cff\n",
      "2017-03-26 15:25\n",
      "@amueller  @geekSiddharth @glemaitre  thank you so much guys :)\n",
      "\n",
      "1\n",
      "3213\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-26 18:42\n",
      "@tonyvanhain check out the replies to @aniked above.\n",
      "I would rather not recommend bootcamps, as you'd have to pay for them usually, and some of them give me money ;)\n",
      "There's really a whole bunch of good free material online, though\n",
      "\n",
      "3\n",
      "3214\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-03-27 11:44\n",
      "The docs for StratifiedShuffleSplit and StratifiedKFold are very similar. One says \"This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.\" and the other says \"This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\"   I might not be the only person who finds this confusing\n",
      "what is the difference?\n",
      "is the only difference that the first one is randomized?\n",
      "\n",
      "3\n",
      "3215\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-03-27 18:15\n",
      "there are no folds in StratifiedShuffleSplit\n",
      "the number of iterations is independent of the training set size\n",
      "\n",
      "2\n",
      "3216\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-03-30 02:04\n",
      "hi all :)\n",
      "\n",
      "1\n",
      "3217\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-03-31 08:00\n",
      "does sklearn have tests that you can run?\n",
      "\n",
      "1\n",
      "3218\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-03-31 09:08\n",
      "@lesshaste Do you mean https://github.com/scikit-learn/scikit-learn#testing\n",
      "\n",
      "1\n",
      "3219\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-03-31 09:54\n",
      "yes thanks\n",
      "\n",
      "1\n",
      "3220\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-03-31 09:57\n",
      "are some errors expected? For example \"bagging.py:747: RuntimeWarning: divide by zero encountered in log \"\n",
      "\n",
      "1\n",
      "3221\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-03-31 11:02\n",
      "Sometimes, you can have error or warning raised by scipy and numpy which will be catch I think\n",
      "the important things is that you get no output as\n",
      "``` Ran 20 tests in 0.238s  FAILED (errors=2) ```\n",
      "or with failure\n",
      "\n",
      "9\n",
      "3222\n",
      "58cd4780d73408ce4f51d3cb\n",
      "2017-03-31 21:31\n",
      "Hello, I am willing to get the frequency of keywords + ngrams over a Russian text. I am having troubles using the CountVectorizer, here is what I did so far:  ```python corpus = open(\"russian-text-file\").read() corpus = nltk.regexp_tokenize(corpus, r'(?u)\\\\b\\\\w+\\\\b', gaps=True) cv = CountVectorizer(ngram_range=(1, 10), vocabulary=[\"list of russian keywords + ngrams\"], token_pattern='(?u)\\\\b\\\\w+\\\\b') results = pd.DataFrame(cv.fit_transform(corpus).toarray(), columns=cv.get_feature_names()) results_sum = results.sum() ```  `results_sum` shows that none of the keywords or ngrams are present in the text. However when I searched manually I could find them. Also, this code snippet worked with english text. Any help is appreciated, thanks!\n",
      "\n",
      "1\n",
      "3223\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-03-31 22:06\n",
      "would someone mind interpreting a learning curve for me? Im not quite sure whether this represents an issue with my data.\n",
      "[![Screen Shot 2017-03-31 at 15.06.27.png](https://files.gitter.im/scikit-learn/scikit-learn/QgsH/thumb/Screen-Shot-2017-03-31-at-15.06.27.png)](https://files.gitter.im/scikit-learn/scikit-learn/QgsH/Screen-Shot-2017-03-31-at-15.06.27.png)\n",
      "it looks like were getting a 7% CV error, which is a bit high, but tolerable for this use case?\n",
      "my p/r values sort of suck\n",
      "\n",
      "4\n",
      "3224\n",
      "58cd4780d73408ce4f51d3cb\n",
      "2017-04-01 10:37\n",
      "@sbromberger You can read more about bias and variance at http://cs229.stanford.edu/materials/ML-advice.pdf\n",
      "\n",
      "1\n",
      "3225\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-01 23:16\n",
      "I understand (or at least I think I do) about bias and variance. What I dont quite have down is whether or not my actual results above indicate excessive variance.\n",
      "\n",
      "1\n",
      "3226\n",
      "588f320bd73408ce4f46ee3d\n",
      "2017-04-02 06:43\n",
      "The overlap of the shaded curve hulls around 150 an 175 training samples would bring doubt to me, if wether this presentation is useful to judge the parameters impact - but I am alien to machine learning ;-)\n",
      "\n",
      "1\n",
      "3227\n",
      "55476cb515522ed4b3dfe7eb\n",
      "2017-04-02 09:19\n",
      "Hy guys, I'm trying to learn scikit clustering, but can not get into final step before giving data into clustering algorithms, hope someone will be able to point me out direction of next step to get two dimmensional array from dataframe so it can be used by algorithms like MeanShift or may be DBSCAN or something else  ``` import pandas as pd df = pd.read_csv('http://sandbox.mac-blog.org.ua/sample.csv') # C1..C5 - categorical, D1..D10 - dates, B1..B28 - binary, 100K rows df = df.drop(['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10'], 1) # do not understand how to deal with this df = df.fillna(False) # looking around, all categorical data has values, treating NaN for all binaries as false  # going to convert all binaries into 0..1 ints for c in df.columns:     if c.startswith('B'):         df[c] = df[c].astype('int')  # totally not sure should such things be done print('Before:', len(df)) # 100000 df = df.drop_duplicates() print('After:', len(df)) # 35944  # not sure is it good idea at all # but after that I have reduced number of columns from 33 to 12 from sklearn.feature_selection import VarianceThreshold sel = VarianceThreshold(threshold=(.8 * (1 - .8))) sel.fit(df) labels = [df.columns[x] for x in sel.get_support(indices=True)] print('Before:', len(df.columns)) # 33 df = pd.DataFrame(sel.fit_transform(df), columns=labels) print('After:', len(df.columns)) # 12  # not sure do I need something like StandardScaler or LabelEncoder?  # every example of clustering algorithms like https://www.youtube.com/watch?v=EQZaSuK-PHs expect 2 dimensional array - kind of stuck here  df.head() ```\n",
      "Seems that have found one way:  step 1 looking around on data   ``` from sklearn.decomposition import PCA      pca = PCA(n_components=2) pca.fit(df) existing_2d = pca.transform(df) plt.scatter(existing_2d.T[0], existing_2d.T[1], c='b') ```  step 2 clustering  in my case i definitely see 4 clusters so using kmeans  ``` from sklearn.cluster import KMeans km = KMeans(n_clusters=4) km.fit(df) df['Cluster'] = pd.Series(clusters, index=df.index) # append cluster column to dataframe ```  step 3 get usefull data  ``` desired = [] for col in df.columns:     if col != 'Cluster':         vals = [] # will contain top 1 value from each cluster         for cluster in list(set(km.labels_)):             vals.append(df[df['Cluster']==cluster][col].value_counts().head(h).reset_index().rename(columns={'index': col, col: 'Count'}).iloc[0][col])         if len(np.unique(vals)) > 1:             desired.append(col) # we are looking only for columns that are changing between clusters  xx = [] for cluster in list(set(km.labels_)):     x = {'Cluster': cluster}     for col in desired:         h=1         z = df[df['Cluster']==cluster][col].value_counts().head(h).reset_index().rename(columns={'index': col, col: 'Count'})         x[col] = z.iloc[0][col]     xx.append(x)      pd.DataFrame(xx) ```  not sure if this is a right way but got answer dataframe with 6 columns (5 categorical and 1 binary) describing top1 from each cluster (4 rows)  hope that may be helpful\n",
      "\n",
      "2\n",
      "3228\n",
      "578a5e75c2f0db084a23470c\n",
      "2017-04-04 01:47\n",
      "Looking at the digits dataset, the `DESCR` says \":Number of Instances: 5620\" but when I examine the shape of the `data`, `target`, and `images`,  they all have 1797 instances.  Is this a documentation error or could I be missing something when I loaded the data?\n",
      "\n",
      "1\n",
      "3229\n",
      "5495ae8fdb8155e6700e17c8\n",
      "2017-04-04 10:29\n",
      "A curious observation: ``` from sklearn.feature_extraction.text import HashingVectorizer v = HashingVectorizer() v.transform([\"a\",\"b\",\"c\"]) ``` The result is  ``` <3x1048576 sparse matrix of type '<class 'numpy.float64'>' \twith 0 stored elements in Compressed Sparse Row format> ``` It seems that HashingVectorizer will transform any single word into zero vector. Do I miss anything?\n",
      "look like the default \"word\" analyzer will transform single character into empty list ``` analyzer = v.build_analyzer() analyzer(\"a\") ```  ``` [] ```\n",
      "\n",
      "2\n",
      "3230\n",
      "58e381c1d73408ce4f55fa94\n",
      "2017-04-04 11:29\n",
      "Hi any good book for Keras\n",
      "\n",
      "1\n",
      "3231\n",
      "5729ef37c43b8c60197119b1\n",
      "2017-04-04 11:35\n",
      "Folks, whats the easiest way to figure out the top features used by RandomForestClassifier for prediction?  Im using DictVectorizer to extract features.\n",
      "\n",
      "1\n",
      "3232\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-04 22:20\n",
      "so, `class_weight` doesnt do what I expected it to do.\n",
      "\n",
      "1\n",
      "3233\n",
      "561a58f7d33f749381a8ff2f\n",
      "2017-04-06 08:56\n",
      "OHE makes me feel like a handicapped person\n",
      "\n",
      "1\n",
      "3234\n",
      "57f1894dd73408ce4f2b2588\n",
      "2017-04-06 17:46\n",
      "is there a sklearn preprocessing function/class that removes data points (examples) with values that are out of range? here is a snippet, i.e. remove all data x<lo or x>up. here is an example https://gist.github.com/ulf1/6810883a985ef464ae9d26833966b4fa\n",
      "\n",
      "1\n",
      "3235\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-04-06 20:23\n",
      "@ulf1 you can use [`numpy.clip`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html)\n",
      "\n",
      "1\n",
      "3236\n",
      "57f1894dd73408ce4f2b2588\n",
      "2017-04-07 08:09\n",
      "@glemaitre thank you for the tip. however it isnt exactly what i was searching for. numpy.clip substitute values that are out of range. my plan is to remove the row (or column) that contain out-of-range values.\n",
      "\n",
      "1\n",
      "3237\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-04-07 10:03\n",
      "There is not transformer which remove samples in scikit-learn I think\n",
      "you could probably use the [`pandas.query`](http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.query.html) to make such request\n",
      "\n",
      "2\n",
      "3238\n",
      "58e381c1d73408ce4f55fa94\n",
      "2017-04-08 06:25\n",
      "welcome to sonnet from DeepMind\n",
      "        https://gitter.im/TF_Sonnet/Lobby?utm_source=share-link&utm_medium=link&utm_campaign=share-link\n",
      "\n",
      "2\n",
      "3239\n",
      "561a58f7d33f749381a8ff2f\n",
      "2017-04-09 22:56\n",
      "I think Ridge normalized=True is broken\n",
      "\n",
      "1\n",
      "3240\n",
      "561a58f7d33f749381a8ff2f\n",
      "2017-04-09 23:00\n",
      "In [44]: Ridge(normalize=True).fit([[1000],[1], [500]], [1000,1,500]).predict([[1000],[1], [500]]) Out[44]: array([ 750.16666667,  250.66666667,  500.16666667])\n",
      "In [45]: Ridge(normalize=False).fit([[1000],[1], [500]], [1000,1,500]).predict([[1000],[1], [500]]) Out[45]: array([ 999.99899867,    1.00100066,  500.00000067])\n",
      "'scikit-learn==0.19.dev0'\n",
      "\n",
      "3\n",
      "3241\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-04-10 04:55\n",
      "@kootenpv Use the issue tracker from GitHub if you have a bug report\n",
      "\n",
      "1\n",
      "3242\n",
      "5845699ed73408ce4f3ad4e7\n",
      "2017-04-11 13:00\n",
      "I am getting a major error in Dictionary Learning.  The error is Segmentation fault (core dumped).\n",
      "It is working fine for 100 Images to train a Dictionary but If I use 200 Images to train a Dictionary then I am  getting Segmentation fault (core dumped).\n",
      "I debuged my code and got this one : :0x00007ffff3059f50 in ATL_dJIK0x0x48NN0x0x0_aX_bX () from /usr/lib/libblas.so.3\n",
      "\n",
      "3\n",
      "3243\n",
      "58e88d54d73408ce4f56ee47\n",
      "2017-04-14 08:27\n",
      "hello, i am trying to use sklearn.svm.svc with l1 or l2 regularization and can't seem to find how to, can anyone please help with me some pointers? please see stackoverflow question  http://stackoverflow.com/questions/43407896/python-sklearn-non-linear-svm-penalty\n",
      "\n",
      "1\n",
      "3244\n",
      "589856bdd73408ce4f4866a2\n",
      "2017-04-15 01:53\n",
      "Guys need some advice on Networkx. What's the best way to learn it. Need to perform some clustering on stackoverflow dump.\n",
      "\n",
      "1\n",
      "3245\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-15 15:16\n",
      "@Pratyush3196_twitter the networkx docs are really good, but be aware that its not memory efficient AT ALL so youre not going to be able to use it at scale.\n",
      "\n",
      "1\n",
      "3246\n",
      "589856bdd73408ce4f4866a2\n",
      "2017-04-15 15:25\n",
      "Should I prefer Gephi over Networkx? @sbromberger\n",
      "\n",
      "1\n",
      "3247\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-15 15:29\n",
      "@Pratyush3196_twitter networkx is good. How big are your graphs?\n",
      "\n",
      "1\n",
      "3248\n",
      "589856bdd73408ce4f4866a2\n",
      "2017-04-16 04:47\n",
      "@sbromberger. I am using it on the 200gb stackexchange dump.\n",
      "Around 180345\n",
      "\n",
      "2\n",
      "3249\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-16 04:48\n",
      "how many nodes/edges?\n",
      "\n",
      "1\n",
      "3250\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-16 04:50\n",
      "nodes?\n",
      "how many edges? and have you tried loading this into networkx?\n",
      "I dont believe it can.\n",
      "\n",
      "3\n",
      "3251\n",
      "589856bdd73408ce4f4866a2\n",
      "2017-04-16 04:50\n",
      "But all of them won't be mapped. Lots of tags contain null so wont map them. Networkx has pretty straightforward documentaion. I read through it but gephi looks quite exquisite.\n",
      "@sbromberger It's my first DS project. Learning by doing so could use some good advice.\n",
      "\n",
      "2\n",
      "3252\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-16 04:55\n",
      "gephi is a different package with different goals. It really depends on what you want to do.\n",
      "if you want to visualize data and play around with it in a gui, gephi is better suited. If you want to run advanced graph analysis, networkx is probably better.\n",
      "but both have limitations.\n",
      "\n",
      "3\n",
      "3253\n",
      "589856bdd73408ce4f4866a2\n",
      "2017-04-16 04:59\n",
      "Can I apply kmeans on the data on the gephi graphs?\n",
      "\n",
      "3\n",
      "3254\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-16 05:07\n",
      "but there are many clustering algorithms available for each.\n",
      "http://stackoverflow.com/questions/40602158/how-to-draw-networkx-graph-based-on-k-means-cluster-label may help you.\n",
      "\n",
      "2\n",
      "3255\n",
      "589856bdd73408ce4f4866a2\n",
      "2017-04-16 05:10\n",
      "Thanks! That was a great help. Is this feature also available in Gephi in some way?\n",
      "\n",
      "2\n",
      "3256\n",
      "58e46e92d73408ce4f562b3b\n",
      "2017-04-17 04:49\n",
      "Is there a way to combine LevenbergMarquardt algorithm with Stochastic Gradient Descent?\n",
      "\n",
      "1\n",
      "3257\n",
      "585cd30bd73408ce4f3ee4d2\n",
      "2017-04-19 06:49\n",
      "~~~ Testing ~~~\n",
      "\n",
      "1\n",
      "3258\n",
      "5586719a15522ed4b3e23add\n",
      "2017-04-19 13:36\n",
      "Hi everyone,  (it seems that this channel changed its purpose...? anyway)  I don't want to bother the group but I recently posted [an issue when running kNN's](http://stackoverflow.com/questions/43284115/sklearn-knn-sklearn-neighbors-kneighbors-function-producing-unexpected-result)? It is in stackoverflow with the suggested changes by other users - still downgraded though.  Does anyone can have a look? If not, which other place would be the best one to post my question? Also: what other tests should I try before posting anywhere else to be sure I did all I could to verify the problem was completely evaluated?\n",
      "\n",
      "1\n",
      "3259\n",
      "5586719a15522ed4b3e23add\n",
      "2017-04-20 09:21\n",
      "^^^  --- Problem above was solve. Thanks for the help.\n",
      "\n",
      "1\n",
      "3260\n",
      "58fbc160d73408ce4f5a4648\n",
      "2017-04-22 20:50\n",
      "Can i talk C# in here?\n",
      "I am new to here\n",
      "\n",
      "2\n",
      "3261\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-04-22 22:00\n",
      "@HamunSunga It does not seem the right gitter room since this is a scikit-learn room\n",
      "\n",
      "1\n",
      "3262\n",
      "58af2e9dd73408ce4f4c72db\n",
      "2017-04-24 13:59\n",
      "Can i use datetime variable as independent variable in case of building a classification model like Random Forest.? I'm a new to data science. Great if someone can help. Thanks\n",
      "\n",
      "1\n",
      "3263\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-24 14:22\n",
      "since your datetime variable will be unique per observation, what do you expect to gain by making it a feature?\n",
      "\n",
      "1\n",
      "3264\n",
      "58af2e9dd73408ce4f4c72db\n",
      "2017-04-25 00:14\n",
      "I agree to with ur point. In my training dataset i see a relationship among the transactions happened with in a less span of time and in morning & evening are classified as suspicious .\n",
      "\n",
      "1\n",
      "3265\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-04-25 00:22\n",
      "so you want to create a set of features based on the timestamp: perhaps the hour of the timestamp only (to determine morning/evening), and then some duration engineered from the rest of the data.\n",
      "\n",
      "1\n",
      "3266\n",
      "58af2e9dd73408ce4f4c72db\n",
      "2017-04-25 00:28\n",
      "Ok. So new features like hour and minute bin like 5 or 10 minute size will have to create.\n",
      "\n",
      "1\n",
      "3267\n",
      "59006cdfd73408ce4f5b1a27\n",
      "2017-04-26 09:50\n",
      "Hi I need to implement a multiclass text classification using python\n",
      "How?\n",
      "\n",
      "2\n",
      "3268\n",
      "5832567dd73408ce4f376bd8\n",
      "2017-04-27 21:49\n",
      "what sites do you recomend for learning c or c++\n",
      "\n",
      "1\n",
      "3269\n",
      "58d7cfdad73408ce4f53c9b6\n",
      "2017-04-29 04:41\n",
      "Is CountVectorizer's vocabulary_ attribute sorted by their occurences in the documents?\n",
      "\n",
      "1\n",
      "3270\n",
      "58c984f9d73408ce4f50e4cb\n",
      "2017-04-30 02:10\n",
      "hi everyone, i was tasked with setting up a dev environment for our teams projects and I accidentally installed the latest version of sklearn instead of .17      is there a way to downgrade myself? or do I have to uninstall and reinstall?\n",
      "\n",
      "1\n",
      "3271\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-05-01 18:13\n",
      "@kaufmak2 with pip or conda or how?\n",
      "\n",
      "1\n",
      "3272\n",
      "57430fe4c43b8c60197476d9\n",
      "2017-05-03 12:04\n",
      "It says on the PyParis page that the scikit-learn dev sprint is going to happen during the conference - but this [link](https://github.com/scikit-learn/scikit-learn/wiki/Upcoming-events) mentions it happening a week before. Will there be 2 sprints in Paris?\n",
      "\n",
      "1\n",
      "3273\n",
      "547efb1fdb8155e6700dad1a\n",
      "2017-05-03 20:28\n",
      "DataScience Digest, Issue #7 - http://bit.ly/2p9NaRc  Please share;)\n",
      "\n",
      "1\n",
      "3274\n",
      "58ca7db4d73408ce4f51357f\n",
      "2017-05-04 10:26\n",
      "Hi\n",
      "\n",
      "1\n",
      "3275\n",
      "590a3126d73408ce4f5cbbf3\n",
      "2017-05-04 14:48\n",
      "@b0bcup_twitter  hello\n",
      "\n",
      "1\n",
      "3276\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-04 20:02\n",
      "@bhargavvader The sprint will be the week before as mentioned in the wiki.\n",
      "\n",
      "1\n",
      "3277\n",
      "58e6bdf2d73408ce4f56a831\n",
      "2017-05-05 18:06\n",
      "Hello @theslothhermit  you may consider the new boston tutorial in youtube.\n",
      "C++ Programming Tutorials Playlist: http://www.youtube.com/playlist?list=PLAE85DE8440AA6B83\n",
      "C Programming Tutorials: http://www.youtube.com/playlist?list=PL6gx4Cwl9DGAKIXv8Yr6nhGJ9Vlcjyymq\n",
      "\n",
      "3\n",
      "3278\n",
      "590e05cdd73408ce4f5d769b\n",
      "2017-05-06 20:59\n",
      "hi all, i'm trying to run n_jobs>1 with a OneVsRestClassifier(LinearSVC()) and am seeing the following... is this due to lambda's creeping in somewhere recently? ``` Traceback (most recent call last):   File \"main.py\", line 24, in <module>     train(df, Y)   File \"/home/ubuntu/model.py\", line 90, in train     mod.fit(docs_train, labels_train) #mod to use grid search or \"model\" for the pipeline only   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py\", line 216, in fit     for i, column in enumerate(columns))   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 768, in __call__     self.retrieve()   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 719, in retrieve     raise exception   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 682, in retrieve     self._output.extend(job.get(timeout=self.timeout))   File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 608, in get     raise self._value   File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 385, in _handle_tasks     put(task)   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\", line 371, in send     CustomizablePickler(buffer, self._reducers).dump(obj) AttributeError: Can't pickle local object 'train.<locals>.<lambda>' ```\n",
      "is there a way to set dill or some other serializer instead?\n",
      "\n",
      "2\n",
      "3279\n",
      "590e05cdd73408ce4f5d769b\n",
      "2017-05-06 21:06\n",
      "nm, found a lambda hidden away in my code somewhere... :)\n",
      "\n",
      "1\n",
      "3280\n",
      "58da7f70d73408ce4f545382\n",
      "2017-05-07 04:49\n",
      "I need some help\n",
      "Means I write codes in pycharm and it will execute in Windows cmd prompt\n",
      "Plzzzzzzzzz\n",
      "Help me\n",
      "\n",
      "4\n",
      "3281\n",
      "58847c11d73408ce4f452e18\n",
      "2017-05-08 03:37\n",
      "hello\n",
      "\n",
      "1\n",
      "3282\n",
      "58fd3abed73408ce4f5a7b40\n",
      "2017-05-10 15:49\n",
      "anybody here with experience handling EEG data ?  I'll be very grateful if you could answer a few questions\n",
      "\n",
      "1\n",
      "3283\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-10 17:54\n",
      "@fel-mazo You have maybe more chance to find people on the mailing list of MNE: http://martinos.org/mne/stable/index.html\n",
      "\n",
      "1\n",
      "3284\n",
      "591414ffd73408ce4f5eaf83\n",
      "2017-05-11 07:41\n",
      "from sklearn.decomposition import PCA  pca = PCA(n_components=2) pca.fit(df) existing_2d = pca.transform(df) plt.scatter(existing_2d.T[0], existing_2d.T[1], c='b')\n",
      "After doing the pca now I want to know which are the columns that have been selected by pca and only use those in my dataset df Any idea how to get the column names selected by pca?\n",
      "*pca = PCA(n_components=29)\n",
      "say I have a large value of columns 400 and I want only 29 of them but I want the name of those 29 columns selected by pca\n",
      "from df\n",
      "can anybody help please?\n",
      "Thanks\n",
      "\n",
      "7\n",
      "3285\n",
      "553d32d715522ed4b3df8b92\n",
      "2017-05-11 09:37\n",
      "Hi, in case I understood the use case right, you might want to look at feature_selection http://scikit-learn.org/stable/modules/feature_selection.html for selecting features since PCA doesnt select particular features but converts to n_components dimensions where each component is a weighted sum of the features. Hope it helps.\n",
      "\n",
      "1\n",
      "3286\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-11 09:41\n",
      "@maniteja123 :+1:  @puneetmathurDS each component is a \"bit of all columns\"\n",
      "\n",
      "1\n",
      "3287\n",
      "58fd3abed73408ce4f5a7b40\n",
      "2017-05-11 10:46\n",
      "@glemaitre thanks !\n",
      "\n",
      "1\n",
      "3288\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-05-12 06:23\n",
      "I have a doubt  in my code ``` from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_breast_cancer from sklearn.cross_validation import train_test_split cancer = load_breast_cancer() X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify=cancer.target, random_state=42) tree = DecisionTreeClassifier(random_state=0) tree.fit(X_train, y_train) print(tree.score(X_train, y_train)) print(tree.score(X_test, y_test)) ``` The first random_state is in test_train_split is used for  getting the same result next time when we are gonna run the code. But I don't get why is there a random_state in DecisionTreeClassifier line ? And how does that work ? @amueller  ..\n",
      "\n",
      "1\n",
      "3289\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-12 08:24\n",
      "@ashiskriz There is some randomness in the decision tree. For instance a subset of feature can be taken at each note with a randomization. Having this in mind the random_state allows to have this part deterministic as well\n",
      "\n",
      "1\n",
      "3290\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-05-12 11:57\n",
      "@glemaitre  - Thank you very much . Thats helpful  And I  want to add that the book I was following  written by @amueller  had a sentence saying \"We fix the random_state in the tree, which is used for tiebreaking internally\"  . I was wondering about the mechanism of tie breaking thing . How is the tie breaking thing happens internally?\n",
      "\n",
      "1\n",
      "3291\n",
      "56ed9a8885d51f252ab9b33f\n",
      "2017-05-12 13:39\n",
      "\n",
      "1\n",
      "3292\n",
      "58e46e92d73408ce4f562b3b\n",
      "2017-05-12 14:21\n",
      "@punitaojha you have posted the exact same message in https://gitter.im/Machine-Learning-Group/chat too. That's I believe a spammy behavior...\n",
      "\n",
      "1\n",
      "3293\n",
      "590c8ffdd73408ce4f5d352d\n",
      "2017-05-12 14:41\n",
      "Hey Guys, I am planning to build a ML and probabilistic modelling libray in Python. Would like to know if anybody is interested to start the project with me?\n",
      "\n",
      "1\n",
      "3294\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-12 22:00\n",
      "@ashiskriz at each node, a random set of feature will be used to find a best split. If there two features leading to a split with the same impurity improvement, you get a tie. Therefore, the first feature which was randomly picked up will be selected.\n",
      "If you try multiple times, you will select one feature or the other which will lead to different trees architectures. Therefore, random_state allows you to pick up always the same feature in case of a tie.\n",
      "\n",
      "2\n",
      "3295\n",
      "55901c1b15522ed4b3e2f949\n",
      "2017-05-12 23:35\n",
      "@anisnouri how will it be different from current existing libraries?\n",
      "\n",
      "1\n",
      "3296\n",
      "590c8ffdd73408ce4f5d352d\n",
      "2017-05-12 23:41\n",
      "@jmschrei  would be much focused on quantifying uncertainty.\n",
      "\n",
      "1\n",
      "3297\n",
      "55901c1b15522ed4b3e2f949\n",
      "2017-05-12 23:42\n",
      "Have you looked into PyMC3 and PyStan, and the libraries built on top of those?\n",
      "\n",
      "2\n",
      "3298\n",
      "55901c1b15522ed4b3e2f949\n",
      "2017-05-12 23:55\n",
      "You should probably reach out in those communities then, if you'll be building on top of it. I think that learning Bayesian models like that is much more niche than classical machine learning.\n",
      "\n",
      "1\n",
      "3299\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-05-13 19:04\n",
      "@glemaitre   Thank you very much  .Really appreciate your help. Great explanation there\n",
      "\n",
      "1\n",
      "3300\n",
      "590d97a3d73408ce4f5d6441\n",
      "2017-05-17 06:05\n",
      "@punitaojha <unconvertable> You know why we are here.\n",
      "\n",
      "1\n",
      "3301\n",
      "5864997ad73408ce4f3fe96c\n",
      "2017-05-21 03:32\n",
      "pbppppppppppapoppppp\n",
      "\n",
      "1\n",
      "3302\n",
      "5921d334d73408ce4f612e46\n",
      "2017-05-21 19:21\n",
      "Hello, I'm looking for some help with a code I wrote. It seems straightforward enough but it takes forever (its not even run it yet and I've had it running for hours) to perform a 5-fold cross validation bit on an 800 x 18 dataframe.\n",
      "This seems to be where my code hangs:\n",
      "\n",
      "2\n",
      "3303\n",
      "5921d334d73408ce4f612e46\n",
      "2017-05-21 19:41\n",
      "``` kf = KFold(800,5) score = [] for i in range(1,1001):      clf = SVC(C=i/100, kernel='linear')      error = []      for train_index, test_index in kf:           X_train, X_test = train_data[train_index], train_data[test_index]           y_train, y_test = train_label[train_index], train_label[test_index]           clf.fit(X_train,y_train)           error.append(1 - clf.score(X_test, y_test))      score.append(sum(error)/5) ```\n",
      "\n",
      "1\n",
      "3304\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-21 19:49\n",
      "you can use the [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to do what you try to do\n",
      "Also I think that you are using the `cross_validation` module which you should move away from since it is deprecated\n",
      "Use the `model_selection` where you have the refactored cross-validation classes\n",
      "long story short, you can write your code as\n",
      "Then it seems that you try to actually find the best `C` parameter probably. To do that you should use the [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class.\n",
      "```python from sklearn import svm, datasets from sklearn.model_selection import GridSearchCV  parameters = {'C': [1, 10, 100, 1000]} svr = svm.SVC(kernel='linear') clf = GridSearchCV(svr, parameters) clf.fit(X, y) ```\n",
      "It will search and select the best `C` parameter\n",
      "\n",
      "7\n",
      "3305\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-21 19:54\n",
      "```python from sklearn.model_selection import KFold from sklearn.model_selection cross_val_score from sklearn.svm import SVC  clf = SVC(kernel='linear') print(cross_val_score(clf, X, y))  ```\n",
      "\n",
      "1\n",
      "3306\n",
      "5921d334d73408ce4f612e46\n",
      "2017-05-21 20:00\n",
      "OK. Thank you so much @glemaitre. I'd try that Yes, I am trying to find the best C parameter\n",
      "\n",
      "1\n",
      "3307\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-21 20:00\n",
      "In your example you are trying all the 1000 possible `C` values and it could be pretty slow. You can also use `n_jobs=-1` to take advantages of all the CPU cores\n",
      "this is actually related to an SVM with different kernel and C values\n",
      "\n",
      "2\n",
      "3308\n",
      "5921d334d73408ce4f612e46\n",
      "2017-05-21 20:02\n",
      "Oh. Sorry, I'm still new at it so, I don't know how to use the n_jobs yet\n",
      "\n",
      "6\n",
      "3309\n",
      "5921d334d73408ce4f612e46\n",
      "2017-05-21 20:03\n",
      "The task was to check for the best C value between 0.01 and 10\n",
      "\n",
      "1\n",
      "3310\n",
      "5921d334d73408ce4f612e46\n",
      "2017-05-21 20:13\n",
      "`GridSearchCV(cv=None, error_score='raise',        estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',   max_iter=-1, probability=False, random_state=None, shrinking=True,   tol=0.001, verbose=False),        fit_params={}, iid=True, n_jobs=1,        param_grid={'C': [1, 10, 100, 1000]}, pre_dispatch='2*n_jobs',        refit=True, return_train_score=True, scoring=None, verbose=0)`\n",
      "I guess this implies the best estimator is C = 1.0 ?\n",
      "\n",
      "5\n",
      "3311\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-21 20:14\n",
      "the returned classifier is this one\n",
      "\n",
      "7\n",
      "3312\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-05-21 20:17\n",
      "`parameters = {'C': np.logspace(-1, 2, num=20)}`\n",
      "Don't hesitate sometimes to check the tutorial\n",
      "http://scikit-learn.org/stable/tutorial/\n",
      "\n",
      "6\n",
      "3313\n",
      "5921d334d73408ce4f612e46\n",
      "2017-05-21 20:19\n",
      "I'm new at sci-kit and machine learning. So sometimes, my solutions are not optimal and my system is rather slow\n",
      "Awesome. I'd go through it\n",
      "Yes, it is :D\n",
      "\n",
      "3\n",
      "3314\n",
      "5921d334d73408ce4f612e46\n",
      "2017-05-22 11:14\n",
      "Please does it make sense to find the optimal gamma for a polynomial kernel?\n",
      "Most of the examples I've seen searching for gamma have the kernel as linear or rbf\n",
      "I tried using grid search, but it doesn't seem to be the right function for getting the gamma in this case (it seems slow in processing)\n",
      "\n",
      "3\n",
      "3315\n",
      "56ed9a8885d51f252ab9b33f\n",
      "2017-05-22 15:02\n",
      "\n",
      "1\n",
      "3316\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-05-25 12:16\n",
      " Hi all,  Can I get some indepth resource on RFE(Recursive features elimination) ? I have went through the documentation but it will be good if I could get any detailed  example which uses RFE.\n",
      "\n",
      "1\n",
      "3317\n",
      "574454a0c43b8c601974a563\n",
      "2017-05-29 19:15\n",
      "[sklearn-porter 0.5.0](https://github.com/nok/sklearn-porter) has been released :sparkles:. The [MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) is the first supported regressor! That's one small step for a man, one giant leap for scientists. :smile:\n",
      "\n",
      "1\n",
      "3318\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-05-30 21:31\n",
      "thats pretty cool, @nok\n",
      "\n",
      "1\n",
      "3319\n",
      "574454a0c43b8c601974a563\n",
      "2017-05-30 21:35\n",
      "Thanks :smile:\n",
      "\n",
      "1\n",
      "3320\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-05-30 22:30\n",
      "Itd be nice to get Julia support into this :)\n",
      "\n",
      "1\n",
      "3321\n",
      "56c4f19ae610378809c1f8ae\n",
      "2017-05-31 06:43\n",
      "does anyone happen to know if scikit-learns implementation of coordinate descent (for stuff like Lasso, ElasticNet, etc.) is stochastic?\n",
      "i.e., does sample order matter?\n",
      "(assuming random seeds are set and all that other stuff)\n",
      "\n",
      "3\n",
      "3322\n",
      "56aba3aee610378809bedf43\n",
      "2017-05-31 08:54\n",
      "I have been trying SVC and have a doubt. C is referred to the penalty term, therefore with increase in the value of C, the algorithm try's to reduce the number of wrong classified and with very high value of C it may overfit the data. Can we say that with overfitting, the number of support vectors would increase. And if we use a lower value of C the number of support vectors will be less ?\n",
      "But the results I got were different, with small value of C, the model was underfit and the number of support vectors were huge\n",
      "\n",
      "2\n",
      "3323\n",
      "56aba3aee610378809bedf43\n",
      "2017-05-31 09:03\n",
      "Can anyone explain, what is wrong with my thinking :P\n",
      "\n",
      "1\n",
      "3324\n",
      "592f0e7ad73408ce4f63b1dd\n",
      "2017-05-31 18:42\n",
      "no\n",
      "don't you know anything?\n",
      "\n",
      "2\n",
      "3325\n",
      "574454a0c43b8c601974a563\n",
      "2017-05-31 23:26\n",
      "@sbromberger Currently Im not familiar with Julia, but it seems to be easy. I will have a look at the necessary parts.\n",
      "\n",
      "1\n",
      "3326\n",
      "574454a0c43b8c601974a563\n",
      "2017-05-31 23:33\n",
      "@amitmanchanda1995 https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n",
      "\n",
      "1\n",
      "3327\n",
      "574454a0c43b8c601974a563\n",
      "2017-05-31 23:37\n",
      "@amitmanchanda1995 Furthermore  the parameter `n_support_`  of an estimator gives you the number of used support vectors for each class: https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/svm/classes.py#L484-L485\n",
      "\n",
      "1\n",
      "3328\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-06-01 00:23\n",
      "@nok - its a very cool language, and it has bindings to sklearn :)\n",
      "\n",
      "1\n",
      "3329\n",
      "56aba3aee610378809bedf43\n",
      "2017-06-01 07:23\n",
      "@nok thanks. Previously I thought only the vectors present on the boundary lines were only considered to be support vectors, but all the misclassified and the vectors on the boundary lines are support vector so if we increase C the number of support vectors will decrease.\n",
      "\n",
      "1\n",
      "3330\n",
      "590709b7d73408ce4f5c2454\n",
      "2017-06-03 02:01\n",
      "S\n",
      "\n",
      "1\n",
      "3331\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-03 10:53\n",
      "@nelson-liu sample order shouldn't matter but feature order. Some pick the coordinates at random, I think.\n",
      "@amitmanchanda1995 C is an upper bound on the dual coefficents, and if you restrict the dual coefficients more (make C smaller) you'll have less zero alphas, i.e. more support vectors\n",
      "\n",
      "2\n",
      "3332\n",
      "56aba3aee610378809bedf43\n",
      "2017-06-05 07:14\n",
      "Thanks @amueller. Can you also help me the following *In predict_proba of SVC* > Notes > The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.  What do we mean by cross validation in case of prediction, what I get by cross validation is that the training data is divided into K sets, and K-1 is used for training while the 1 set is used for testing.\n",
      "And it is repeated K times\n",
      "\n",
      "2\n",
      "3333\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-05 07:15\n",
      "check out the paper by platt\n",
      "it's linked to in the description\n",
      "predict_proba uses platt scaling internally\n",
      "\n",
      "3\n",
      "3334\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-05 07:22\n",
      "Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods, J. Platt, (1999)\n",
      "\n",
      "2\n",
      "3335\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-05 07:23\n",
      "maybe also Predicting Good Probabilities with Supervised Learning, A. Niculescu-Mizil & R. Caruana, ICML 2005\n",
      "\n",
      "1\n",
      "3336\n",
      "551c051b15522ed4b3de2fea\n",
      "2017-06-05 21:49\n",
      "`DBSCAN` takes a long time relative to K-means.\n",
      "\n",
      "1\n",
      "3337\n",
      "593679b8d73408ce4f65082e\n",
      "2017-06-06 11:34\n",
      "Hello!   Am I right that https://github.com/scikit-learn/scikit-learn/pull/6015 is also within the sprint scope? Can I take it?\n",
      "\n",
      "1\n",
      "3338\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-06 11:35\n",
      "@n0mad review it you mean?\n",
      "\n",
      "1\n",
      "3339\n",
      "54e47f0815522ed4b3dc2640\n",
      "2017-06-06 11:39\n",
      "Anyone tackling this already https://github.com/scikit-learn/scikit-learn/issues/8899 ? Seems easy and would be useful for hmmlearn as well.\n",
      "\n",
      "2\n",
      "3340\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-06-06 11:45\n",
      "@superbobry You can go ahead if there is not PR yet\n",
      "\n",
      "1\n",
      "3341\n",
      "54e07d0815522ed4b3dc0850\n",
      "2017-06-07 10:33\n",
      "@jnothman : do you want to do a video hangout at some point, mostly to say hi and join the excitement?\n",
      "@jnothman I believe that it's 8:30PM your time. We are currently having the lunch break. So maybe in a little while will be good. We can put you on a big screen\n",
      "\n",
      "2\n",
      "3342\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-06-07 10:35\n",
      "I'm okay with small screens. Sounds like fun. I'm currently having my dinner too.\n",
      "\n",
      "1\n",
      "3343\n",
      "54e07d0815522ed4b3dc0850\n",
      "2017-06-07 10:41\n",
      "OK, after dinner? Or will you be busy?\n",
      "If you want: ping me on google hangout\n",
      "\n",
      "2\n",
      "3344\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-06-07 10:49\n",
      "Should be alright when all the feasting is\n",
      "over\n",
      "\n",
      "2\n",
      "3345\n",
      "54e07d0815522ed4b3dc0850\n",
      "2017-06-07 10:50\n",
      "Everybody is gone to take a break outside now :)\n",
      "\n",
      "1\n",
      "3346\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-06-07 11:08\n",
      "No hurry on my part! There's washing up to do.\n",
      "\n",
      "3\n",
      "3347\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-06-07 11:23\n",
      "I'll be with you in a couple of mins\n",
      "\n",
      "1\n",
      "3348\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-07 11:23\n",
      "@tguillemot also https://github.com/scikit-learn/scikit-learn/pull/9030 https://github.com/scikit-learn/scikit-learn/pull/9029\n",
      ";)\n",
      "\n",
      "2\n",
      "3349\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-07 11:35\n",
      "@jnothman I think @mblondel is going to ICML\n",
      "\n",
      "2\n",
      "3350\n",
      "54e07d0815522ed4b3dc0850\n",
      "2017-06-07 12:29\n",
      "All sprinters: we are having a sample-props discussion from 15:00 to 16:00, and a group picture at 16:00 hopefully on the roof\n",
      "\n",
      "1\n",
      "3351\n",
      "56b0a775e610378809bf7a7c\n",
      "2017-06-07 12:38\n",
      "if you want you can find a spec for sample-props  I have done several month ago to launch the discussion : https://github.com/tguillemot/sample_props_spec/blob/master/sample_props_spec_v2.md\n",
      "\n",
      "1\n",
      "3352\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-07 12:39\n",
      "thanks @tguillemot, I'll reread\n",
      "\n",
      "1\n",
      "3353\n",
      "54e07d0815522ed4b3dc0850\n",
      "2017-06-07 12:41\n",
      "Update: group picture: 16:30 sharp: Ibrahim will be there to bring us up to the roof\n",
      "\n",
      "1\n",
      "3354\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-07 12:51\n",
      "simple fix for making BaseSearchCV (more) API compatible: https://github.com/scikit-learn/scikit-learn/pull/9038\n",
      "\n",
      "1\n",
      "3355\n",
      "55502d0c15522ed4b3e03330\n",
      "2017-06-07 19:06\n",
      "hi all, I wish to calculate the confidence of an `SVC` prediction, but I have a hard time understanding the result I have fitting data like this ```python X = np.array([ \t\t[0, 0, 0, 0], \t\t[1, 1, 1, 1], \t\t[2, 2, 2, 2], \t\t[3, 3, 3, 3] \t])  y = np.array([ \t0, 1, 2, 3 \t])  clf = svm.SVC(C=1, kernel='rbf', probability=True) clf.fit(X, y) ``` Then I predict the class for input `x = np.array([1, 2, 3, 4])`, and I the output makes sence ```python clf.predict([x]) # array([1]) ```  But the probability does not make sense at all ```python clf.predict_proba([x]) # array([[ 0.2550524 ,  0.16488868,  0.25497241,  0.3250865 ]]) ```  According to the probabilities, class `1` has the lowest possibility. Why is that?\n",
      "\n",
      "1\n",
      "3356\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-07 19:10\n",
      "what's x?\n",
      "@nlhkh check out the docs, it tells you that there can be inconsistencies between the probability estimate and the prediction, because of the platt scaling that is used to create the predictions\n",
      "you can also look at decision_function and that should be consistent\n",
      "\n",
      "4\n",
      "3357\n",
      "55502d0c15522ed4b3e03330\n",
      "2017-06-07 19:13\n",
      "```python x = np.array([1, 1, 1, 1]) ```\n",
      "I tried the `decision_function` too\n",
      "Could you tell me a bit more what it means?\n",
      "\n",
      "3\n",
      "3358\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-07 19:14\n",
      "I can explain to you what happens: the probability estimate uses cross-validation to create the probability estimates, but there's only one data point for each class, so the models will be pretty useless\n",
      "because as soon as you leave out some points you leave out the whole class\n",
      "if you're trying to understand the svm algorithm, don't look at the probabilities\n",
      "what is your goal?\n",
      "on this dataset?\n",
      "ok. well then it will work\n",
      "well the example is done in a way that breaks the method that is used to get the uncertainty\n",
      "\n",
      "7\n",
      "3359\n",
      "55502d0c15522ed4b3e03330\n",
      "2017-06-07 19:15\n",
      "my goal is to determine how confident a prediction is\n",
      "so as to declare that a prediction is not reliable\n",
      "no\n",
      "\n",
      "3\n",
      "3360\n",
      "55502d0c15522ed4b3e03330\n",
      "2017-06-07 19:15\n",
      "on a much bigger dataset\n",
      "this is just for example\n",
      "\n",
      "2\n",
      "3361\n",
      "55502d0c15522ed4b3e03330\n",
      "2017-06-07 19:16\n",
      "so when I do this `clf.decision_function([x])`, I get this  ``` array([[-0.0296443 , -0.22257708, -0.22257708, -0.19293278, -0.19293278,         0.        ]]) ```\n",
      "how should I interprete these numbers?\n",
      "\n",
      "2\n",
      "3362\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-07 19:17\n",
      "predict_proba they are estimated probabilities of the classes\n",
      "for decision_function they are unnormalized, so higher means better but there is no absolute scale\n",
      "\n",
      "2\n",
      "3363\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-07 19:19\n",
      "```python import numpy as np from sklearn import svm X = np.array([         [0, 0, 0, 0],         [1, 1, 1, 1],         [2, 2, 2, 2],         [3, 3, 3, 3]     ]) X = np.repeat(X, 10, axis=0)  y = np.array([     0, 1, 2, 3     ]) y = np.repeat(y, 10)  clf = svm.SVC(C=1, kernel='rbf', probability=True) clf.fit(X, y) print(clf.predict([[1, 1, 1, 1]])) print(clf.predict_proba([[1, 1, 1, 1]])) ```\n",
      "\n",
      "3\n",
      "3364\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-07 19:23\n",
      "in your example, yes\n",
      "\n",
      "22\n",
      "3365\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-07 19:25\n",
      "there's a 3.5 for class 1, that's why class 1 is predicted\n",
      "it's always the argmax of the decision function\n",
      "you're running different code then ;)\n",
      "\n",
      "3\n",
      "3366\n",
      "55502d0c15522ed4b3e03330\n",
      "2017-06-07 19:27\n",
      "it does say this \"The decision_function_shape default value will change from 'ovo' to 'ovr' in 0.19. This will change the shape of the decision function returned by SVC. <unconvertable> :)\n",
      "\n",
      "12\n",
      "3367\n",
      "584bec30d73408ce4f3c127e\n",
      "2017-06-08 05:16\n",
      "Hello, I need some help with C++ Coding\n",
      "\n",
      "1\n",
      "3368\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-08 09:15\n",
      "@MoreToDo have you tried stackoverflow?\n",
      "\n",
      "1\n",
      "3369\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-06-08 10:04\n",
      "I've requested some very minor changes at #8096 and gather that @dalmia won't complete it. A volunteer to take it over?\n",
      "\n",
      "1\n",
      "3370\n",
      "53135b495e986b0712efc453\n",
      "2017-06-08 11:54\n",
      "Need any reviews anyone?\n",
      "\n",
      "1\n",
      "3371\n",
      "547d8325db8155e6700da60b\n",
      "2017-06-08 12:12\n",
      "@jnothman  do you still need someone for #8096 ??\n",
      "\n",
      "2\n",
      "3372\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-06-08 12:20\n",
      "@raghavrv I will need one soon\n",
      "https://github.com/scikit-learn/scikit-learn/pull/9058 he it goes\n",
      "\n",
      "2\n",
      "3373\n",
      "53135b495e986b0712efc453\n",
      "2017-06-08 12:28\n",
      "Aye master on it :)\n",
      "\n",
      "1\n",
      "3374\n",
      "547d8325db8155e6700da60b\n",
      "2017-06-08 12:54\n",
      "@jnothman see #9059\n",
      "\n",
      "1\n",
      "3375\n",
      "5634e15116b6c7089cb8f9f2\n",
      "2017-06-08 13:57\n",
      "hey all, please point me to a \"Definitive g\"\n",
      "hey all, don't mean to bother but please I'd appreciate if I could be pointed to a \"Definitive guide to using Scikit-Learn\" -style tutorial walkthrough. Peace <unconvertable>\n",
      "\n",
      "2\n",
      "3376\n",
      "59366dabd73408ce4f6504fd\n",
      "2017-06-09 12:02\n",
      "Did you miss this http://scikit-learn.org/stable/ ?\n",
      "\n",
      "1\n",
      "3377\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-09 17:34\n",
      "@Ij888  There is http://scikit-learn.org/stable/tutorial/index.html or you can buy a book such as: http://shop.oreilly.com/product/0636920030515.do\n",
      "\n",
      "1\n",
      "3378\n",
      "5634e15116b6c7089cb8f9f2\n",
      "2017-06-10 07:16\n",
      "thanks @dohmatob and @ogrisel . I tried working through the docs on  http://scikit-learn.org/stable/tutorial/basic/tutorial.html but it got rather dense at the **Learning and predicting** section especially at `clf.fit(digits.data[:-1], digits.target[:-1])`. I am at a loss to what some of the parameters in `SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)` represent.\n",
      "\n",
      "1\n",
      "3379\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-10 09:41\n",
      "you don't need to know the meaning of them all at first but you should check the API documentation of the SVC class at some point\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
      "\n",
      "2\n",
      "3380\n",
      "5634e15116b6c7089cb8f9f2\n",
      "2017-06-10 11:11\n",
      "thanks for the tip @ogrisel  <unconvertable>\n",
      "\n",
      "1\n",
      "3381\n",
      "5784d86dc2f0db084a229ae0\n",
      "2017-06-10 12:08\n",
      "Hey guys! I have an idea for an app and I was hoping i could find rediculously passionate people who could come work with me. it has machine learning integrated to bring progress to mankind.\n",
      "\n",
      "1\n",
      "3382\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-10 17:11\n",
      "I think people in this room are ridiculously passionate (and incredibly busy) in building tools for other people to build apps to bring progress to mankind.\n",
      ";)\n",
      "\n",
      "2\n",
      "3383\n",
      "547d8325db8155e6700da60b\n",
      "2017-06-10 17:12\n",
      "ajajajajaaj\n",
      "\n",
      "1\n",
      "3384\n",
      "5784d86dc2f0db084a229ae0\n",
      "2017-06-10 19:00\n",
      "@ogrisel Haha yeah that's pretty evident :') so you wouldn't be able to help me?\n",
      "\n",
      "1\n",
      "3385\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-10 19:06\n",
      "I am sorry, no. You should probably setup a proof of concept of your own if you want to recruit others to join your project.\n",
      "\n",
      "1\n",
      "3386\n",
      "593c448fd73408ce4f66554c\n",
      "2017-06-10 19:32\n",
      "Hi, I am a veteran R/Rcpp developer wanting to transition to being a sklearn contributor. There are a lot of classes and I think a consistent coding style in the sklearn code base - I'm wondering if there are any docs or presentations on the overall architecture (class hierarchy, the way helper functions should be organized, etc) that could help me understand the flow and be able to contribute?\n",
      "\n",
      "1\n",
      "3387\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-06-10 20:19\n",
      "@jeffwong I would check the tutorial http://scikit-learn.org/stable/tutorial/index.html The introduction will give you the convention and bases regarding the estimator\n",
      "\n",
      "1\n",
      "3388\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-11 10:59\n",
      "@jeffwong and the contributors doc: http://scikit-learn.org/stable/developers/contributing.html\n",
      "\n",
      "1\n",
      "3389\n",
      "54908b42db8155e6700dfe2d\n",
      "2017-06-12 12:04\n",
      "Is there any way to incorporate cost matrix in SVM classification as is available in weka https://weka.wikispaces.com/CostSensitiveClassifier\n",
      "\n",
      "1\n",
      "3390\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-12 16:00\n",
      "@axay possibly the class_weight parameter, though that doesn't allow specifying a full matrix, only relative weights\n",
      "It also depends a bit on whether you want to do that for training or for predicting with a trained model or both. It looks like weka influences the training.\n",
      "false positives for that label?\n",
      "I think it would make most sense to calibrate the prediction thresholds for that, but scikit-learn doesn't have tools for that yet.\n",
      "you can try to change the class-weights, but the control they give is a bit indirect\n",
      "do you know how weka implements this?\n",
      "the scikit-learn class_weights for SVC are multipliers to C so that you get one C per class, as described in the libsvm paper\n",
      "\n",
      "7\n",
      "3391\n",
      "54908b42db8155e6700dfe2d\n",
      "2017-06-12 16:03\n",
      "weka supports both afaik. My aim is to prevent misclassification of other labels as a target label\n",
      "for example if i have 3 labels A, B and C, I don't want A and C to be predicted as B\n",
      "yeah\n",
      "no, haven't gone through their code yet.\n",
      "\n",
      "4\n",
      "3392\n",
      "593f94a1d73408ce4f66e9df\n",
      "2017-06-13 07:35\n",
      "Hi, this is probably not the right forum to ask it, but not sure where else to go: How do I get permission to use the scikit-learn machine learning map (ML-map.png) in a published document? Obviously with full citation!  Thanks. Available from this link: http://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
      "\n",
      "1\n",
      "3393\n",
      "56aba3aee610378809bedf43\n",
      "2017-06-13 09:45\n",
      "Hi, is there any way to incorporate self-learning in SVC. By self-learning I mean if a datapoint is being predicted class A with accurary 0.7, then if I add same datapoint again in the corpus, then the prediction accurary should increase from 0.7 like in case of Neural Nets. But looking at the mathematics, if the datapoint is not a support vector then it is not considered in cost function. Then what should I do to increase the accuracy?\n",
      "predict_proba of SVC gives me a confidence score of 0.7. Can I seed it back into your model and hope that the next time confidence score for the same datapoint increase\n",
      "\n",
      "5\n",
      "3394\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-13 09:46\n",
      "Self learning is a meta-algorithm for semi-supervised learning and should work with any supervised model.\n",
      "I'm not sure what you mean by a point being predicted with accuracy 0.7\n",
      "\n",
      "2\n",
      "3395\n",
      "56aba3aee610378809bedf43\n",
      "2017-06-13 09:53\n",
      "yeah sorry that was a mistake. what I want is once I deploy a model in production, it somehow keeps on improving. I was thinking that if the confidence score of an unseen data point is above a certain threshold (say 0.7) i add it to the training set and retrain . Is it a good idea to use this with SVC ?\n",
      "\n",
      "1\n",
      "3396\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-13 12:09\n",
      "yeah that can work. self learning can have problems with concept drift\n",
      "\n",
      "1\n",
      "3397\n",
      "584bec30d73408ce4f3c127e\n",
      "2017-06-14 01:31\n",
      "Hello, I need some assistance with C++ homework.  let me know if anyone can assist!\n",
      "\n",
      "1\n",
      "3398\n",
      "57379367c43b8c601972f35d\n",
      "2017-06-14 02:03\n",
      "hey\n",
      "\n",
      "1\n",
      "3399\n",
      "55e777330fc9f982beaf78c9\n",
      "2017-06-14 15:28\n",
      "anyone seen an error like this when building from source? I get this with conda Cython but not pip Cython (0.25.2 in both cases, Python 3.6.1) ``` [ 2/39] Cythonizing sklearn/_isotonic.pyx Traceback (most recent call last):   File \"setup.py\", line 267, in <module>     setup_package() ... \"/Users/brettnaul/miniconda3/envs/py361/lib/python3.6/copy.py\", line 169, in deepcopy     rv = reductor(4) TypeError: can't pickle Cython.Compiler.FlowControl.NameAssignment objects ```\n",
      "\n",
      "1\n",
      "3400\n",
      "58e53928d73408ce4f565aa3\n",
      "2017-06-15 11:03\n",
      "hi guys ,i m coding in c ,and i want to store values from a txt file to an array any idea ?\n",
      "\n",
      "1\n",
      "3401\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-06-15 11:11\n",
      "@Krikbov  Bad chance that scikit-learn is in Python\n",
      "\n",
      "1\n",
      "3402\n",
      "5941c1b4d73408ce4f6764d6\n",
      "2017-06-16 01:11\n",
      "Hello my name is priyam and i am interested in contributing to scikit-learn.Can anyone help me i am completely new in open source world.\n",
      "\n",
      "1\n",
      "3403\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-16 07:55\n",
      "@bnaul that's really weird. Do you have the full Traceback? It seems that the interesting part is under the setup_package function.\n",
      "\n",
      "1\n",
      "3404\n",
      "57433a8cc43b8c6019747d9e\n",
      "2017-06-16 15:19\n",
      "@satishjasthi Hi all, Anyone knows how to visualize high dimensional data in 2d image\n",
      "We have 25 instances each of dimension 1800. We want to visualize each instance separately and then build a classifier on it.\n",
      "Please help\n",
      "can We visualize each instance separately in t-SNE\n",
      "\n",
      "4\n",
      "3405\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-16 15:21\n",
      "You can do a scatter plot of a t-SNE embedding to get some intuition on the structure of your data.  But you should not train the classifier on the 2D project. It's very likely that you will get much higher predictive accuracy by training the classifier on the original high dimensional data.\n",
      "There is a TSNE class in scikit-learn but it has many bugs. We would like to solve them for the next release but it's not ready yet. I would advise to have a look at https://github.com/lvdmaaten/bhtsne .\n",
      "The scatter plot will put on dot in the 2D plane for each instance.\n",
      "What you visualize is the neighborhood structure of your data.\n",
      "http://distill.pub/2016/misread-tsne/\n",
      "You should also try with 2D PCA of your data. This is much faster to compute alhough generally the neighborhood structure can be much blurier.\n",
      "I don't understand what it means to \"visualize a 2D image or a single 1800 dimensional vector\".\n",
      "what are you features ? which data types ? what is the physical meaning of each of them ?\n",
      "you already said that, please answer my question if you want me to be able to help you.\n",
      "There is no generic way to visualize a single feature vector of a CNN.\n",
      "\n",
      "10\n",
      "3406\n",
      "57433a8cc43b8c6019747d9e\n",
      "2017-06-16 15:25\n",
      "No. We dont want a dot for each instance. We want an altogether different 2D image for each instance\n",
      "\n",
      "1\n",
      "3407\n",
      "57433a8cc43b8c6019747d9e\n",
      "2017-06-16 15:28\n",
      "We have 1800 features for each instance. We then need to visualize these 1800 features in a single image. Repeat this process for each instance. And then build a classifier on it\n",
      "we got these 1800 features after doing CNN on the image\n",
      "RGB image, we performed CNN, and then extracted features are of dimension 1800\n",
      "We have 25 such images\n",
      "so it becomes 25X1800\n",
      "\n",
      "5\n",
      "3408\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-16 15:32\n",
      "If you want to visualize the distribution of your 25 images embedded in the 1800 dimensional CNN space,  then you can use PCA or TSNE, for instance using http://projector.tensorflow.org/ (online web interface) or https://github.com/lvdmaaten/bhtsne to do that programmatically.\n",
      "Also if you want to train an image classifier, I would advise you to build a training set with at list 100 images per class. 25 is probably far too few even if you do binary classification.\n",
      "\n",
      "2\n",
      "3409\n",
      "5634e8e116b6c7089cb8fa99\n",
      "2017-06-17 03:12\n",
      "Hi all, has anyone in scikit-learn tried to import some of the surrogate model methods used by pyKriging or inspyred?\n",
      "\n",
      "1\n",
      "3410\n",
      "57433a8cc43b8c6019747d9e\n",
      "2017-06-17 06:40\n",
      "Hi all, So we have 25 images of dimension 400X488. We have labels assigned to each image as 1/0 . There are 14 1s and 11 0s. What we need to do is we need to find pixels that differentiates 1 from 0. Also along with these differentiating pixels we also need to allot ranking to them based on their frequency in images with label 1.  Any thoughts on how to do it?\n",
      "@ogrisel Thank you\n",
      "Hi all, So we have 25 images of dimension 400X488. We have labels assigned to each image as 1/0 . There are 14 1s and 11 0s. What we need to do is we need to find pixels that differentiates 1 from 0. Also along with these differentiating pixels we also need to allot ranking to them based on their frequency in images with label 1.  Any thoughts on how to do it?\n",
      "\n",
      "3\n",
      "3411\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-21 09:05\n",
      "@satishjasthi please do not spam this channel with repeated questions. The way you pose your image classification problem is very non standard. What does ranking pixels mean? Why do you want to do that in the first place? Is this a natural image classification problem? In that case you will need much more than 25 labeled images to do anything useful as I already told you earlier. If those are not natural images then you should probably give more details on the nature of the images. Which objects are in the images? What are the two labels 0 and 1 (what do they mean)? Have the images been recorded with a traditional camera with three color channels or is it a very specific recording device?\n",
      "\n",
      "1\n",
      "3412\n",
      "55502d0c15522ed4b3e03330\n",
      "2017-06-26 09:43\n",
      "Hi all, I want to revisit the question regarding how to interpret the output of `decision_function`into something meaningful. With the output from `decision_function`, how can I determine if a prediction is trust worthy or not, given some threshold `alpha`?\n",
      "\n",
      "1\n",
      "3413\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-26 09:45\n",
      "There is no generic method, it depends on your estimator and the kind of multiclass reduction used by the underlying model (e.g. One vs One for SVC, One vs All for most other models such as LinearSVC).\n",
      "You might also have heteroschedastic prediction errors: some regions of your feature space might lead to a higher error rate and this heteroschedasticity might not be part of the model assumptions (or maybe cannot be handled properly by the model capacity).\n",
      "\n",
      "2\n",
      "3414\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-26 09:51\n",
      "Which model family are you interested in? One interesting thing to consider is using a calibrated classifier (especially for binary classification): http://scikit-learn.org/stable/modules/calibration.html This way you can better interpret the output of `clf.predict_proba`. This won't solve any heteroschedasticity issues though.\n",
      "What you can also do is plot the precision / recall curve, select an admissible precision level (e.g. 0.8) according to business considerations and select the model and threshold that maximizes the recall at that precision level. We currently don't have a high level API to do this but the http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html function of scikit-learn returns the thresholds for each point along the PR curve.\n",
      "Once you have fitted your best model, you can compute its precision and recall on a held out test set to check that this is still fine, and further analyse that you don't have strong heteroschedasticity in your precision / recall metrics. For instance if you use your classifier to build a recommender systems for users, you can check that you get approximately similar performance for various ways to split your user base, e.g.: male vs female users, age groups, geography, very engaged users vs casual users...\n",
      "\n",
      "3\n",
      "3415\n",
      "55502d0c15522ed4b3e03330\n",
      "2017-06-26 12:23\n",
      "@ogrisel thanks Olivier :smile: I'll give it a try\n",
      "I am doing facial recognition, so it's multiclass\n",
      "\n",
      "2\n",
      "3416\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-26 12:25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you should definitely try to label your test dataset with additional info such as facial hair, glasses, long hair vs short air, gender, maybe ethnicity so as to compare the accuracy of your model for different groups of people.\n",
      "\n",
      "1\n",
      "3417\n",
      "59533ac5d73408ce4f6a9616\n",
      "2017-06-28 05:15\n",
      "i have  some customers data what information can i get from there using machine learning\n",
      "hi there\n",
      "@ogrisel  hi there\n",
      "\n",
      "3\n",
      "3418\n",
      "58bf9a31d73408ce4f4f2137\n",
      "2017-06-28 13:39\n",
      "Lol wut\n",
      "\n",
      "1\n",
      "3419\n",
      "55502d0c15522ed4b3e03330\n",
      "2017-06-29 13:00\n",
      "when I use `GridSearchCV`, how can I find the actual parameters being used for a classifier?\n",
      "the `get_params` method seems to print everything in the search grid\n",
      "thanks Olivier :)\n",
      "\n",
      "3\n",
      "3420\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-29 13:01\n",
      "`grid_search.best_params_` after fit\n",
      "\n",
      "2\n",
      "3421\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-29 14:09\n",
      "test #9032\n",
      "\n",
      "1\n",
      "3422\n",
      "5572219415522ed4b3e17f0c\n",
      "2017-06-29 22:35\n",
      "I'm using sklearn '0.15.0b1'.  I think I found a bug - when using lm.Ridge, if XtX is singular, the problem is still solved but sample_weight is ignored.  Is this worth filing an issue for?  I tried searching to see if this has been reported/resolved but I didn't find anything\n",
      "\n",
      "1\n",
      "3423\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-29 22:36\n",
      "@hhuuggoo that's a *really* old version and a beta release. Can you please update to master and see if the problem persists?\n",
      "\n",
      "2\n",
      "3424\n",
      "5572219415522ed4b3e17f0c\n",
      "2017-06-29 22:48\n",
      "ok cool looks like it's correct in sklearn 0.18.2\n",
      "\n",
      "2\n",
      "3425\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-06-29 23:43\n",
      "I'm close to defeating my arch enemy, dot: https://user-images.githubusercontent.com/449558/27715089-d81ef462-5d02-11e7-806b-87ac0753ecbe.png\n",
      "(pure matplotlib)\n",
      "\n",
      "2\n",
      "3426\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-30 06:01\n",
      ":)\n",
      "\n",
      "1\n",
      "3427\n",
      "541a528b163965c9bc2053de\n",
      "2017-06-30 06:04\n",
      "Maybe you should make all boxes have the same width and left align the content:  ```text petal width(cm) split: <= 1.65 entropy: 0.041 samples: 48 value: 0, 47, 1 class:  versicolor ```\n",
      "\n",
      "1\n",
      "3428\n",
      "5864997ad73408ce4f3fe96c\n",
      "2017-07-01 02:32\n",
      "cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccp\n",
      "\n",
      "1\n",
      "3429\n",
      "558ca3d015522ed4b3e2cf85\n",
      "2017-07-02 02:09\n",
      "Hello\n",
      "\n",
      "1\n",
      "3430\n",
      "536ba85d048862e761fa0abb\n",
      "2017-07-03 07:32\n",
      "l\n",
      "\n",
      "1\n",
      "3431\n",
      "5799ed1c40f3a6eec05ce0b3\n",
      "2017-07-04 08:46\n",
      "Hello good people! If I may enquire,  is there any advantage for using one-hot encoding over label encoding with GradientBoosting algorithm?\n",
      "\n",
      "1\n",
      "3432\n",
      "590c8ffdd73408ce4f5d352d\n",
      "2017-07-04 16:14\n",
      "Hi, Is there a particular clustering algorithm that works best with time series?\n",
      "\n",
      "1\n",
      "3433\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-05 04:53\n",
      "@arnawldo, LabelEncoder is not intended for features.\n",
      "\n",
      "1\n",
      "3434\n",
      "5799ed1c40f3a6eec05ce0b3\n",
      "2017-07-05 06:02\n",
      "@jnothman so given my feature is categorical,  should I just replace the levels with integers, or spread it across columns?\n",
      "\n",
      "1\n",
      "3435\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-05 06:44\n",
      "We have had discussions on this topic on the mailing list. Either should work alright with trees. Integers will require deeper trees to select for (or against) a single category. So the model parameters need to be tuned to whichever approach you take.\n",
      "\n",
      "1\n",
      "3436\n",
      "5799ed1c40f3a6eec05ce0b3\n",
      "2017-07-05 06:56\n",
      "@jnothman great. How can I see this discussion and learn more. I'm not on this list\n",
      "\n",
      "1\n",
      "3437\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-05 07:00\n",
      "https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/ suggests you are better off with integers, fwiw\n",
      "\n",
      "1\n",
      "3438\n",
      "5799ed1c40f3a6eec05ce0b3\n",
      "2017-07-05 07:06\n",
      "@jnothman Thanks a lot\n",
      "\n",
      "1\n",
      "3439\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-07-05 10:33\n",
      "Hello\n",
      "\n",
      "1\n",
      "3440\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-07-05 10:35\n",
      "Docs for nearest neighbors says \"The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset...\". Why it is so?\n",
      "\n",
      "1\n",
      "3441\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-05 10:47\n",
      "Because BallTree and KDTree require true metrics and dense data; and because brute force pairwise distances may be faster for small datasets.\n",
      "\n",
      "1\n",
      "3442\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-07-05 10:49\n",
      "I mean, why it is pairwise (O(dn^2))? Why you have to compute distances between all pairs of points? I'm trying to understand the time complexity of the kNN, that's why I ask.\n",
      "isn't it then like O(knd), not O(dn^2) ?\n",
      "\n",
      "2\n",
      "3443\n",
      "5683764a16b6c7089cc092dc\n",
      "2017-07-05 10:50\n",
      "hmm, it is hard to say. it is the nature of kNN. you can find some visualization of kNN on the Internet and you will understand\n",
      "well, let's say for  one new data point\n",
      "how do you know the k-nearest neighbors of this new one?\n",
      "exactly\n",
      "so for one data point you need n calculation\n",
      "\n",
      "18\n",
      "3444\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-07-05 10:51\n",
      "calculate distance from this new point to all training points and keep k closest?\n",
      "\n",
      "2\n",
      "3445\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-07-05 10:58\n",
      "Yes, I was just reading that... the example I gave was from that\n",
      "(crossvalidated I mean)\n",
      "\n",
      "2\n",
      "3446\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-07-05 11:00\n",
      "It seems that people just think it different ways. And probably different implementations.\n",
      "That's why they differ\n",
      "Well, the answer at the Cross Validated says it depends on \"algorithmic choices\"\n",
      "Doesn't it mean different implementations?\n",
      "\n",
      "4\n",
      "3447\n",
      "5683764a16b6c7089cc092dc\n",
      "2017-07-05 11:01\n",
      "I wouldnt say so. brute-force kNN is very straightforward\n",
      "\n",
      "1\n",
      "3448\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-05 11:07\n",
      "O(knd) is referring to the cost *per query*. If you have n queries, then the cost is O(kn^2d)\n",
      "and by per query I mean per instance in your query\n",
      "With a binary tree index, the k neighbor search should be O(k log n) *per query* (ignoring some factor related to d)\n",
      "\n",
      "3\n",
      "3449\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-07-05 11:15\n",
      "Ok, it's just confusing when the complexity of brute force is stated to be O(DN^2) i.e. N queries in O(DN) time but for KD-tree it's stated to be O(log N) i.e. only for one query. Or am I missing something here?\n",
      "Should it be made more clear in the docs that it refers to computing nearest neighbors for N query points?\n",
      "\n",
      "2\n",
      "3450\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-07-05 11:37\n",
      "Also, I looked at code and if I'm not mistaken, it  calculates pairwise distances between query points and training points using pairwise_distances() function. In this case, if there's M query points, it would be O(DMN) and only if M is close to N in size, then it's O(DN^2), right?\n",
      "\n",
      "1\n",
      "3451\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-05 11:40\n",
      "sure. a pull request is welcome which removes the ^2. I think the current implementation is actually something like O(knd log k)...\n",
      "\n",
      "1\n",
      "3452\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2017-07-05 11:52\n",
      "Ok. Where does the log k come from?\n",
      "\n",
      "1\n",
      "3453\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-05 12:15\n",
      "we sort the k neighbors so nearest is output first. which we perhaps need not be doing.\n",
      "\n",
      "2\n",
      "3454\n",
      "55f3c8020fc9f982beb0733a\n",
      "2017-07-06 13:02\n",
      "Hi All! Is there an option to run k-median incremental algorithm with the library? Thanks\n",
      "\n",
      "1\n",
      "3455\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-06 13:39\n",
      "It is not provided in the library, but there might be a compatible implementation outside. There is an implementation of K Medoids under review, but this is not the same thing.\n",
      "\n",
      "2\n",
      "3456\n",
      "5938174cd73408ce4f656da8\n",
      "2017-07-07 09:32\n",
      "Hi all, I have a program to run pipeline with featureunion infinitely by putting into fit_transform function and training data. So may I know how to know program running status? I am using anaconda Jupiter notebook to run that python script. Million thanks\n",
      "Any verbose features available on fit transform and pipeline?\n",
      "\n",
      "2\n",
      "3457\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-07 14:51\n",
      "@bryanleekw it depends on the models in the pipeline\n",
      "there is a pull request for verbosity in pipelines, but I don't think it's merged yet\n",
      "\n",
      "2\n",
      "3458\n",
      "55366f4c15522ed4b3df5040\n",
      "2017-07-07 21:30\n",
      "Hi, does anybody know how should I go if I want to work on an open PR from another person?\n",
      "\n",
      "1\n",
      "3459\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-07 21:51\n",
      "you can add them as a remote and pull their branch and open your own pr\n",
      "@gsmafra\n",
      "\n",
      "2\n",
      "3460\n",
      "5938174cd73408ce4f656da8\n",
      "2017-07-08 02:58\n",
      "Thanks Andreas, may I know any links in sklearn that i can know detail?\n",
      "\n",
      "1\n",
      "3461\n",
      "5960d940d73408ce4f6c33f4\n",
      "2017-07-08 13:12\n",
      "hi guys, i am new to gitter. Not sure if this is right way and channel to post the doubt. but if it is, here is my doubt: I have to create a model to assign sales representatives in like 105 territories across united states based on sales-and other business rules like geo location - in each territory. But i am having hard time finding a solution to limit the size of each cluster based on sales, vicinity and few other parameters. So my question is how do i stop the cluster from growing when it meets a certain defined criteria/conditions.\n",
      "\n",
      "1\n",
      "3462\n",
      "54e07d6515522ed4b3dc0858\n",
      "2017-07-11 13:29\n",
      "If you truly want to dynamically \"stop growing a cluster\" based on custom criteria, I don't think any of the scikit-learn algorithms have an API for that. Moreover, it would only apply to algorithms that incrementally grow clusters, e.g. hierarchical clustering. If your dataset is not huge and you don't need to fit many different models, you could try implementing your own hierarchical clustering with simple python logic, and add your own constraints. It's a fairly straightforward algorithm.\n",
      "\n",
      "1\n",
      "3463\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-12 01:52\n",
      "As its agglomerative, you dont lose a great deal by not stopping. You should just process the output of linkage_tree to adhere to your criteria.\n",
      "\n",
      "1\n",
      "3464\n",
      "5581814615522ed4b3e20c6a\n",
      "2017-07-12 10:01\n",
      "Its is possible to retrain a Sklearn classifier once loaded back from a pickled filed\n",
      "\n",
      "1\n",
      "3465\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-12 10:02\n",
      "Yes but it will forget everything from the previous training set.\n",
      "\n",
      "1\n",
      "3466\n",
      "5581814615522ed4b3e20c6a\n",
      "2017-07-12 10:02\n",
      "So its again a fresh training?\n",
      "\n",
      "3\n",
      "3467\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-12 10:07\n",
      "There is no generic way, it depends on the model. Some models have a `partial_fit` method, it might or might not do want you want. Read the documentation, the reference papers of the literature (usually referenced in the docstring of the class) and the source code of the model to determine that whether it can help accomplish what you are looking for.\n",
      "\n",
      "5\n",
      "3468\n",
      "5581814615522ed4b3e20c6a\n",
      "2017-07-12 10:10\n",
      "So you mean to keep the training set as a back up and letter merge the new dataset with old and fit\n",
      "\n",
      "3\n",
      "3469\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-12 14:39\n",
      "@ogrisel did you tag at some point?\n",
      "We should definitely tag before the sprint\n",
      "looks like no branch yet\n",
      "ok that\n",
      "also good with me @ogrisel\n",
      "\n",
      "5\n",
      "3470\n",
      "59533ac5d73408ce4f6a9616\n",
      "2017-07-12 16:01\n",
      "sir can i ask You something\n",
      "hi there\n",
      "\n",
      "6\n",
      "3471\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-12 16:02\n",
      "@amueller no, not yet I got busy on other thing and now I am back at trying to debug a precision issue in the error computation of the TSNE model that has an impact on the stopping criterion.\n",
      "\n",
      "5\n",
      "3472\n",
      "59533ac5d73408ce4f6a9616\n",
      "2017-07-12 16:04\n",
      "@amueller  I have an interviw  in a startup\n",
      "@amueller I clear technical part but if somebody ask about is there any questions for me  what should i ask\n",
      "\n",
      "2\n",
      "3473\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-12 16:05\n",
      "This is not really a forum to discuss interview practices\n",
      "\n",
      "1\n",
      "3474\n",
      "5964e3b4d73408ce4f6c9e02\n",
      "2017-07-13 12:08\n",
      "Hey! Did someone deal with this preprocessing dilemma when working with timeseries? https://stackoverflow.com/questions/45080001/how-to-preprocess-timeseries-test-data-to-make-a-classification-prediction\n",
      "\n",
      "1\n",
      "3475\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-13 15:42\n",
      "if anyone wants to help the sprints run smoothly, please help tag issues appropriately with \"easy\" \"need contributor\" and \"sprint\"\n",
      "\n",
      "1\n",
      "3476\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-13 15:52\n",
      "looks like all \"easy\" \"need contributor\" issues already have PRs awaiting review...\n",
      "so review sprint?\n",
      ";)\n",
      "\n",
      "3\n",
      "3477\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-13 16:24\n",
      "If you have experienced macOS users who know a thing or two about numerical stability, clang, Accelerate / OpenBLAS.\n",
      "https://github.com/scikit-learn/scikit-learn/issues/9351\n",
      "this is blocking the 0.19b1 wheels on macOS\n",
      "there is also a 32 bit linux issue in the feature importance test\n",
      "the test is probably too strict, I am currently investigating with docker\n",
      "the saga solver on  macOS is a real bug but it's probably not easy to debug.\n",
      "\n",
      "6\n",
      "3478\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-13 16:25\n",
      "sprint will be on saturday ;)\n",
      "hm yeah it would be cool to have some easy issues but it looks like the issue tracker doesn't really have a lot right now :-/\n",
      "\n",
      "2\n",
      "3479\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-13 16:29\n",
      "I will add one to improve CI on master to test 32 bit linux and another to test macOS\n",
      "\n",
      "10\n",
      "3480\n",
      "57ec309f40f3a6eec067e511\n",
      "2017-07-13 18:18\n",
      "Hi @amueller just saw your tweet about advocating the need for fair and un-biased prediction. We at datascience.com have been looking into this topic and open sourced our first step in that direction. https://github.com/datascienceinc/Skater. If possible checkout our roadmap. How do others feel about the idea ?\n",
      "\n",
      "1\n",
      "3481\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-13 18:31\n",
      "@pramitchoudhary so that builds upon lime? what functionality are you adding to it?\n",
      "\n",
      "1\n",
      "3482\n",
      "57ec309f40f3a6eec067e511\n",
      "2017-07-13 18:34\n",
      "@amueller so the idea is to balance between global and local interpretation. At global level: model agnostic pdp, model agnostic variable importance as of now(one flavor of it, work is in progress to add other flavors); local level: its currently our forked version of LIME(_some improvements in the way local samples are generated_). There is also support for InMemoryModel(_one has access to the environment in which model is build_) and DeployedModel(_model is deployed in the wild_)\n",
      "here is a nice example of DeployedModel to evaluate third part models  https://github.com/datascienceinc/Skater/blob/master/examples/third_party_model/algorithmia_indico.ipynb.\n",
      "\n",
      "2\n",
      "3483\n",
      "57ec309f40f3a6eec067e511\n",
      "2017-07-13 18:39\n",
      "InMemoryModel example: https://github.com/datascienceinc/Skater/blob/master/examples/credit_analysis/Credit%20Analysis.ipynb\n",
      "\n",
      "1\n",
      "3484\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-13 18:49\n",
      "@ogrisel did you break master ;)\n",
      "\n",
      "1\n",
      "3485\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-13 18:52\n",
      "@amueller it's green: https://travis-ci.org/scikit-learn/scikit-learn/branches\n",
      "\n",
      "4\n",
      "3486\n",
      "574454a0c43b8c601974a563\n",
      "2017-07-14 22:48\n",
      "Then couldt we start the rebuild again?\n",
      "\n",
      "1\n",
      "3487\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-15 03:08\n",
      "@nok yeah but it doesn't matter\n",
      "it will restart with the next commit\n",
      "\n",
      "2\n",
      "3488\n",
      "574454a0c43b8c601974a563\n",
      "2017-07-15 06:48\n",
      "Okay :smile:\n",
      "\n",
      "1\n",
      "3489\n",
      "5581814615522ed4b3e20c6a\n",
      "2017-07-17 14:29\n",
      "Consider if i am having a column of integers is there any strong suggestion whether to do classification or regression. @ogrisel @amueller\n",
      "normally i use pandas dtypes to differentiate the problem\n",
      "if its object or int64 - classification\n",
      "if its float64 - regression\n",
      "is that right?\n",
      "@ogrisel i can understand. Is there any specific automation in such cases\n",
      "I wrote a function to see uniques of integer columns counts if its above 75% then i consider is regression\n",
      "is it a right thing\n",
      "\n",
      "8\n",
      "3490\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-17 14:31\n",
      "it depends : if the integers encodes target classes / categories, then classification, if they represent a target quantity (e.g. ratings, prices...),  then a regression model should work\n",
      "it depends on what they mean, not their physical types\n",
      "\n",
      "2\n",
      "3491\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-17 14:35\n",
      "pandas has a categorical datatype that you could use to make the distinction instead of using ambiguous integers.\n",
      "\n",
      "3\n",
      "3492\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-17 14:36\n",
      "e.g. predicting ratings, you might get 80% of \"4\" in your training set and treat the probleme as classification instead of regression.\n",
      "\n",
      "3\n",
      "3493\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-17 14:37\n",
      "but that should work well enough anyway. But using classification metrics might not be the best metric.\n",
      "\n",
      "1\n",
      "3494\n",
      "57ab46cf40f3a6eec05eccac\n",
      "2017-07-17 14:51\n",
      "Hi, I have a quick question for you experts: I'm trying to use the function \"model_selection.train_test_split()\" but I need to make sure that only there are no transactions whose \"coreID\" appears both in the test and in the train set.  I generally have 100 transactions per core and I have a coreID for every transaction.  I basically want each \"core\" (I have thousands of them) to be either in train or in test. Any ideas to help?\n",
      "Thanks!!\n",
      "(my dataset is at the single transaction level though)\n",
      "\n",
      "3\n",
      "3495\n",
      "547d8325db8155e6700da60b\n",
      "2017-07-17 16:55\n",
      "@mario_avevo_twitter can you make it with the parameter **group** of split ?\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html\n",
      "\n",
      "3\n",
      "3496\n",
      "57ab46cf40f3a6eec05eccac\n",
      "2017-07-17 18:49\n",
      "thank you @massich I'll look into it!!\n",
      "\n",
      "1\n",
      "3497\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-07-18 07:11\n",
      "If after 6th epoch of perceptron ,  our perceptron is getting converged but I have set n_iter parameter as 10 , will the  values of weights that are obtained at 6th epoch as well as 10th epoch  be optimal or Is it like the weight values at 6th epoch be more optimal than 10th epoch? Need some clarifications .\n",
      "Ok I'll share the code soon...\n",
      "\n",
      "2\n",
      "3498\n",
      "596d505dd73408ce4f6d9602\n",
      "2017-07-18 08:05\n",
      "I don't know the perceptron implementation of scikit-learn but convergence with Delta rule is guaranteed only for linearly separable problem\n",
      "\n",
      "1\n",
      "3499\n",
      "596d505dd73408ce4f6d9602\n",
      "2017-07-18 08:25\n",
      "<unconvertable> I will take a look, I'm dev a perceptron in go (on github) and I want also to implement a multilevel + backprop framework (so guys, if you are interested, please join :D :D)\n",
      "About convergence: I don't remember maths related to \"is monotonous descent or not\"\n",
      "But definitely is guaranteed under the assumption of linearly separability (I have demonstration, if you want)\n",
      "\n",
      "3\n",
      "3500\n",
      "5581814615522ed4b3e20c6a\n",
      "2017-07-18 16:56\n",
      "@ogrisel I have pickled my model using Joblib. Now i am pickling the LabelEncoder used to build the dataset also. So i can use it when prediction. Is it a right path?\n",
      "\n",
      "1\n",
      "3501\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-18 17:18\n",
      "How did you use the LabelEncder? To transform input features or the target variable? Most scikit-learn classifiers will automatically use a label encoder internally so you don't need to do it externally.\n",
      "If it's used to transform  input features it's better to use a pipeline.\n",
      "there is a PR for a ColumnTransformer under way.\n",
      "You can copy the code in your own project if you want it to be compatible with sklearn 0.18.2 and the future 0.19. ColumnTransformer will be part of 0.20.\n",
      "\n",
      "6\n",
      "3502\n",
      "5581814615522ed4b3e20c6a\n",
      "2017-07-18 17:19\n",
      "Only for couple of categorical variables\n",
      "\n",
      "4\n",
      "3503\n",
      "5581814615522ed4b3e20c6a\n",
      "2017-07-18 17:23\n",
      "I am currently planning serving prediction via RESTapi\n",
      "So mostly ppl will upload their dataset\n",
      "my models automatically are preprocessed (imputing, labelEncoding for Object types, Scaling)\n",
      "as its encoded i need to have those encoding instance for cross validating my new predictions right\n",
      "Thats perfect\n",
      "Thank you @ogrisel  for instant reply :)\n",
      "exactly.. thank u :)\n",
      "\n",
      "7\n",
      "3504\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-18 17:25\n",
      "Then write all the preprocessing logics in a transformer (as done in ColumnTransformer for instance) and use a pipeline: http://scikit-learn.org/stable/modules/pipeline.html to combine it with the supervised classification or regression model.\n",
      "Then you can pickle the full pipeline for deployment.\n",
      "\n",
      "2\n",
      "3505\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-18 17:26\n",
      "Don't forget to snapshot the training for a given version of the model to be deployed. This way you can make sure you can retrain a similar model from the same data when you decide to upgrade the scikit-learn version.\n",
      "\n",
      "3\n",
      "3506\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-18 17:32\n",
      "@BastinRobin BTW if you deploy your model with several python processes running on the same host (e.g. gunicorn workers), you should use `joblib.dump(pipeline, '/path/to/store/model.pkl')` to save the model and `joblib.load('/path/to/store/model.pkl', mmap_mode='r')` to load the model parameters in read-only shared memory and save memory usage on your production servers.\n",
      "\n",
      "3\n",
      "3507\n",
      "56f4f5b885d51f252ababa4e\n",
      "2017-07-18 18:36\n",
      "@amueller Noticing your latest issue, there is another malformed class at: http://scikit-learn.org/stable/modules/cross_validation.html#group-k-fold\n",
      "\n",
      "1\n",
      "3508\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-07-23 18:22\n",
      "@made2591  .. Thanks for the help . I did the maths manually and got it cleared ..\n",
      "\n",
      "1\n",
      "3509\n",
      "596d505dd73408ce4f6d9602\n",
      "2017-07-23 19:30\n",
      "@ashiskriz actually I didn't anything ^^ u r welcome!\n",
      "\n",
      "1\n",
      "3510\n",
      "541a528b163965c9bc2053de\n",
      "2017-07-25 07:58\n",
      "@SebastinSanty it's fixed in the master branch: http://scikit-learn.org/dev/modules/cross_validation.html#group-k-fold\n",
      ":)\n",
      "\n",
      "2\n",
      "3511\n",
      "56f4f5b885d51f252ababa4e\n",
      "2017-07-25 09:29\n",
      "@ogrisel Yes, I fixed it! :-)\n",
      "\n",
      "1\n",
      "3512\n",
      "5948fba7d73408ce4f68b048\n",
      "2017-07-27 07:28\n",
      "Hi guys are there any c++ projects I can get involved in\n",
      "if yes please link me below\n",
      "\n",
      "2\n",
      "3513\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-07-27 15:44\n",
      "@PriyaChincholikar lol this is a python package. We have a bunch of Cython though. But you can check out Shogun or MLPack\n",
      "or xgboost\n",
      "\n",
      "2\n",
      "3514\n",
      "5581814615522ed4b3e20c6a\n",
      "2017-07-28 08:12\n",
      "@ogrisel  Is there a better recommendation on using timeseries data in realtime. Is it good to store it or use any tools to get realtime inputs\n",
      "\n",
      "1\n",
      "3515\n",
      "5695d48216b6c7089cc24ae7\n",
      "2017-07-30 14:41\n",
      "Hi all, any open easy issues need to be solved?\n",
      "\n",
      "1\n",
      "3516\n",
      "582cbfc5d73408ce4f365e2b\n",
      "2017-07-30 22:36\n",
      "Hello does anyone know if there are any plans to add factor rotation to FA ?\n",
      "\n",
      "1\n",
      "3517\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-07-31 04:03\n",
      "@ibrahimsharaf, https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+label%3A%22Need+Contributor%22+label%3AEasy includes: #9341, #9336, #9325. #4458, among other stalled pull requests, should not take much work to finish up...\n",
      "\n",
      "1\n",
      "3518\n",
      "56f8122085d51f252abb1414\n",
      "2017-08-02 18:52\n",
      "Hi all, I was wondering if it is somehow possible to include a model into the mean of a Gaussian Process Regressor (just like one can choose a kernel) a fit for the mean as well?\n",
      "\n",
      "1\n",
      "3519\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-08-02 18:53\n",
      "No, I don't think so. That's rarely done in GPs afaik\n",
      "you can remove the global mean beforehand if you like\n",
      "\n",
      "2\n",
      "3520\n",
      "56f8122085d51f252abb1414\n",
      "2017-08-02 19:08\n",
      "Oh ok, thanks for the quick reply. I dont want to subtract the mean because sometimes subtracting things out of a dataset somehow changes its random nature...\n",
      "\n",
      "1\n",
      "3521\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-08-04 00:01\n",
      "A toy for anyone who uses grid search, and particularly @amueller: https://github.com/jnothman/searchgrid\n",
      "very much in alpha status\n",
      "\n",
      "2\n",
      "3522\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-08-04 14:37\n",
      "@jnothman nice!\n",
      "\n",
      "1\n",
      "3523\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-08-04 14:38\n",
      "What I'm thinking about right now / talking about with some people is to make it easier to implement ask/tell interfaces with BaseGridSearch. Maybe not in sklearn, but I think it should be possible to use a coroutine approach that allows implementing arbitrary parallel search strategies with a common interface\n",
      "\n",
      "1\n",
      "3524\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-08-05 08:09\n",
      "I implemented a callback-based parameter search variety of grid search years ago. It wasn't hard then, but it might be now ;) Not sure if that's all you mean...\n",
      "\n",
      "1\n",
      "3525\n",
      "5759dd0dc2f0db084a1d128d\n",
      "2017-08-07 17:46\n",
      "I'm looking at our GMM and the BIC derivation looks weird\n",
      "never mind... It's the score that is weird, and the BIC function corrects for that.\n",
      "\n",
      "2\n",
      "3526\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-08-08 01:37\n",
      "I hope thats a good thing ;)\n",
      "\n",
      "1\n",
      "3527\n",
      "598aa547d73408ce4f70a1d1\n",
      "2017-08-09 06:11\n",
      "I recently built a rudimentary classifier that identifies the news topic of a given article. I scraped lots of news articles - as training data - off a website, with their labels. In terms of text-processing, I removed stopwords, punctuations, lemmatized all words, then computed the tf-idf values of the remaining words. I represented each article as a doc of its 25 words with the highest tf-idf values. Then I decided on adopting a knn approach to the problem: upon getting the input article and determining its most frequent words (after the text-processing), I find the 5 articles that have most words in common with the input article - and then I perform a majority vote. If a majority of the 5 articles are tech, then the input article is tech. What are some ways I can improve upon this approach?\n",
      "\n",
      "1\n",
      "3528\n",
      "553d0d7b15522ed4b3df8a67\n",
      "2017-08-11 15:54\n",
      "would there be a way to include some of the changes suggested by https://stackoverflow.com/questions/26851553/sklearn-agglomerative-clustering-linkage-matrix into agglomerative clustering to plot dendrograms of the model?\n",
      "\n",
      "1\n",
      "3529\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-08-11 15:55\n",
      "@jzf2101 can you open an issue, or check if there's already one? I think there were previous requests to draw dendrograms, which I think would be a great feature\n",
      "I haven't read the question in full but there were some issues in supporting this in the past\n",
      "\n",
      "2\n",
      "3530\n",
      "54c084dbdb8155e6700eed4c\n",
      "2017-08-12 04:47\n",
      "Hey all, chipping away at #5653 and was wondering if there's any places in the codebase that check if fitted on any input estimator (where the est type is unknown) ?\n",
      "https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/partial_dependence.py#L125 from #7464 is a little tricky to generalise\n",
      "\n",
      "2\n",
      "3531\n",
      "54c084dbdb8155e6700eed4c\n",
      "2017-08-12 05:13\n",
      "hrmm. i think i can sneak it into the logic for ensembles and then check for other estimators in the fit stage. never mind!\n",
      "\n",
      "1\n",
      "3532\n",
      "57afd94d40f3a6eec05f5ad7\n",
      "2017-08-12 11:13\n",
      "hi\n",
      "\n",
      "1\n",
      "3533\n",
      "5986271dd73408ce4f703d62\n",
      "2017-08-12 15:06\n",
      "Hi, I've generated a big set of very high-dimensional embeddings (7300 verbs with 1700 dimensions each) from a QnA dataset. I'm trying to visualize them and what I do is to apply [`truncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD) (a PCA which centers the data) to obtain 50 dimensions and then pass those to [`TSNE`](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) in order to visualize them in 2d. The problem is that those embeddings include lots of verb conjugations and synonyms, so I would like to apply some type of algorithm to those high-dimensional embeddings (like kNN or cosine similarity) in order to obtain clusters or groups of similar embeddings represented by a single vector for each of those groups (without reducing their dimensionality since that is done later through SVD and TSNE). Does anybody know how to obtain that?\n",
      "\n",
      "1\n",
      "3534\n",
      "553d0d7b15522ed4b3df8a67\n",
      "2017-08-12 20:17\n",
      "@amueller issue already exists but pr is still left open\n",
      "\n",
      "1\n",
      "3535\n",
      "584e0ea7d73408ce4f3c6163\n",
      "2017-08-14 15:03\n",
      "Can anyone help me with a calculus issue using the SVM lost function\n",
      "\n",
      "1\n",
      "3536\n",
      "5683764a16b6c7089cc092dc\n",
      "2017-08-15 09:22\n",
      "hi everyone\n",
      "in SGDClassifier\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
      "how can I determine the baseline model?\n",
      "the doc said: \"Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\"\n",
      "how can I choose between SVM and logistic regression?\n",
      "\n",
      "6\n",
      "3537\n",
      "5717ab2f659847a7aff3b583\n",
      "2017-08-15 10:45\n",
      "it depends in your data\n",
      "and what you want to predict\n",
      "continous data\n",
      "\n",
      "10\n",
      "3538\n",
      "5683764a16b6c7089cc092dc\n",
      "2017-08-15 10:46\n",
      "hi\n",
      "yes, but I couldn't find\n",
      "that's why I ask here\n",
      "\n",
      "3\n",
      "3539\n",
      "5683764a16b6c7089cc092dc\n",
      "2017-08-15 10:47\n",
      "it is SVM?\n",
      "but how can I use SVM as a base model with SGDClassifier?\n",
      "or suppose SVM is by default, how can I switch to logistic regression?\n",
      "no problem, thanks for your help\n",
      "\n",
      "4\n",
      "3540\n",
      "5717ab2f659847a7aff3b583\n",
      "2017-08-15 10:51\n",
      "or sorry , i didn't have depth knowledge in machine learning\n",
      "how to choose the algorithm\n",
      "i'am begineer\n",
      "i just follow some moocs\n",
      "@vinhqdang tell me your experience with machine learning ?\n",
      "\n",
      "5\n",
      "3541\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-08-15 10:55\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier\n",
      "@vinhqdang you have to set the `loss` properly\n",
      "`loss=hinge` correspond to the SVM while `loss=log` to logistic regression\n",
      "\n",
      "6\n",
      "3542\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-08-15 10:58\n",
      "modified_huber is a smooth hinge loss\n",
      "\n",
      "5\n",
      "3543\n",
      "5717ab2f659847a7aff3b583\n",
      "2017-08-15 10:59\n",
      "@glemaitre hi can you contact you in private\n",
      "?\n",
      "@vinhqdang\n",
      "\n",
      "3\n",
      "3544\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-08-15 11:03\n",
      "@vinhqdang If really you want to play with the loss\n",
      "they are defined there https://github.com/scikit-learn/scikit-learn/blob/ef5cb84a805efbe4bb06516670a9b8c690992bd7/sklearn/linear_model/sgd_fast.pyx\n",
      "in case that you want to implement your own\n",
      "\n",
      "7\n",
      "3545\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-08-15 21:11\n",
      "@vinhqdang I wouldn't use SGDClassifier for custom loss functions for learning. Just implement your own in pure python. It will not be fast but it's a good learning experience and much easier to understand\n",
      "\n",
      "1\n",
      "3546\n",
      "58176b11d73408ce4f321426\n",
      "2017-08-24 07:35\n",
      "Anyone have experience predicting multiple time series with similar inputs and outputs? My problem is as follows: I have a bunch of products, and a bunch of locations at which the products are sold. Each of these locations has a sales history, so essentially I have a time series for each product at each location. My goal is to forecast future sales of each product at each location. I have tried approaching this problem as a time-series forecasting problem however I can't wrap my head around how to build a generalized single model from multiple time series. Would love to chat with someone if they've had experience with this before. thanks!\n",
      "\n",
      "1\n",
      "3547\n",
      "56652eda16b6c7089cbdb710\n",
      "2017-08-24 09:09\n",
      "@tblazina you should probably consider it as a time series data for each location. that way you will be only considering sales as a function of time and thus make your initial model simple.\n",
      "once you are able to make predictions on that  model and are able to understand it probably you can scale this model to encompass addtional dimensions like geography. point is to take only one location at a time. that should help you\n",
      "\n",
      "2\n",
      "3548\n",
      "58176b11d73408ce4f321426\n",
      "2017-08-24 09:14\n",
      "@infinite-Joy  thanks for the feedback, yes that was my general approach before having a model for each location but unfortunately then I'm reducing the amount of data significantly,  from >300,000 data points in all for all locations, versus having sometimes <<1000 for newer locations, I was wondering if there was somehow a way to reframe the problem so as to only have one model which generalizes to all locations.\n",
      "cool, i'll have a look at it\n",
      "thanks!\n",
      "\n",
      "3\n",
      "3549\n",
      "56652eda16b6c7089cbdb710\n",
      "2017-08-24 09:32\n",
      "this one seems a close match. not sure if this will help though. also not in python https://stats.stackexchange.com/questions/23036/estimating-same-model-over-multiple-time-series\n",
      "\n",
      "1\n",
      "3550\n",
      "54c084dbdb8155e6700eed4c\n",
      "2017-09-01 11:27\n",
      "The #9551 merge seems to be causing fails in a lot of new PRs\n",
      "\n",
      "1\n",
      "3551\n",
      "5571fe1015522ed4b3e17d90\n",
      "2017-09-01 11:28\n",
      "Hmmm really do you have examples?\n",
      "\n",
      "1\n",
      "3552\n",
      "54c084dbdb8155e6700eed4c\n",
      "2017-09-01 11:32\n",
      "#4197 https://travis-ci.org/scikit-learn/scikit-learn/builds/270779099?utm_source=github_status&utm_medium=notification\n",
      "#9147 sorry... feeling dlysexic this evening\n",
      "#5653 will fail shortly too I'm guessing (for a variety of reasons lol... but this complex check will be one of them)\n",
      "\n",
      "3\n",
      "3553\n",
      "5571fe1015522ed4b3e17d90\n",
      "2017-09-01 12:07\n",
      ":confused: #9147 has not been merged yet, right?\n",
      "I think I understand what you are saying. I'll fix it.\n",
      "\n",
      "2\n",
      "3554\n",
      "54c084dbdb8155e6700eed4c\n",
      "2017-09-01 12:14\n",
      "the fails aren't related to the PRs that are failing... #9551 seems to be the source unrelated to the new code\n",
      "\n",
      "1\n",
      "3555\n",
      "5571fe1015522ed4b3e17d90\n",
      "2017-09-01 12:21\n",
      "Should be fixed now: https://github.com/scikit-learn/scikit-learn/commit/deaa96452a981e3e54dc302fc14cb1c83cb2e399\n",
      "\n",
      "1\n",
      "3556\n",
      "54c084dbdb8155e6700eed4c\n",
      "2017-09-02 02:11\n",
      "Looking good now  @lesteve  :beers:\n",
      "\n",
      "1\n",
      "3557\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-09-03 18:50\n",
      "hi.. I want to do cross-validation but my data comes in groups. I have 10,000,000 rows in total and the sizes of the groups vary a lot. So I would really like to sample 100,000 rows first in proportion to the group sizes and then do the group cross-validation.  Is that possible?\n",
      "\n",
      "1\n",
      "3558\n",
      "56f8122085d51f252abb1414\n",
      "2017-09-03 19:43\n",
      "Is scikit-learn taking part on the Google Summer of Code this year? (I couldnt find anything for this year)\n",
      "\n",
      "1\n",
      "3559\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-09-03 20:02\n",
      "@mirca not this year\n",
      "\n",
      "1\n",
      "3560\n",
      "58b47262d73408ce4f4d4113\n",
      "2017-09-04 11:57\n",
      "@glemaitre  And what about next year? Is scikit-learn taking part next year?\n",
      "\n",
      "1\n",
      "3561\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-09-04 12:24\n",
      "@thechargedneutron no idea. I think that the core devs will keep the community informed.\n",
      "\n",
      "2\n",
      "3562\n",
      "56f8122085d51f252abb1414\n",
      "2017-09-04 18:16\n",
      "@glemaitre thanks\n",
      "\n",
      "1\n",
      "3563\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-09-05 07:41\n",
      "I am confused by scikit-learn's support for pandas dataframes.  Can you just use them like 2n numpy arrays in scikit learn and if so, is there still any need for sklearn-pandas?\n",
      "\n",
      "1\n",
      "3564\n",
      "541a528b163965c9bc2053de\n",
      "2017-09-05 07:51\n",
      "@lesshaste you can use pandas dataframes with numeric values as input to most scikit-learn estimators and model selection tools (e.g. cross_val_score and parameter search tools). sklearn-pandas can still be useful to do per-column feature preprocessing although this also should be improved by default in future scikit-learn versions.\n",
      "\n",
      "1\n",
      "3565\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-09-05 07:53\n",
      "@ogrisel  thanks!  I can't tell by looking at the docs which parts of sklearn will work with dataframes and which won't. For example http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
      "should I just assume all classifier and regression functions will work?\n",
      "\n",
      "15\n",
      "3566\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-09-05 10:55\n",
      "Is there any way to get feature importance (for a random forest classifier) based on auc (area under the curve)?\n",
      "\n",
      "1\n",
      "3567\n",
      "584778b4d73408ce4f3b440c\n",
      "2017-09-05 18:49\n",
      "why is auc important?\n",
      "\n",
      "1\n",
      "3568\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-09-05 18:50\n",
      "@lesshaste what do you mean by that? you could do sequential feature selection for any arbitrary metric, but that requires fitting multiple models\n",
      "\n",
      "1\n",
      "3569\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-09-05 20:19\n",
      "ah ok.  Looks like it just stalled on that observation https://github.com/scikit-learn/scikit-learn/pull/7663#issuecomment-307566895\n",
      "\n",
      "4\n",
      "3570\n",
      "59ad481bd73408ce4f743eb0\n",
      "2017-09-07 11:05\n",
      "Hey I am a newbie. Would love to contribute to the organization. Can anyone guide me on this?\n",
      "\n",
      "1\n",
      "3571\n",
      "58b47262d73408ce4f4d4113\n",
      "2017-09-07 11:16\n",
      "You may start by solving easy issues. Its available at https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue+is%3Aopen+label%3AEasy\n",
      "Solve the ones which is not currently being attempted by anyone else. (I'm a newbie too :P )\n",
      "\n",
      "2\n",
      "3572\n",
      "59ad481bd73408ce4f743eb0\n",
      "2017-09-07 13:34\n",
      "Alright. Thanks :smile:\n",
      "\n",
      "1\n",
      "3573\n",
      "59b56a87d73408ce4f751f7b\n",
      "2017-09-11 04:47\n",
      "hi i am new here can anyone guide me..!\n",
      "\n",
      "1\n",
      "3574\n",
      "58db7784d73408ce4f5484eb\n",
      "2017-09-11 15:23\n",
      "Hi, i am quite new to machine learning and am thinking about a good approach for my ML analysis. I have several tables with data in a DB. Some of this data are name, adresses and email and so on and some are 'other' data, mostly numbers and configuration data. Both kind of data can be in the same table. What I want to analyse now is, has a column address like data or number/config data. As I undersatand it with most ML examples you have a number of test data in tables and each table row is a data point. When I want to predict something, I take data that has the data structure of one table row and see that I predict the wanted data based on the test data training. In my case I have the table + column metadata (column name, length, etc) plus the actual content of the table column. My first idea is now to create a new table with (table name, colunm name, length, ctable column content as a string ,lets say) and then copy the column content column by column into this structture. This would give me individual rows to analyse. It just feels a bit clonky as the meta data (table/comuln name, ..) would be naturally the same for a whole column and only the column content differs and so there is not much variation in this table. Is there a more clever way of prepading the data ?\n",
      "\n",
      "1\n",
      "3575\n",
      "5967287ad73408ce4f6cf2a7\n",
      "2017-09-13 07:05\n",
      "Could I have Kmeans example convert data frame and output cluster prediction with Plot visualization?\n",
      "\n",
      "1\n",
      "3576\n",
      "59b94206d73408ce4f7598d4\n",
      "2017-09-13 14:41\n",
      "like this? https://beta.gryd.us/notebook/published/fJevxtDmjFfo5nGYs4A4bC/\n",
      "\n",
      "1\n",
      "3577\n",
      "5967287ad73408ce4f6cf2a7\n",
      "2017-09-13 15:23\n",
      "and how about convert string data frame to vector? Thanks anyway\n",
      "\n",
      "1\n",
      "3578\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-09-14 11:29\n",
      "anyone here have authority to update the topic in the IRC channel? (#scikit-learn on freenode)\n",
      "\n",
      "1\n",
      "3579\n",
      "5571fe1015522ed4b3e17d90\n",
      "2017-09-15 08:43\n",
      "@mirca @thechargedneutron aboug Google Summer of Code, the consensus amongst the core developers seems to be that we need all of the three conditions: a good student, a good focussed project with accomplishable goals that will produce something within the time constraints, and one mentor (ideally two mentors actually) who has enough spare bandwidth during this time. It just was not the case last year. It's hard to tell what will happen next year for sure. Potentially interested GSoC students are more than encouraged to get involved in scikit-learn so they get to know the project and we get to know them.\n",
      "\n",
      "1\n",
      "3580\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-09-15 08:44\n",
      "@lesteve  maybe add something to the scikit front page about this?\n",
      "It's always good to advertise :)\n",
      "\n",
      "2\n",
      "3581\n",
      "58b47262d73408ce4f4d4113\n",
      "2017-09-15 08:48\n",
      "@lesteve  Thanks for the information. I am continuously trying to understand the codebase by solving bugs. It's more comfortable for me now. Thanks :)\n",
      "\n",
      "1\n",
      "3582\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-09-18 14:19\n",
      "@lesshaste yeah I could update the topic if I find the time. Maybe the update should be \"check gitter instead\"\n",
      "\n",
      "1\n",
      "3583\n",
      "5717ab2f659847a7aff3b583\n",
      "2017-09-29 17:14\n",
      "hi\n",
      "\n",
      "1\n",
      "3584\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-09-29 17:28\n",
      "@Rebaiahmed hi\n",
      "\n",
      "1\n",
      "3585\n",
      "5717ab2f659847a7aff3b583\n",
      "2017-09-29 17:28\n",
      "i need some help\n",
      "for scholarship project\n",
      "just i'm searching about\n",
      "sickit learn\n",
      "with django\n",
      "i want to implement machine learning\n",
      "into web project\n",
      "but i didn\"t have any idea\n",
      "for the application\n",
      "or the avalaible dataset\n",
      "\n",
      "10\n",
      "3586\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-09-29 17:29\n",
      "you can post your question here, but I recommend going to stackexchange and tag it with sklearn\n",
      "\n",
      "3\n",
      "3587\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-09-29 17:30\n",
      "for an introduction, check out @jakevdp's free book: https://github.com/jakevdp/PythonDataScienceHandbook\n",
      "\n",
      "1\n",
      "3588\n",
      "5717ab2f659847a7aff3b583\n",
      "2017-09-29 17:31\n",
      "i have an experience with django and sickit-learn\n",
      "they are separated\n",
      "i want an idea o combine them\n",
      "good idea ;)\n",
      "why ?\n",
      "what are the advantages ?\n",
      "ok good :D\n",
      "\n",
      "7\n",
      "3589\n",
      "541a528b163965c9bc2053de\n",
      "2017-09-29 17:33\n",
      "Build a recommender system for suggesting movies based on the movie lens data as a web application\n",
      "or a book recommender system using https://www.kaggle.com/zygmunt/goodbooks-10k\n",
      "I would recommend you to use [lightfm](https://github.com/lyst/lightfm) or [spotlight](https://github.com/maciejkula/spotlight) instead of scikit-learn though :)\n",
      "Those libraries are dedicated to building recommender systems. Factorization machines and neural networks with categorical embeddings are known to be very good for building recommender systems but are not implemented in scikit-learn.\n",
      "(yet)\n",
      "\n",
      "5\n",
      "3590\n",
      "5717ab2f659847a7aff3b583\n",
      "2017-09-29 17:37\n",
      "there is any other suggestion for other dataset\n",
      "for example\n",
      "client -ecommerce dataset\n",
      "something ike this\n",
      "\n",
      "4\n",
      "3591\n",
      "5717ab2f659847a7aff3b583\n",
      "2017-09-29 17:38\n",
      "what do you think about Twitter Sentiment Analysis ?\n",
      "\n",
      "1\n",
      "3592\n",
      "541a528b163965c9bc2053de\n",
      "2017-09-29 17:39\n",
      "e-commerce: I don't know any out the time of my mind. For twitter you can use the sentiment140 dataset but I find sentiment analysis pretty useless personally.\n",
      "I have to go. Good luck for your project.\n",
      "\n",
      "2\n",
      "3593\n",
      "59bacd63d73408ce4f75cb22\n",
      "2017-09-30 11:27\n",
      "> Build a recommender system for suggesting movies based on the movie lens data as a web applicationLove  this one!\n",
      "\n",
      "1\n",
      "3594\n",
      "59d4936ed73408ce4f7883b1\n",
      "2017-10-04 20:27\n",
      "@Rebaiahmed Sentiment analysis is iffy at best. Twitter is particularly bad. Training data is typically nice and clean, but Twitter users are are an insanely sarcastic bunch which makes this so unreliable. Plus it's pretty worthless. The move to 280 characters might improve this in the future but we'll need to get new training data.  What could be interesting is to do questionaries, and demographics and other data, then build a model that predicts opinions and responses based on that data. You could through in sentiment analysis here for longer paragraph answers if it really interests you.\n",
      "\n",
      "1\n",
      "3595\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-10-06 21:04\n",
      "is it possible to use  the pearson correlation coefficient as the loss function when doing regression in scikit-learn?\n",
      "\n",
      "1\n",
      "3596\n",
      "59146cabd73408ce4f5ec527\n",
      "2017-10-08 23:13\n",
      "To look at feature importance, I extract it from the `feature_importance_` attribute, sort, and visualize. Is there a way to pretty print / plot automatically? In R, `plot(random_forest)` does the trick\n",
      "If there's not a way to do this in scikit-learn, is there a reason why and does the community welcome PR in this regard?\n",
      "\n",
      "2\n",
      "3597\n",
      "59d187cfd73408ce4f782f22\n",
      "2017-10-09 06:02\n",
      "@LaDilettante you need to use matplotlib to plot your results\n",
      "\n",
      "1\n",
      "3598\n",
      "59d187cfd73408ce4f782f22\n",
      "2017-10-09 06:04\n",
      "Sentiment analysis is good for articles, but yes I do agree it's bad for Twitter.\n",
      "\n",
      "1\n",
      "3599\n",
      "595a8627d73408ce4f6b61ba\n",
      "2017-10-10 13:18\n",
      "perhaps use bayesian classifier if predicting something from tweets\n",
      "it worked in some works ive come across\n",
      "\n",
      "2\n",
      "3600\n",
      "5757de1ec43b8c6019787b6c\n",
      "2017-10-11 06:46\n",
      "@rishavroy1264bitmesra Hey guys ! Can someone help me to find a way to find similar meaning sentences sentence-A is known and I have to develop an implementation to find all sentences meaning same to sentence-A in coming input data(paragraphs)\n",
      "\n",
      "1\n",
      "3601\n",
      "577ebc96c2f0db084a21f545\n",
      "2017-10-12 17:49\n",
      "@rishavroy1264bitmesra  I am not sure how you can exactly achieve this but what you are trying to achieve is a very hard problem. I feel WordNet shall be used for calculating the word distances for similarity/dissimilarity and even use tf-idf for sentence scoring. I know this is very vague for you've got some work to do..\n",
      "\n",
      "1\n",
      "3602\n",
      "58d98495d73408ce4f5421c4\n",
      "2017-10-12 23:43\n",
      "Does someone have experience with using SE(3) poses as inputs? I am trying to find a good representation for the orientations. Some pros/cons, along with usage guidelines would be much appreciated as well. Thanks in advance\n",
      "\n",
      "1\n",
      "3603\n",
      "59d187cfd73408ce4f782f22\n",
      "2017-10-16 04:54\n",
      "Hey guys anyone here familiar with University of Edinburgh's grad school program?\n",
      "Looking for recommendations for European graduate schools\n",
      "\n",
      "2\n",
      "3604\n",
      "59e0d687d73408ce4f79e8c1\n",
      "2017-10-16 16:21\n",
      "Hi. Was wondering if anyone knows of any academic papers on marketing attribution modelling with offline source (assigning a website visitor a probability of having come to the site due to a TV ad)?\n",
      "\n",
      "1\n",
      "3605\n",
      "59c93e97d73408ce4f774b70\n",
      "2017-10-17 14:45\n",
      "HI, I am trying to prepare training data for hand written recognition for Tibetan language. As i have to prepare thousand of it. I want to make sure i am making the correct format of pixel values for Tibetan character for machine learning. I tried a single Tibetan alphabet image of size 32*32 pixel. I got the pixel values but i am not able to plot its back in python using matplotlib.pyplot.\n",
      "\n",
      "1\n",
      "3606\n",
      "59d187cfd73408ce4f782f22\n",
      "2017-10-17 16:51\n",
      "hey guys I have X_test and X_train data. Does anyone know how I can divide it so I can have two different two of each for two different models?\n",
      "trying to make a pipeline\n",
      "nope I didn't use test_train_split to make X_test and X_train\n",
      "\n",
      "3\n",
      "3607\n",
      "59c93e97d73408ce4f774b70\n",
      "2017-10-17 16:52\n",
      "@angelotc  you got one data file ?\n",
      "\n",
      "1\n",
      "3608\n",
      "56d2f977e610378809c40828\n",
      "2017-10-20 15:55\n",
      "Hi, a recent ML enthusiast trying to contribute. I just forked the scikit-learn repo and got a very easy PR merged.\n",
      "\n",
      "1\n",
      "3609\n",
      "56d2f977e610378809c40828\n",
      "2017-10-20 16:00\n",
      "I set up a dev environment using virtualenv, so that it does not interfere with my existing scikit-learn installation. But I see that the docs does not mention it at all. Is there any particular reason? Or am I missing something obvious here?\n",
      "\n",
      "1\n",
      "3610\n",
      "59ea3107d73408ce4f7af81c\n",
      "2017-10-20 17:26\n",
      "@angelotc  : First import  from sklearn.model_selection import train_test_split then  X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42) X is a dataframe having only features and y is a dataframe having only target from your original dataset\n",
      "\n",
      "1\n",
      "3611\n",
      "58a5767cd73408ce4f4abd3f\n",
      "2017-10-21 00:49\n",
      "Are you guys familiar with C++? I need some help...\n",
      "\n",
      "1\n",
      "3612\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-10-22 18:19\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/yuhz/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/yuhz/image.png)\n",
      "Hello everybody , I have a doubt - Why is the number of nodes in hidden layer always 1 more than number of nodes in input layer ?\n",
      "How do we set that number of nodes in hidden layer . In most cases I see this type of representation as shown in diagram\n",
      "I mean in most cases that I came across   @mikegraham\n",
      "\n",
      "4\n",
      "3613\n",
      "55a487245e0d51bd787b4e45\n",
      "2017-10-22 18:21\n",
      "@CaptainAshis \"Always\"?\n",
      "\n",
      "1\n",
      "3614\n",
      "55a487245e0d51bd787b4e45\n",
      "2017-10-22 18:23\n",
      "@CaptainAshis I am not aware of this being a standard practice. Do you mean in actual cases or just diagramatically?\n",
      "\n",
      "1\n",
      "3615\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-10-22 18:25\n",
      "I mean diagrammatically .In most youtube videos I see this type of representation @mikegraham\n",
      "\n",
      "2\n",
      "3616\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-10-22 18:27\n",
      "@mikegraham  . So is there any standard rule to know how many nodes we can set for the hidden layer ?\n",
      "\n",
      "1\n",
      "3617\n",
      "55a487245e0d51bd787b4e45\n",
      "2017-10-22 18:36\n",
      "@CaptainAshis It varies wildly what ends up working well. There is an old rule of thumb for single-hidden-layer models of 'mean of input nodes and output nodes', but that is even rougher than most rules of thumb.\n",
      "\n",
      "2\n",
      "3618\n",
      "5634e15116b6c7089cb8f9f2\n",
      "2017-10-24 12:16\n",
      "@CaptainAshis hot damn! How can I implement a hello world style for this ![alt](https://files.gitter.im/scikit-learn/scikit-learn/yuhz/image.png)...\n",
      "\n",
      "1\n",
      "3619\n",
      "595a8627d73408ce4f6b61ba\n",
      "2017-10-24 13:22\n",
      "https://www.youtube.com/watch?v=bxe2T-V8XRs\n",
      "@Ij888\n",
      "\n",
      "2\n",
      "3620\n",
      "5634e15116b6c7089cb8f9f2\n",
      "2017-10-24 15:48\n",
      "yo @tms1337 woot woot\n",
      "\n",
      "1\n",
      "3621\n",
      "574454a0c43b8c601974a563\n",
      "2017-10-24 18:20\n",
      "@Ij888 e.g. https://github.com/rushter/MLAlgorithms\n",
      "\n",
      "1\n",
      "3622\n",
      "54ea6b6b15522ed4b3dc55a2\n",
      "2017-10-27 17:07\n",
      "`<SPAM>` In case someone is interested in visiting Colombia and presenting something, or maybe just attending ;-)  https://www.pycon.co And sorry for the spam `</SPAM>`\n",
      "\n",
      "1\n",
      "3623\n",
      "5910079ed73408ce4f5dc467\n",
      "2017-10-29 11:47\n",
      "@lj888 hope you got your answer  .. :)\n",
      "\n",
      "1\n",
      "3624\n",
      "53eb987c107e137846baa89d\n",
      "2017-10-30 16:39\n",
      "I am doing this ``` In [1]: from sklearn.datasets import fetch_olivetti_faces In [2]: import pandas as pd In [3]: ol_faces = fetch_olivetti_faces() In [4]: ol_faces_df = pd.DataFrame(ol_faces.data) In [5]: pd.tools.plotting.scatter_matrix(ol_faces_df, c=ol_faces.target, figsize=(8, 8)) ``` But this programs tends to keep increasing memory usage, is something wrong that I am doing? I am not trying to do anything in particular on my own. I was just trying to 'replicate' (on self exercise type question) for what Alexandre Gram tried to do in SciPy 2017 tutorial.\n",
      "I am doing this on pandas version 0.19. I think we have `pd.plotting.scatter_matrix` in versions higher than 0.19 (i.e. development version I guess)\n",
      "\n",
      "2\n",
      "3625\n",
      "58b47262d73408ce4f4d4113\n",
      "2017-11-04 17:25\n",
      "Hello, I am quite free after my semester ending examinations. I want to contribute more to scikit-learn. Apart from solving bugs (which currently I am doing with 6-8 merged PRs), how else can I contribute to scikit-learn?\n",
      "\n",
      "1\n",
      "3626\n",
      "598c6a46d73408ce4f70da78\n",
      "2017-11-07 15:26\n",
      "Hi, is there a way to remove samples using a pipeline? I was looking at FunctionTransformer but I don't see how it will modify the y values\n",
      "\n",
      "1\n",
      "3627\n",
      "576e76e2c2f0db084a1fdb14\n",
      "2017-11-07 15:45\n",
      "@cs_hanes_twitter  `imbalanced-learn` has a `Pipeline` object that lets you remove samples. Also, there is WIP for a `FunctionSampler` that lets you use arbitrary functions confirming the `scikit-learn`'s object oriented API.\n",
      "\n",
      "1\n",
      "3628\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-08 14:05\n",
      "How do I run tests (unit tests) locally after making some changes?\n",
      "Would running `make` at the top-level directory be sufficient?\n",
      "\n",
      "2\n",
      "3629\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-08 14:43\n",
      "Though I did that now. But I would still like to know how to do it cleanly.\n",
      "\n",
      "1\n",
      "3630\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-11-08 21:08\n",
      "make test\n",
      "\n",
      "1\n",
      "3631\n",
      "564106cd16b6c7089cba16c7\n",
      "2017-11-08 21:27\n",
      "Hi all, could you tell what you think of this approach  : https://www.kaggle.com/jankoch/scikit-learn-pipelines-and-pandas/notebook ?\n",
      "\n",
      "1\n",
      "3632\n",
      "53232ac75e986b0712efe3af\n",
      "2017-11-08 22:28\n",
      "@ncouturier I didn't go through your notebook in detail, but I think you might be interested in looking at https://github.com/scikit-learn/scikit-learn/pull/9012/\n",
      "That certainly relates to those transformers you implemented to apply certain transformers to certain columns\n",
      "(you call it DataFrameFeatureUnion)\n",
      "Regarding the dummy encoder transformer, you also might want to look at https://github.com/scikit-learn/scikit-learn/pull/9151 that implements a CategoricalEncoder\n",
      "Further, you might be interested in https://github.com/scikit-learn-contrib/sklearn-pandas (but I am also not very familiar with that, so not sure how the functionality there relates to the transformers you implemented)\n",
      "(so as a summary, I agree with you that it currently is hard to do pandas-like preprocessing in sklearn pipelines, but we are working to improve that. Feedback on those linked PRs / project is always welcome!)\n",
      "\n",
      "6\n",
      "3633\n",
      "564106cd16b6c7089cba16c7\n",
      "2017-11-08 22:36\n",
      "ok. Thx for your answer.\n",
      "\n",
      "1\n",
      "3634\n",
      "5a04163fd73408ce4f7dd6f7\n",
      "2017-11-09 08:52\n",
      "<unconvertable> Hi, this is the link to my blog: http://dhrubajitdas44.blogspot.in/ <unconvertable>  It contains my machine learning/ deep learning projects, few as of now. More coming. Any kind of criticism/suggestions/corrections are very much welcome, as it will help me learn. I am a beginner. If any experts/instructors would like to give a review on the projects, that would be great. And the students, follow the blog if you find it useful and for future updates. Thank you\n",
      "\n",
      "1\n",
      "3635\n",
      "58b46af0d73408ce4f4d3f2c\n",
      "2017-11-12 00:54\n",
      "Hi everyone.\n",
      "I'm a statistics graduate student and I'm interested getting some FOSS (and programming) experience.\n",
      "Would it be a worthwhile endeavor to implement Dirichlet Process mixture models in sklearn?\n",
      "Yikes, sorry just noticed it's already implemented!\n",
      "\n",
      "4\n",
      "3636\n",
      "58ec2bf4d73408ce4f577ca0\n",
      "2017-11-12 16:19\n",
      "what would probably be useful on the other hand is a python framework for generalized finite mixture models\n",
      "eg. where you specify a set of distributions and parameters to estimate and run the model without hardcoding the maximum likelihood equation yourself\n",
      "\n",
      "2\n",
      "3637\n",
      "58b46af0d73408ce4f4d3f2c\n",
      "2017-11-14 20:03\n",
      "@VHRanger as an extension to sklearn?\n",
      "\n",
      "1\n",
      "3638\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-11-17 16:37\n",
      "@VHRanger you mean pomegranate?\n",
      "\n",
      "1\n",
      "3639\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-11-17 16:38\n",
      "@ssequeira if your interested in DPs, maybe check out our LDA? I feel it could use some love, and I also think having a gibbs sampling version would be nice\n",
      "not sure if we also want a gibbs sampling version of the GMM\n",
      "\n",
      "2\n",
      "3640\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-19 09:19\n",
      "Are the tags 'help wanted' tagged issues for beginners? How do they differ from 'Easy' tags on issues?\n",
      "\n",
      "1\n",
      "3641\n",
      "5a05409fd73408ce4f7dfdd9\n",
      "2017-11-20 16:59\n",
      "Ok\n",
      "\n",
      "1\n",
      "3642\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-11-21 14:30\n",
      "if you use minmaxscaler like this: scaler = MinMaxScaler(feature_range=(0, 1)) train = scaler.fit_transform(train) test = scaler.transform(test)\n",
      "are we guaranteed that test is in the range 0 to 1?\n",
      "\n",
      "2\n",
      "3643\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-11-21 15:11\n",
      "@lesshaste the training set yes, the test-set  not.\n",
      "\n",
      "17\n",
      "3644\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-11-21 15:29\n",
      "in a related question.. how would you do CV for time series data?  The only obvious method that I can think of is to randomly split the data into the first x% of samples and the last (100-x)% of samples and train and test accordingly. But that clearly has much less randomness than a normal CV and the different folds overlap hugely\n",
      "I am confused by this example to visualize a decision tree. http://scikit-learn.org/stable/modules/tree.html#tree In a script, how do you get to see the picture? The example just finishes with >>> graph = graphviz.Source(dot_data)   >>> graph\n",
      "\n",
      "2\n",
      "3645\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-11-21 20:05\n",
      "in jupyter you can just put the graph object in a cell\n",
      "it'll show as image\n",
      "\n",
      "2\n",
      "3646\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-11-22 09:11\n",
      "@amueller  right.. it would be nice to have an example that worked in a script though as many newbie people will just copy and paste into their code\n",
      "@amueller  I got it work myself so this is a suggestion for others\n",
      "\n",
      "2\n",
      "3647\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-11-22 14:59\n",
      "@lesshaste you mean for the tree plotting? The real solution is here: https://github.com/scikit-learn/scikit-learn/pull/9251\n",
      "\n",
      "1\n",
      "3648\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-11-22 17:22\n",
      "@amueller  Thanks. Should this work for regression trees too?\n",
      "\n",
      "2\n",
      "3649\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-11-22 17:34\n",
      "great! I look forward to it's being merged\n",
      "\n",
      "1\n",
      "3650\n",
      "5a09a93cd73408ce4f7e626f\n",
      "2017-11-23 14:45\n",
      "@amueller  When i ingest data, set sharing to true, what is shared?\n",
      "@amueller The distributed ingest data in geomesa is the MapReduce method, why not use spark?\n",
      "\n",
      "2\n",
      "3651\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-23 20:45\n",
      "Ah, never mind. I found about 'help wanted' here: http://scikit-learn.org/stable/developers/contributing.html#issue-tracker-tags\n",
      "\n",
      "1\n",
      "3652\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-24 09:12\n",
      "Using `make html` in the `doc` directory I get the following error (on master branch) ``` Exception occurred:   File \"/home/gxyd/anaconda3/lib/python3.6/site-packages/sphinx_gallery/gen_gallery.py\", line 322, in sumarize_failing_examples     \"\\n\" + \"-\" * 79) ValueError: Here is a summary of the problems encountered when running the examples  Unexpected failing examples: /home/gxyd/dev/scikit-learn/examples/ensemble/plot_feature_transformation.py failed leaving traceback: Traceback (most recent call last):   File \"/home/gxyd/anaconda3/lib/python3.6/site-packages/sphinx_gallery/gen_rst.py\", line 450, in execute_code_block     exec(code_block, example_globals)   File \"<string>\", line 15, in <module> ImportError: cannot import name 'CategoricalEncoder'   /home/gxyd/dev/scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py failed leaving traceback: Traceback (most recent call last):   File \"/home/gxyd/anaconda3/lib/python3.6/site-packages/sphinx_gallery/gen_rst.py\", line 450, in execute_code_block     exec(code_block, example_globals)   File \"<string>\", line 40, in <module> TypeError: __init__() got an unexpected keyword argument 'validation_fraction' ```\n",
      "\n",
      "1\n",
      "3653\n",
      "53232ac75e986b0712efe3af\n",
      "2017-11-24 09:13\n",
      "Are you sure you installed the development version?\n",
      "\n",
      "1\n",
      "3654\n",
      "53232ac75e986b0712efe3af\n",
      "2017-11-24 09:14\n",
      "It seems as you are building the master docs but with a released version of sklearn for running the examples\n",
      "\n",
      "4\n",
      "3655\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-24 09:17\n",
      "I get things like ``` /home/gxyd/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:docstring of sklearn.model_selection.GridSearchCV:166: WARNING: Undefined substitution referenced: \"param_kernel|param_gamma|param_degree|split0_test_score\". /home/gxyd/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:docstring of sklearn.model_selection.GridSearchCV:166: WARNING: Undefined substitution referenced: \"...\". ``` While running `make html` which made me inclined towards the thing you are saying.\n",
      "As I can see for `-e` option: ``` Install  a  project  in editable mode (i.e.  setuptools \"develop               mode\") from a local project path or a VCS url. ```\n",
      "\n",
      "2\n",
      "3656\n",
      "53232ac75e986b0712efe3af\n",
      "2017-11-24 11:12\n",
      "@gxyd I typically have a 'development' environment in which I install the master version (with `pip install -e .`)\n",
      "\n",
      "1\n",
      "3657\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-24 11:58\n",
      "@jorisvandenbossche thanks. That seems to be working just fine for me right now.\n",
      "\n",
      "1\n",
      "3658\n",
      "5a1847bed73408ce4f801f28\n",
      "2017-11-24 16:27\n",
      "hello\n",
      "\n",
      "1\n",
      "3659\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-25 06:40\n",
      "Does running tests using `make test` differs from tests run using `pytest`?\n",
      "\n",
      "1\n",
      "3660\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-11-25 12:41\n",
      "@gxyd Check the Makefile: https://github.com/scikit-learn/scikit-learn/blob/master/Makefile#L39\n",
      "\n",
      "1\n",
      "3661\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-25 13:05\n",
      "So that would mean `make test` actually runs `pytest` with different parameters (or options)?\n",
      "\n",
      "3\n",
      "3662\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-11-25 15:24\n",
      "``` pytest --showlocals -v sklearn pytest --showlocals -v doc/sphinxext/ pytest $(shell find doc -name '*.rst' | sort) ```\n",
      "\n",
      "1\n",
      "3663\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-25 16:04\n",
      "Yup, I understand now. Thanks.\n",
      "\n",
      "1\n",
      "3664\n",
      "570d14d6187bb6f0eadf1582\n",
      "2017-11-26 14:15\n",
      "Hi, I want to pickup https://github.com/scikit-learn/scikit-learn/pull/7694 again - which branch should I merge to the my code -  0.19.X, master ?\n",
      "\n",
      "1\n",
      "3665\n",
      "53eb987c107e137846baa89d\n",
      "2017-11-26 14:22\n",
      "I think 'master' branch should be the one to go with.\n",
      "\n",
      "1\n",
      "3666\n",
      "570d14d6187bb6f0eadf1582\n",
      "2017-11-26 20:20\n",
      "That's what I try, but make seems to fail: https://gist.github.com/Kornel/11e69ef9fd2e9380a21991029fbecaf9#file-gistfile1-txt-L9473\n",
      "I'm using python 3.6.3,\n",
      "``` python -c \"import numpy as np; import scipy as sp; print(np.__version__); print(sp.__version__)\" 1.13.3 1.0.0```\n",
      "and running simply make\n",
      "\n",
      "4\n",
      "3667\n",
      "56f8122085d51f252abb1414\n",
      "2017-11-26 21:53\n",
      "Hi everyone, simple question here: is the prior in, e.g., Ridge Regression, also applied to the intercept? See here: http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression\n",
      "By \"prior\" I mean the L2 regularization term.\n",
      "\n",
      "2\n",
      "3668\n",
      "58b47262d73408ce4f4d4113\n",
      "2017-12-01 17:57\n",
      "Hello all, I want to take up a moder\n",
      "Hello all, I want to take up a moderate issue or something significant not currently being worked upon by anyone. Is there an issue or feature which needs quick attention?\n",
      "\n",
      "2\n",
      "3669\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-01 18:01\n",
      "@thechargedneutron have you worked on anything before? if not, go with those that say \"good first issue\"\n",
      "oh actually you did, sorry\n",
      "your name looked familiar lol\n",
      "\n",
      "3\n",
      "3670\n",
      "58b47262d73408ce4f4d4113\n",
      "2017-12-01 18:02\n",
      "Yeah, I have like 8-9 issues solved. And currently working on a moderate, but almost done. Hence looking for something more than just a bug\n",
      "\n",
      "4\n",
      "3671\n",
      "5a234aead73408ce4f816c50\n",
      "2017-12-03 00:58\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/7foy/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/7foy/image.png)\n",
      "\n",
      "1\n",
      "3672\n",
      "5a234aead73408ce4f816c50\n",
      "2017-12-03 00:59\n",
      "Does anyone know if you need to be a repo owner to access CircleCI artifacts? Supposedly you should be able to access the full documentation built by CircleCI, but I don't see the tab in the build results...\n",
      "\n",
      "1\n",
      "3673\n",
      "5a23f69dd73408ce4f81799e\n",
      "2017-12-03 13:06\n",
      "need some help please\n",
      "\n",
      "1\n",
      "3674\n",
      "5a23f69dd73408ce4f81799e\n",
      "2017-12-03 13:06\n",
      "Hi,  Have you used Sklearn TImeSeriesSplit before?  I was wondering how you'd actually implement it?  # Splitting Time-series dataset into Training set and Test set using TimeSeriesSplit from sklearn.model_selection import TimeSeriesSplit tscv = TimeSeriesSplit(n_splits=10) print(tscv) for train_index, test_index in tscv.split(X):     print(\"TRAIN:\", train_index, \"TEST:\", test_index)          X_train =     X_test =     y_train =     y_test =  So after you have the indexes built, how do you actually use this to get your X_train/test and y_train/test? From this point onwards, do you fit it to your model the same way?  Thanks, Joe\n",
      "ahh no worries, solved it\n",
      "misread documentation\n",
      "\n",
      "3\n",
      "3675\n",
      "564e507e16b6c7089cbb6551\n",
      "2017-12-03 17:33\n",
      "Hi everyone, I'm classifying an 8Million of pixels image using supervised classifiers in scikit-learn. I observed that SGD is that only classifier that converges in a reasonable time; the other classifiers tend to either run endlessly or to get stuck somehow. Is it true what's stated in this page (https://datascience.stackexchange.com/a/996/19222) that above 200.000 observations, one should stick with linear learning (i.e. those that implement partial_fit, although I'm really using it and the SGD is still working correctly)?\n",
      "\n",
      "1\n",
      "3676\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-04 20:13\n",
      "quick poll: did you all notice the conda compiler package update and have you trashed all your legacy environments? Am I the only one that didn't hear about that?\n",
      "are you mixing old and new packages by any chance?\n",
      "\n",
      "2\n",
      "3677\n",
      "53232ac75e986b0712efe3af\n",
      "2017-12-04 20:16\n",
      "I have had a lot of problems lately with conda, but I don't know if they are related to the new compiler packages. Basically whatever small change I want to do, it wants to downgrade/upgrade a whole set of other packages (eg it always wants to downgrade my conda-forge numpy 1.13 to defaults numpy 1.11 when installing a package that is in no way depending (also no through its deps) on numpy)\n",
      "\n",
      "11\n",
      "3678\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-12-05 16:29\n",
      " I am trying (X,y) = make_classification(n_features=20, n_samples=1000, n_classes=3) but it says ValueError: n_classes * n_clusters_per_class must be smaller or equal 2 ** n_informative\n",
      "I just want to make a random classification problem with 3 classes. How can I do that?\n",
      "\n",
      "2\n",
      "3679\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-05 16:42\n",
      "How about ``` make_classification(n_classes=3, n_redundant=0, n_informative=20) ``` (I am also a beginner, but just trying here. You can ignore if you want someone more experienced to comment)\n",
      "There are quite a few other (default)arguments which probably would need to be changed. See http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
      "\n",
      "2\n",
      "3680\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-12-05 16:43\n",
      "thanks.. (trainX, trainY) = make_classification(n_informative=20, n_redundant=0, n_samples=50000, n_classes=120) works!\n",
      "this is slightly less intuitive than normal for scikit learn\n",
      "because (trainX, trainY) = make_classification(n_informative=20, n_samples=50000, n_classes=120)  does not work\n",
      "\n",
      "3\n",
      "3681\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-05 16:46\n",
      "To be true, I am new to scikit-learn and to machine learning as well. This is what intrigues me a lot that for every function we find a lot (a lot) of arguments. I would have thought to simply use `**kwargs` and extract out the important information from it. But I think may be developers would have already given a good thought into this.\n",
      "\n",
      "1\n",
      "3682\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-12-05 17:00\n",
      "Even if scikit-learn is intuitive, @lesshaste do not forget to read the doc: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
      "\n",
      "1\n",
      "3683\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-12-05 17:09\n",
      "so if we still wish to have 2 clusters per classes\n",
      "you need at least `n_informative=6` minimum\n",
      "\n",
      "2\n",
      "3684\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-12-05 17:12\n",
      "I really want 120\n",
      "thanks\n",
      "\n",
      "2\n",
      "3685\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-12-05 17:17\n",
      "> To be true, I am new to scikit-learn and to machine learning as well. This is what intrigues me a lot that for every function we find a lot (a lot) of arguments. I would have thought to simply use `**kwargs` and extract out the important information from it. But I think may be developers would have already given a good thought into this.  You can refer to [this talk](https://www.youtube.com/watch?v=MQMbnhSthZQ) to see ONE of the problem of the kwarg :).\n",
      "\n",
      "1\n",
      "3686\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-05 17:24\n",
      "I haven't seen the video completely (but in between I liked to point out). But I have spent quite sometime with SymPy, and I definitely agree I used a lot of `*args, **kwargs`.\n",
      "I think I could say, that for a library having other libraries as dependencies it is better to use hard-coded arguments instead of `*args, **kwargs`.\n",
      "Thanks for the video, I can sleep tight now :)\n",
      "\n",
      "3\n",
      "3687\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-12-06 15:40\n",
      "Apologies for being slightly OT but...does anyone know what I am doing wrong with xgboost? It seems amazingly slow when you have a large number of classes.  https://bpaste.net/show/ef817a256658 shows the problem\n",
      "thanks to the wonderful scikit-learn make_classification\n",
      "whereas as RandomForestClassifier is still super fast\n",
      "\n",
      "3\n",
      "3688\n",
      "5623dda816b6c7089cb76c0f\n",
      "2017-12-06 16:51\n",
      "Hi all. First of all sorry this question since it is not a sci-kit learn related, but I thought someone can help me out. I have some pet projects that Id like to start, but I am having some troubles to start. Can anyone point me out? or bring some ideas? Both involve unsupervised learning.  1. Id like to classify students using some economic information such as income, number of family members, etc. The classification should the economic status, e.g. low resources, medium, rich. The problem is that I dont have any labeled data. Can I approach this problem in a unsupervised way? (I tried with k-means)  2. I want classify academic papers using a taxonomy. Many librarians tag their documents in a specific way. In most cases, this tagging is different for each library. Tagging documents with a specific taxonomy will help search engines to retrieve documents. However, I found a taxonomy called Unesco nomenclature, but I dont know a way to match a document with an element/elements of the taxonomy.\n",
      "\n",
      "1\n",
      "3689\n",
      "56f8122085d51f252abb1414\n",
      "2017-12-07 23:00\n",
      "Hello everyone. Anyone can point me to a mathematical proof of the objective function used in sklearn for the logistic regression? http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression Im trying to demonstrate it starting from the Bernoulli likelihood in which the probability of a success is a sigmoid function, however, the final likelihood expression that I arrive is not equivalent to sklearns.\n",
      "\n",
      "1\n",
      "3690\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-07 23:01\n",
      "@mirca there's nothing sklearn specific about this, this is *the* objective for binary logistic regression (with penalty)\n",
      "Elements of statistical learning, chapter 4.4\n",
      "https://web.stanford.edu/~hastie/ElemStatLearn/\n",
      "\n",
      "3\n",
      "3691\n",
      "541a528b163965c9bc2053de\n",
      "2017-12-07 23:05\n",
      "The objective function for binary classification logistic regression stems from the negative log likelihood of a linear parametrization the log odd ratio of the Bernoulli model.\n",
      "the linear parametrization (w.T . x + b)  is for the log odd ratio log(p / (1 - p)) instead of p directly, where p is the parameter of the Bernoulli function.\n",
      "I edited my comment as I made a mistake :)\n",
      "\n",
      "3\n",
      "3692\n",
      "56f8122085d51f252abb1414\n",
      "2017-12-07 23:09\n",
      "@amueller Im arriving exactly at expression (4.20) of the book you mention. Am I mistaken or (4.20) is different from http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression ? (disconsider the regularization/prior term)\n",
      "\n",
      "1\n",
      "3693\n",
      "56f8122085d51f252abb1414\n",
      "2017-12-07 23:14\n",
      "@ogrisel exactly, through these assumptions I arrive at (4.20) ^, which, to me, looks a little different from http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Perhaps I am missing out something?\n",
      "\n",
      "1\n",
      "3694\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-07 23:17\n",
      "I think you can simplify the 2 class case further.  let me catch up with the notation\n",
      "\n",
      "1\n",
      "3695\n",
      "541a528b163965c9bc2053de\n",
      "2017-12-07 23:20\n",
      "IIRC y_i take values in {-1, 1} for the negative and positive classes respectively. This is not clear in the scikit-learn doc.\n",
      "\n",
      "7\n",
      "3696\n",
      "56f8122085d51f252abb1414\n",
      "2017-12-07 23:36\n",
      "its not clear to me how (4.20) and sklearns docs are equivalent\n",
      "Hum, ok! That looks to be the answer\n",
      "thank you!\n",
      "Will do! =)\n",
      "\n",
      "4\n",
      "3697\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-07 23:37\n",
      "have you tried replacing y by -1 and 1 like ogrisel said?\n",
      "or maybe easier, try to expand sklearns doc for  y=-1 and y=1 and then try to match that to the two terms in 4.20\n",
      "\n",
      "2\n",
      "3698\n",
      "541a528b163965c9bc2053de\n",
      "2017-12-07 23:39\n",
      "Feel free to submit a pull request to improve the doc :)\n",
      "https://github.com/scikit-learn/scikit-learn/blob/master/doc/modules/linear_model.rst?utf8=%E2%9C%93#logistic-regression\n",
      "Just mentioning explicitly the -1 and +1 values for y_i might be enough.\n",
      "\n",
      "3\n",
      "3699\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2017-12-09 22:36\n",
      "I am new to open source development but have worked with python for many year. I have found an issue I would like to solve. https://github.com/scikit-learn/scikit-learn/issues/10279 I have read a fair bit on how to submit pull requests but I hesitate to do so because of my inexperience. Is anybody interested in doing a review of my code before I submit so that I do not burden the community at large?\n",
      "\n",
      "1\n",
      "3700\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-12-09 22:46\n",
      "@DrEhrfurchtgebietend until your are fixing the issue and try to follow the contributing guide as much as possible, you can submit the PR. The community will review the code such that it follows the scikit-learn standard :)\n",
      "\n",
      "1\n",
      "3701\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2017-12-09 22:49\n",
      "OK thanks. I will do my best. I am trying to get up to speed on something small then I plan to tackle the more complex issue I discussed with you before https://github.com/scikit-learn/scikit-learn/issues/9947\n",
      "\n",
      "1\n",
      "3702\n",
      "58bd4178d73408ce4f4eaf1e\n",
      "2017-12-11 11:33\n",
      "Hello Everyone,I am new to scikit-learn.I followed the advanced install instructions but when I run pytest sklearn. I get module not found error.So can anyone help me.\n",
      "\n",
      "1\n",
      "3703\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-11 16:11\n",
      "@DrEhrfurchtgebietend omg that name lol\n",
      "\n",
      "1\n",
      "3704\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2017-12-11 16:46\n",
      " By far my favorite word. German\n",
      "has most of my favorite words\n",
      "\n",
      "2\n",
      "3705\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-11 16:46\n",
      "@DrEhrfurchtgebietend I know, I'm german ;)\n",
      "\n",
      "1\n",
      "3706\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2017-12-11 16:49\n",
      "@amueller I had guessed based on your name. I use it in place of \"awesome\" just to get a reaction\n",
      "\n",
      "1\n",
      "3707\n",
      "5a1450bed73408ce4f7fa504\n",
      "2017-12-12 15:54\n",
      "Hello from Berlin!\n",
      "\n",
      "1\n",
      "3708\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-12 18:50\n",
      "How can I debug using the simple 'print' commands? This is what I tried:, I first put a `print( (blaaa, bllllaaaa))` in my code. Then if I run the tests using `pytest path/to/file.py` then I don't get output from the `print` statement, I get whatever was expected without the `print` statements. (I'm sure that running those test would definitely reach those `print` statements).  Has this got something to do with `*.pyx` files?\n",
      "\n",
      "1\n",
      "3709\n",
      "59d4f81ed73408ce4f789336\n",
      "2017-12-12 19:19\n",
      "Hello developers! Am a newbie. Can someone provide me instructions link to build & run scikit-learn from source code. Am messing up with things.\n",
      "nope\n",
      "sklearn/tests/test_docstring_parameters.py:150: AssertionError\n",
      "``1 failed, 8361 passed, 16 skipped, 5024 warnings in 293.63 seconds``\n",
      "m newbie. understanding issues and working is still tough despite working in python for long\n",
      "\n",
      "5\n",
      "3710\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-12 19:25\n",
      "I guess this http://scikit-learn.org/stable/developers/advanced_installation.html should probably work.\n",
      "\n",
      "1\n",
      "3711\n",
      "59d4f81ed73408ce4f789336\n",
      "2017-12-12 20:02\n",
      "Am not getting how to put the cloned code to work. Help\n",
      "\n",
      "1\n",
      "3712\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-12 20:17\n",
      "are you able to do `make test` (without errors) on root scikit-learn directory?\n",
      "\n",
      "1\n",
      "3713\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-12 20:24\n",
      "if you could paste into a pastebin or somrthing that might help\n",
      "\n",
      "1\n",
      "3714\n",
      "59d4f81ed73408ce4f789336\n",
      "2017-12-12 20:29\n",
      "https://pastebin.com/raw/1Q8PfyRf\n",
      "\n",
      "1\n",
      "3715\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-12 20:34\n",
      "@ai-coder don't worry about that, that looks like ups messing up.\n",
      "we need to fix that, though\n",
      "though master if working\n",
      "hm...\n",
      "so might be that you're mixing different installations?\n",
      "meant \"us messing up\" though I double checked, and it's correct\n",
      "\n",
      "6\n",
      "3716\n",
      "59d4f81ed73408ce4f789336\n",
      "2017-12-12 20:36\n",
      "working now\n",
      "\n",
      "1\n",
      "3717\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-12 20:36\n",
      "It says: `1 failed, 8361 passed, 16 skipped, 5024 warnings in 293.63 seconds`, it does contain 'passed' non-zero value, does it indicate ruling out mixing installations?\n",
      "\n",
      "4\n",
      "3718\n",
      "59d4f81ed73408ce4f789336\n",
      "2017-12-12 20:43\n",
      "what time will it take to get familiar with things over here?\n",
      "\n",
      "2\n",
      "3719\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-12 20:45\n",
      "well depends on what you mean with \"getting familiar with things over here\"... I've been doing machine learning for err.. 8 years and learn new stuff most days ;)\n",
      "\n",
      "4\n",
      "3720\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-12 20:48\n",
      "@amueller since you are here. Can you please answer this query of mine https://gitter.im/scikit-learn/scikit-learn?at=5a3024da540c78242db7aaaf ? (I think otherwise it might get lost upward)\n",
      "\n",
      "10\n",
      "3721\n",
      "56f8122085d51f252abb1414\n",
      "2017-12-13 02:54\n",
      "pytest -s -v name-of-file-to-test.py should be enough\n",
      "\n",
      "1\n",
      "3722\n",
      "57224267659847a7aff4ffce\n",
      "2017-12-13 07:26\n",
      "Is this page for development using scikit or contributing to the project or both?\n",
      "\n",
      "1\n",
      "3723\n",
      "58b47262d73408ce4f4d4113\n",
      "2017-12-13 07:31\n",
      "For any type of queries and suggestions related to scikit-learn.\n",
      "There's a special dev room for developers of the project.\n",
      "\n",
      "6\n",
      "3724\n",
      "58b47262d73408ce4f4d4113\n",
      "2017-12-13 07:37\n",
      ":smile:\n",
      "\n",
      "1\n",
      "3725\n",
      "5a1450bed73408ce4f7fa504\n",
      "2017-12-14 08:48\n",
      "Hi guys! I want to contribute to the scikit learn open source. I am new to this. Can anyone give me any direction as to where to start with?\n",
      "nice! a lot of information!! thanks  @ogrisel  !\n",
      "\n",
      "5\n",
      "3726\n",
      "541a528b163965c9bc2053de\n",
      "2017-12-14 08:49\n",
      "@ashish-ram have a look at the contributors guide: http://scikit-learn.org/stable/developers/contributing.html\n",
      "\n",
      "1\n",
      "3727\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-12-14 17:58\n",
      "@gxyd  `pytest  -s -v ` will display something only if the test fail. You can always put a `raise`after your printing :) or use a proper debugger maybe\n",
      "\n",
      "1\n",
      "3728\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-14 17:59\n",
      "No, I think it worked. None of the test failed, but I simply put a `print` statement and it worked just fine.\n",
      "I am thinking you confuse that with something else.\n",
      "\n",
      "2\n",
      "3729\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-12-14 18:04\n",
      "ups right, this is `-l` that does show only at `raise`\n",
      "\n",
      "1\n",
      "3730\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-14 18:05\n",
      "Seeing in documentation? (Refer me to it if you are).\n",
      "\n",
      "1\n",
      "3731\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-12-14 18:07\n",
      "`-l, --showlocals      show locals in tracebacks (disabled by default).`\n",
      "\n",
      "2\n",
      "3732\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-15 21:05\n",
      "How can I run just doctests using pytest? Searching over the web I reached https://github.com/scikit-learn/scikit-learn/pull/9697, but still it isn't clear as how to do that.\n",
      "Perhaps it is better if documented in http://scikit-learn.org/stable/developers/advanced_installation.html#testing  also?\n",
      "\n",
      "2\n",
      "3733\n",
      "564789be16b6c7089cbab8b7\n",
      "2017-12-16 20:20\n",
      "is there  a suggested replacement for the deprecated http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html ?\n",
      "or http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLasso.html ?\n",
      "\n",
      "2\n",
      "3734\n",
      "55d21ee30fc9f982beadabb8\n",
      "2017-12-19 13:24\n",
      "@lesshaste I think that you can check the discussion: https://github.com/scikit-learn/scikit-learn/issues/9657\n",
      "\n",
      "1\n",
      "3735\n",
      "5a37fc62d73408ce4f83e015\n",
      "2017-12-20 04:13\n",
      "Hello everyone, how can I help with this project?\n",
      "\n",
      "1\n",
      "3736\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-20 10:37\n",
      "Here in `make_blobs` http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html , what is centers? It says: ``` centers : int or array of shape [n_centers, n_features], optional      (default=3) The number of centers to generate, or the fixed center locations. ``` The number of centers to generate. I don't understand this.\n",
      "@MartinEliasQ see http://scikit-learn.org/stable/developers/contributing.html\n",
      "\n",
      "2\n",
      "3737\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-20 12:00\n",
      "I think I got some idea using its different values in plots. (centers of normal distribution)\n",
      "\n",
      "1\n",
      "3738\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2017-12-20 22:14\n",
      "Yep. The center of each blob (cluster)\n",
      "\n",
      "2\n",
      "3739\n",
      "54d4a1d6db8155e6700f853b\n",
      "2017-12-21 21:09\n",
      "@jorisvandenbossche can you do me a favor and explain https://github.com/pandas-dev/pandas/issues/18801 to me?\n",
      "or maybe @jnothman understands and can enlighten me\n",
      "\n",
      "2\n",
      "3740\n",
      "53eb987c107e137846baa89d\n",
      "2017-12-21 21:12\n",
      "I wonder if Joel ever comes here, always busy with issues/PR. :)\n",
      "\n",
      "1\n",
      "3741\n",
      "54b2524adb8155e6700e8a8e\n",
      "2017-12-22 07:25\n",
      "I sometimes forget to open gitter and dont see its notifications...\n",
      "\n",
      "1\n",
      "3742\n",
      "57379367c43b8c601972f35d\n",
      "2017-12-24 15:55\n",
      "hey\n",
      "\n",
      "1\n",
      "3743\n",
      "58b46af0d73408ce4f4d3f2c\n",
      "2018-01-01 06:50\n",
      "Does scikit learn have support for sparse Gaussian processes, like with variational learning?\n",
      "Is that a feature that's on the list?\n",
      "*to be implemented list\n",
      "\n",
      "3\n",
      "3744\n",
      "595a4f26d73408ce4f6b5a31\n",
      "2018-01-02 07:50\n",
      "Hi everyone.  I joined this room first time today,  nice to meet you all\n",
      "\n",
      "1\n",
      "3745\n",
      "5a4e181cd73408ce4f8611a6\n",
      "2018-01-04 12:04\n",
      "hi,\n",
      "\n",
      "1\n",
      "3746\n",
      "57379367c43b8c601972f35d\n",
      "2018-01-06 03:46\n",
      "Hello\n",
      "\n",
      "1\n",
      "3747\n",
      "58faf245d73408ce4f5a2675\n",
      "2018-01-08 07:50\n",
      "hey,\n",
      "\n",
      "1\n",
      "3748\n",
      "58faf245d73408ce4f5a2675\n",
      "2018-01-08 07:51\n",
      "i have some doubt in estimator_check can anyone plz explain the purpose of that file\n",
      "\n",
      "1\n",
      "3749\n",
      "53eb987c107e137846baa89d\n",
      "2018-01-08 12:26\n",
      "I think it contains various routines to check the correctness of input to the various estimators. In particular see: http://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.check_estimator.html\n",
      "\n",
      "1\n",
      "3750\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-09 01:26\n",
      "sklearn/utils/estimator_checks.py includes assertions run on each estimator to check that it behaves according to Scikit-learns conventions.\n",
      "\n",
      "1\n",
      "3751\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-09 02:55\n",
      "@ssequeira, I dont think we have any planned enhancements to gaussian processes except for easier access to a linear kernel in #8373\n",
      "\n",
      "1\n",
      "3752\n",
      "53eb987c107e137846baa89d\n",
      "2018-01-09 06:37\n",
      "@jnothman (as you are aware) I'm currently working on https://github.com/scikit-learn/scikit-learn/pull/10083 and the other remaining PR is https://github.com/scikit-learn/scikit-learn/pull/10273 (has been inactive for sometime, but I'll get to that), do you think this would be a good time for me to get to medium-level issues?  If the answer to above question is somwhat yes, then, I've been searching for a medium-level issue (non-documentation issue), one which will require a few months of work. Better if there are a series of medium-level issues on the same topic that you guys need someone to contribute on?\n",
      "\n",
      "1\n",
      "3753\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-10 06:08\n",
      "Sorry, I cant think of anything right now. I think you would benefit from a few more easy issues. https://github.com/scikit-learn/scikit-learn/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+-label%3A%22good+first+issue%22+ might help, but as you know, were much better at adding the <unconvertable> help wanted <unconvertable> tag than removing it. Do you want to help me build a bot to manage that?? I have ideas...\n",
      "Ping @gxyd\n",
      "\n",
      "2\n",
      "3754\n",
      "53eb987c107e137846baa89d\n",
      "2018-01-10 10:18\n",
      "Yes, I'll am willing to work on that.\n",
      "Will you open an issue for that?\n",
      "\n",
      "2\n",
      "3755\n",
      "55fc44590fc9f982beb11bbf\n",
      "2018-01-10 15:53\n",
      "Please let me know if any assistance is needed\n",
      "\n",
      "2\n",
      "3756\n",
      "55fc44590fc9f982beb11bbf\n",
      "2018-01-10 19:37\n",
      "https://travis-ci.org/scikit-learn/scikit-learn/jobs/327310192\n",
      "Why can't I use scipy.sparse.random?\n",
      "Works fine locally\n",
      "\n",
      "3\n",
      "3757\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-01-10 20:15\n",
      "scipy.sparse.random was not there in scipy 0.13.3 which is the minimal version required by scikit-learn\n",
      "\n",
      "1\n",
      "3758\n",
      "55fc44590fc9f982beb11bbf\n",
      "2018-01-11 03:09\n",
      "Alroght thanks I'll change it with something else\n",
      "Alright *\n",
      "\n",
      "2\n",
      "3759\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-11 05:42\n",
      "#9099 is also a fairly large, non-core project... Designing a better way to print out estimators.\n",
      "\n",
      "1\n",
      "3760\n",
      "55fc44590fc9f982beb11bbf\n",
      "2018-01-11 15:29\n",
      "Is github down?\n",
      "\n",
      "1\n",
      "3761\n",
      "58faf245d73408ce4f5a2675\n",
      "2018-01-11 15:51\n",
      " @maykulkarni nope\n",
      "\n",
      "1\n",
      "3762\n",
      "55fc44590fc9f982beb11bbf\n",
      "2018-01-11 15:52\n",
      "It's working now, it was d os n\n",
      "It was down a while ago\n",
      "\n",
      "2\n",
      "3763\n",
      "5a477efed73408ce4f85632f\n",
      "2018-01-11 16:09\n",
      "@maykulkarni - Were you trying to access using Jio internet because I faved the same problem\n",
      "*faced\n",
      "\n",
      "2\n",
      "3764\n",
      "55fc44590fc9f982beb11bbf\n",
      "2018-01-11 16:09\n",
      "No github was down https://status.github.com/messages\n",
      "\n",
      "1\n",
      "3765\n",
      "5a429497d73408ce4f84e60d\n",
      "2018-01-11 17:10\n",
      "guys does anyone have a cool idea for a simple machine learning project?\n",
      "using sciket-learn which is never done before?\n",
      "\n",
      "2\n",
      "3766\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-01-12 05:44\n",
      "Do you mean something which has never been done with scikit-learn or something that has never been done in general? If the former then just look for feature request. If the latter I think it is safe to say all simple things have been done long ago.\n",
      "\n",
      "1\n",
      "3767\n",
      "5a429497d73408ce4f84e60d\n",
      "2018-01-12 05:45\n",
      "Yeah its all done\n",
      "\n",
      "1\n",
      "3768\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-01-12 05:49\n",
      "Well the intersection of novel and simple is nearly nonexistant in most developed fields\n",
      "\n",
      "1\n",
      "3769\n",
      "58bd1933d73408ce4f4ea7d6\n",
      "2018-01-12 12:13\n",
      "am i supposed to know the internal functioning of all the scikit learn models?\n",
      "because there are too many models\n",
      "i just did the andrew ng course on coursera\n",
      "he taught a few basic things.\n",
      "\n",
      "4\n",
      "3770\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-01-13 05:28\n",
      "Supposed to by who?\n",
      "\n",
      "1\n",
      "3771\n",
      "53eb987c107e137846baa89d\n",
      "2018-01-13 05:49\n",
      "I'm currently getting some errors in my scikit-learn branch (made some changes). Running tests via pytest, produces some output on terminal, but since numpy arrays contains large number of elements, they are printed with a threshold. ``` $ pytest sklearn/metrics/tests/test_common.py ============================================================= test session starts =============================================================  sklearn/metrics/tests/test_common.py:917: in check_averaging     y_pred_binarize, is_multilabel) sklearn/utils/testing.py:311: in wrapper     return fn(*args, **kwargs) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  metric = <functools.partial object at 0x7fa62c5d5b50> y_true = array([0, 1, 0, 1, 1, 2, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 0, 1, 0,   ...0, 2, 0, 1, 1, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0,        1, 1, 2, 0]) y_pred = array([0, 1, 0, 1, 2, 2, 0, 1, 1, 1, 1, 2, 2, 2, 0, 2, 1, 0, 1, 2, 0, 0, 2,   ...0, 0, 0, 2, 0, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 2, 0,        2, 0, 1, 2]) y_true_binarize = array([[1, 0, 0],        [0, 1, 0],        [1, 0, 0],        [0, 1, 0],       ...0, 0],        [0, 1, 0],        [0, 1, 0],        [0, 0, 1],        [1, 0, 0]]) ``` Now the problem is how can I print the complete array's (i.e with `threshold=np.nan`).  I tried to put `np.set_printoptions(threshold=np.nan)` in concerned test file as well as in skearn/utils/testing.py but with no success. Do I need some pytest option?\n",
      "See here `y_true` is printed with some threshold size.\n",
      "Well, one thing I can do is, use the `print` statement and then use `pytest` with `-s - v` option. But I don't think this is a good solution.\n",
      "\n",
      "3\n",
      "3772\n",
      "5717ab2f659847a7aff3b583\n",
      "2018-01-13 12:21\n",
      "hi\n",
      "\n",
      "1\n",
      "3773\n",
      "55fc44590fc9f982beb11bbf\n",
      "2018-01-13 14:51\n",
      "#10443 Ready to MRG, needs review.\n",
      "\n",
      "1\n",
      "3774\n",
      "58b47262d73408ce4f4d4113\n",
      "2018-01-13 17:22\n",
      "Is scikit-learn going to take part in GSoC 2018? If yes, I saw a couple of projects in GSoC 2017 wiki. Since, there were no participant last year, is that project list still valid or are you guys going to come up with some other projects? I would be willing to explore available/possible projects.\n",
      "\n",
      "1\n",
      "3775\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-15 00:19\n",
      "@gxyd, try `np.set_printoptions(threshold=np.inf)` rather than `np.nan`, or perhaps `np.set_printoptions(threshold=np.iinfo(np.int).max)`\n",
      "\n",
      "1\n",
      "3776\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-15 00:24\n",
      "@thechargedneutron, our problem with GSoC is assuring mentor availability, as well as developing well-defined projects and students we are confident will complete the work without too much hand-holding. We have much less core dev availability than a few years ago, and many fewer well-defined, coherent project options that do not require substantial expertise. For example, we have a few big API things that could do with attention (see https://github.com/scikit-learn/scikit-learn/projects/9), but they mostly require a lot of familiarity with Scikit-learn API design issues. At this stage of maturity we have less need for an <unconvertable> implement this kind of algorithm <unconvertable> or <unconvertable> optimise that <unconvertable> kind of project. If a compelling student/project candidate approached, I think we would consider it.\n",
      "\n",
      "1\n",
      "3777\n",
      "58b47262d73408ce4f4d4113\n",
      "2018-01-15 05:51\n",
      "@jnothman  Thanks for the reply. I am indeed interested in working in API things but need a good understanding of it. Since there's some time before the GSoC, I would like to orient myself in the direction of the current requirements. Can you advise me on how to acquaint me with the current issues and possible solutions. Thanks :smile:\n",
      "\n",
      "1\n",
      "3778\n",
      "53eb987c107e137846baa89d\n",
      "2018-01-16 08:56\n",
      "@jnothman neither of them work for me. BTW, which files should I put them in? I tried to put it both files sklearn/utils/testing.py and sklearn/metrics/tests/test_common.py.\n",
      "\n",
      "1\n",
      "3779\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-16 09:36\n",
      "Modify conftest.py if youre doing this for testing??\n",
      "\n",
      "1\n",
      "3780\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-16 12:34\n",
      "Is there a contributor who wants to work on an Easy issue with a moderate amount of work? #9726: create a new sklearn.impute module for imputation\n",
      "\n",
      "1\n",
      "3781\n",
      "58b47262d73408ce4f4d4113\n",
      "2018-01-16 12:36\n",
      "I am relatively free. I have currently #10206 which may get completed by the end of this week. So, I am willing to take it.\n",
      "\n",
      "1\n",
      "3782\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-16 12:54\n",
      "For a longer project (potentially a GSoC), contributors might be interested in resampling API: https://github.com/scikit-learn/scikit-learn/issues/3855#issuecomment-357949997\n",
      "\n",
      "1\n",
      "3783\n",
      "56333d0d16b6c7089cb8d5c7\n",
      "2018-01-16 14:05\n",
      "@jnothman I can work on #9726.\n",
      "\n",
      "1\n",
      "3784\n",
      "589208e2d73408ce4f4770c4\n",
      "2018-01-17 22:49\n",
      "[![Screenshot from 2018-01-17 22-43-24.png](https://files.gitter.im/scikit-learn/scikit-learn/aUpR/thumb/Screenshot-from-2018-01-17-22-43-24.png)](https://files.gitter.im/scikit-learn/scikit-learn/aUpR/Screenshot-from-2018-01-17-22-43-24.png)\n",
      "\n",
      "1\n",
      "3785\n",
      "589208e2d73408ce4f4770c4\n",
      "2018-01-17 22:50\n",
      "Hi everyone. an SVM model i created returns an error when i fit with my x and y data. below is a print out of my x and y values and the error message. I transformed the y values to onehotencoder values as well.\n",
      "the same error is generated when i use random forest and naive bayes\n",
      "\n",
      "2\n",
      "3786\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-01-17 23:22\n",
      "Don't OneHotEncode `y`\n",
      "and it will work\n",
      "\n",
      "2\n",
      "3787\n",
      "55fc44590fc9f982beb11bbf\n",
      "2018-01-18 04:09\n",
      "Just curious,\n",
      "Should every bug fix / enhancement go into what's new?\n",
      "\n",
      "2\n",
      "3788\n",
      "589208e2d73408ce4f4770c4\n",
      "2018-01-18 08:51\n",
      "@glemaitre: i the data is a multiclass data so i labeled the classes 0,1 ,2. will is it okay to use them as it is without onehotencoding?\n",
      "\n",
      "1\n",
      "3789\n",
      "589208e2d73408ce4f4770c4\n",
      "2018-01-18 09:34\n",
      "i encoded them as [001],[010] and [100] and that is what generated the error\n",
      "\n",
      "1\n",
      "3790\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-01-19 13:14\n",
      "http://scikit-learn.org/stable/modules/multiclass.html\n",
      "@maykulkarni most of the time yes\n",
      "@jotes35 SVM does not handle multi-label. But I am not sure that you have to hot one encode\n",
      "\n",
      "3\n",
      "3791\n",
      "55fc44590fc9f982beb11bbf\n",
      "2018-01-19 13:17\n",
      "#10478 #10443 completed, needs review\n",
      "\n",
      "1\n",
      "3792\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-01-19 13:39\n",
      "@maykulkarni I put 2 additional tests to do because I am almost sure that the solution in #10443 is not working if we don't introduce a dtype to the transformer as proposed [here](https://github.com/glemaitre/scikit-learn/commit/4e5e1f06ed1b90c0ffb00584db81a4e8c77e1dff)\n",
      "\n",
      "2\n",
      "3793\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-01-22 20:25\n",
      "Anybody know if there is a wrapper or python equivalent to RankLib\n",
      "?\n",
      "\n",
      "2\n",
      "3794\n",
      "59afadccd73408ce4f748432\n",
      "2018-01-24 22:12\n",
      "@DrEhrfurchtgebietend  It seems to be LEROT in Python. You have more details here : https://www.quora.com/What-are-the-alternatives-to-RankLib\n",
      "\n",
      "1\n",
      "3795\n",
      "551d48fa15522ed4b3de4121\n",
      "2018-01-26 15:14\n",
      "Guys, the GaussianMixture model's score_samples() method return log-probabilities, I'm not sure how to calculate regular/percentage-wise probabilities from these, can anyone enlighten me?\n",
      "\n",
      "1\n",
      "3796\n",
      "57af6d0540f3a6eec05f5297\n",
      "2018-01-27 08:19\n",
      "how to make spacemacs work behind https_proxy?\n",
      "\n",
      "1\n",
      "3797\n",
      "57af6d0540f3a6eec05f5297\n",
      "2018-01-27 08:26\n",
      "I tried `(setq url-proxy-services         '((\"no_proxy\" . \"^\\\\(localhost\\\\|127.*\\\\)\")           (\"http\" . \"127.0.0.1:1087\")           (\"https\" . \"127.0.0.1:1087\")))   )`,but it didnt work\n",
      "\n",
      "1\n",
      "3798\n",
      "57af6d0540f3a6eec05f5297\n",
      "2018-01-27 08:41\n",
      "Sorry,i send my message to wrong room,sorry again\n",
      "\n",
      "1\n",
      "3799\n",
      "598e3be6d73408ce4f710e38\n",
      "2018-01-27 08:43\n",
      "hi what are you doing now\n",
      "\n",
      "1\n",
      "3800\n",
      "54b2524adb8155e6700e8a8e\n",
      "2018-01-28 10:19\n",
      "@JVanloofsvelt np.exp is the inverse of np.log... Just apply that to get probabilities.\n",
      "\n",
      "1\n",
      "3801\n",
      "5a6e6050d73408ce4f8a8f8b\n",
      "2018-01-28 23:45\n",
      "hello world\n",
      "\n",
      "1\n",
      "3802\n",
      "551d48fa15522ed4b3de4121\n",
      "2018-01-30 09:39\n",
      "@jnothman thanks!\n",
      "\n",
      "1\n",
      "3803\n",
      "551d48fa15522ed4b3de4121\n",
      "2018-01-30 09:40\n",
      "using GaussianMixtureModel.predict I'm able to guess what cluster/distribution the sample(s) belong(s) to, but how do I get the probability of the given sample occurring given that distribution (not a weighted average of all clusters)?\n",
      "\n",
      "1\n",
      "3804\n",
      "59e7b052d73408ce4f7aa075\n",
      "2018-01-31 19:59\n",
      "I would like to include data loading/augmentation as the first step of a pipeline, so I can optimize parameters. But I see no way of doing so because of the signature of TransformerMixin (no place for class labels to be returned) . Any ideas?\n",
      "\n",
      "1\n",
      "3805\n",
      "5a314ce4d73408ce4f83281f\n",
      "2018-02-01 18:59\n",
      "Hi, anyone replicated IDL SMOOTH function in python successfully ? I infact want to use smooth2 function adopted by JHUAPL (https://github.com/callumenator/idl/blob/master/external/JHUAPL/SMOOTH2.PRO) which uses smooth function of IDL. Eagerly waiting !\n",
      "\n",
      "1\n",
      "3806\n",
      "58b46af0d73408ce4f4d3f2c\n",
      "2018-02-02 01:00\n",
      "As  a statistics graduate student, I'd love to add inferential tools to scikit-learn.\n",
      "\n",
      "1\n",
      "3807\n",
      "58b46af0d73408ce4f4d3f2c\n",
      "2018-02-02 01:58\n",
      "Would it be a worthwhile effort to implement conformal inference (http://www.stat.cmu.edu/~ryantibs/papers/conformal.pdf) ?\n",
      "\n",
      "1\n",
      "3808\n",
      "5921d334d73408ce4f612e46\n",
      "2018-02-03 03:16\n",
      "Hello, I'm trying to understand the RCV1 dataset, so I can run some analysis on it. Please has anyone worked with it before?\n",
      "I'd also be grateful to receive help in understanding how to work with CSR\n",
      "\n",
      "2\n",
      "3809\n",
      "5a764508d73408ce4f8b90d6\n",
      "2018-02-03 23:27\n",
      "Any good book for machine learning with python?\n",
      "\n",
      "1\n",
      "3810\n",
      "5a774845d73408ce4f8ba7bb\n",
      "2018-02-04 17:55\n",
      "hello\n",
      "\n",
      "1\n",
      "3811\n",
      "5a774845d73408ce4f8ba7bb\n",
      "2018-02-04 17:56\n",
      "i want to ask which is more advantageous to use dummy variables or one hot encoding?\n",
      "\n",
      "1\n",
      "3812\n",
      "59b159b1d73408ce4f74c00d\n",
      "2018-02-09 08:02\n",
      "@itsmegaurav check out https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn\n",
      "\n",
      "1\n",
      "3813\n",
      "55a487245e0d51bd787b4e45\n",
      "2018-02-10 17:30\n",
      "@xidorn5 I can't speak for the sklearn team, but generally sklearn is intended to target the most widespread, widely-cited, well-established methods for the core package, and to allow third-party packages a lot of freedom in implementing less-established advancements from the literature.\n",
      "\n",
      "1\n",
      "3814\n",
      "58dabed4d73408ce4f5463b2\n",
      "2018-02-13 09:23\n",
      "@Pimp_Fada_twitter https://books.google.com/books/about/Introduction_to_Machine_Learning_with_Py.html\n",
      "\n",
      "1\n",
      "3815\n",
      "58dabed4d73408ce4f5463b2\n",
      "2018-02-13 09:25\n",
      "Introduction to Machine Learning with Python: A Guide for Data Scientists https://www.amazon.com/dp/1449369413/ref=cm_sw_r_cp_apa_47QGAbZNXX58Z\n",
      "\n",
      "1\n",
      "3816\n",
      "5a867bd5d73408ce4f8d6091\n",
      "2018-02-16 06:43\n",
      "hi i am working on ml that determine the emergency situation in vehicle(cars) by getting the input from the sensors of the vehicle . so i need a real time OBD2 logged csv file for training in the ml. any help ?\n",
      "\n",
      "1\n",
      "3817\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2018-02-20 10:18\n",
      "Hello, I tried to use PredefinedSplit as CV for GridSearchCV. I have two sets, first is for training and second is for validation. I made indices array so that for training set indices I put -1 and 0 for testing set. I get reasonable results to grid.cv_results_, but when I test it with the second (testing) set, I get .99 which is clearly not correct. Why is that?\n",
      "\n",
      "1\n",
      "3818\n",
      "57a061aa40f3a6eec05d8d26\n",
      "2018-02-22 00:02\n",
      "I get warning that some columns are collinear. Is there any utility to find out which columns they are?\n",
      "\n",
      "1\n",
      "3819\n",
      "572f08a2c43b8c601971bc45\n",
      "2018-02-26 09:19\n",
      "Hello, I am considering applying for a [GSoC project](https://github.com/rstats-gsoc/gsoc2018/wiki/SAGA-sparse-linear-models) with the aim of adapting the SAGA algorithm for R, possibly by porting existing code from scitkit-learn. The project, however, needs another co-mentor and I am wondering if there is any scitkit-developer here that would interested in participating? You can message me directly or contact Toby, who is signed up as mentor.\n",
      "\n",
      "1\n",
      "3820\n",
      "56d53608e610378809c45c8c\n",
      "2018-02-26 23:01\n",
      "Hello, I want to use the http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html but with the decision boundary always going through the origin. The API doesn't allow this so I wonder if there is a way around this.\n",
      "\n",
      "1\n",
      "3821\n",
      "56d53608e610378809c45c8c\n",
      "2018-02-27 04:14\n",
      "I guess I can just use the normal SVM and mirror one example along the origin.\n",
      "\n",
      "1\n",
      "3822\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-03-03 19:40\n",
      "I would like to cluster about 100,000  vectors according to the Pearson distance https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#Pearson's_distance  . Which clustering method would support that?\n",
      "many of them seem to want the Euclidean distance\n",
      "\n",
      "2\n",
      "3823\n",
      "59afadccd73408ce4f748432\n",
      "2018-03-03 19:48\n",
      "@lesshaste\n",
      "\n",
      "2\n",
      "3824\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-03-03 19:50\n",
      "@guyome80  I am hoping you might be about to tell me about clustering...\n",
      "\n",
      "2\n",
      "3825\n",
      "570d14d6187bb6f0eadf1582\n",
      "2018-03-03 19:53\n",
      "You can try k-medoids\n",
      "\n",
      "1\n",
      "3826\n",
      "59afadccd73408ce4f748432\n",
      "2018-03-03 19:53\n",
      "I was thinking about a preprocessing method (Standardscaler) instead of clustering...\n",
      "\n",
      "1\n",
      "3827\n",
      "570d14d6187bb6f0eadf1582\n",
      "2018-03-03 19:54\n",
      "K-medoids isnt released yet, but it support any distance metric you wish. you might copy paste the class or build from the branch. The complexity is worse then k means, for 100k should be good\n",
      "\n",
      "1\n",
      "3828\n",
      "57379367c43b8c601972f35d\n",
      "2018-03-03 23:01\n",
      "hi\n",
      "\n",
      "1\n",
      "3829\n",
      "5a70791bd73408ce4f8ad6e1\n",
      "2018-03-05 11:53\n",
      "Hi, further on my spell correct algo: Is there a function that gives you the number of occurences of a word (BoW-style)? I would expect that frequent words (preposition) are often in the neighbourhood of other words (nouns)\n",
      "@aph61 Further to my question  before, can you also work with word vectors, like, count the number of vectors for a given word? Makes actually more sense\n",
      "\n",
      "2\n",
      "3830\n",
      "59ff9cd4d73408ce4f7d44f2\n",
      "2018-03-06 17:53\n",
      "Hi guys\n",
      "\n",
      "1\n",
      "3831\n",
      "59ff9cd4d73408ce4f7d44f2\n",
      "2018-03-06 17:54\n",
      "What's the... eh, correct way to test C extensions without exposing the original class?\n",
      "oops, wrong room, sorry :worried:\n",
      "\n",
      "2\n",
      "3832\n",
      "5a895aedd73408ce4f8da2b8\n",
      "2018-03-07 05:27\n",
      "Hello all! I just finished compiling a survey with more than 350 open source software project members which, together with some communication theory, we used to define a set of **best practices and guidelines for using Gitter** (in fact, it probably applies to *any chat-like platform*).  If anybody would be interest on it, or in applying them in this community, or your other projects, there is a [small survey](http://bit.ly/survey-and-guidelines) you can fill to get the guidelines. The survey takes something between just *10 seconds to a maximum of 2 minutes to fill* and it is intended to help us validate the guidelines in the future.  There is also a small [article](https://medium.com/@fabiomolinar/validating-chat-communication-tools-guidelines-and-best-practices-fb8852f319da) I wrote with a really short description about the study; in case you would be curious about it.  Of course, feel free to send the [survey link](http://bit.ly/survey-and-guidelines) to anyone you would like to share these guidelines with.\n",
      "\n",
      "1\n",
      "3833\n",
      "57433a8cc43b8c6019747d9e\n",
      "2018-03-07 12:04\n",
      "Guys I'm new to contributing to open source projects and I'm trying to resolve issue #10689, the issue is about replacing a depreciated function with new function But when I try to replace the depreciated issue with new issue, and create a PR, my commits are facing merge conflicts because of circle ci and lgtome tools  Can anyone please help me?\n",
      "\n",
      "1\n",
      "3834\n",
      "57007657187bb6f0eadd96e8\n",
      "2018-03-09 17:53\n",
      "try a rebase\n",
      "\n",
      "1\n",
      "3835\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-03-13 16:35\n",
      "Why are partial dependence plots not in the units of the prediction variable? https://stackoverflow.com/questions/49247796/understanding-partial-dependence-for-gradient-boosted-regression-trees\n",
      "\n",
      "1\n",
      "3836\n",
      "58412a9ed73408ce4f3a3004\n",
      "2018-03-20 17:56\n",
      "Hi everyone! I have been looking in to multi-output regression the last few days. I know that the scikit-learn package offers a class called multioutput regressor. This class will give every algorithm multi-output support by fitting model for every variable. My questions : Does this package take the relationship between the input variables into account? Which algorithm would work better , a model with a multi-output regression or without (some models support it natively ?\n",
      "\n",
      "1\n",
      "3837\n",
      "5796009e40f3a6eec05c5aa0\n",
      "2018-03-21 17:53\n",
      "@satishjasthi , I would use Pycharm if the command line version of git is giving you trouble.  Then, do the [visual merge conflict resolution](https://www.jetbrains.com/help/idea/resolving-conflicts.html).\n",
      "\n",
      "1\n",
      "3838\n",
      "57e0cda740f3a6eec0662f30\n",
      "2018-03-22 12:14\n",
      "How many documents scikit k means clustering can process at a time?\n",
      "\n",
      "1\n",
      "3839\n",
      "5ab421aad73408ce4f92b961\n",
      "2018-03-22 21:44\n",
      "Just discover gitter. Nice room to join :-)\n",
      "\n",
      "1\n",
      "3840\n",
      "59e7b052d73408ce4f7aa075\n",
      "2018-03-23 22:09\n",
      "Do \"train\" and \"fit\" mean exactly the same thing or do they have different connotations?\n",
      "\n",
      "1\n",
      "3841\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-03-23 22:10\n",
      "@bsheline they mean the same. I think train mostly comes from the neural net community, while fit comes more from statistics\n",
      "but I'm not sure\n",
      "\n",
      "2\n",
      "3842\n",
      "58f51045d73408ce4f5906f0\n",
      "2018-03-24 02:24\n",
      "i know this is python but does anyone know how to do C#?\n",
      "\n",
      "1\n",
      "3843\n",
      "57cdefdf40f3a6eec063af10\n",
      "2018-03-24 20:12\n",
      "hi all\n",
      "\n",
      "1\n",
      "3844\n",
      "5ab7916ad73408ce4f93091f\n",
      "2018-03-25 12:15\n",
      "hi\n",
      "send help\n",
      "\n",
      "2\n",
      "3845\n",
      "5ab8e610d73408ce4f9327af\n",
      "2018-03-26 12:23\n",
      "Hi guys\n",
      "\n",
      "1\n",
      "3846\n",
      "5842913cd73408ce4f3a6993\n",
      "2018-03-26 18:36\n",
      "<unconvertable> <unconvertable> <unconvertable> <unconvertable>\n",
      "\n",
      "1\n",
      "3847\n",
      "58b72049d73408ce4f4dc16c\n",
      "2018-03-29 17:39\n",
      "Hey there\n",
      "\n",
      "1\n",
      "3848\n",
      "5abd978cd73408ce4f93b55a\n",
      "2018-03-30 01:54\n",
      "Hey there\n",
      "\n",
      "1\n",
      "3849\n",
      "5abeb33fd73408ce4f93d1a6\n",
      "2018-03-30 22:09\n",
      "hello guys im new in data science can any one please guide me with some books or anything that i should learn\n",
      "\n",
      "1\n",
      "3850\n",
      "57c1cad340f3a6eec061a142\n",
      "2018-03-31 14:03\n",
      "hi guys, I got some question about dimensional reduction using LDA\n",
      "\n",
      "1\n",
      "3851\n",
      "5ab421aad73408ce4f92b961\n",
      "2018-03-31 14:10\n",
      "@idahmed One of my latest favourites is Hands on machine learning with Scikit-Learn and Tensor flow by Aurelien Geron\n",
      "@hndr91 there is an issue opened at Github with some info to take into account\n",
      "\n",
      "2\n",
      "3852\n",
      "5a873b5fd73408ce4f8d784b\n",
      "2018-04-01 14:15\n",
      "@idahmed : also try this https://developers.google.com/machine-learning/crash-course/\n",
      "\n",
      "1\n",
      "3853\n",
      "57e0cda740f3a6eec0662f30\n",
      "2018-04-02 05:17\n",
      "I have to classify docs into two categories and if the docs does not belong to these two, then i have to identify them in other category i have training data for two. Classes What algo or approach i can use?\n",
      "\n",
      "1\n",
      "3854\n",
      "570665ba187bb6f0eade50c8\n",
      "2018-04-03 08:33\n",
      "Hi , I am a beginner and I try to predict an anomaly in different systems, I have different parameters, which I can use to do more precise prediction. I have a general question, is there a way to do prediction without know with parameters should I use ? sometimes, I don't have all parameters values, so I am thinking if there is a away to do prediction with a minimum parameters\n",
      "\n",
      "1\n",
      "3855\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-04-03 18:14\n",
      "Google feature selection\n",
      "\n",
      "1\n",
      "3856\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-04-03 19:37\n",
      "@naaioa @DrEhrfurchtgebietend more like imputation?\n",
      "\n",
      "1\n",
      "3857\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-04-03 19:39\n",
      "@idahmed the \"python data science handbook\" is a great introduction and available online for free and in notebook format.\n",
      "@mcasl if I may ask, have you looked at mine as well? I feel the two are somewhat similar in scope, but tbh I haven't looked into Aurelien's book in that much detail\n",
      "\n",
      "2\n",
      "3858\n",
      "5ab421aad73408ce4f92b961\n",
      "2018-04-05 19:35\n",
      "@amueller\n",
      "\n",
      "1\n",
      "3859\n",
      "5ab421aad73408ce4f92b961\n",
      "2018-04-05 19:37\n",
      "@amueller Sure! I have it on my to do list. Just browsed it and seems an impressive work. Eager to read it\n",
      "@amueller Besides of books, I would recommend anyone visiting the materials you have posted regarding your University course on machine learning. I specially liked  the fact of introducing unitary tests, continuos integration engines and git as essentials. Keep up the good work!\n",
      "\n",
      "2\n",
      "3860\n",
      "551c051b15522ed4b3de2fea\n",
      "2018-04-06 21:56\n",
      "hi all\n",
      "\n",
      "1\n",
      "3861\n",
      "551c051b15522ed4b3de2fea\n",
      "2018-04-06 21:56\n",
      "I am doing some testing with gradient descent and labeled data. For X, I have selected two features: one sort of nonsense, and one the EXACT (binary) label. I train on a 66% subset of this data my precision/recall for `y = 1` is 0.00. how is this possible?\n",
      "```              precision    recall  f1-score   support            0       0.94      1.00      0.97      4194           1       0.00      0.00      0.00       247  avg / total       0.89      0.94      0.92      4441 ```\n",
      "\n",
      "2\n",
      "3862\n",
      "589208e2d73408ce4f4770c4\n",
      "2018-04-09 23:46\n",
      "Hi,all I am using scikit 0.19.1 I generated a training model using random forest and saved the model. These were done on ubuntu 16.01 x86_64. I copied the model to a windows 10 64 bit machine and wanted to reuse the saved model. But unfortunately i get the following Traceback (most recent call last): File \"C:\\Users\\PC\\Documents\\Vincent\\nicholas\\feverwizard.py.py\", line 19, in rfmodel=joblib.load(modelfile) File \"C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py\", line 578, in load obj = _unpickle(fobj, filename, mmap_mode) File \"C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py\", line 508, in _unpickle obj = unpickler.load() File \"C:\\Python27\\lib\\pickle.py\", line 864, in load dispatchkey File \"C:\\Python27\\lib\\pickle.py\", line 1139, in load_reduce value = func(*args) File \"sklearn\\tree_tree.pyx\", line 601, in sklearn.tree._tree.Tree.cinit ValueError: Buffer dtype mismatch, expected 'SIZE_t' but got 'long long'  What could be happening? Is it because of a switch from ubuntu to windows? However i am able to reuse the model in my ubuntu.\n",
      "\n",
      "1\n",
      "3863\n",
      "5a4f6d1cd73408ce4f864c42\n",
      "2018-04-10 13:48\n",
      "@jotes35 i think the issue isn't fixed.  the only feasible solution is retrain it over there in the new architecture, and yes it seems to be because of different architectures, not because it's windows and ubuntu, it happens even on ubuntu and another version of ubuntu\n",
      "have a look at this https://github.com/scikit-learn/scikit-learn/issues/7891\n",
      "\n",
      "2\n",
      "3864\n",
      "551c051b15522ed4b3de2fea\n",
      "2018-04-10 17:37\n",
      "in general, what are good starting points for `min_samples_leaf` in random forest classifiers? I find that `0.005` gives me decent results but Im afraid Im overfitting.\n",
      "\n",
      "1\n",
      "3865\n",
      "589208e2d73408ce4f4770c4\n",
      "2018-04-11 02:45\n",
      "Thanks @greed2411 . Retraining on the new architecture works.\n",
      "\n",
      "1\n",
      "3866\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-04-11 18:06\n",
      "I have a few thousand points on a line and most of them are in a dense part with a few hundred spread out more widely. What is a good way to find the dense part?\n",
      "This looks like a clustering problem with one cluster plus noise\n",
      "\n",
      "2\n",
      "3867\n",
      "5acfdfffd73408ce4f95738d\n",
      "2018-04-12 22:43\n",
      "How to get weight for signs(Perceptron, binary classification)? In what function they are?\n",
      "\n",
      "1\n",
      "3868\n",
      "5886855fd73408ce4f4586d8\n",
      "2018-04-13 05:13\n",
      "hello everyone\n",
      "\n",
      "1\n",
      "3869\n",
      "56bb7a56e610378809c0cb2c\n",
      "2018-04-13 05:13\n",
      "`D3XT3R` HoL\n",
      "`D3XT3R` a\n",
      "\n",
      "2\n",
      "3870\n",
      "5886855fd73408ce4f4586d8\n",
      "2018-04-13 05:50\n",
      "did everyone really stop talking  as soon as I showed up?\n",
      "\n",
      "1\n",
      "3871\n",
      "5a4f6d1cd73408ce4f864c42\n",
      "2018-04-13 17:16\n",
      "@quant12345 you can look into the class's attributes. Say if you are using an instance called `mlp` belonging to ` MLPClassifier` class, you can check them at `mlp.coefs_` and `mlp.intercepts_` after fitting/training the model. More on this here : http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
      "\n",
      "1\n",
      "3872\n",
      "5acfdfffd73408ce4f95738d\n",
      "2018-04-13 17:37\n",
      "@greed2411                                                                                                                                          Thank you, for the answer. That is, the \"mlp.coefs_ \" weights for sign, \"mlp.intercepts_ \" predicted class labels. There is an example with three classes. I multiplied the signs by weight. The values of each class were obtained: 1.\t0.40-043 2.\t0.47-0.51 3.\t0.24-0.35 Is it possible to get from the function thresholds, which determine the belonging to each class?  regards\n",
      "\n",
      "1\n",
      "3873\n",
      "5a4f6d1cd73408ce4f864c42\n",
      "2018-04-14 00:28\n",
      "I don't think mlp works that way. You can't determine which neuron made it predict that output @quant12345 . But try googling it.im not entirely sure either.\n",
      "\n",
      "1\n",
      "3874\n",
      "541a528b163965c9bc2053de\n",
      "2018-04-20 08:15\n",
      "@lesshaste if you dimension is low enough (e.g. max 300, the lower the better), then density based clustering such as DBSCAN or https://github.com/scikit-learn-contrib/hdbscan should work well for that case.\n",
      "\n",
      "1\n",
      "3875\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-04-20 09:24\n",
      "@ogrisel  Thanks.. in my case the dimension is 1\n",
      "I will see what dbscan gives you. Is it really happy to find just one cluster and ignore the outliers?\n",
      "@ogrisel  ok thanks. dbscan in 1d it is :)\n",
      "\n",
      "3\n",
      "3876\n",
      "541a528b163965c9bc2053de\n",
      "2018-04-20 09:26\n",
      "it's worth a try with DBSCAN. You do not choose the number of cluster but the typical distance between two points that are to be considered as \"core points\" that is point in high density regions.\n",
      "There are also a bunch of other hyperparams. Read the scikit-learn docs and the hdbscan docs to learn more about this family of estimators.\n",
      "\n",
      "5\n",
      "3877\n",
      "5802c9ebd73408ce4f2e8431\n",
      "2018-04-24 23:03\n",
      "hello guys anybody here ?\n",
      "\n",
      "1\n",
      "3878\n",
      "595a8627d73408ce4f6b61ba\n",
      "2018-04-27 17:36\n",
      "hey\n",
      "\n",
      "1\n",
      "3879\n",
      "5962cf1ad73408ce4f6c5b9a\n",
      "2018-05-01 02:03\n",
      "I'm trying to understand when `fit_params` would be used in `_fit()` in a `Pipeline`.  ```python def _fit(self, X, y=None, **fit_params):     ... ```  Are there any example Pipelines that make use of this functionality? Is the point of this to allow passing additional data to a `fit`? If the point is to pass parameters what is the use case over adding these parameters to the estimators init? It seems like passing parameters would kind of go against https://github.com/scikit-learn/scikit-learn/issues/1975 no?\n",
      "\n",
      "1\n",
      "3880\n",
      "5962cf1ad73408ce4f6c5b9a\n",
      "2018-05-01 02:41\n",
      "I was also curious why `BaseEstimator` supports recursive `get_params`? Are there times when an estimator will have another estimator as a parameter?\n",
      "\n",
      "1\n",
      "3881\n",
      "5796939b40f3a6eec05c6fe0\n",
      "2018-05-03 04:57\n",
      "\n",
      "1\n",
      "3882\n",
      "572ccae9c43b8c60197181b7\n",
      "2018-05-03 21:15\n",
      "Is there any way to get the uncertainty on the fit parameters out of a `RANSACRegressor` or an underlying `base_estimator`?\n",
      "\n",
      "1\n",
      "3883\n",
      "5a97c979d73408ce4f8f616e\n",
      "2018-05-06 12:58\n",
      "Is there Any existing function for computing partial correlation coefficients ?\n",
      "\n",
      "1\n",
      "3884\n",
      "5717ab2f659847a7aff3b583\n",
      "2018-05-09 12:44\n",
      "hi\n",
      "\n",
      "1\n",
      "3885\n",
      "5986271dd73408ce4f703d62\n",
      "2018-05-10 18:47\n",
      "Hi guys\n",
      "\n",
      "1\n",
      "3886\n",
      "5986271dd73408ce4f703d62\n",
      "2018-05-10 18:48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm getting an error when I try to train a MultiOutputRegressor using sparse (csr) numpy matrices for both X and y\n",
      "I get: ``` AttributeError: 'MultiOutputRegressor' object has no attribute 'fit_transform' ```\n",
      "\n",
      "2\n",
      "3887\n",
      "5af5ab1fd73408ce4f98f85e\n",
      "2018-05-11 19:14\n",
      "HI\n",
      "\n",
      "1\n",
      "3888\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-05-11 21:55\n",
      "@gonesbuyo_twitter it is a regressor. The meaningful method is `predict`\n",
      "\n",
      "1\n",
      "3889\n",
      "5acfdfffd73408ce4f95738d\n",
      "2018-05-11 22:06\n",
      "Hi! Read all that I found about RandomForestClassifier. Is it possible in any way from 100 trees to make 1 common? The fact that I want to move the logic of the tree with cut-off thresholds in another program. With one tree it is easy to do. And here is how to be with 100 trees from a random forest? Only  if  operator will get a few hundred. Or maybe to calculate importance of characteristics using random forest and then build 1 the  decision tree with relevant signs? What can you advise on this problem in General?\n",
      "\n",
      "1\n",
      "3890\n",
      "5a6cb8cdd73408ce4f8a6a10\n",
      "2018-05-12 06:34\n",
      "@quant12345 numirate your questions, please [Answer](https://stackoverflow.com/questions/7152470/how-i-can-merge-two-binary-trees) on the first question\n",
      "Maybe with one tree it's incomplicated problem, but I think after concationations, your tree will be so big\n",
      "\n",
      "2\n",
      "3891\n",
      "5acfdfffd73408ce4f95738d\n",
      "2018-05-12 10:57\n",
      "@istom1n_twitter  Looked graphs of all 100 trees. They have different architectures. Do even, if possible to unite them, the tree will be huge. regards\n",
      "\n",
      "1\n",
      "3892\n",
      "5acfdfffd73408ce4f95738d\n",
      "2018-05-12 11:08\n",
      "1.  To take advantage of a random forest, for later use in a single decision tree graph(diagram). Getting significant features from random forests. Then I use only these features in one solution tree. What do you think about this? 2.  Is there a need to balance classes in the random forest and decision tree(the number of signs in class 1-two times more than in the 2nd class)? If so, what features should I use?\n",
      "\n",
      "1\n",
      "3893\n",
      "5a6cb8cdd73408ce4f8a6a10\n",
      "2018-05-12 12:46\n",
      "@quant12345 2. Balabcing, [first in search](https://www.geeksforgeeks.org/how-to-determine-if-a-binary-tree-is-balanced/)\n",
      "1. I don't understand your questions, if you mean reduce your tree after connections, I think you can get Dijkstras Algorithm, Minimum Spanning Trees\n",
      "I found good [explanation](https://web.eecs.umich.edu/~akamil/teaching/sp03/041403.pdf)\n",
      "\n",
      "3\n",
      "3894\n",
      "5acfdfffd73408ce4f95738d\n",
      "2018-05-12 18:18\n",
      "@istom1n_twitter  Thank you! On the second issue was meant another. Already found a solution: class_weight =balanced\n",
      "\n",
      "1\n",
      "3895\n",
      "5a6cb8cdd73408ce4f8a6a10\n",
      "2018-05-12 18:25\n",
      "depends about what tree we're talking, if binary, them balanced is in the left and in the right, we have equal amount of nodes\n",
      "Im only now get your message, strange\n",
      "Alright, thats correct, you are welcome\n",
      "\n",
      "3\n",
      "3896\n",
      "5a6cb8cdd73408ce4f8a6a10\n",
      "2018-05-12 18:36\n",
      "Question in that, if we connected 100 tree, for example, we get a graph, not tree obviously, and our problem is to find minimum spanning tree\n",
      "\n",
      "1\n",
      "3897\n",
      "57ed4d1c40f3a6eec0680da9\n",
      "2018-05-21 06:40\n",
      "Hello everyone,\n",
      "\n",
      "1\n",
      "3898\n",
      "5b029d9cd73408ce4f9a1afe\n",
      "2018-05-21 10:25\n",
      "hii\n",
      "\n",
      "1\n",
      "3899\n",
      "5b029d9cd73408ce4f9a1afe\n",
      "2018-05-21 10:26\n",
      "anyone use Named Entity Recognizer ??\n",
      "\n",
      "1\n",
      "3900\n",
      "5b018bb2d73408ce4f9a05a7\n",
      "2018-05-25 15:19\n",
      "anyone active here?\n",
      "Hello?\n",
      "Hello Prady.\n",
      "\n",
      "3\n",
      "3901\n",
      "577ebc96c2f0db084a21f545\n",
      "2018-05-25 15:23\n",
      "Yes\n",
      "\n",
      "1\n",
      "3902\n",
      "577ebc96c2f0db084a21f545\n",
      "2018-05-25 15:24\n",
      "Hello Dark Knight (Bruce Wayne) <unconvertable>\n",
      "By the way I love Dark Knight\n",
      "\n",
      "3\n",
      "3903\n",
      "5b018bb2d73408ce4f9a05a7\n",
      "2018-05-25 15:25\n",
      "So are you working on machine learning?\n",
      "\n",
      "1\n",
      "3904\n",
      "577ebc96c2f0db084a21f545\n",
      "2018-05-25 15:28\n",
      "Yes I am\n",
      "\n",
      "2\n",
      "3905\n",
      "5aa26b7ad73408ce4f90978b\n",
      "2018-05-29 11:00\n",
      "@pradyumnad   you kid me?\n",
      "\n",
      "1\n",
      "3906\n",
      "5b05ffccd73408ce4f9a806c\n",
      "2018-05-30 00:47\n",
      "I am here to start journey\n",
      "\n",
      "1\n",
      "3907\n",
      "5af8e5f8d73408ce4f99309c\n",
      "2018-05-31 18:01\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/VVFY/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/VVFY/image.png)\n",
      "\n",
      "1\n",
      "3908\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-01 22:39\n",
      "As anybody encountered PicklingError when trying to use pipeline.predict with pool.apply_async ?\n",
      "PicklingError: Can't pickle <function Pipeline.predict at 0x1c655cd08>: it's not the same object as sklearn.pipeline.Pipeline.predict\n",
      "The code that I'm using is -  result_train = pool.apply_async( \t\t \t\t\testimator.predict, \t\t \t\t\t(train_data['X'], ), \t\t \t)\n",
      "I tried wrapping it into a function. It still throws the same error.\n",
      "def wrapper(*args): \tfunc = args[0] \tfunc(*args[1:])\n",
      "result_train = pool.apply_async( \t\t\t\t\twrapper, \t\t\t\t\t(estimator.predict, train_data['X']), \t\t\t)\n",
      "Okay, let me try that.\n",
      "Yes, it works.\n",
      "Thanss @amueller. Where can I read more about this error ?\n",
      "\n",
      "9\n",
      "3909\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-06-01 22:39\n",
      "what's the error?\n",
      "\n",
      "1\n",
      "3910\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-06-01 22:41\n",
      "yeah you can't pickle instance methods like that\n",
      "write a new function that is not a method and does the prediction.\n",
      "make estimator the argument, not estimator.predict\n",
      "\n",
      "3\n",
      "3911\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-06-01 22:49\n",
      "I don't know.  google pickling instance methods, and you'll see that's not possible\n",
      "sklearn.pipeline.Pipeline.predict is a bit of a wild beast and not a normal class method, so you can't pickle it\n",
      "\n",
      "2\n",
      "3912\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-02 02:28\n",
      "Is there a reason Scikit doesnt parallelize predict ?\n",
      "\n",
      "1\n",
      "3913\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-02 02:31\n",
      "Since estimator wont mutate as a part of the computation, would it be safe to use it across the threads ?\n",
      "\n",
      "1\n",
      "3914\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-06-02 03:32\n",
      "it's often slower, depending on estimator and dataset size\n",
      "\n",
      "1\n",
      "3915\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-03 21:32\n",
      "I am working with text data. I'm using TfIdf right off the shelf. I noticed that MaxAbsScaler gives a better performance than StandardScaler(with_mean = False). I found this link: http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py. This prompted me to try out different scaling transformers. Is there anybody who has worked with text data before ? I wanted to know if \"selecting scaler\" is worth time spending on ?\n",
      "If it is, the link uses feature distributions, outliers to select a scaler ? Do the above concepts make sense in the space after TfIDF transforms the text data ?\n",
      "\n",
      "2\n",
      "3916\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-06-04 02:04\n",
      "@AMaini503 I think we should deprecate StandardScaler(mean=False). It seems weird to me\n",
      "but I would just try a bunch and see what makes sense. There's no general rule\n",
      "\n",
      "2\n",
      "3917\n",
      "5b018bb2d73408ce4f9a05a7\n",
      "2018-06-05 05:00\n",
      "Hello\n",
      "\n",
      "1\n",
      "3918\n",
      "5841d067d73408ce4f3a4ecf\n",
      "2018-06-05 12:57\n",
      "@kelux19 Hey people! ***   string[] name = {\"Malin\", \"Manar\", \"Stefan\", \"Ali\", \"Alexandra\", \"Robert\",\"Suzana\" }; string[] property = { \"kind\", \"bad\", \"talentfull\", \"helpfull\", \"nice\", \"melancholy\", \"egoistic\" }; string[] role = { \"teacher\", \"system developer\", \"student\", \"musician\", \"programmer\", \"actor\", \"doctor\"};  *** how can i pick from these three arrays randomly, for example \"Ali is a kind teacher\"?\n",
      "its c# btw\n",
      "\n",
      "2\n",
      "3919\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-05 16:37\n",
      "@kelux19  I don't think so there is any api to scikit-learn in c#. But you might want to wait for others' answers as well.\n",
      "\n",
      "1\n",
      "3920\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-05 17:30\n",
      "I'm using  a custom tokenizer for the TfIDF. I trained the model and saved the pipeline object to a pickle file using joblib. However, when I try to load the pickle file in a different script, it throws an error: module '__main__' has no attribute 'MyTokenizer'. Am I pickling the model correctly ?\n",
      "\n",
      "1\n",
      "3921\n",
      "541a528b163965c9bc2053de\n",
      "2018-06-06 11:21\n",
      "You should put the code for the MyTokenizer class in an importable module that his installed somewhere in the python path in the Python where you load the model.\n",
      "Alternatively, you can use `cloudpickle.dump / load` instead of joblib. It will be slightly less efficient if your model has large numpy arrays as attributes but this is probably not a problem in practice.\n",
      "\n",
      "2\n",
      "3922\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-06 17:24\n",
      "The first solution that you mention works. I read that joblib pickles by remembering the paths to objects\n",
      "So, if I create a function in an interactive session, there is no path to that function. Putting that into an importable module makes it work\n",
      "\n",
      "2\n",
      "3923\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-06 20:48\n",
      "@ogrisel Coming back to the pickling issue. The way I was using it in another script was by redefining the function.\n",
      "\n",
      "1\n",
      "3924\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-06 20:50\n",
      "Now, I'm trying to use the pipeline object inside a class. In the __init__, I try to load the model, it throws the same error again. I tried defining the function inside the __init__, but still the pickle doesn't see the function. Any workaround for this ?\n",
      "Here is the paste: https://pastebin.com/7rLmaxxJ.\n",
      "Open to suggestions from others.\n",
      "\n",
      "3\n",
      "3925\n",
      "541a528b163965c9bc2053de\n",
      "2018-06-07 07:51\n",
      "@AMaini503  please provide a Minimal Complete Verifiable Example in your paste (http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports)\n",
      "\n",
      "1\n",
      "3926\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-07 21:58\n",
      "I'm trying to get cluster assignments from the linkage matrix. So far, I know that I can use the fcluster to do that. But, how do I specify the n_clusters ?\n",
      "\n",
      "1\n",
      "3927\n",
      "5b018bb2d73408ce4f9a05a7\n",
      "2018-06-09 11:58\n",
      "hello\n",
      "\n",
      "1\n",
      "3928\n",
      "5af3b5e8d73408ce4f98c40d\n",
      "2018-06-09 16:34\n",
      "I'm facing an issue in importing svm\n",
      "\n",
      "1\n",
      "3929\n",
      "5af3b5e8d73408ce4f98c40d\n",
      "2018-06-09 16:34\n",
      "\n",
      "0\n",
      "3930\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-06-10 01:45\n",
      "Is there a reason sklearn.ensemble.partial_dependence only works on gradient boosting?\n",
      "\n",
      "1\n",
      "3931\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-11 17:06\n",
      "@ucalyptus https://stackoverflow.com/questions/15274696/importerror-in-importing-from-sklearn-cannot-import-name-check-build\n",
      "\n",
      "1\n",
      "3932\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-06-11 17:14\n",
      "Can anyone help me with this: https://stats.stackexchange.com/questions/350520/evaluation-of-machine-learning-model-in-production ?\n",
      "TL;DR - I'm trying to evaluate a model in production. I need advice on what metrics to use ( excluding explicit evaluation on a data set ).\n",
      "\n",
      "2\n",
      "3933\n",
      "5634e8e116b6c7089cb8fa99\n",
      "2018-06-13 23:06\n",
      "Hi all, do the kernel density estimators in scikit-learn allow for diagonal (D class) bandwidths, and do any of them also use a leave-one-out bandwidth estimation?\n",
      "\n",
      "1\n",
      "3934\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-06-16 15:54\n",
      "@tanimislam you can use gridsearchcv for that.\n",
      "(bandwidth estimation)\n",
      "there's no efficient implementation if that was the question\n",
      "\n",
      "3\n",
      "3935\n",
      "5796939b40f3a6eec05c6fe0\n",
      "2018-06-17 09:42\n",
      "\n",
      "1\n",
      "3936\n",
      "5824aa0dd73408ce4f3501a2\n",
      "2018-06-17 11:40\n",
      "Hi everyone, I am Florian and I am a co-organizer of the Python sprints meetup in London (at least I hope so as this is my first time next week). We are planning on doing a small documentation sprint on a few open source libraries. The main organiser is planning to address some issues and structural changes in the pandas documentation and I wanted to help a separate group work on a different library. I thought that maybe https://github.com/scikit-learn/scikit-learn/issues/10453 would be a good issue to work on with a small group of people for about 2-3 hours since it overlaps nicely with the other group. Is that something that would be helpful and is there someone working on it already (did not see anyone on github picking it up so far)?\n",
      "\n",
      "1\n",
      "3937\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-06-17 11:50\n",
      "I don't see anyone working on the issue for the moment and it seems a good opportunity.\n",
      "It would be beneficial for the project since we start to have some feature (e.g. ColumnTransformer) which can benefit from the pandas.\n",
      "\n",
      "2\n",
      "3938\n",
      "5824aa0dd73408ce4f3501a2\n",
      "2018-06-17 11:58\n",
      "ok great, I see where we can get to on Thursday\n",
      "\n",
      "1\n",
      "3939\n",
      "5b27cec1d73408ce4f9dbcaf\n",
      "2018-06-18 15:57\n",
      "What is the difference between logloss and cross entropy? Sorry I am a newbie in this field\n",
      "\n",
      "1\n",
      "3940\n",
      "5a36ab82d73408ce4f83bb10\n",
      "2018-06-18 22:27\n",
      "hey guys, I am trying to visualize a high dimensional RNA dataset with TSNE\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/6rZd/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/6rZd/image.png)\n",
      "However I get the error\n",
      "ValueError: could not convert string to float: 'sample_800'\n",
      "Any inputs on this?\n",
      "\n",
      "5\n",
      "3941\n",
      "5b27cec1d73408ce4f9dbcaf\n",
      "2018-06-19 01:28\n",
      "@ziweiwu which line is the error on?\n",
      "\n",
      "1\n",
      "3942\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-06-19 08:12\n",
      "Apparently you have string in your data which you try to convert into numeric\n",
      "\n",
      "1\n",
      "3943\n",
      "5a36ab82d73408ce4f83bb10\n",
      "2018-06-20 19:31\n",
      "I see. I have solved this issue, what I did is to convert my df to df.values.\n",
      "so only numerical datas are inputted into the algorithm\n",
      "\n",
      "2\n",
      "3944\n",
      "5a32b032d73408ce4f835880\n",
      "2018-06-20 21:01\n",
      "Hey all, Is there a way to pass a sentence like play some music to an AI model and after that chrome tab will open with a random YouTube song Ive been trying to figure out a way to do this for a while and I appreciate any advices or help\n",
      "Just curious if anyone has a way or approach how to wrap this up, advice is highly appreciated\n",
      "\n",
      "2\n",
      "3945\n",
      "5b2cf725d73408ce4f9e4022\n",
      "2018-06-22 13:19\n",
      "Hi guys trying to analyze a data set for predicting employee absenteeism\n",
      "Can you please suggest that linear regression is good or I need to use a time series model. My dataset is\n",
      "https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work\n",
      "\n",
      "3\n",
      "3946\n",
      "541a528b163965c9bc2053de\n",
      "2018-06-22 13:34\n",
      "Based on its description, the dataset you linked does not look like timeseries data: it's a table of feature data and one can probably make the i.i.d assumption. This is not a series of events with a timestamp for each with a progressing time dependency.  For supervised regression of the number of hours of absenteism, I would indeed try linear regress ion (+ some feature engineering) and compare with a more complex model such as RandomForestRegressor\n",
      "\n",
      "1\n",
      "3947\n",
      "5a5a272ad73408ce4f880912\n",
      "2018-06-24 09:41\n",
      "Hello! I have taken machine learning class from udacity and now there's a problem while loading a pickle file. The file is just 15MB and I've 4GB RAM. Anyone who can get me through this problem?\n",
      "\n",
      "1\n",
      "3948\n",
      "5957e40fd73408ce4f6b27e0\n",
      "2018-06-26 07:10\n",
      "@loginofdeath Hi, I am using this pipeline object pipe = Pipeline( \t\t[ \t\t\t('fresh', \t\t\t FeatureAugmenter(column_id='index', column_sort='tick', default_fc_parameters=MinimalFCParameters())), \t\t\t('scaler', StandardScaler())  \t\t]) If I fit_transform it on the train data and get some features, will I get the same features on the test data too so that I can give it  to a classifier?\n",
      "\n",
      "1\n",
      "3949\n",
      "530c03e25e986b0712efafb8\n",
      "2018-06-27 17:07\n",
      "I apologize for what I'm sure is a common question, but is there a schedule for a 0.20 release?  \"No\" or \"a long time from now\" are both useful answers.  I don't want this to be interpretted as \"please do a release\", I'm just planning some activities and knowing this would help\n",
      "\n",
      "1\n",
      "3950\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-06-28 08:06\n",
      "As fast as possible :)\n",
      "\n",
      "1\n",
      "3951\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-06-28 08:07\n",
      "We are trying to finish up couple of issues and we would like to have a release candidate for SciPy\n",
      "However we might have some delay depending on how it will go :)\n",
      "\n",
      "2\n",
      "3952\n",
      "530c03e25e986b0712efafb8\n",
      "2018-06-28 14:25\n",
      "Thanks for the response.   Given the \"trying to have a release candidate for scipy\" statement I'm interpretting this as \"weeks away\", not days and not months\n",
      "\n",
      "2\n",
      "3953\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-07-02 16:59\n",
      "Hi folks, excuse my ignorant question. I was wondering if it is possible to pass a tensorflow model into scikit OneVsRestClassifier, if I expose fit and predict methods in the tensorflow model? I couln't find an example when I googled it.\n",
      "\n",
      "1\n",
      "3954\n",
      "5b11caa3d73408ce4f9b96b5\n",
      "2018-07-03 16:56\n",
      "@kirk86  I don't know about tensorflow. But keras has wrappers to make models a part of sklearn workflow. I think you can take a look at the source for these wrappers. Here's the link: https://keras.io/scikit-learn-api/\n",
      "\n",
      "1\n",
      "3955\n",
      "5957e40fd73408ce4f6b27e0\n",
      "2018-07-04 07:28\n",
      "can knn be used for One Class classification?\n",
      "\n",
      "1\n",
      "3956\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-07-04 08:29\n",
      "Would it make sense for http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html to have a transform method?\n",
      "Currently you can't train it on a training set and then use it on a test set it seems\n",
      "\n",
      "2\n",
      "3957\n",
      "564e507e16b6c7089cbb6551\n",
      "2018-07-05 20:19\n",
      "Hey guys, is there a nice book or a course online to understand how the MLP and the SGD classifiers implemented in Scikit-learn work?\n",
      "I'm looking at the code, but I can't understand it, without having a basic understanding of the theory behind\n",
      "\n",
      "2\n",
      "3958\n",
      "5b2f34c7d73408ce4f9e6615\n",
      "2018-07-06 08:34\n",
      "https://chat.whatsapp.com/EY44e81jzzCFLXEkY0tjFc\n",
      "\n",
      "1\n",
      "3959\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-07-06 20:11\n",
      "I have some 2d data which I think would be well classified by two straight intersecting lines. One straight line is logistric regression I believe but how can you get two straight lines as the decision boundary?\n",
      "\n",
      "1\n",
      "3960\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-07-07 19:11\n",
      "oh verbosity = 2 does that\n",
      "is it possible to get tpot to optimize the ROC when doing classification?\n",
      "oh that is there too\n",
      "sorry\n",
      "\n",
      "4\n",
      "3961\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-07-07 19:59\n",
      "If you have this code:\n",
      "tpot = TPOTClassifier(generations=20, population_size=50, scoring='roc_auc', verbosity=2, n_jobs=-1) tpot.fit(X_train, y_train) print(\"TPOT score\", tpot.score(X_test, y_test))\n",
      "is the score being printed the roc_auc score?\n",
      "\n",
      "3\n",
      "3962\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-07-09 14:15\n",
      "I am going to guess it is\n",
      "\n",
      "1\n",
      "3963\n",
      "5b437334d73408ce4fa02901\n",
      "2018-07-09 14:38\n",
      "Hello, I'm just not getting anywhere. I have a regression ANN with four inputs and want to draw a contour plot.  So two variable variables and two fixed. Does anyone have an example or can you give me a hint how I do this?\n",
      "\n",
      "1\n",
      "3964\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-09 14:39\n",
      "> is the score being printed the roc_auc score?\n",
      "accuracy for classifier and r2 for regressor\n",
      "\n",
      "2\n",
      "3965\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-07-09 14:40\n",
      "@glemaitre  sorry I was really asking a  a tpot question. I think it prints whatever you used when defining TPOTClassifier\n",
      "\n",
      "6\n",
      "3966\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-07-09 14:46\n",
      "of course.. sorry for asking a slightly off topic question here\n",
      "\n",
      "1\n",
      "3967\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-09 14:47\n",
      "no problem but looking quickly at their base class, score is calling the associated string score from sklearn\n",
      "so I think that your guess was good\n",
      "\n",
      "3\n",
      "3968\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-07-09 14:48\n",
      "on another topic, I have lots (1000s) of pairs of vectors. For each pair on the left there is exactly one pair on the right that it should be associated with\n",
      "I could make a classification problem out of it by concatenating the vectors and labelling them with 0 or 1 but 1/1000s of the labels would be 1.  That is the fraction of 1 labels would be tiny\n",
      "is there a standard way to approach this sort of problem?\n",
      "oh that sounds interesting\n",
      "thanks.. I will see if that is appropriate  The main restriction is that I need to do out of sample prediction\n",
      "I did also look at scikit learn's kernel pca\n",
      "might that work?\n",
      "you would need to feed in a full set of distances which would only ever be 0 or 1\n",
      "I don't know if that is generally a bad idea for kernel pca\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
      "me neither!\n",
      "\n",
      "11\n",
      "3969\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-09 14:50\n",
      "I am not really familiar\n",
      "but it could something linked to metric learning\n",
      "in which you want to know the relative distance between pairs\n",
      "and know which pairs have the minimal distance\n",
      "https://github.com/metric-learn/metric-learn\n",
      "I don't have enough background there :)\n",
      "\n",
      "6\n",
      "3970\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-09 14:55\n",
      "I would presume that having a continuous distance is what you would need\n",
      "\n",
      "3\n",
      "3971\n",
      "5b437ec8d73408ce4fa02a9a\n",
      "2018-07-11 08:50\n",
      "```  def get_age(df):     return df[['age']]  preprocess_pipeline = Pipeline([             ('get_cols', FunctionTransformer(get_age, validate=False)),             ('median_impute', Imputer(strategy='median')),             ('min_max_scale', MinMaxScaler())         ])  with open('./preprocess_pipeline.pkl', 'rb') as handle:     preprocess_pipeline = pickle.dump(handle) ``` I want to dump and load this pipeline but it requires me to define `get_age` function again when I want to load. I don't want to duplicate my `get_age` function in 2 different files, how can I include only 1 column (`age` in this case) in the pipeline dump/load friendly way?\n",
      "I've tried using lambda function and joblib function of sklearn, didn't work\n",
      "\n",
      "2\n",
      "3972\n",
      "564e507e16b6c7089cbb6551\n",
      "2018-07-12 11:07\n",
      "Anyone knows on which paper are based the formulas in: http://scikit-learn.org/stable/modules/sgd.html#id1 for the `weights` and the `learning rate` of the `SGDClassifier`? I couldn't find the exact same formulas in the papers linked at the bottom of those sections.\n",
      "\n",
      "1\n",
      "3973\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-12 18:15\n",
      "@h4k1m0u probably bottou's sgd code\n",
      "\n",
      "1\n",
      "3974\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-07-13 00:13\n",
      "@AMaini503  thanks for the pointers I was trying to achieve the same outcome but with pure tf. Not luck so far :(\n",
      "\n",
      "1\n",
      "3975\n",
      "59e7b052d73408ce4f7aa075\n",
      "2018-07-13 18:21\n",
      "I am fitting several SVRS to images and get ~3MB pickled model file size for ~20MB image data. Does the pickle contain any redundant data (training or otherwise) or is that a reasonable size?\n",
      "\n",
      "1\n",
      "3976\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-14 14:02\n",
      "Sprints!!!\n",
      "\n",
      "1\n",
      "3977\n",
      "56f8122085d51f252abb1414\n",
      "2018-07-14 14:08\n",
      "Hi all, I would like to know whether there is interest in adding a least absolute deviation regression with L1 penalty to sklearn. The optimization problem is very similar to the Lasso one, but it uses a L1 likelihood rather than L2.\n",
      "\n",
      "2\n",
      "3978\n",
      "56f8122085d51f252abb1414\n",
      "2018-07-14 14:36\n",
      "@amueller yes, thats mostly for robust regression. I dont have the answer on how it compares to huber right now, but I would guess least absolute deviation would work better on sparse settings. I will investigate that and follow up on the mailing list. Thanks!\n",
      "\n",
      "2\n",
      "3979\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-14 14:56\n",
      "we're in 105 for now\n",
      "\n",
      "1\n",
      "3980\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-14 15:02\n",
      "A list of issues/stalled PRs are tagged \"sprint\" which could be good selection for the sprint.\n",
      "https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3ASprint\n",
      "\n",
      "2\n",
      "3981\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-14 15:03\n",
      "also if you're new the ones tagged \"good first issue\" https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22+sort%3Aupdated-desc\n",
      "\n",
      "1\n",
      "3982\n",
      "59b1b233d73408ce4f74ce2d\n",
      "2018-07-14 15:12\n",
      "thanks @amueller and @glemaitre !\n",
      "\n",
      "1\n",
      "3983\n",
      "59b1b233d73408ce4f74ce2d\n",
      "2018-07-14 16:40\n",
      "I have a question about the issue here: https://github.com/scikit-learn/scikit-learn/issues/9352 Where should docker be run? https://github.com/scikit-learn/scikit-learn/blob/master/.travis.yml#L62 I assume that we should copy over install and testing scripts and run in a docker container, is that the right path?\n",
      "\n",
      "1\n",
      "3984\n",
      "59b1b233d73408ce4f74ce2d\n",
      "2018-07-14 16:48\n",
      "ran tests in the CentOS32 bit docker image. 13 failed, investigating why first\n",
      "\n",
      "1\n",
      "3985\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-14 16:52\n",
      "@jrmlhermitte yeah I think travis\n",
      "@jrmlhermitte olivier is at the back of the room ;)\n",
      "\n",
      "2\n",
      "3986\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-07-14 16:54\n",
      "@jrmlhermitte you can report the errors you get in that issue about 32bit bits in any case-- it might be helpful for future reference..\n",
      "\n",
      "1\n",
      "3987\n",
      "59b1b233d73408ce4f74ce2d\n",
      "2018-07-14 17:01\n",
      "thanks i posted them here : https://github.com/scikit-learn/scikit-learn/pull/11515 waiting for travis to build but i thikn i can start trying to debug locally\n",
      "\n",
      "1\n",
      "3988\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-14 17:16\n",
      "I'm hungry\n",
      "food?\n",
      "where should we go?\n",
      "\n",
      "3\n",
      "3989\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-14 17:16\n",
      "I agree this is time\n",
      ":)\n",
      "Whatever works where there is a vegetarian option\n",
      "\n",
      "3\n",
      "3990\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-14 17:23\n",
      "the vietnamese place or the indian place? or the food carts if they are there?\n",
      "\n",
      "1\n",
      "3991\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-14 19:09\n",
      "can someone merge this please? https://github.com/scikit-learn/scikit-learn/pull/11289#pullrequestreview-137246243\n",
      "\n",
      "1\n",
      "3992\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-14 19:19\n",
      "done\n",
      "\n",
      "1\n",
      "3993\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-14 19:21\n",
      "https://sci-hub.tw/https://doi.org/10.1002/sim.1822\n",
      "\n",
      "1\n",
      "3994\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-14 19:25\n",
      "Can someone merge this one https://github.com/scikit-learn/scikit-learn/pull/11391\n",
      ":)\n",
      "\n",
      "2\n",
      "3995\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-07-14 19:54\n",
      "@amueller if you are OK with https://github.com/scikit-learn/scikit-learn/pull/11431 should we merge it?\n",
      "\n",
      "1\n",
      "3996\n",
      "5b4a54fdd73408ce4fa0e3c7\n",
      "2018-07-14 19:55\n",
      "Hi all, quick question: how do I run a single test locally? For example the docttests? Thanks!\n",
      "\n",
      "4\n",
      "3997\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-07-14 20:02\n",
      "To run a single test you can also run, ``` pytest sklearn/file_path..  -k part_of_test-name ``` providing both the path to the file and a part of the test name will make test collection faster.\n",
      "See http://scikit-learn.org/dev/developers/tips.html#useful-pytest-aliases-and-flags\n",
      "\n",
      "2\n",
      "3998\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-14 20:10\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11469\n",
      "I push some changes @amueller @rth\n",
      "\n",
      "2\n",
      "3999\n",
      "5571fe1015522ed4b3e17d90\n",
      "2018-07-14 20:16\n",
      "James Bourbeau (@jrbourbeau) would like to chat with us about how to make GridSearchCV play nicer with dask-ml. If you have a preferred time today, let me know! Otherwise we'll probably just drop by at a random time :-).  To sum up, in GridSearchCV something check that the score returned by the metric is a number, so they dask-ml needs to call `.compute` explicitly which has a bit of friction with the rest of their API (everything is lazy).\n",
      "\n",
      "1\n",
      "4000\n",
      "5b4a54fdd73408ce4fa0e3c7\n",
      "2018-07-14 20:19\n",
      "Hi all, I'm afraid I am still a bit test-confused (newbie here). When running `make test`, I can see errors (like `scikit-learn/sklearn/impute.py:547: DocTestFailure`) related to the docstring in the Imputation, but `make test-doc` does not surface these.\n",
      "\n",
      "1\n",
      "4001\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-14 20:28\n",
      "@lesteve we you want\n",
      "\n",
      "1\n",
      "4002\n",
      "5b4a54fdd73408ce4fa0e3c7\n",
      "2018-07-14 20:33\n",
      "Ok, `pytest sklearn/impute.py::sklearn.impute.ChainedImputer`  does the trick! I guess that was in the docs, thanks again!\n",
      "\n",
      "1\n",
      "4003\n",
      "5571fe1015522ed4b3e17d90\n",
      "2018-07-14 20:43\n",
      "FYI we'll drop by in 5 -10 minutes\n",
      "\n",
      "1\n",
      "4004\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-14 21:52\n",
      "Sorry I missed you @lesteve .\n",
      "I merged the Python 3.7 ABC warning  fix in 0.19.X and triggered a new build on the MacPython/scikit-learn-wheels repo\n",
      "if it goes well I can push those wheels to pypi.org\n",
      "\n",
      "3\n",
      "4005\n",
      "569fe132e610378809bd5552\n",
      "2018-07-14 22:51\n",
      "I'm getting a doctest failure when building from source on master\n",
      "It's getting raised from modules/compose.rst\n",
      "Has anyone else here run into this?\n",
      "\n",
      "3\n",
      "4006\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-14 22:52\n",
      "sprint will end in 10 minutes\n",
      "also: if you want to ask me something, you have 10 minutes because then I'll f off\n",
      "\n",
      "2\n",
      "4007\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-15 15:04\n",
      "does anyone understand what's happening with lgtm? seems like pypi timeouts? @jnothman ?\n",
      "\n",
      "1\n",
      "4008\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-15 15:37\n",
      "@ogrisel do you have an opinion on https://github.com/scikit-learn/scikit-learn/pull/11469#discussion_r202540743 ?\n",
      "@lesteve is there an issue tracking the joblib pickle thing?\n",
      "\n",
      "2\n",
      "4009\n",
      "5571fe1015522ed4b3e17d90\n",
      "2018-07-15 16:53\n",
      "Yep, let me find it.\n",
      "https://github.com/scikit-learn/scikit-learn/issues/11408\n",
      "\n",
      "2\n",
      "4010\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-15 19:36\n",
      "There is to pending PR waiting for an extra review to be merged\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11469\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11391\n",
      "\n",
      "3\n",
      "4011\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-07-15 20:31\n",
      "Anyone interested in a reviewing a PR on PyPy support ?  https://github.com/scikit-learn/scikit-learn/pull/11010 it's mostly a CI setup + very light changes to make tests pass .. Maybe @lesteve ? :)\n",
      "\n",
      "1\n",
      "4012\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-15 20:37\n",
      "I gave my +1\n",
      "\n",
      "1\n",
      "4013\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-15 21:01\n",
      "FYI I disabled the lgtm webhook (or tried to?) because it's confusing for the sprinters\n",
      "\n",
      "1\n",
      "4014\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-15 21:14\n",
      "Any additional review on the MissingIndicator is welcomed\n",
      "https://github.com/scikit-learn/scikit-learn/pull/8075\n",
      "@amueller It seems it was done on purpose\n",
      "https://github.com/scikit-learn/scikit-learn/commit/b4561f09e6e0ff8a2e16f09be21b3202012bbdd7\n",
      "oh right\n",
      "\n",
      "5\n",
      "4015\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-15 21:31\n",
      "Anybody can review this one\n",
      "https://github.com/scikit-learn/scikit-learn/pull/9616\n",
      "\n",
      "2\n",
      "4016\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-15 21:33\n",
      "does anyone know what happened here? https://github.com/scikit-learn/scikit-learn/pull/11504\n",
      "there's something funky going on with the pooling_func deprecation\n",
      "\n",
      "2\n",
      "4017\n",
      "5b4a4873d73408ce4fa0e326\n",
      "2018-07-15 21:39\n",
      "@amueller\n",
      "\n",
      "1\n",
      "4018\n",
      "5b4a4873d73408ce4fa0e326\n",
      "2018-07-15 21:39\n",
      "@amueller  When I post a future warning, am I to assume we are  currently on version 0.20 of sci-kit learn?\n",
      "\n",
      "3\n",
      "4019\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-15 21:41\n",
      "fixed in #11537\n",
      "it's unused in agglomerative clustering but not feature agglomeration which inherits the fit...\n",
      "\n",
      "2\n",
      "4020\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-15 21:44\n",
      "(and that's why I don't want deprecation warnings in the examples and tests ;)\n",
      "\n",
      "1\n",
      "4021\n",
      "5b4a4873d73408ce4fa0e326\n",
      "2018-07-15 21:57\n",
      "@amueller  When I do a pull request, should it be against the master branch?\n",
      "\n",
      "3\n",
      "4022\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-16 10:02\n",
      "An update from Paris: we have a dozen of people working on a variety of small issues or reviewing (they have marked the issues that they are working on).\n",
      "I think that some of us are going to prioritize issues needed for the release.\n",
      "\n",
      "2\n",
      "4023\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-16 13:01\n",
      "great\n",
      "\n",
      "1\n",
      "4024\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-16 13:02\n",
      "we will in the office in 30 minutes or so ready to light up\n",
      "\n",
      "1\n",
      "4025\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-16 13:03\n",
      "Awesome. I am busy finishing details on small PRs that are important for the release.\n",
      "I would like to move a bunch of issues to the next milestone. I think that we should not delay the release. If needed, we can discuss this quickly.\n",
      "I think that we should delay KMedoids\n",
      "It would be good to have it, but this release is already a big one\n",
      "\n",
      "4\n",
      "4026\n",
      "53232ac75e986b0712efe3af\n",
      "2018-07-16 13:15\n",
      "it will not be in 30 min, Guillaume was a bit optimistic :)\n",
      "\n",
      "1\n",
      "4027\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-16 13:26\n",
      "It's good to be optimistic!\n",
      "\n",
      "1\n",
      "4028\n",
      "54e07d1515522ed4b3dc0852\n",
      "2018-07-16 13:38\n",
      "anyone looking for a simple issue that is pure doc: https://github.com/scikit-learn/scikit-learn/issues/9196\n",
      "\n",
      "1\n",
      "4029\n",
      "560313510fc9f982beb1a331\n",
      "2018-07-16 14:24\n",
      "Anyone able to review a small PR using BLAS? ( https://github.com/scikit-learn/scikit-learn/pull/11420 )\n",
      "\n",
      "1\n",
      "4030\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-16 14:41\n",
      "but we eventually got in the office now\n",
      "\n",
      "1\n",
      "4031\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-16 14:49\n",
      "OK. Looking at the list of issues for the next release: https://github.com/scikit-learn/scikit-learn/milestone/24, I would like to postpone a few. Things like #11520 , #8642, #9098. My goal is to move us forward to the release.\n",
      "Any objections?\n",
      "Also, #11408 needs a review. It fixes one of the problems of the build\n",
      "@amueller : OK, but this list is too long. We are going to have to make choices. The delay in the release is not good for our users. I am happy focusing on things to get them done (including #11520), but we will not be able to address all this.\n",
      "Great. I'll have a look at #11520 right now\n",
      "@amueller is adding issues to the milestone faster than we can close them :)\n",
      "Yes, but it's quite depressing to see the percentage going down as we work. I really worry about feature creep. I was expecting this release to be out a couple months ago.\n",
      "\n",
      "7\n",
      "4032\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 14:49\n",
      "I don't want to postpone the Yeo-Johnson because it will be the default for PowerTransform\n",
      "\n",
      "1\n",
      "4033\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 14:50\n",
      "basically if we don't do #11520 then we'll release with power transform only working on non-negative data and we need a deprecation cycle to change the default to yeo-johnson\n",
      "\n",
      "1\n",
      "4034\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 14:51\n",
      "I'm also untagging some things right now, and tagging other things as blockers\n",
      "yes I know it is\n",
      "I think joris is creating a github project board right now\n",
      "\n",
      "3\n",
      "4035\n",
      "5b3cc3c9d73408ce4f9f9bf8\n",
      "2018-07-16 14:54\n",
      "Hi . please help me about visualization of SVM, cross validation and its performance measures\n",
      "\n",
      "1\n",
      "4036\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 14:57\n",
      "what' about #9723 ?\n",
      "\n",
      "3\n",
      "4037\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 15:00\n",
      "yeah looks like it from a glance\n",
      "@GaelVaroquaux I kinda wanted estimator tags in, but I guess we might not be able to do that :-/\n",
      "KMedoids? #11099  I'd say delay\n",
      "#10058 NCA I'd say delay as well\n",
      "\n",
      "4\n",
      "4038\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-16 15:05\n",
      "I do think that NCA is delayed :(\n",
      "\n",
      "1\n",
      "4039\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-16 15:06\n",
      "we need a second review on https://github.com/scikit-learn/scikit-learn/pull/11464\n",
      "\n",
      "4\n",
      "4040\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-16 15:16\n",
      "We also need to fix the MICE/ChainedImputer/IterativeImputer\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11350\n",
      "basically the remaining question would be link to the default imputer to use (RidgeCV or RandomForest)\n",
      "\n",
      "3\n",
      "4041\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-16 15:28\n",
      "I commented on this choice directly in the PR, to keep the discussion on github.\n",
      "\n",
      "1\n",
      "4042\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 16:07\n",
      "@GaelVaroquaux do you have opinions on https://github.com/scikit-learn/scikit-learn/issues/11536 ?\n",
      "there is tons of convergence warnings everywhere currently\n",
      "also, the examples and tests have tons of warnings about iid. Should we do something about that? not sure if it's worth in the examples, in the test we should probably catch\n",
      "@ogrisel also would love to hear your thoughts on https://github.com/scikit-learn/scikit-learn/issues/11536\n",
      "\n",
      "4\n",
      "4043\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 16:26\n",
      "@GaelVaroquaux sorry :-/ only thing that I'm noticing now that we broke\n",
      "\n",
      "1\n",
      "4044\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 16:55\n",
      "what feature creep? you mean scope of sklearn or scope of this release?\n",
      "I'm just going through examples and dogfooding all the changes we did and make sure the user experience is sane\n",
      "like the iid change can easily lead to a wall of deprecations. I guess the user can always catch them...\n",
      "@glemaitre the one you just posted lol\n",
      "this is related but not the consistency: #6425\n",
      "this is the actual issue #7242\n",
      "\n",
      "6\n",
      "4045\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-07-16 16:57\n",
      "Opinions of what should be done about the blocking  euclidean_distance in 32 bit issue would be welcome https://github.com/scikit-learn/scikit-learn/issues/9354#issuecomment-405314164\n",
      "\n",
      "1\n",
      "4046\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 17:26\n",
      "this is ready for reviews: https://github.com/scikit-learn/scikit-learn/pull/11561\n",
      "\n",
      "1\n",
      "4047\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-16 17:30\n",
      "This could be merged (MRG+2): https://github.com/scikit-learn/scikit-learn/pull/9616\n",
      "\n",
      "4\n",
      "4048\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-16 18:03\n",
      "OK, we are wrapping up here in Paris. I am signing off, as I have \"real work\" to do for tonight. Sorry\n",
      "\n",
      "1\n",
      "4049\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 19:14\n",
      "thanks!\n",
      "\n",
      "1\n",
      "4050\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-16 19:18\n",
      "@amueller https://hub.github.com/\n",
      "\n",
      "1\n",
      "4051\n",
      "5571fe1015522ed4b3e17d90\n",
      "2018-07-16 19:18\n",
      "or this without dependencies: git push https://github.com/lmcinnes/scikit-learn pr/8554:sparse-LLE-Isomap -f\n",
      "\n",
      "1\n",
      "4052\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-16 19:20\n",
      "@amueller You seem to be interested in merging this one since the test are passing\n",
      "https://github.com/scikit-learn/scikit-learn/pull/8075\n",
      "\n",
      "2\n",
      "4053\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 19:23\n",
      "what about https://github.com/scikit-learn/scikit-learn/pull/8760 ?\n",
      "anyone working on that?\n",
      "is that a regression?\n",
      "\n",
      "3\n",
      "4054\n",
      "5a83f33dd73408ce4f8d133f\n",
      "2018-07-16 19:41\n",
      "I'm working on it (PR related to issue #8720 which I commented)\n",
      "\n",
      "1\n",
      "4055\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 19:44\n",
      "@rth do you wanna review the openml loader? #11419 ?\n",
      "anyone else have a quick review for #11561 ?\n",
      "\n",
      "4\n",
      "4056\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-16 19:54\n",
      "@amueller : feature creep = scope of release. Not scope of scikit-learn. I think that we are doing good on this.\n",
      "\n",
      "2\n",
      "4057\n",
      "53232ac75e986b0712efe3af\n",
      "2018-07-16 19:54\n",
      "@amueller https://github.com/scikit-learn/scikit-learn/pull/10198 (OneHotEncoder.get_feature_names, was not tagged apparently)\n",
      "\n",
      "1\n",
      "4058\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 19:56\n",
      "someone look at #11567 ? ;)\n",
      "(still opening issues and PRs quicker than people can close them)\n",
      "\n",
      "2\n",
      "4059\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-16 20:23\n",
      "Hey Austin, while both y'all and we all are awake, how about we schedule a hangout for tomorrow\n",
      "I miss you all!\n",
      "\n",
      "2\n",
      "4060\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-16 20:24\n",
      "Alright, which time would be fine with you?\n",
      "\n",
      "1\n",
      "4061\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 20:57\n",
      "jan: https://github.com/scikit-learn/scikit-learn/pull/11570\n",
      "\n",
      "1\n",
      "4062\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-16 21:10\n",
      "@rth https://github.com/scikit-learn/scikit-learn/pull/11561 merge this since your comments have been addressd\n",
      "\n",
      "1\n",
      "4063\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 21:21\n",
      "@rth do you know about this? 11572\n",
      "#11572\n",
      "it's about AUC reorder\n",
      "\n",
      "3\n",
      "4064\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-16 21:47\n",
      "@amueller https://github.com/scikit-learn/scikit-learn/pull/10495/files\n",
      "\n",
      "1\n",
      "4065\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-16 21:52\n",
      "can someone change the working in https://github.com/scikit-learn/scikit-learn/pull/10495 from interpret to convert?\n",
      "\n",
      "1\n",
      "4066\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-16 22:04\n",
      "I'll do a PR, rewording it\n",
      "\n",
      "1\n",
      "4067\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-16 22:11\n",
      "Second review needed on the SimpleImputer which was buggy for sparse matrix\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11496\n",
      "\n",
      "2\n",
      "4068\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 05:27\n",
      "Sorry, I missed the reply about the hangout. Let's do it when you guys are up, and maybe an hour or so after you have started. That gives you time to get organized.\n",
      "\n",
      "1\n",
      "4069\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 12:57\n",
      "https://github.com/scikit-learn/scikit-learn/issues/11536 is blocker and needs a discussion\n",
      "\n",
      "1\n",
      "4070\n",
      "5a280cdcd73408ce4f820b9c\n",
      "2018-07-17 13:43\n",
      "scikit-learn/scikit-learn#11557 ready for review\n",
      "\n",
      "1\n",
      "4071\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 14:16\n",
      "@amueller #11577\n",
      "\n",
      "1\n",
      "4072\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-07-17 14:40\n",
      "@ogrisel  could you have a look at a small BLAS related change in https://github.com/scikit-learn/scikit-learn/pull/11420 by @jakirkham Thanks.\n",
      "\n",
      "1\n",
      "4073\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-17 14:54\n",
      "quick reviews on #11593 please?\n",
      "\n",
      "1\n",
      "4074\n",
      "53232ac75e986b0712efe3af\n",
      "2018-07-17 14:54\n",
      "@amueller https://github.com/scikit-learn/scikit-learn/pull/11592\n",
      "\n",
      "1\n",
      "4075\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 15:09\n",
      "@amueller  maybe you can merge #11124\n",
      "(or anyone who can click the green button !0\n",
      ")\n",
      "\n",
      "3\n",
      "4076\n",
      "54e07d1515522ed4b3dc0852\n",
      "2018-07-17 15:13\n",
      "review https://github.com/scikit-learn/scikit-learn/pull/11535 ?\n",
      "\n",
      "3\n",
      "4077\n",
      "5b4c47d0d73408ce4fa101b0\n",
      "2018-07-17 15:15\n",
      "@GaelVaroquaux can you have a look at #11589 ?\n",
      "\n",
      "1\n",
      "4078\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 15:17\n",
      "The travis backlog is starting to be a major problem. Does someone have a contact at travis to ask them if they could bump our plan for the day?\n",
      "@jbschiratti : on it\n",
      "\n",
      "2\n",
      "4079\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 15:17\n",
      "Can I get comments in #11563 maybe it can be merged without CI\n",
      "\n",
      "3\n",
      "4080\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-17 15:25\n",
      "@GaelVaroquaux are you contacting travis?\n",
      "\n",
      "1\n",
      "4081\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 15:35\n",
      "What contact should I use?\n",
      "For travis?\n",
      "\n",
      "2\n",
      "4082\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-17 15:39\n",
      "By the way I'll like to have discussion regarding Stacking and the issue of fit.transform != fit_transform such that we can unlock the PR\n",
      "I know this is not for the coming release thought\n",
      "\n",
      "2\n",
      "4083\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-17 15:39\n",
      "they have an email thing on the website @GaelVaroquaux\n",
      "support@travis-ci.com\n",
      "well but this sounds like we need a slep\n",
      "so the outcome of this discussion is: we disagree so far\n",
      "I'll write a slep, we can discuss it and we can vote\n",
      "\n",
      "5\n",
      "4084\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 15:40\n",
      "If we want to do a hangout, it's in the next hour or so.\n",
      "@amueller : on it\n",
      "Good with the call.\n",
      "Do you want to call me\n",
      "When you do, I'll run in the room next door, and whoever wants will join me\n",
      "\n",
      "5\n",
      "4085\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-17 15:40\n",
      "are there particular topics for the hangout? the stacking fit_transform? pandas column names? the governance doc?\n",
      "\n",
      "2\n",
      "4086\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 15:44\n",
      "Done for travis. We'll see what they reply\n",
      "\n",
      "1\n",
      "4087\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-17 15:53\n",
      "whether we should backport estimator-tags to the RC branch if I can finish it within a week lol ;)\n",
      "(semi-kidding)\n",
      "should we do the call?\n",
      "\n",
      "3\n",
      "4088\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-17 16:05\n",
      "I'll try to call you.\n",
      "\n",
      "1\n",
      "4089\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-17 16:37\n",
      "we can call you by phone on a free mobile\n",
      "\n",
      "1\n",
      "4090\n",
      "560313510fc9f982beb1a331\n",
      "2018-07-17 16:52\n",
      "> The travis backlog is starting to be a major problem.  There is an auto-cancellation feature for PRs with outdated commits  ref: https://blog.travis-ci.com/2017-03-22-introducing-auto-cancellation\n",
      "There's a similar feature for PRs on AppVeyor\n",
      "HTH\n",
      "\n",
      "3\n",
      "4091\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-17 16:53\n",
      "I think that we have that\n",
      "activated\n",
      "\n",
      "2\n",
      "4092\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-07-17 16:55\n",
      "yes, both in Travis and AppVeyor, but not circle ci..\n",
      "\n",
      "1\n",
      "4093\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-17 16:56\n",
      "@amueller The PR regarding the n_estimators=100 in Forest is ready for review\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11542\n",
      "\n",
      "2\n",
      "4094\n",
      "53232ac75e986b0712efe3af\n",
      "2018-07-17 16:59\n",
      "@GaelVaroquaux and other people there: are you OK with switching the default in `ColumnTransformer` from `remainder='passthrough'` to `remainder='drop'` ?\n",
      "\n",
      "1\n",
      "4095\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-17 17:01\n",
      "The motivation is that silently passing through uniquely identifying columns (e.g. a user id) will lead to catastrophic  overfitting that beginner users will have a hard time to debug.\n",
      "\n",
      "3\n",
      "4096\n",
      "5b4e20f7d73408ce4fa13b54\n",
      "2018-07-17 17:03\n",
      "appveyor is failing very quickly (less than 1min) for a lot of PRs\n",
      "\n",
      "1\n",
      "4097\n",
      "560313510fc9f982beb1a331\n",
      "2018-07-17 17:07\n",
      "We have a script for fast cancelling old builds in c-f. Here's a [usage example]( https://github.com/conda-forge/numpy-feedstock/blob/0d8089f7d346fabaa8bf1ea948d89a9c42e9f9c8/.circleci/fast_finish_ci_pr_build.sh#L3-L4 ). This works on Travis CI, AppVeyor, and CircleCI as long as the proper environment variables are supplied  ref: https://github.com/conda-forge/conda-forge-ci-setup-feedstock/blob/master/recipe/ff_ci_pr_build.py\n",
      "\n",
      "1\n",
      "4098\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-17 17:14\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11583\n",
      "\n",
      "1\n",
      "4099\n",
      "5a280cdcd73408ce4f820b9c\n",
      "2018-07-17 17:17\n",
      "scikit-learn/scikit-learn#11585 on sparse PCA needs review\n",
      "\n",
      "2\n",
      "4100\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 17:20\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11599\n",
      "\n",
      "1\n",
      "4101\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-07-17 17:22\n",
      "Thanks @jakirkham , that's very useful. I'll look into it.\n",
      "\n",
      "1\n",
      "4102\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 17:24\n",
      "here's how appveyor kills all but last commit https://github.com/conda-forge/staged-recipes/blob/master/.appveyor.yml#L19-L23\n",
      "https://github.com/conda-forge/staged-recipes/blob/master/.circleci/fast_finish_ci_pr_build.sh\n",
      "here's for CircleCI\n",
      "\n",
      "3\n",
      "4103\n",
      "5a83f33dd73408ce4f8d133f\n",
      "2018-07-17 17:33\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11526 is also ready for a last review/merge\n",
      "\n",
      "1\n",
      "4104\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 17:40\n",
      "Travis bumped our resources!!\n",
      "They rock\n",
      "We'll need to thank them\n",
      "\n",
      "3\n",
      "4105\n",
      "5a280cdcd73408ce4f820b9c\n",
      "2018-07-17 18:15\n",
      "I'm off until tomorrow guys.  You can have a look at scikit-learn/scikit-learn#11596 and continue if you wanna merge quickly.\n",
      "\n",
      "1\n",
      "4106\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 18:26\n",
      "Hey, does anybody understand what the problem is on appveyor: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.23610/job/fab9oqavus2wy8sa ?\n",
      "It's happening on several PR. I don't love the smell of it.\n",
      "\n",
      "2\n",
      "4107\n",
      "5a83f33dd73408ce4f8d133f\n",
      "2018-07-17 18:48\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11578 is almost green on tests, ready for a second review\n",
      "\n",
      "1\n",
      "4108\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-17 19:00\n",
      "looks like travis bumped up to 10 according to @jorisvandenbossche\n",
      "@GaelVaroquaux did you tweet about it?\n",
      "we should also tweet to enthought maybe?\n",
      "\n",
      "3\n",
      "4109\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 19:03\n",
      "Yes, travis did bump up to 10.\n",
      "It makes a big different\n",
      "I didn't tweet, but I'll thank them in the blog post\n",
      "\n",
      "3\n",
      "4110\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-17 19:05\n",
      "can someone riddle me this? https://github.com/scikit-learn/scikit-learn/pull/11542\n",
      "SAG test failure on python2.7 in the n_estimators=100 branch?!\n",
      "@GaelVaroquaux you mean the appveyor, not the tranvis I mentioned, right?\n",
      "ok cool\n",
      "\n",
      "4\n",
      "4111\n",
      "5571fe1015522ed4b3e17d90\n",
      "2018-07-17 19:06\n",
      "No idea about the AppVeyor problem but it does seem to happen quite often.\n",
      "\n",
      "1\n",
      "4112\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-17 19:06\n",
      "@GaelVaroquaux the appveyor issue is recent. One possibility might be that have changed something in their API and that breaks and that broke our trick to automatically cancel / skip builds on PRs that have received new push events in the mean time.\n",
      "\n",
      "3\n",
      "4113\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-17 19:09\n",
      "I don't know if it's random with a very high likelihood or if it's a deterministic failure. It might be a temporary outage.\n",
      "\n",
      "2\n",
      "4114\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-07-17 19:12\n",
      "Somebody to open the door of Enthought ;)\n",
      "\n",
      "1\n",
      "4115\n",
      "5571fe1015522ed4b3e17d90\n",
      "2018-07-17 19:12\n",
      "I have AppVeyor enabled in my fork and I have some green builds recently: https://ci.appveyor.com/project/lesteve/scikit-learn/history\n",
      "This could well be the HTTP request we use which is problematic.\n",
      "\n",
      "2\n",
      "4116\n",
      "5571fe1015522ed4b3e17d90\n",
      "2018-07-17 19:18\n",
      "I'll look at the AppVeyor problem more in details.\n",
      "\n",
      "1\n",
      "4117\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-17 19:22\n",
      "Thanks Loic!\n",
      "\n",
      "1\n",
      "4118\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-17 19:30\n",
      "my current reading is that we shouldn't have done the error_score deprecation in ``_fit_and_score`` but in BaseSearchCV. But it depends on what we want to do in learning_curve, cross_validate and cross_val_score (and other places I haven't thought of)\n",
      "https://github.com/scikit-learn/scikit-learn/issues/11576\n",
      "\n",
      "2\n",
      "4119\n",
      "53232ac75e986b0712efe3af\n",
      "2018-07-17 19:51\n",
      "@amueller column transformer remainder change: https://github.com/scikit-learn/scikit-learn/pull/11603\n",
      "\n",
      "1\n",
      "4120\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 19:52\n",
      "https://github.com/scikit-learn/scikit-learn/pull/11604\n",
      "try to fix appveyor !\n",
      "\n",
      "2\n",
      "4121\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 20:13\n",
      "@amueller https://github.com/scikit-learn/scikit-learn/pull/11570/files#r203161922\n",
      "\n",
      "1\n",
      "4122\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-17 20:34\n",
      "so @rth made the good point that the deprecation warning thing should be removed in the release\n",
      "\n",
      "1\n",
      "4123\n",
      "5a83f33dd73408ce4f8d133f\n",
      "2018-07-17 20:40\n",
      "my PR is green now if someones wants to be the 2nd reviewer: https://github.com/scikit-learn/scikit-learn/pull/11578\n",
      "\n",
      "1\n",
      "4124\n",
      "5571fe1015522ed4b3e17d90\n",
      "2018-07-17 20:44\n",
      "We looked at AppVeyor with Sik. There is a work-around in master and all the current running AppVeyor builds have been killed. Just push a new commit in your branch if you want AppVeyor to run again or ask me while I have a sklearn-ci tab opened.\n",
      "\n",
      "1\n",
      "4125\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-17 20:53\n",
      "I will also cancel all the circle ci builds as the queue is far too long. We can restart new builds on new PR (preferably recently opened ones, or those with a recent master merge commit).\n",
      "\n",
      "1\n",
      "4126\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 22:01\n",
      "any comments here https://github.com/scikit-learn/scikit-learn/pull/11213\n",
      "??\n",
      "\n",
      "2\n",
      "4127\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 22:06\n",
      "@lesteve https://github.com/scikit-learn/scikit-learn/pull/11213\n",
      "sorry https://github.com/scikit-learn/scikit-learn/pull/11552\n",
      "\n",
      "2\n",
      "4128\n",
      "547d8325db8155e6700da60b\n",
      "2018-07-17 22:42\n",
      "@lesteve https://github.com/scikit-learn/scikit-learn/pull/11563\n",
      "\n",
      "1\n",
      "4129\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-18 05:15\n",
      "Hum, CI is a bit in a mess. There is going to be some work there\n",
      "\n",
      "1\n",
      "4130\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-07-18 05:23\n",
      "In the lastest commit on master there is just 1 test failing in one build.  Attempting to fix that in https://github.com/scikit-learn/scikit-learn/pull/11617 Appveyor and Circle CI should be hopefully OK, the issue is mostly a very long queue..\n",
      "\n",
      "1\n",
      "4131\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-18 05:32\n",
      "Great! I was about to look at this face issue! Thanks!\n",
      "OK, @rth: I reviewed it. Travis will be done with it in 20mn. It needs a second review\n",
      " scikit-learn/scikit-learn#11617  is green and needs a second review\n",
      "\n",
      "3\n",
      "4132\n",
      "5571fe1015522ed4b3e17d90\n",
      "2018-07-18 06:09\n",
      "Travis is green in master :fireworks: !\n",
      "https://travis-ci.org/scikit-learn/scikit-learn/builds/405208107\n",
      "\n",
      "2\n",
      "4133\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-18 17:45\n",
      "It seems that travis is failing again on one of the configuration. I have made a PR that should fix that: https://github.com/scikit-learn/scikit-learn/pull/11625\n",
      "It fixes travis indeed. It would be good to have reviews +merge, as many PRs are red because of this small issue\n",
      "\n",
      "2\n",
      "4134\n",
      "54e07d0815522ed4b3dc0850\n",
      "2018-07-18 18:42\n",
      "OK, I now realize that it was fixed in the mean time by downgrading the joblib requirement on travis :). Still good to have!\n",
      "\n",
      "1\n",
      "4135\n",
      "56a5a8fce610378809be08d1\n",
      "2018-07-19 06:24\n",
      "Hi, is it possible to use different distance metrics for `kmeans`? I noticed that `cosine` and `manhattan` are already implemented in `metrics/pairwise.py`.  Is there any quick dirty way to somehow specify these metrics instead of euclidean distance for `kmeans`?\n",
      "\n",
      "1\n",
      "4136\n",
      "560313510fc9f982beb1a331\n",
      "2018-07-19 20:08\n",
      "Is there a way to check what BLAS scikit-learn is built with? Maybe like an info function or something?\n",
      ":tada:\n",
      "Thank you\n",
      "\n",
      "3\n",
      "4137\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-19 20:08\n",
      "@jakirkham there's a pr for that ;)\n",
      "but it's whatever numpy is build with, I think\n",
      "\n",
      "6\n",
      "4138\n",
      "560313510fc9f982beb1a331\n",
      "2018-07-19 20:13\n",
      "If it says `cblas`, does that mean scikit-learn's internal BLAS?\n",
      "\n",
      "1\n",
      "4139\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-19 20:25\n",
      "I think we stopped shipping blas, didn't we?\n",
      "\n",
      "1\n",
      "4140\n",
      "5b4c9e4bd73408ce4fa10b88\n",
      "2018-07-19 20:33\n",
      "@amueller  not merged yet :)\n",
      "\n",
      "1\n",
      "4141\n",
      "5a280cdcd73408ce4f820b9c\n",
      "2018-07-19 21:25\n",
      "@jakirkham the method you are looking for is `sklearn._build_utils.get_blas_info()`\n",
      "\n",
      "1\n",
      "4142\n",
      "560313510fc9f982beb1a331\n",
      "2018-07-19 21:59\n",
      "Great thanks!\n",
      "\n",
      "1\n",
      "4143\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-07-22 06:59\n",
      "Hello - for BallTree can I use a user defined metric as great circle distance calculated from PyProj ? I believe the answer is YES but just confirming it here on gitter\n",
      "as shown in this SO answer - https://stackoverflow.com/questions/21052509/sklearn-knn-usage-with-a-user-defined-metric\n",
      "\n",
      "2\n",
      "4144\n",
      "541a528b163965c9bc2053de\n",
      "2018-07-23 06:42\n",
      "We still ship a bunch of cblas and actually use it in the windows and linux wheels. The easiest way to get rid of that would be to use scipy cython BLAS functions API but this won't be possible before we bump up the dependencies.\n",
      "\n",
      "1\n",
      "4145\n",
      "560313510fc9f982beb1a331\n",
      "2018-07-23 17:10\n",
      "Sounds like issue ( https://github.com/scikit-learn/scikit-learn/issues/11638 ) :wink:\n",
      "\n",
      "1\n",
      "4146\n",
      "59cbaaecd73408ce4f779597\n",
      "2018-07-23 19:16\n",
      "Hello peoplw\n",
      "I want to begin with scikit, how should I begin ?\n",
      "Please suggest\n",
      "\n",
      "3\n",
      "4147\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-23 19:18\n",
      "@geekyharshal https://jakevdp.github.io/PythonDataScienceHandbook/\n",
      "\n",
      "1\n",
      "4148\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-07-23 22:19\n",
      "Hey guys, are there any prebuild Docker images to work with sagemaker for sklearn? They support Apache MXNet or TensorFlow directly but not sklearn. https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb\n",
      "This is an option if not https://github.com/jupyter/repo2docker\n",
      "\n",
      "2\n",
      "4149\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-24 11:58\n",
      "that example shows you what to do, doesn't it?\n",
      "Don't think anyone here has used sagemaker\n",
      "\n",
      "2\n",
      "4150\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-07-24 17:55\n",
      "I can build it myself but its more a question of if there are standard images. Maybe this is more or a conda question since it is about making a consistent collection of packages.\n",
      "I would suggest you guys take a look at sagemaker. There are not any good exampke for scikit learn for the whole train test deploy. Would be good for adoption of sklearn. Althoughj, i am sure you have no shortage of projects\n",
      "\n",
      "2\n",
      "4151\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-07-24 17:58\n",
      "I was visiting the sagemaker team like a month ago\n",
      "\n",
      "1\n",
      "4152\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-07-24 18:37\n",
      "Cool. My team uses sklearn a lot and we are just getting started with sagemaker for deployment. We do everything locally now but will move to AWS for train and predict. We will try to come up with a good example for the deployment and maybe add it to their existing examples.\n",
      "\n",
      "1\n",
      "4153\n",
      "5a5a272ad73408ce4f880912\n",
      "2018-07-26 13:36\n",
      "Hello! My name is Tushar.  I recently finished a course on machine learning and now I'm making some projects. One of them is object classification, I have googled all I could.  The problem is, I am confused on how to preprocess the images to work. I have worked on text. Now I am having troubles with images. All the things  I saw were using deep learning and tensorFlow. Articles with sklearn were using the dataset already provided in datasets. And that wasn't helping. If anyone can guide me, I'd appreciate it. Thanks\n",
      "\n",
      "1\n",
      "4154\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-07-27 03:34\n",
      "Typically each pixel is encoded as rgb so 3 features per pixel. Sklearn can try to classify this with a number of algorithms but I suspect you will not get far. The pooling and covolutional layers in a NN are what gives the power for image classification. Sklearn does not have any NN beyond an MLP so you lose that ability.\n",
      "\n",
      "1\n",
      "4155\n",
      "5b52e812d73408ce4fa1c2c2\n",
      "2018-07-28 18:22\n",
      "hey i am here to contibute pls assign me some task\n",
      "\n",
      "1\n",
      "4156\n",
      "5a5a272ad73408ce4f880912\n",
      "2018-07-28 19:41\n",
      "Thank you very much Keith!\n",
      "\n",
      "1\n",
      "4157\n",
      "5a5a272ad73408ce4f880912\n",
      "2018-07-28 19:42\n",
      "I actually finished a machine learning course and I'd like to make a few projects before I move on to deep learning and neural networks.\n",
      "\n",
      "1\n",
      "4158\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-07-29 05:35\n",
      "Then don't do image recognition\n",
      "\n",
      "1\n",
      "4159\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-07-29 18:21\n",
      "@Ishaan28malik Tasks are more claimed than assigned. Go to the open issues on github and find one you can do. Ask if anybody is activly working on it. If not then you can give it a try\n",
      "\n",
      "1\n",
      "4160\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-08-01 09:18\n",
      "Hello - are we allowed to put links to SO questions here ?\n",
      "@amueller Ok then here you go - https://stackoverflow.com/questions/51627721/typeerror-with-scikit-learns-balltree\n",
      "\n",
      "2\n",
      "4161\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-01 09:19\n",
      "@winash12 you can but I wouldn't encourage it, I guess?\n",
      "Generally it's better to ask on stackoverflow\n",
      "\n",
      "10\n",
      "4162\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-08-01 09:25\n",
      "So like in scipy it must be the coordinates of point\n",
      "in this case x and y\n",
      "hang on let me run my toy example\n",
      "type(matches) is a list\n",
      "\n",
      "11\n",
      "4163\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-01 09:27\n",
      "ok that's what I thought. bt.data is a memory view, the docs are wrong :-/\n",
      "\n",
      "1\n",
      "4164\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-08-01 09:33\n",
      "@amueller  An honor to converse with the author of scikit-learn !\n",
      "Thank you for opening the issue on github !!\n",
      "\n",
      "6\n",
      "4165\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-01 09:37\n",
      "you can just do np.array on bt.data to get a numpy array for now, but you already have the array as ``points`` so there's really no need...\n",
      "\n",
      "8\n",
      "4166\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-01 09:44\n",
      "that wiki pages says the harvesine formula is more stable?\n",
      "not my area of expertise but look at the wiki page\n",
      "that's probably what pyproj is using or should be using?\n",
      "\n",
      "3\n",
      "4167\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-08-01 09:48\n",
      "Can I raise this as a issue on github as people on my project want to use pyproj ?\n",
      "\n",
      "1\n",
      "4168\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-01 09:54\n",
      "you can ask on pyproj? who did you talk to as sklearn? Jake?\n",
      "\n",
      "5\n",
      "4169\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-01 09:59\n",
      "jake is unlikely to reply to an issue on github\n",
      "\n",
      "1\n",
      "4170\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-08-01 10:00\n",
      "By the way I used points in that code example and I get another error -` File \"testballtree.py\", line 23, in main     x1,y1 = bt.points[matches].T AttributeError: 'sklearn.neighbors.ball_tree.BallTree' object has no attribute 'points'`\n",
      "np.array gives the same exception as the one I showed on SO\n",
      "\n",
      "2\n",
      "4171\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-01 10:01\n",
      "not bt.points. your array points\n",
      "\n",
      "2\n",
      "4172\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-08-01 10:58\n",
      "I think PyProj uses the Vincenty formula\n",
      "\n",
      "1\n",
      "4173\n",
      "572d9bc3c43b8c601971985f\n",
      "2018-08-01 13:42\n",
      "hello\n",
      "\n",
      "1\n",
      "4174\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-08-02 07:00\n",
      "@amueller I got BallTree to work with PyProj's inv()\n",
      "\n",
      "1\n",
      "4175\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-08-02 07:14\n",
      "@amueller As mentioned by you in this issue - https://github.com/scikit-learn/scikit-learn/issues/6256 you are suggesting that the custom myfunc can be written in Cython which I am willing to do. But we are shipping a product. What happens when there is a new version of scikit-learn and what will happen to my Cython extension ?\n",
      "\n",
      "1\n",
      "4176\n",
      "58d1e48dd73408ce4f52af1b\n",
      "2018-08-03 02:18\n",
      "The second question I had is the following - in my field the requirement is that the Nearest Neighbors search for a particular grid point of say 2D data must be include contributions from above and below the surface in question. Supposing i have three 2-D surfaces. top and below the current surface. Then I do a radius of influence query for the three surfaces separately. The interpolated value for the grid point in the current surface must include contributions from all three surfaces from above and below the current surface as well as the current surface\n",
      "\n",
      "1\n",
      "4177\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-08-07 22:26\n",
      "Hey, I notices something strange in the documentation. For the GradientBoostingClassifier is says the criterion=friedman_mse for the default. I would have expected criterion=gini like in RandomForestClassifier since mse is typically used for regression not classificaiton. The text is the same as in GradientBoostingRegressor. Is it possible that it was copied by accident?\n",
      "\n",
      "1\n",
      "4178\n",
      "560313510fc9f982beb1a331\n",
      "2018-08-10 17:27\n",
      "Has anyone played with memory pools or custom allocators in the context of scikit-learn?\n",
      "\n",
      "1\n",
      "4179\n",
      "57957e3e40f3a6eec05c4d2a\n",
      "2018-08-13 23:06\n",
      "Hey guys. I'm dealing with an older version of sklearn (0.13.1) (not able to upgrade at the time) where the class attribute `_label` for `svm.SVC` object is used instead of `classes_`. So where I'd have `'classes_': array(['compatible', 'incompatible'], dtype=object)` in my fitted svm object, how would I replace that with `_label`? Simply exchanging the key names doesn't do the trick. I get the error message: `dtype mismatch, expected 'int32_t' but got Python object`.  If you guys could help me figure this out, you're be even more awesome than you already are.\n",
      "\n",
      "1\n",
      "4180\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-08-20 10:06\n",
      "@jakirkham not to my knowledge. What approaches were you considering and why do you think it might be beneficial?\n",
      "\n",
      "1\n",
      "4181\n",
      "530c03e25e986b0712efafb8\n",
      "2018-08-20 15:17\n",
      "Is there an estimate on a release date of 0.20 ?  No pressure or expectations, I'm just doing some planning.  \"No estimate\" is also a fine response.\n",
      "\n",
      "1\n",
      "4182\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-08-20 15:53\n",
      "@mrocklin See https://github.com/scikit-learn/scikit-learn/pull/11838 for the 0.20.rc1 , probably several weeks later for the 0.20 final .\n",
      "\n",
      "3\n",
      "4183\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-20 16:06\n",
      "hope to do the RC today or tomorrow but I'm an optimist ;)\n",
      "I need to convince some more people that I'm right and they are wrong before we can do that, though\n",
      "\n",
      "2\n",
      "4184\n",
      "560313510fc9f982beb1a331\n",
      "2018-08-21 18:25\n",
      "> not to my knowledge. What approaches were you considering and why do you think it might be beneficial?   There are cases where we seem to be allocating very similar sized blocks for arrays repeatedly (e.g. enforcing Fortran order on inputs, applying LASSO repeatedly on similar sized arrays). The time for the allocations here is larger than I would naively expect, which makes me think that whatever default memory allocation scheme is running into issues.  Am debating the value of allocating a larger array that includes N such blocks and distributes sliced chunks of it for these allocation operations. This may be too naive, but this is my first thought. WDYT?\n",
      "\n",
      "1\n",
      "4185\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-21 18:52\n",
      "does anyone get what I'm doing wrong with appveyor?\n",
      "https://ci.appveyor.com/project/sklearn-wheels/scikit-learn-wheels/build/job/8ij4qa5mcayg8xr8\n",
      "\n",
      "2\n",
      "4186\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-22 01:51\n",
      "the appveyor thing is mostly what's preventing me from doing the RC right now :-/\n",
      "\n",
      "1\n",
      "4187\n",
      "5582e83c15522ed4b3e21bef\n",
      "2018-08-22 07:01\n",
      "@amueller there seem to be a few different issues. Very strange.\n",
      "There are unicode differences. 'foo' vs. u'foo'\n",
      "and there are also a lot of `, dtype=int32)`vs  `, dtype=int64)`\n",
      "\n",
      "3\n",
      "4188\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-22 15:21\n",
      "@bgruening that's actually not the problem I was having, @jnothman has fixed my problem :)\n",
      "We're supposed to be skipping doctests on 32bit and possibly on py27. or maybe there was a fix in the doctests for py27 changes? Anyone remember lol?\n",
      "\n",
      "2\n",
      "4189\n",
      "560313510fc9f982beb1a331\n",
      "2018-08-23 14:13\n",
      "If anyone has time to look at PR ( https://github.com/scikit-learn/scikit-learn/pull/11896 ) and/or PR ( https://github.com/scikit-learn/scikit-learn/pull/11898 ), that would be greatly appreciated. The former is just a slimmed down version of PR ( https://github.com/scikit-learn/scikit-learn/pull/11507 )\n",
      "\n",
      "1\n",
      "4190\n",
      "5b52e812d73408ce4fa1c2c2\n",
      "2018-08-24 08:45\n",
      "Ok thanks @DrEhrfurchtgebietend  any link for the issues .pls\n",
      "\n",
      "1\n",
      "4191\n",
      "5b52e812d73408ce4fa1c2c2\n",
      "2018-08-24 16:58\n",
      "@DrEhrfurchtgebietend  any link for joining the org at github like any invitation or anything ..\n",
      "\n",
      "1\n",
      "4192\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-08-31 19:11\n",
      "Is there a reason why LinearSVC doesn't have predict_proba?\n",
      "oh!  Hmm...\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html has predict_proba\n",
      "@amueller  I am confused why one is a probabilistic model but the other isn't\n",
      "I mean there is no randomness in the decision tree classifier\n",
      "\n",
      "5\n",
      "4193\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-31 19:12\n",
      "@lesshaste yeah it's not a probabilistic model\n",
      "\n",
      "1\n",
      "4194\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-31 19:51\n",
      "randomness has nothing to do with whether it's a probabilistic model. there's no randomness in logistic regression\n",
      "\n",
      "1\n",
      "4195\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-08-31 20:59\n",
      "@amueller  Yes sorry my mistake. Could you give a short reason why one is a probabilistic model and the other isn't?\n",
      "Please\n",
      "\n",
      "2\n",
      "4196\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-08-31 21:08\n",
      "not really? There is a way to interpret a tree model in a probabilistic way but there is no way to interpret an SVM in a probabilistic way\n",
      "\n",
      "1\n",
      "4197\n",
      "564789be16b6c7089cbab8b7\n",
      "2018-09-01 10:16\n",
      "@amueller  Thanks, that's interesting. I noticed that you can always use  http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html to make probabilities\n",
      "\n",
      "1\n",
      "4198\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-09-01 16:02\n",
      "yes you can\n",
      "\n",
      "1\n",
      "4199\n",
      "5b6fe633d73408ce4fa45408\n",
      "2018-09-01 18:56\n",
      "I tagged my pull request with WIP. However,  right now, Id like if someone would have a look over it. Im not sure if my approach is the desired one. I could do some stuff on documentation of course, but it might be that my approach is declined entirely. So should I change the name of the request? For completeness: this is the pull request (https://github.com/scikit-learn/scikit-learn/pull/11891). Of course I know that time is precious for all of us.\n",
      "\n",
      "1\n",
      "4200\n",
      "5b85ed51d73408ce4fa6261d\n",
      "2018-09-03 14:55\n",
      "I know this is out of topic here but trying to get as much as data we can. Can you please help me understand your use of agile methods by completing this one minute survey https://www.surveymonkey.com/r/98JMTJ2\n",
      "\n",
      "1\n",
      "4201\n",
      "5582e83c15522ed4b3e21bef\n",
      "2018-09-04 10:46\n",
      "Hi all! What is the current recommended was to save models and redistribute them. We tried https://github.com/uchicago-cs/deepdish and similar concepts, but all do pickle the object at some point. This is not relocatable and breaks with different python versions etc ... is there any emerging standard. Any hint how the ML community is tackling this at the moment or in the future?\n",
      "@DrEhrfurchtgebietend I consider this a very bad hack :)\n",
      "Is there no emerging standard :(\n",
      "\n",
      "3\n",
      "4202\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-09-04 16:48\n",
      "@bgruening Use pickle from joblib and build in a docker container. It seems to be the standatd method but is not without flaws. In a standard local server deployment i rarely use the docker containers and just keep track of package versions.\n",
      "\n",
      "1\n",
      "4203\n",
      "55d21ee30fc9f982beadabb8\n",
      "2018-09-04 19:29\n",
      "I would refer to the talk of Alejandro Saucedo at EuroSciPy couple of days ago https://axsauze.github.io/scalable-data-science/#/\n",
      "and more precisely\n",
      "https://github.com/axsauze/awesome-machine-learning-operations\n",
      "However, I did not check all the solution and I cannot ensure you that you will not get the pickling issue that you mentioned.\n",
      "\n",
      "4\n",
      "4204\n",
      "5b6fe633d73408ce4fa45408\n",
      "2018-09-04 19:43\n",
      "thx for sharing Guillaume\n",
      "\n",
      "1\n",
      "4205\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-09-07 20:40\n",
      "Much of that has to do with code versioning not really the deployment method. PKL + docker is a common method mentioned in the Guillaume's links\n",
      "\n",
      "1\n",
      "4206\n",
      "5b9b749bd73408ce4fa82693\n",
      "2018-09-14 08:44\n",
      "Hi. I'm a student and am new to sklearn. One of my peers told me that once I get a job, I can't and shouldn't use sklearn for professional projects. Is this true? Is not sklearn completely open source and free to use?\n",
      "which license does it come under?\n",
      "\n",
      "2\n",
      "4207\n",
      "5b9b749bd73408ce4fa82693\n",
      "2018-09-14 08:48\n",
      "If the response is too detailed to be mentioned in a chat, please respond on my email : ujjawalpanchal32@gmail.com Thanks for your help. Looking forward to replies.\n",
      "\n",
      "1\n",
      "4208\n",
      "5582e83c15522ed4b3e21bef\n",
      "2018-09-14 10:00\n",
      "@Ujjawal-K-Panchal just look at github: https://github.com/scikit-learn/scikit-learn/blob/master/COPYING\n",
      "\n",
      "1\n",
      "4209\n",
      "5b9b749bd73408ce4fa82693\n",
      "2018-09-15 03:34\n",
      "Thanks!\n",
      "\n",
      "1\n",
      "4210\n",
      "561a58f7d33f749381a8ff2f\n",
      "2018-09-15 11:20\n",
      "interesting question: https://stats.stackexchange.com/questions/367051/how-to-learn-new-clusters-on-residuals-of-kmeans\n",
      "given you have 40k frozen centroids, how to learn 40k more?\n",
      "`partial_fit` and `_mini_batch_step` don't feel very modular\n",
      "\n",
      "3\n",
      "4211\n",
      "5b953136d73408ce4fa76b1f\n",
      "2018-09-15 14:54\n",
      "hello\n",
      "\n",
      "1\n",
      "4212\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-09-18 12:15\n",
      "question: does it make sense to go through the docstrings and make sure the default values are mentioned for each optional parameter, preferably with a consistent format? Or should we just try to enforce it for new PRs and leave the rest alone? There are some funny cases though (http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor)\n",
      "@amueller why would it create a conflict with existing PRs, if those PRs haven't change the line corresponding to the parameter? And if they have, it does make sense for them to follow whatever convention we choose anyway, doesn't it?\n",
      "\n",
      "4\n",
      "4213\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-09-18 12:16\n",
      "@adrinjalali making sure defaults are there would be great. consistent formatting has the issue that there's gonna be a lot of changes creating lots of conflicts with existing PRs.\n",
      "but it'll be hard to search for missing defaults without a consistent format :-/\n",
      "\n",
      "2\n",
      "4214\n",
      "58bef343d73408ce4f4f062f\n",
      "2018-09-22 05:52\n",
      "Hello everybody! I just wrote an article about Machine Learning. Let me know what you think about it :wink: https://medium.freecodecamp.org/how-to-predict-likes-and-shares-based-on-your-articles-title-using-machine-learning-47f98f0612ea\n",
      "\n",
      "1\n",
      "4215\n",
      "5657989e16b6c7089cbc5309\n",
      "2018-09-24 08:03\n",
      "@flaviohenriquecbc nice report! Two suggestions. One, it would be nice to explain what the red and blue colors mean. I didn't get a strong sense of what those were when reading the Medium post or your final report PDF. And two, if you were to improve this accuracy of your models, maybe consider feature selection to narrow which words you use in your model, instead of using all of them. From your final report, it appears you used all of them, but if I missed that part of your analysis, forgive me. Thanks for sharing your work and results!\n",
      "@flaviohenriquecbc And feel free to share in the [freeCodeCamp DataScience](https://gitter.im/FreeCodeCamp/DataScience) room as well :wink:\n",
      "\n",
      "2\n",
      "4216\n",
      "58bef343d73408ce4f4f062f\n",
      "2018-09-24 08:25\n",
      "thank you for your feedback, @erictleung .. i will try to implement what you said :)\n",
      "\n",
      "1\n",
      "4217\n",
      "58bef343d73408ce4f4f062f\n",
      "2018-09-24 08:43\n",
      "actually the colors didn't mean anything.. it was a style that i used for the graphs\n",
      "\n",
      "1\n",
      "4218\n",
      "564e507e16b6c7089cbb6551\n",
      "2018-09-24 17:43\n",
      "@amueller checked Bottou's sgd code in C++ (https://leon.bottou.org/projects/sgd), I'm not sure where that same formula used for the learning rate can be found there.\n",
      "\n",
      "1\n",
      "4219\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-09-24 20:21\n",
      "@h4k1m0u which one?\n",
      "\n",
      "1\n",
      "4220\n",
      "564e507e16b6c7089cbb6551\n",
      "2018-09-25 13:13\n",
      "@amueller I cannot find this one $\\eta = \\frac{1}{\\alpha (t_0 + t)}$ (http://scikit-learn.org/stable/modules/sgd.html#id1) in Bottou's source code\n",
      "\n",
      "1\n",
      "4221\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-09-25 14:45\n",
      "Isn't that the same as svmsgd?\n",
      "\n",
      "1\n",
      "4222\n",
      "564e507e16b6c7089cbb6551\n",
      "2018-09-25 15:03\n",
      "@amueller I've opened `svm/svmsgd.cpp`, I can't locate that same formula. I found this one which is different: `double eta = eta0 / (1 + lambda * eta0 * t);`\n",
      "\n",
      "3\n",
      "4223\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-09-25 15:35\n",
      ":+1:\n",
      "\n",
      "1\n",
      "4224\n",
      "54de281a15522ed4b3dbfc22\n",
      "2018-09-25 18:23\n",
      "For demonstration purposes, I would like to track the kmeans centers during each iteration of the fit. Is that possible with sklearn kmeans, e.g. with a callback?\n",
      "\n",
      "1\n",
      "4225\n",
      "54de281a15522ed4b3dbfc22\n",
      "2018-09-25 18:50\n",
      "I will try to run kmeans for one iteration at a team, initializing the centers with the centers from the previous kmeans. That should do the trick.\n",
      "\n",
      "3\n",
      "4226\n",
      "5b7904d9d73408ce4fa50f7a\n",
      "2018-09-27 12:15\n",
      "Hello guys, I wish to start contributing to sci-kit learn. As I am a beginner, so can anybody suggest me how to start contributing.\n",
      "\n",
      "1\n",
      "4227\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-09-27 16:09\n",
      "There are issues on GitHub marked with \"easy\"/\"good first issue\" https://github.com/scikit-learn/scikit-learn/issues\n",
      "\n",
      "1\n",
      "4228\n",
      "58a84478d73408ce4f4b3982\n",
      "2018-09-28 07:38\n",
      "Number of features of the model must match the input. Model n_features is 13 and input n_features is 2\n",
      "What does it mean?\n",
      "\n",
      "2\n",
      "4229\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-09-28 15:57\n",
      "@Praful-cs can you please say what's unclear about that so we can improve the error message?\n",
      "The model was trained with 13 features and you give it data with 2 features\n",
      "\n",
      "2\n",
      "4230\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-10-03 15:58\n",
      "@nicolashug you here?\n",
      "so in lbfgs alpha = 1/C\n",
      "I'm trying to figure out the loss computation for lbfgs first because we can most easily access the function\n",
      "\n",
      "3\n",
      "4231\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2018-10-03 15:58\n",
      "yup\n",
      "\n",
      "1\n",
      "4232\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-10-03 16:18\n",
      "@NicolasHug https://pastebin.com/5Vv72rLc\n",
      "these two are identical and identical to what's internally used\n",
      "\n",
      "2\n",
      "4233\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-10-03 16:21\n",
      "and if you want SGD to be equivalent to LogisticRegression, you have indeed to set alpha = 1/(C  * n_samples)\n",
      "\n",
      "1\n",
      "4234\n",
      "5bbc5d22d73408ce4faacee2\n",
      "2018-10-11 17:20\n",
      "Hi. Can you anyone tell me if there is any tutorial for ML ?\n",
      "\n",
      "1\n",
      "4235\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-10-11 18:58\n",
      "@KoulickS https://jakevdp.github.io/PythonDataScienceHandbook/\n",
      "https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners\n",
      "\n",
      "2\n",
      "4236\n",
      "5b15820ad73408ce4f9bec06\n",
      "2018-10-12 19:41\n",
      "Greetings everyone, anyone knows about bitcoin price prediction algorithms ?\n",
      "\n",
      "1\n",
      "4237\n",
      "5b3f6eedd73408ce4f9fd9e7\n",
      "2018-10-13 09:36\n",
      "Hi, i will try to predict customer lifetime period. My dataset has information about customer lifetime period. Should i set this column as target and do regression for predict it?\n",
      "\n",
      "1\n",
      "4238\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-10-13 15:16\n",
      "@MahmoudElsayad you probably would need algorithms which deal well with time series challenges, such as HMM or RNN (mostly LSTM related) models, which are not included in scikit-learn. You may find more information in the following places: - https://hmmlearn.readthedocs.io/en/latest/ - https://machinelearningmastery.com/make-predictions-time-series-forecasting-python/ - https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
      "@talatccan that would be a start. But please note that issues such as preprocessing and scaling your inputs and/or outputs, and hyperparameters of your models, may substantially affect your results. You can refer to the resources posted above your message for more information.\n",
      "\n",
      "2\n",
      "4239\n",
      "5bbc5d22d73408ce4faacee2\n",
      "2018-10-13 16:48\n",
      "What all do you think is the future scope of ML and AI in the field of data science\n",
      "\n",
      "1\n",
      "4240\n",
      "5bc6614fd73408ce4fabae05\n",
      "2018-10-17 00:58\n",
      "https://docs.google.com/viewerng/viewer?url=https://s3.amazonaws.com/acadgildsite/course/masteringdatascience/session23/ACD_MDS_V2_Session_23_Project_1_Main.pdf\n",
      "Someone help with this assigment\n",
      "Ir could be pais\n",
      "\n",
      "3\n",
      "4241\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-10-17 03:19\n",
      "Not cool\n",
      "\n",
      "1\n",
      "4242\n",
      "5bbdfc48d73408ce4faaf97c\n",
      "2018-10-18 07:46\n",
      "  Write a function to return the intercept as a float (rounded to the nearest 3 integers) of a linear regression model  def lin_reg_intercept(X_train, y_train):\n",
      "\n",
      "1\n",
      "4243\n",
      "5abc16fed73408ce4f938d6d\n",
      "2018-10-20 18:45\n",
      "@GONZALORUIZR_twitter what help do u need?\n",
      "\n",
      "1\n",
      "4244\n",
      "5bc6614fd73408ce4fabae05\n",
      "2018-10-21 01:16\n",
      "I need somebody do the assigment and send me via open repository\n",
      "\n",
      "1\n",
      "4245\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-10-24 15:14\n",
      "Is it for course credit? Are you going to pay?\n",
      "\n",
      "1\n",
      "4246\n",
      "5bd4b048d73408ce4facfb79\n",
      "2018-10-28 04:54\n",
      "Hi guys.  I just have a doubt.  Is it better to scale down the target values before using it as ground truth for training a model or can we use the target values as such?\n",
      "My target values are in the range of 100's currently.\n",
      "\n",
      "2\n",
      "4247\n",
      "5668c71116b6c7089cbe1ea3\n",
      "2018-10-28 10:23\n",
      "@MVenkat_28_gitlab if you have one target that you are predicting, scaling should have no real effect. What models/approaches are you using ... maybe there is something I am not thinking about?\n",
      "so rounding error would be a concern if you had massive range in your targets\n",
      "And it is generally easier to start with a classifier rather than regressor ...\n",
      "\n",
      "3\n",
      "4248\n",
      "5bd4b048d73408ce4facfb79\n",
      "2018-10-28 14:38\n",
      "I have only one target.  I'm currently using a regression neural network.\n",
      "@cottrell how do you say that for one target scaling will make no effect?\n",
      "\n",
      "2\n",
      "4249\n",
      "5b7c5fe0d73408ce4fa555ef\n",
      "2018-10-28 17:17\n",
      "hi, everyone i'm new here so anything i need to know?\n",
      "\n",
      "1\n",
      "4250\n",
      "5668c71116b6c7089cbe1ea3\n",
      "2018-10-29 06:59\n",
      "@MVenkat_28_gitlab well, I guess I mean for unregularized regression. Linear scaling mathematically should not have a direct impact for most models and methods. I think the linear scaling will just factor out of everything. Of course, it might help with numerical stability if the target has extreme values.  Basically, intuition is that for regression methods, min F(X) - y is same problem as min F(X) - \\alpha * y but with a modified X. For example, you could just scale the weights of the last layer to get the different Y.   With SGD or whatever method you are using to solve, you could of course get different results with scaled y.   This is not a proof of course ... just found this: https://roamanalytics.com/2016/11/17/translation-and-scaling-invariance-in-regression-models/#Scaling-does-not-affect-unregularized-regression  which is potentially a good illustration of the details.\n",
      "@nisnt2411 Hi, I dunno I jumped in recently without even asking anything :). Seems like a mix of questions from people starting and a few technical discussions. (human) Latency is pretty high. Just try to help people who are stuck if you know some answers to questions I guess.\n",
      "\n",
      "2\n",
      "4251\n",
      "5b7c5fe0d73408ce4fa555ef\n",
      "2018-10-29 08:31\n",
      "@cottrell thanks! I thought there might be some prerequisites .\n",
      "\n",
      "1\n",
      "4252\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-10-29 20:56\n",
      "@nisnt2411 wanting to contribute. Python and ML knowledge is helpful ;)\n",
      "start with the contributors guide and \"good first issue\" tag if you want to start helping\n",
      "and yes, review latency is terribly high unfortunately\n",
      "\n",
      "3\n",
      "4253\n",
      "54fdd51a15522ed4b3dd04b7\n",
      "2018-11-05 07:48\n",
      "Hi all, i see a bunch of `unused variables` alert  in lgtm. https://lgtm.com/projects/g/scikit-learn/scikit-learn/alerts/?mode=tree&ruleFocus=6780086 Would this be a useful PR?\n",
      "\n",
      "1\n",
      "4254\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-11-05 15:20\n",
      "@whiletruelearn not if they are in externals or backports. otherwise yes.\n",
      "\n",
      "1\n",
      "4255\n",
      "5bc6614fd73408ce4fabae05\n",
      "2018-11-05 16:40\n",
      "How wants to code in a kaggle team!\n",
      "\n",
      "1\n",
      "4256\n",
      "5be10c63d73408ce4fadfb10\n",
      "2018-11-06 03:40\n",
      "Hi all! How can I see a error in this build?   https://circleci.com/gh/gilbertoolimpio/scikit-learn/11?utm_campaign=workflow-failed&utm_medium=email&utm_source=notification  Can you help me?\n",
      "\n",
      "1\n",
      "4257\n",
      "5b9b523bd73408ce4fa82230\n",
      "2018-11-06 08:58\n",
      "Hi all I am a newbie here only have coding experience but I would like to contribute in this project so where should I start with\n",
      "\n",
      "1\n",
      "4258\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-11-06 09:01\n",
      "@dwibedis the contributing guides (http://scikit-learn.org/dev/developers/contributing.html) and the \"good first issue\" ones on github (https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) are not a bad place to start, if you haven't checked them already.\n",
      "@OlimpioGilberto_twitter you can download the build log there, and look for errors (if there's any). But I'm not sure why you're looking in that build log. those are mostly for the docs, and that particular one is for python2, which is deprecated in master (it's still there cause future 0.20.xxx releases still support python2). If you're trying to see some build logs and to check the tests, you probably need to check the travis-ci logs, not circle-ci.\n",
      "\n",
      "2\n",
      "4259\n",
      "5be10c63d73408ce4fadfb10\n",
      "2018-11-06 09:36\n",
      "@adrinjalali Thank you! I am looking in the circle-ci because my PR was labeled with an error, because of this problem and I did not find which was the error that was triggered. But now, I suppose this problem is not so serious! Tks!\n",
      "\n",
      "3\n",
      "4260\n",
      "5be10c63d73408ce4fadfb10\n",
      "2018-11-06 09:46\n",
      "@adrinjalali here is my PR number #12524 Tks!\n",
      "\n",
      "1\n",
      "4261\n",
      "541a528b163965c9bc2053de\n",
      "2018-11-07 09:39\n",
      "Hi all, I am trying to update the DNS of scikit-learn.org to enable https, and it broke. Working on fixing it.\n",
      "For reference I was trying to follow the instructions from https://github.com/scikit-learn/scikit-learn/issues/12278\n",
      "It works again \\o/\n",
      "I did nothing more, it just took a bit of time apparently.\n",
      "\n",
      "4\n",
      "4262\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-11-07 09:42\n",
      "woohooo\n",
      "now computer says yes :P\n",
      "\n",
      "2\n",
      "4263\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2018-11-07 09:49\n",
      "Now I won't be able  to use scikit-learn.org with Airports wifi one of the remaining websites that doesn't use https to get redirected to the network sign-in. So this can break some use cases :) Great that it happened, can confirm it works!\n",
      "\n",
      "5\n",
      "4264\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-11-07 15:13\n",
      "sweet!\n",
      "example.com walso works @rth\n",
      "\n",
      "2\n",
      "4265\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-11-07 20:31\n",
      "hi folks, sorry to bother. I've came across an old scikit example on GPs for which I can't seem to find any documentation or example on how to translate it to the current version: ``` from sklearn.gaussian_process import GaussianProcess gp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1E-1,                      random_start=100) xfit = np.linspace(0, 10, 1000) yfit, MSE = gp.predict(xfit[:, np.newaxis], eval_MSE=True) ``` I understand that's GPR but the parameters have changed, so `corr=cubic` and `thetas` don't really exist. Anyone has any idea how to translate this to version 0.20?\n",
      "\n",
      "1\n",
      "4266\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-11-08 11:01\n",
      "@kirk86 you need to find the appropriate kernels from `sklearn.gaussian_process.kernels` and construct the equivalent kernel to construct your covariance matrix for the GP. I haven't looked into them to know if there's a one to one mapping between the old ones and the new implementations. Alternatively, you can try and construct the solution to your usecase from scratch using tutorials such as: https://scikit-learn.org/dev/modules/gaussian_process.html#gaussian-process\n",
      "\n",
      "1\n",
      "4267\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-11-08 11:54\n",
      "@amueller in less than two weeks we're going to have a meetup for people who are new and interested in open source development. I'll be introducing them to scikit-learn's development process, for which it'll be nice to have the issue/pr labels updated. The main issue which also troubles myself whenever I'm looking for something to work on, is that the \"help wanted\" labels are very rarely removed from the issues once they're put there. Going through the first 25 issues tagged with \"help wanted\", only 7 actually really need help, and for most of them there's already an open PR to fix the issue. I'm not sure how it can be improved in a sustainable way, but it'd be nice if somebody could update the labels at least for now :)\n",
      "this is our meetup: https://www.meetup.com/opensourcediversity/events/255369540/\n",
      "\n",
      "2\n",
      "4268\n",
      "5b6f95c3d73408ce4fa450d4\n",
      "2018-11-08 11:56\n",
      "@adrinjalali I second that.\n",
      "Is there any proposed date for this? @adrinjalali ?\n",
      "\n",
      "2\n",
      "4269\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-11-08 15:38\n",
      "@adrinjalali thank you for taking time to respond. I was thinking the same think that you're suggesting but I can't find any `cubic` kernel in the list of available kernels: ``` >>> sklearn.gaussian_process.kernels. sklearn.gaussian_process.kernels.ABCMeta(                sklearn.gaussian_process.kernels.PairwiseKernel(         sklearn.gaussian_process.kernels.kv( sklearn.gaussian_process.kernels.CompoundKernel(         sklearn.gaussian_process.kernels.Product(                sklearn.gaussian_process.kernels.math sklearn.gaussian_process.kernels.ConstantKernel(         sklearn.gaussian_process.kernels.RBF(                    sklearn.gaussian_process.kernels.namedtuple( sklearn.gaussian_process.kernels.DotProduct(             sklearn.gaussian_process.kernels.RationalQuadratic(      sklearn.gaussian_process.kernels.np sklearn.gaussian_process.kernels.ExpSineSquared(         sklearn.gaussian_process.kernels.StationaryKernelMixin(  sklearn.gaussian_process.kernels.pairwise_kernels( sklearn.gaussian_process.kernels.Exponentiation(         sklearn.gaussian_process.kernels.Sum(                    sklearn.gaussian_process.kernels.pdist( sklearn.gaussian_process.kernels.Hyperparameter(         sklearn.gaussian_process.kernels.WhiteKernel(            sklearn.gaussian_process.kernels.signature( sklearn.gaussian_process.kernels.Kernel(                 sklearn.gaussian_process.kernels.abstractmethod(         sklearn.gaussian_process.kernels.six sklearn.gaussian_process.kernels.KernelOperator(         sklearn.gaussian_process.kernels.cdist(                  sklearn.gaussian_process.kernels.squareform( sklearn.gaussian_process.kernels.Matern(                 sklearn.gaussian_process.kernels.clone(                   sklearn.gaussian_process.kernels.NormalizedKernelMixin(  sklearn.gaussian_process.kernels.gamma( ```\n",
      "\n",
      "1\n",
      "4270\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-11-08 17:31\n",
      "A question for the developers, how can someone add a custom kernel to gaussian process, I have custom method but keep getting error when adding it directly do we need to wrap it with some base estimator because it complains about the `get_params` method\n",
      "``` (Pdb) gp.fit(np.random.randn(10, 3), np.random.randn(10)) *** TypeError: Cannot clone object 'array([0.99946371, 0.99859908, 0.99818002, 0.9994673 , 0.99954341,        0.99969851, 0.99905772, 0.99932515, 0.99906479, 0.99983097])' (type <class 'numpy.ndarray'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods. ```\n",
      "I guess this is still an issue https://stackoverflow.com/questions/49188159/how-to-create-a-custom-kernel-for-a-gaussian-process-regressor-in-scikit-learn\n",
      "\n",
      "3\n",
      "4271\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-11-09 16:54\n",
      "@kirk86 can you please open an issue? I don't think there's an issue open on that\n",
      "\n",
      "1\n",
      "4272\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-11-10 17:39\n",
      "@amueller just did #12558\n",
      "thanks\n",
      "\n",
      "2\n",
      "4273\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-11-10 18:15\n",
      "Another question for the developers. It seems that `sklearn.feature_extraction.text.CountVectorizer` is lacking an option to pass a 2d numpy array of strings or objects. It seems that it's impossible at the moment. It would be nice because at some point someone might need to do text wrangling and cleaning on another tool such as pandas or spark and be able to dump that as 2d numpy array in order to further analyze.\n",
      "\n",
      "1\n",
      "4274\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-11-10 19:17\n",
      "@kirk86 I'm not sure I understand the usecase. You want several separate text-fields and have each vectorized separately? Or together?\n",
      "\n",
      "1\n",
      "4275\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-11-10 21:57\n",
      "Apologies for not clearly explaining the usecase. Let's say that our dataset is like this: ``` X = np.array([['this is a text'], ['the brown fox jumped over the fence'], ['This is a longer string for just showcasing an example'], ['Dummy text here'], ...]) X.shape = (1000, 10) ``` Each sample or row from `X` creates a vector representing the bag of words. Each vector is of variable size because it depends on the length of the sting a.k.a how many times a word is present in the string. One could pad with zeros all of those vectors in order to have the same dimensions. Ideally it would be nice to give `X` to `sklearn.feature_extraction.text.CountVectorizer ` and get back a matrix `X_new .shape = (1000, m)`  where `m` represents that each row vector in `X_new` has the same length. Is that a bit more clear?\n",
      "\n",
      "1\n",
      "4276\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-11-10 22:03\n",
      "sorry, what's the 10 in your example? Is each example a single string or multiple strings?\n",
      "\n",
      "1\n",
      "4277\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-11-10 22:35\n",
      "the 10 is the number of columns and each of them may or may not contain multiple stings\n",
      "\n",
      "1\n",
      "4278\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-11-11 17:00\n",
      "Can you give an example of such an X? Sorry I'm being slow. The X example you gave above has shape (n_samples, 1)\n",
      "\n",
      "1\n",
      "4279\n",
      "5a1fcc56d73408ce4f810256\n",
      "2018-11-11 18:44\n",
      "What are the typical approaches used for time series based classification decisions. Appreciate any pointers. I am planning to start with decision trees to somehow learn features based on class transitions. ( I only expect a couple of class transitions amongst\n",
      "Classes which got renamed over time but Essentially carry similar features - say classes a1, a2,a3) besides the vanilla classification problem against classes b1,b2 and c1.\n",
      "\n",
      "2\n",
      "4280\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-11-12 13:42\n",
      "@amueller here's an example: ``` array(['dsny', 'bcc - brooklyn south', 'sanitation condition',        '15 street cond/dump-out/drop-off', 'street', '218 31 street',        '31 street', 'brooklyn', 'closed', '07 brooklyn', 'brooklyn'],       dtype=object) ```\n",
      "[![example.png](https://files.gitter.im/scikit-learn/scikit-learn/LYOA/thumb/example.png)](https://files.gitter.im/scikit-learn/scikit-learn/LYOA/example.png)\n",
      "here `X` is (n_samples, 11)\n",
      "\n",
      "3\n",
      "4281\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-11-12 16:29\n",
      "these two are different examples, though?! Ok so do you want one vectorizer for each column?\n",
      "``make_column_transformer(*[(c, CountVectorizer()) for c in X.columns])`` should do that.\n",
      "but really status here is probably a categorical variable - thought I guess CountVectorizer also works for those\n",
      "Well unless there's a space in one of the category names - so explictly saying these are categorical might be better\n",
      "\n",
      "4\n",
      "4282\n",
      "579618a040f3a6eec05c5e42\n",
      "2018-11-12 17:26\n",
      "> these two are different examples, though?  They are the same in the sense that the picture is `X` and the array above corresponds just to the first row of `X`.   > but really status here is probably a categorical variable  True, its a categorical value.\n",
      "Thanks!\n",
      "\n",
      "2\n",
      "4283\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-11-12 17:27\n",
      "yeah well treating a single column is different from treating the table, and the single column example is exactly what CountVectorizer does\n",
      "\n",
      "1\n",
      "4284\n",
      "5bbdfc48d73408ce4faaf97c\n",
      "2018-11-13 10:13\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/2epS/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/2epS/image.png)\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/l8Ri/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/l8Ri/image.png)\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/HwWK/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/HwWK/image.png)\n",
      "\n",
      "3\n",
      "4285\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-11-14 01:09\n",
      "@Ngamlana this seems entirely unrelated. Please don't spam the channel\n",
      "\n",
      "1\n",
      "4286\n",
      "599feaf2d73408ce4f72e9d5\n",
      "2018-11-17 13:18\n",
      "Hello. I am new to this community. I would love to contribute. Can someone help me start\n",
      "\n",
      "1\n",
      "4287\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-11-17 14:28\n",
      "@Naman9639 the contributing guide (https://scikit-learn.org/dev/developers/contributing.html) is a very good place to start, and then you can try some of the \"good first issue\" (https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) ones.\n",
      "\n",
      "1\n",
      "4288\n",
      "599feaf2d73408ce4f72e9d5\n",
      "2018-11-17 15:36\n",
      "Thanks I will start\n",
      "\n",
      "1\n",
      "4289\n",
      "5bbc5d22d73408ce4faacee2\n",
      "2018-11-22 12:39\n",
      "How to access groups under NetCDF4 files?\n",
      "\n",
      "1\n",
      "4290\n",
      "5b3f6eedd73408ce4f9fd9e7\n",
      "2018-12-05 13:26\n",
      "Hi, i get following error when im trying to apply one hot to categoric columns. I didnt understand what is problem exactly. Error is: TypeError: Wrong type for parameter `n_values`. Expected 'auto', int or array of ints, got <class 'numpy.ndarray'>\n",
      "\n",
      "1\n",
      "4291\n",
      "5b3f6eedd73408ce4f9fd9e7\n",
      "2018-12-05 13:33\n",
      "and here is the my one hot code: col_index = ([train.columns.get_loc(c) for c in train.columns if c in temp_cat_cols]) print('OneHot Uygulanacak Columns: ', col_index) ohe = OneHotEncoder(categorical_features=col_index, handle_unknown='ignore') X_train = ohe.fit_transform(X_train).toarray() X_test = ohe.transform(X_test).toarray()\n",
      "\n",
      "1\n",
      "4292\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-12-13 00:31\n",
      "@talatccan can you give a minimum example to reproduce?\n",
      "and what version of sklearn are you using?\n",
      "\n",
      "2\n",
      "4293\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-12-14 11:05\n",
      "master's circle-ci badge says failing, but I fail to find the build which fails, is the badge wrong? Or am I looking at the wrong place?\n",
      "\n",
      "1\n",
      "4294\n",
      "541a528b163965c9bc2053de\n",
      "2018-12-14 16:58\n",
      "There is an issue with the ssh key in the deploy step in the master branch: https://circleci.com/gh/scikit-learn/scikit-learn/tree/master\n",
      "Here is the end of the log of the last failure:  ``` ...  rewrite dev/searchindex.js (73%) + git push ERROR: The key you are authenticating with has been marked as read only. fatal: Could not read from remote repository.  Please make sure you have the correct access rights and the repository exists. Exited with code 128 ```\n",
      "\n",
      "2\n",
      "4295\n",
      "541a528b163965c9bc2053de\n",
      "2018-12-14 17:12\n",
      "The sklearn-ci github user has a user ssh key named \"sklearn-docbuilder\" that should be able to push to the scikit-learn.github.io repo. However I don't understand how the cicleci job is supposed to have access to this ssh private key.\n",
      "The recent changes in the `scikit-learn/scikit-learn/.circleci/config.yml` do not seem to be related in any way to the ssh key configuration.\n",
      "\n",
      "2\n",
      "4296\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-12-14 19:16\n",
      "shouldn't that be inside a section in circle-ci holding \"secrets\"? And then giving access to those values to jobs via environment variables or something? I have no idea how circle-ci works though.\n",
      "\n",
      "1\n",
      "4297\n",
      "541a528b163965c9bc2053de\n",
      "2018-12-14 23:54\n",
      "yes I believe so but I did not see anything related to this in the circle CI settings menus and I am not the one who configured it in the first place so I am not sure what has changed and what should be done to restore the push.\n",
      "\n",
      "1\n",
      "4298\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-12-15 09:10\n",
      "On circle-ci, when I look at our jobs, it says \"Projects currently running on CircleCI 1.0 are no longer supported. Please migrate to CircleCI 2.0.\" up there, and the migrate thingy is a hyperlink. We already use circle-ci 2 in our pipeline scripts, but it'd be nice if we upgraded there as well I suppose.\n",
      "\n",
      "1\n",
      "4299\n",
      "5c03c22dd73408ce4fb0b61d\n",
      "2018-12-15 12:03\n",
      "Just posted in pydata/pandas but someone suggested I ask here too:   Does anyone where the joblib folks hang out? I am trying some custom store backend stuff with pyarrow serializers and want to see if anyone else is messing around in this space. Feels like am between dask, joblib and pyarrow communities and want to make sure I'm not missing something someone else is already doing.\n",
      "\n",
      "1\n",
      "4300\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-12-15 16:08\n",
      "@david-cottrell_gitlab joblib folks are here ;) I think it's primarily @lesteve and @ogrisel ?\n",
      "pretty sure they haven't thought about pyarrow though\n",
      "(but @ogrisel is probably the best person to know these three communities)\n",
      "\n",
      "3\n",
      "4301\n",
      "5c03c22dd73408ce4fb0b61d\n",
      "2018-12-15 16:34\n",
      "@lesteve @ogrisel @amueller basically just wondering if there is some hidden trove of custom store backend hackers out there, there are some interesting use cases with using something like hyperdb (decentralized) as a store_backend for sharing the cache globally but it means swapping out all the pkl for something safer.  I am just playing around but I would suprised if someone hadn't gone down this route before.\n",
      "\n",
      "1\n",
      "4302\n",
      "5c171a90d73408ce4fb225f1\n",
      "2018-12-17 06:34\n",
      "guys need some help\n",
      "\n",
      "1\n",
      "4303\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2018-12-17 07:36\n",
      "You would be better of posting on SO and putting a link here\n",
      "\n",
      "1\n",
      "4304\n",
      "541a528b163965c9bc2053de\n",
      "2018-12-18 08:35\n",
      "@david-cottrell_gitlab I am not aware of any arrow-serializer aware store for joblib. As far as I know there is only pkl based things.\n",
      "\n",
      "1\n",
      "4305\n",
      "5c03c22dd73408ce4fb0b61d\n",
      "2018-12-18 10:00\n",
      "@ogrisel Thanks.\n",
      "\n",
      "1\n",
      "4306\n",
      "5c0f2d1bd73408ce4fb18b9f\n",
      "2018-12-20 02:13\n",
      "Hi, sorry if this is a silly question, very new to SVM in general. Was wondering if it's possible to create a model for one-class classification (ie. training on normal data, testing on normal and novel data), where there are more than 2 features for each example. All the code I've seen online seem to only consider 2 features\n",
      "\n",
      "1\n",
      "4307\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-12-20 10:13\n",
      "Those are hand-made minimal examples mostly for presentation purposes. I've applied SVMs to 22k+ features and have gotten good results. I guess that answers the question :) You may find the \"Learning with Kernels\" book useful if you decide to get deeper into the topic.\n",
      "\n",
      "1\n",
      "4308\n",
      "541a528b163965c9bc2053de\n",
      "2018-12-20 10:15\n",
      "Also if you interest is novelty detection, don't restrict yourself to OneClassSVM. You should also give IsolationForest and LOF a try: https://scikit-learn.org/stable/modules/outlier_detection.html\n",
      "\n",
      "1\n",
      "4309\n",
      "5b97b76ed73408ce4fa7b2da\n",
      "2018-12-21 18:52\n",
      "Hello everyone , I am a noobie with scikit , interested in datascience , can anyone give me trusted quick reference and reading material link , Thanks\n",
      "Thanks @amueller\n",
      "\n",
      "2\n",
      "4310\n",
      "54d4a1d6db8155e6700f853b\n",
      "2018-12-21 18:53\n",
      "https://jakevdp.github.io/PythonDataScienceHandbook/ @Ritzing (free online)\n",
      "\n",
      "1\n",
      "4311\n",
      "5634e8e116b6c7089cb8fa99\n",
      "2018-12-24 05:01\n",
      "In scikit-learn, where `gpr` is a GaussianProcessRegressor object. The dimension of the $$Y$$ variable is $$N$$. $$X$$ consists of $$M$$ points.  In this code below ```python y_pre, y_cov = gpr.predict( X,  return_cov=True ) ``` Why isn't `y_cov` an array of $M$ covariance matrices of size $$N \\times N$$, where $N$ is the dimensionality of the $$Y$$ variable? Why is `y_cov` an array of $$M$$ scalar values?\n",
      "\n",
      "1\n",
      "4312\n",
      "5810cd4cd73408ce4f3101ce\n",
      "2018-12-24 16:57\n",
      "Hey... why is `DecisionTreeClassifier/Regressor` using a `random_state` from my understanding  greedy tree building algorithms dont need random?\n",
      "\n",
      "1\n",
      "4313\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-12-25 17:59\n",
      "@yupbank there can be some randomness in choosing the splits in the trees as you can see [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
      "\n",
      "1\n",
      "4314\n",
      "567f5d7716b6c7089cc043a8\n",
      "2018-12-25 18:02\n",
      "@g_abhishek10_twitter probably doing some research on named entity recognition would give you some good hints.\n",
      "\n",
      "1\n",
      "4315\n",
      "5810cd4cd73408ce4f3101ce\n",
      "2019-01-02 16:14\n",
      "why would there be any randomness in choosing splits? since decision_tree is  a greedy algorithm, find the best feature to split\n",
      "@adrinjalali ^\n",
      "\n",
      "2\n",
      "4316\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-01-02 16:19\n",
      "@yupbank  I suggest you look for `random_state` in [here](https://github.com/scikit-learn/scikit-learn/blob/7389dbac82d362f296dc2746f10e43ffa1615660/sklearn/tree/_splitter.pyx) to better understand how it's working.\n",
      "\n",
      "2\n",
      "4317\n",
      "5810cd4cd73408ce4f3101ce\n",
      "2019-01-03 20:35\n",
      "https://gist.github.com/yupbank/1e0c2f50d5ed571e10559a681e7bb76f should be fun for some people\n",
      "\n",
      "1\n",
      "4318\n",
      "5799a0a940f3a6eec05cd618\n",
      "2019-01-06 19:19\n",
      "Any pull requests or changes/additions/stars to this repository would be very much beneficial, please help https://github.com/gautam1858/python-awesome\n",
      "\n",
      "1\n",
      "4319\n",
      "54fdd51a15522ed4b3dd04b7\n",
      "2019-01-08 15:35\n",
      "Hi, I have been seeing some conversation in the mailing list regarding an upcoming Dev sprint. I have made a couple of contributions at the tail end of last year and it's one of my goals this year to see if I can contribute more.\n",
      "Will this be an event which I will be able to participate remotely as well or do I need to be France to participate?\n",
      "\n",
      "2\n",
      "4320\n",
      "5bd837dbd73408ce4fad4032\n",
      "2019-01-08 17:26\n",
      "I have a minor question on making open-source contributions.. Traditionally, before I decide to change a file, I ALWAYS FIRST do a `git pull` to get the updated version. I then switch to the branch I created initially, and then make the changes, and do the `git add .`,  commit and push, and then submit a pull request. Correct?\n",
      "\n",
      "1\n",
      "4321\n",
      "5bd837dbd73408ce4fad4032\n",
      "2019-01-08 20:42\n",
      "Is the SAME procedure followed for making contribution to this library? I do 'git pull' everytime?\n",
      "\n",
      "1\n",
      "4322\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2019-01-08 21:32\n",
      "Is this a bug? https://stackoverflow.com/questions/54098749/how-can-i-make-gradientboostingregressor-work-with-a-baseestimator-in-scikit-lea\n",
      "\n",
      "1\n",
      "4323\n",
      "5a1fcc56d73408ce4f810256\n",
      "2019-01-09 04:21\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/gww6/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/gww6/image.png)\n",
      "Can someone help me on why words of 1 characters are not available as features here?\n",
      "\n",
      "2\n",
      "4324\n",
      "5c34f182d73408ce4fb408a6\n",
      "2019-01-09 08:49\n",
      "Hello World,I am new.Can someone link me up on how I can start contributing.\n",
      "\n",
      "1\n",
      "4325\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-01-09 09:55\n",
      "@sameshl https://scikit-learn.org/dev/developers/index.html\n",
      "\n",
      "1\n",
      "4326\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2019-01-11 06:26\n",
      "@whiletruelearn Yes, of course it's is possible to participate in the sprint remotely.  I guess list of participants on the wiki is mostly there for organization reasons (booking the room of the right size etc). If you want to participate just comment on gitter (here or on the dev channel) during the sprint and someone will help get you started.\n",
      "@rbhambriiit Word of one character are ignored because typically they are stop words (i.e. have no predictive power). If you want to keep them you can change the regexp in the `token_pattern`.\n",
      "@DrEhrfurchtgebietend Thanks for the heads up. Yes, it should be fixed by https://github.com/scikit-learn/scikit-learn/pull/12436 I think\n",
      "\n",
      "3\n",
      "4327\n",
      "54fdd51a15522ed4b3dd04b7\n",
      "2019-01-12 05:28\n",
      "Thanks @rth . Looking forward to it\n",
      "\n",
      "1\n",
      "4328\n",
      "5c3b550cd73408ce4fb482cc\n",
      "2019-01-14 04:29\n",
      "Hello Everyone! I am New here, My name is Vedang and currently studying at IET-DAVV,Indore. Can someone please tell how can I start contributing. Thanks\n",
      "\n",
      "1\n",
      "4329\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-01-14 12:56\n",
      "@vedangj044 https://scikit-learn.org/dev/developers/index.html\n",
      "\n",
      "1\n",
      "4330\n",
      "5c3b550cd73408ce4fb482cc\n",
      "2019-01-14 13:05\n",
      "@adrinjalali thanks, i would look into it.\n",
      "\n",
      "1\n",
      "4331\n",
      "564e507e16b6c7089cbb6551\n",
      "2019-01-23 15:07\n",
      "Hi everyone, I'm currently doing a pixel-based supervised classification of an image with the SGDClassifier. I want to include the spatial context between pixels in the classification besides the pixel intensity. So, I just found out about `sklearn.feature_extraction.image.grid_to_graph()`, and was wondering if there was a way to include this information (or the graph adjacency matrix) in the classification?\n",
      "\n",
      "1\n",
      "4332\n",
      "5c459020d73408ce4fb56d8d\n",
      "2019-01-24 05:04\n",
      "A good source for data set for malware detection using ML?\n",
      "\n",
      "1\n",
      "4333\n",
      "5b4da81ed73408ce4fa12924\n",
      "2019-01-25 14:39\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/CO5c/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/CO5c/image.png)\n",
      "why in the lst  line this throws an error \"value eror\"\n",
      "string to float?\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/Jl8c/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/Jl8c/image.png)\n",
      "??\n",
      "\n",
      "5\n",
      "4334\n",
      "59afadccd73408ce4f748432\n",
      "2019-01-28 15:42\n",
      "@farman32 the OneHotEncoder should be used on numerical features. So, for instance, use it on your r LabekEncoder, but reshaped : OneHotEncoder.fit_transform(r.reshape(-1,1).toarray(=\n",
      "Oups... toarray()!\n",
      "\n",
      "2\n",
      "4335\n",
      "59afadccd73408ce4f748432\n",
      "2019-01-28 16:43\n",
      "@ronitneve_twitter interesting http://arxiv.org/abs/1802.10135\n",
      "\n",
      "1\n",
      "4336\n",
      "5c516662d73408ce4fb6565a\n",
      "2019-01-30 09:21\n",
      "Salaam everyone! I am currently working on my final year project which includes 3 modules out of which one is 'Topic Generation'. Right now i am stucked in the results. I am using LDA mallet model but the results are not accurate. Help needed!\n",
      "\n",
      "1\n",
      "4337\n",
      "597cbd65d73408ce4f6f49d7\n",
      "2019-02-01 20:29\n",
      "Can anyone recommend a pretrained model for text summarization with for instance Apache license for commercial use?\n",
      "\n",
      "1\n",
      "4338\n",
      "5c5f2b37d73408ce4fb75701\n",
      "2019-02-09 19:38\n",
      "hi all\n",
      "\n",
      "1\n",
      "4339\n",
      "5ba621efd73408ce4fa90312\n",
      "2019-02-12 21:01\n",
      "Good sources to learn scikit for beginners?\n",
      "\n",
      "1\n",
      "4340\n",
      "559c8ad915522ed4b3e39cec\n",
      "2019-02-13 18:19\n",
      "@Akash-Sharma-1 try intro to machine learning course on udacity. Its free and uses skikit library.\n",
      "\n",
      "1\n",
      "4341\n",
      "5ba621efd73408ce4fa90312\n",
      "2019-02-13 19:24\n",
      "@anandvimal thanks for help\n",
      "I am gonna jump straight into it\n",
      "\n",
      "2\n",
      "4342\n",
      "559c8ad915522ed4b3e39cec\n",
      "2019-02-13 19:37\n",
      "It also has a full project in the end to practice the complete ml flow.\n",
      "\n",
      "1\n",
      "4343\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-02-14 02:02\n",
      "There's also Jake Vanderplas' data science handbook\n",
      "\n",
      "1\n",
      "4344\n",
      "5c660e14d73408ce4fb7d8fc\n",
      "2019-02-15 01:16\n",
      "just fun\n",
      "\n",
      "1\n",
      "4345\n",
      "57922eec40f3a6eec05c04ef\n",
      "2019-02-22 19:24\n",
      "Hi together, I like to write custom transformers which should not rely on sklearn as a dependency but should be usable in sklearn pipelines anyway (if necessary). Is that possible by simply providing `fit` and `fit_transform` methods? I assume not looking at [BaseEstimator](https://github.com/scikit-learn/scikit-learn/blob/7389dbac82d362f296dc2746f10e43ffa1615660/sklearn/base.py#L129). Thanks for any help.\n",
      "\n",
      "1\n",
      "4346\n",
      "5bec622ed73408ce4faeed82\n",
      "2019-02-22 21:16\n",
      "@Akash-Sharma-1 also look into hands on machine learning with scikitl-learn and tensorflow\n",
      "\n",
      "1\n",
      "4347\n",
      "54e07d6515522ed4b3dc0858\n",
      "2019-02-25 12:39\n",
      "@mansenfranzen you need to provide `transform` as well, since there are cases where `transform` is called on different data than `fit`; and otherwise make sure you follow the custom estimator guidelines, if you have any parameters to set: https://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator\n",
      "\n",
      "1\n",
      "4348\n",
      "5c73c654d73408ce4fb8d721\n",
      "2019-02-25 13:08\n",
      "sprint Paris: I am continuing #5862\n",
      "\n",
      "1\n",
      "4349\n",
      "57922eec40f3a6eec05c04ef\n",
      "2019-02-25 13:44\n",
      "@vene Thanks alot for the link to the documentation - it's exactly what I was looking for :-)\n",
      "\n",
      "1\n",
      "4350\n",
      "580fa6b0d73408ce4f30ca61\n",
      "2019-02-25 19:06\n",
      "oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"] can anyone explain the last line of code? I can't understand this line properly.\n",
      "'''\n",
      "''' def prepare_country_stats(oecd_bli, gdp_per_capita):     oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"] ''' can anyone explain the last line of code? I can't understand this line properly.\n",
      "\n",
      "3\n",
      "4351\n",
      "541a528b163965c9bc2053de\n",
      "2019-02-26 09:39\n",
      "@Pritom-Mazhi `oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]` selects the all the lines of a pandas dataframe named `oecd_bli` where the `INEQUALITY` column has value `\"TOT\"` All the other lines are dropped.\n",
      "\n",
      "1\n",
      "4352\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-02-27 09:04\n",
      "I'm having no luck with this building. Can someone tell me which contineny were in? Second floor, tight?\n",
      "\n",
      "1\n",
      "4353\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-02-27 09:06\n",
      "7th floor\n",
      "should I come down to get you?\n",
      "african continent\n",
      "\n",
      "3\n",
      "4354\n",
      "5988340fd73408ce4f706113\n",
      "2019-02-27 13:01\n",
      "Doing a presentation on ONNX right now\n",
      "for people at the sprint\n",
      "\n",
      "2\n",
      "4355\n",
      "5c73c654d73408ce4fb8d721\n",
      "2019-02-27 15:40\n",
      "@adrinjalali, what is the difference between `min_samples` and `min_cluster_size` in optics ?\n",
      "\n",
      "1\n",
      "4356\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-02-27 18:06\n",
      "@assiaben by convention (I think dbscan uses the same thing) `min_samples` is the parameter used to do the nearest neighbor part, and `min_cluster_size` is used when _extracting_ the clusters from the reachability distances.\n",
      "we probably need to find a way to better explain these\n",
      "\n",
      "2\n",
      "4357\n",
      "5c6af6cad73408ce4fb82b1c\n",
      "2019-02-27 21:51\n",
      "hello guys! Im working on a movie dataset for item-item collaborative filtering. Im able to get my cosine_similarity function working with for loop on a trunkated part of the dataset. I hear that utilizing apply(lambda x: <func>) is much faster. Anyone familar with converting?\n",
      "\n",
      "1\n",
      "4358\n",
      "5c70487dd73408ce4fb8a95a\n",
      "2019-02-28 21:16\n",
      "@troykirin a minimal example copypasta would be helpful to get us started; usually its as simple as you described `apply(lambda x: func(x))` assuming all records in the loop are independent\n",
      "\n",
      "1\n",
      "4359\n",
      "5c6af6cad73408ce4fb82b1c\n",
      "2019-02-28 22:15\n",
      "@tylerkontra-system1 this is the loop im trying to work with ```  for pair in combinations(np_array_movieandratings[:,],2):     x1 = pair[0]     x2 = pair[1]     np.append.(cos_sim(x1,x2) ```\n",
      "\n",
      "1\n",
      "4360\n",
      "5c70487dd73408ce4fb8a95a\n",
      "2019-02-28 23:57\n",
      "it might make more sense to do something like this ``` import numpy as np from itertools import combinations  def my_func(p):     return np.sum(p) my_array = np.array(range(100)) my_combin = np.array(tuple(combinations(my_array, 2))) my_result = [my_func(pair) for pair in my_combin] ``` since the biggest performance boost is likely to be from simply using numpys data structures (re: https://stackoverflow.com/questions/38709313/numpy-vectorize-multidimensional-function)\n",
      "\n",
      "1\n",
      "4361\n",
      "541a528b163965c9bc2053de\n",
      "2019-03-01 20:15\n",
      "scikit-learn 0.20.3 is online! https://scikit-learn.org/stable/whats_new.html#version-0-20-3\n",
      "\n",
      "1\n",
      "4362\n",
      "5860d0cbd73408ce4f3f55a5\n",
      "2019-03-03 09:05\n",
      "why do we use random numbers in machine learning ?\n",
      "I don't quite get it\n",
      "\n",
      "2\n",
      "4363\n",
      "5c667cfad73408ce4fb7e11a\n",
      "2019-03-03 10:30\n",
      "@Devosource there can be many uses of random numbers. For example if you are working on a dataset of a hundred thousand images and you need to check the result randomly, then you might use a randInt() function i guess.   Also, that was just an example. I didnt quite understood the context of your question. Hope you got your answer but if not then you could be more specific and someone would give you the answer you are looking for.\n",
      "\n",
      "1\n",
      "4364\n",
      "5860d0cbd73408ce4f3f55a5\n",
      "2019-03-03 15:26\n",
      "i was going through keras beginner's tutorial and abit of CNN model tutorial. I found that the first thing to do after importing modules is to fix random seed for reproducibility , for random numbers. That's what I don't understand\n",
      "``` from keras.layers import Dense import numpy # fix random seed for reproducibility numpy.random.seed(7) ```\n",
      "\n",
      "2\n",
      "4365\n",
      "5c667cfad73408ce4fb7e11a\n",
      "2019-03-03 17:44\n",
      "@Devosource  So basically when you are watching a tutorial and coding along with it, you can use fix seed for the same result as in the tutorial you are watching. Just for the sake of same random number generation throughout the code whenever a random number is generated.\n",
      "\n",
      "1\n",
      "4366\n",
      "5860d0cbd73408ce4f3f55a5\n",
      "2019-03-04 08:41\n",
      "@algo-circle thanks\n",
      "\n",
      "1\n",
      "4367\n",
      "5c667cfad73408ce4fb7e11a\n",
      "2019-03-04 08:49\n",
      "@Devosource You are welcome sir! <unconvertable>\n",
      "\n",
      "1\n",
      "4368\n",
      "5c03c22dd73408ce4fb0b61d\n",
      "2019-03-10 19:41\n",
      "Anyone know anything about why the keras.io room is blocked? Says github users only. I think there is a glitch?\n",
      "\n",
      "1\n",
      "4369\n",
      "5668c71116b6c7089cbe1ea3\n",
      "2019-03-10 19:42\n",
      "Yes, it is a glitch with gitter. They link accounts but sign-in still matters.\n",
      "\n",
      "1\n",
      "4370\n",
      "5824457cd73408ce4f34ebbd\n",
      "2019-03-15 17:47\n",
      "Hello everyone, I have a quick question regarding normalization (min-max-scaling) of output values. Usually you are supposed to use normalization only on the training data set and then apply those stats to the validation and test set. But for instance, my prediction variable is a single percentage value ranging [0, 100%]. And I know for sure that in the \"real world\" regarding my problem statement, that I will get samples ranging form 60 - 100%. But my training set is to small and does not contain enough data points including all possible output values. So here comes my question: Should I stay with my initial statement (normalization only on training data) or should I apply the maximum possible value of 100% for my output value to max()-value of the normalization step?\n",
      "\n",
      "1\n",
      "4371\n",
      "57007657187bb6f0eadd96e8\n",
      "2019-03-15 17:51\n",
      "Are you doing regression? Typically minn max scaling is used on the feature vector values that are inputs into the classifier. Not necessarily on the outputs of the classifier\n",
      "\n",
      "2\n",
      "4372\n",
      "5824457cd73408ce4f34ebbd\n",
      "2019-03-15 17:54\n",
      "Another silly idea was to squash my output value into the range [0,1] by applying target/100. Does that make sense?\n",
      "\n",
      "6\n",
      "4373\n",
      "5824457cd73408ce4f34ebbd\n",
      "2019-03-15 18:00\n",
      "Maybe I'm just dump. So, my target value ranges from something like 60% - 100% and I'm trying to predict it by using MLP/CNN. My understanding is that I have to scale my output value that it matches my output activation function (ReLU). But I got cought up with normalization in general and the correct usage on training,validation and test data.\n",
      "That's how I got confused\n",
      "\n",
      "2\n",
      "4374\n",
      "57007657187bb6f0eadd96e8\n",
      "2019-03-15 18:01\n",
      "ok, yeah so any scaling that you do on your training data. You will want to apply those same normalization to the validation and test data\n",
      "you can also set the normalization of the training to be what you expect from your real data\n",
      "\n",
      "2\n",
      "4375\n",
      "5824457cd73408ce4f34ebbd\n",
      "2019-03-15 18:09\n",
      "Exactly, but what if my training set doesn't include all possible outcomes for my output value due to the small sample size. Usually that would just mean, I would have to include more samples. But that is not possible at this point. So furthermore this leads to the situation that my test sample will have output values which are not present in the training set.  One solution would be to give the model the information in advance that the max possible range of the output value will be 100%.\n",
      "But this idea contradicts the Literature\n",
      "\n",
      "2\n",
      "4376\n",
      "5860d0cbd73408ce4f3f55a5\n",
      "2019-03-17 13:11\n",
      "jupyter notebook crashes when running opneAI's gym env\n",
      "any fix for this ?\n",
      "\n",
      "2\n",
      "4377\n",
      "5c8e506dd73408ce4fbaeb3b\n",
      "2019-03-17 13:51\n",
      "Hello Guys and Girls DO you Wanna Meme Based Group Only Of Data Science and Machine Learning I have Created One Dumbily For You People To Join https://www.facebook.com/groups/2084179105032029/ Join in With your Facebook Account\n",
      "\n",
      "1\n",
      "4378\n",
      "5c9bc1dad73408ce4fbc0304\n",
      "2019-03-27 18:46\n",
      "Hi anyone can please assist me to\n",
      "Install anaconda on Ubuntu\n",
      "\n",
      "2\n",
      "4379\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-03-27 20:42\n",
      "have you tried an online tutorial and failed @SadiaAman ?\n",
      "\n",
      "1\n",
      "4380\n",
      "5c9b282cd73408ce4fbbf1ed\n",
      "2019-03-28 06:51\n",
      "Can i join in with my github account?\n",
      "\n",
      "1\n",
      "4381\n",
      "5c99f27ad73408ce4fbbd772\n",
      "2019-03-28 09:05\n",
      "@wwf6688 just login with github account\n",
      "\n",
      "1\n",
      "4382\n",
      "5c9b282cd73408ce4fbbf1ed\n",
      "2019-03-28 09:06\n",
      "I can't open the website in China\n",
      "https://www.facebook.com/groups/2084179105032029/\n",
      "\n",
      "2\n",
      "4383\n",
      "5c99f27ad73408ce4fbbd772\n",
      "2019-03-28 09:30\n",
      "Try using vpn\n",
      "Shadowsocks or any other way\n",
      "\n",
      "2\n",
      "4384\n",
      "5c9b282cd73408ce4fbbf1ed\n",
      "2019-03-29 03:49\n",
      "ok\n",
      "\n",
      "1\n",
      "4385\n",
      "542e902b163965c9bc208763\n",
      "2019-04-08 22:02\n",
      "I'm looking to take a set of attributes about an object in the real world, and then determine a set of products that would best fit it based on these attributes. Would this be a good fit for using a decision tree?\n",
      "these set of products would be values calculated from the attributes themselves\n",
      "I am not looking for any type of prediction though, just a method of representing logic branches nicely in some pretty complex business rules\n",
      "\n",
      "3\n",
      "4386\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2019-04-14 10:54\n",
      "Hi, what's the reason why none of the CV iterators support single splits (i.e. `n_splits=1`)?\n",
      "\n",
      "1\n",
      "4387\n",
      "558c0fd615522ed4b3e2b943\n",
      "2019-04-15 09:44\n",
      "n_splits is the number of chunks produced. So n_splits=1 means not to split the data at all, leave it in one chunk\n",
      "\n",
      "1\n",
      "4388\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2019-04-15 13:06\n",
      "That's not right if I understand you correctly, running  ``` cv = KFold(n_splits=2) len(list(cv.split(np.arange(10)))) ``` returns 2, i.e. 2 tuples of two arrays, not  a single tuple.\n",
      "\n",
      "1\n",
      "4389\n",
      "558c0fd615522ed4b3e2b943\n",
      "2019-04-16 18:44\n",
      "Take a look at the list itself and it'll become clear. You have 2 chunks. First you use the first chunk as train and second chunk as test set. Then, you use the second chunk as train and first chunk as test set.\n",
      "I should not call them chunks. They're folds. Two folds :)\n",
      "\n",
      "2\n",
      "4390\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2019-04-16 19:03\n",
      "Yes, I understand that, but why can it not give me a single train and a single test set (`n_splits=1`) as `split_train_test` does? Because it's then not really folds anymore?\n",
      "\n",
      "1\n",
      "4391\n",
      "558c0fd615522ed4b3e2b943\n",
      "2019-04-16 19:14\n",
      "I guess `n_splits` is a bit of a misnomer. `n_folds` would've been clearer maybe.\n",
      "\n",
      "1\n",
      "4392\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2019-04-17 16:10\n",
      "Yes, okay, but still why not return a single split? Any underlying design choice/complication?\n",
      "\n",
      "1\n",
      "4393\n",
      "564789be16b6c7089cbab8b7\n",
      "2019-04-18 18:32\n",
      "I know this is going to sound like I am complaining, but I am not. In 2015 I suggested adding Gower similarity/dissimilarity to our metrics. https://github.com/scikit-learn/scikit-learn/issues/5884 . It is now April 2019 and the PR is still in the works https://github.com/scikit-learn/scikit-learn/pull/9555\n",
      "It feels like someone  could have coded up the dissimilarity score in a day and written tests in a week. Is this sort of 4 year period normal or something worth exploring?\n",
      "In fact it was implemented in 2017 I see in the issue as a jupyter notebook\n",
      "\n",
      "3\n",
      "4394\n",
      "558c0fd615522ed4b3e2b943\n",
      "2019-04-19 08:48\n",
      "It's easy for PRs to get buried.\n",
      "\n",
      "1\n",
      "4395\n",
      "5aecc12bd73408ce4f982900\n",
      "2019-04-20 22:42\n",
      "Hi, I have a question about what `GridSearchCV` (or, in my case, `RidgeCV`) passes to the scorer for evaluation. I created a custom scorer that takes in `y_true` and `y_pred` and returns the correlation (see below). However, I stuck a `print` statement in there to verify the shape of what's being passed to the scorer. I'm using `KFold` cross-validation with e.g. _k_ = 10, which for 200 samples will return splits with shapes `y_train` = 180 and `y_test` = 20. It seems that both arrays of shape 180 and 20 are being passed to the scorer for evaluation. As if the scorer is being run on both the test set (expected) and the train set (unexpected, for me). Is this actually what's going on? If so, is this the desired behavior? Maybe my implementation of the scorer is screwed up, but couldn't figure this out from the documentation. Thanks, and sorry for any confusion! ```python def correlation_metric(y_true, y_pred):         y_true_demean = y_true - np.mean(y_true, axis=0)     y_pred_demean = y_pred - np.mean(y_pred, axis=0)     numerator = np.sum(y_true_demean * y_pred_demean, axis=0)     denominator = np.sqrt(np.sum(y_true_demean ** 2, axis=0) *                           np.sum(y_pred_demean ** 2, axis=0))     print(f\"True shape: {y_true.shape}; Predicted shape: {y_pred.shape} \"           f\"Correlation: {numerator/denominator}\")     return numerator / denominator correlation_scorer = make_scorer(correlation_metric) ```\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/vOzF/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/vOzF/image.png)\n",
      "\n",
      "2\n",
      "4396\n",
      "5b4b30e3d73408ce4fa0eefc\n",
      "2019-04-22 14:38\n",
      "Hello, is there a simple way how to make a polynomial regression of given degree?\n",
      "\n",
      "1\n",
      "4397\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-04-24 19:55\n",
      "@Borda take a look at  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
      "@snastase yes GridSearchCV has a `return_train_score` attribute (the default will change in the next version)\n",
      "\n",
      "2\n",
      "4398\n",
      "5aecc12bd73408ce4f982900\n",
      "2019-04-25 17:05\n",
      "Ah got itthanks! @NicolasHug\n",
      "\n",
      "1\n",
      "4399\n",
      "5ba59281d73408ce4fa8fbb5\n",
      "2019-04-25 18:45\n",
      "hi anyone here?\n",
      "while using librosa I 'm getting this error ImportError: cannot import name '_inplace_contiguous_isotonic_regression'\n",
      "I tried googling it but I got nothing\n",
      "please help\n",
      "hmmm ok thanks let me try it on librosa\n",
      "can u plz give a one liner pip code to downgrade it\n",
      "?\n",
      "\n",
      "7\n",
      "4400\n",
      "541a528b163965c9bc2053de\n",
      "2019-04-25 18:48\n",
      "This is private API that is not guaranteed to stay stable across scikit-learn releases, maybe librosa was meant to be used with a specific version of scikit-learn. Please check with its documentation or asking its developers and make sure that you have the correct version of scikit-learn.\n",
      "\n",
      "1\n",
      "4401\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-04-25 18:48\n",
      "https://github.com/librosa/librosa/blob/master/setup.py#L50\n",
      "could be that > 0.19 could be problematic\n",
      "\n",
      "2\n",
      "4402\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-04-25 18:49\n",
      "pip install scikit-learn==0.18\n",
      "\n",
      "2\n",
      "4403\n",
      "5ba59281d73408ce4fa8fbb5\n",
      "2019-04-25 18:57\n",
      "now I m getting another error ImportError: cannot import name 'astype'\n",
      "plz help\n",
      "I have python 3.8.Does it matter by that too ?\n",
      "\n",
      "3\n",
      "4404\n",
      "5cc7f07cd73408ce4fbf0399\n",
      "2019-04-30 06:55\n",
      "hi all\n",
      "\n",
      "1\n",
      "4405\n",
      "5cb72452d73408ce4fbdf818\n",
      "2019-04-30 07:41\n",
      "*Hi all*\n",
      "\n",
      "1\n",
      "4406\n",
      "5b3ed273d73408ce4f9fcb4e\n",
      "2019-04-30 08:04\n",
      "Hi, I have a doubt on `TfidfVectorizer` in the `sklearn.feature_extraction.text` package.\n",
      "I see that stopwords like the which occur frequently in all documents I am trying, have an IDF value of 1\n",
      "Shouldnt the IDF value be 0 (because log of 1 is 0)?\n",
      "The example documents I am using are:\n",
      "> [\"The quick brown fox jumped over the lazy dog.\", \t\t\"The dog.\", \t\t\"The fox\"]\n",
      "I suspect the log is not being taken, how do I configure the vectorizer to take the log and get an IDF of 0?\n",
      "\n",
      "6\n",
      "4407\n",
      "5b3ed273d73408ce4f9fcb4e\n",
      "2019-04-30 09:58\n",
      "I think I figured out he IDF calculation. It seems to be `ln(N/df) + 1`. Where `N` is the total number of documents and `df` is the number of documents a particular term appears in\n",
      "So for the word the it is `ln(3/3) + 1` = `0+1`, which is why the value is 1. How do I configure the vectorizer not to add the 1?\n",
      "\n",
      "2\n",
      "4408\n",
      "5a720f91d73408ce4f8b1403\n",
      "2019-05-05 19:43\n",
      "can you tell me what is the api name for congressional voting records datasets?\n",
      "as i have to import the dataset what i should write?\n",
      "data = datasets.load_?????\n",
      "\n",
      "3\n",
      "4409\n",
      "5a720f91d73408ce4f8b1403\n",
      "2019-05-05 20:01\n",
      "please tell me how to find the right dataset name api\n",
      "\n",
      "1\n",
      "4410\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2019-05-06 07:21\n",
      "@aarck you could try to see if that dataset is uploaded on OpenML in which case it would be `datasets.load_openml`. If not, it's up to you to write a loader for this dataset (or upload it to OpenML).\n",
      "\n",
      "1\n",
      "4411\n",
      "5ccfd89fd73408ce4fbf7a43\n",
      "2019-05-07 12:01\n",
      "hello everyone is there someone who knows about scikit-multiflow library ???\n",
      "\n",
      "1\n",
      "4412\n",
      "5cdd97bed73408ce4fc07acf\n",
      "2019-05-16 17:07\n",
      "@Praful-cs  , Hiii, let me help you with this.\n",
      "\n",
      "1\n",
      "4413\n",
      "58de4778d73408ce4f551e04\n",
      "2019-05-20 08:57\n",
      "Hello all, I have a question regarding joblib's \"vendor\" distribution present in scikit-learn `0.20.X` : is it up to date with the latest release of joblib? I know that the latest version of scikit-learn (`0.21.X` and above) are now using joblib as a dependency directly but I need to use scikit-learn `0.20.X` for python 2.7 support. Thank you! :)\n",
      "Thank you @ogrisel for this quick answer!\n",
      "\n",
      "2\n",
      "4414\n",
      "541a528b163965c9bc2053de\n",
      "2019-05-20 08:59\n",
      "@jjerphan scikit-learn 0.20.3 is embedding joblib 0.13.0: https://github.com/scikit-learn/scikit-learn/blob/0.20.X/sklearn/externals/joblib/__init__.py#L109\n",
      "\n",
      "1\n",
      "4415\n",
      "541a528b163965c9bc2053de\n",
      "2019-05-20 09:01\n",
      "if we release 0.20.4 we should think of upgrading the joblib version as well. You can also set the `SKLEARN_JOBLIB_SITE=1` environment variable to use an independently installed version of joblib instead of the vendored version.\n",
      "Note however that future versions of joblib might stop supporting python 2 as well, so better start thinking of upgrading to Python 3 in any case :)\n",
      "\n",
      "6\n",
      "4416\n",
      "5bb35f10d73408ce4faa17f3\n",
      "2019-05-24 00:20\n",
      "@zzj0402_gitlab What is cross correlation? What is the relationship between it and convolution? I am not sure if it's cross product between data and convolution is just flipping the signal from both data sets to negative?\n",
      "\n",
      "1\n",
      "4417\n",
      "58663b41d73408ce4f402d24\n",
      "2019-05-28 04:10\n",
      "Hey guys!!! Can someone suggest me any dataset available on kaggle which contains both numerical and textual data? I have to apply machine learning as well as nlp concepts on it.\n",
      "\n",
      "1\n",
      "4418\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-05-31 07:50\n",
      "Quick question, when using TfIdf vectorizer, would you consider it beneficial to lemmatize the body of the document before fitting?\n",
      "\n",
      "1\n",
      "4419\n",
      "5b3ed273d73408ce4f9fcb4e\n",
      "2019-05-31 07:52\n",
      "Lemmatization is always a good idea. Maybe you can run the vectorizer first and see in how many cases you got a wrong prediction because of no lemmatization and then take a call\n",
      "\n",
      "2\n",
      "4420\n",
      "5c7f95bad73408ce4fb9ca5e\n",
      "2019-06-01 12:05\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/DQTb/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/DQTb/image.png)\n",
      "how to handle with this error?\n",
      "\n",
      "2\n",
      "4421\n",
      "5c7f95bad73408ce4fb9ca5e\n",
      "2019-06-01 12:12\n",
      "Hey guys!!! Can someone suggest me any dataset available on kaggle which contains both numerical and textual data? I have to apply machine learning as well as nlp concepts on it.  refer this----->>>>https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling\n",
      "@mukul09\n",
      "\n",
      "2\n",
      "4422\n",
      "5cf106a0d73408ce4fc1d8b2\n",
      "2019-06-04 14:11\n",
      "Can anybody please explain me batch decent gradient and the procedure to calculate it\n",
      "I have just started machine learning\n",
      "In python\n",
      "\n",
      "3\n",
      "4423\n",
      "5c13ca6dd73408ce4fb1f2d5\n",
      "2019-06-04 14:18\n",
      "Batch gradient descent uses the whole dataset to calculate the gradient vector unlike mini-batch or stochastic gradient descent, thats the key point. The procedure in common words is: calculate partial derivatives for a cost function with respect to each coefficient using the whole dataset, make a gradient step, update coefficients\n",
      "\n",
      "1\n",
      "4424\n",
      "5c651687d73408ce4fb7c2c8\n",
      "2019-06-04 14:28\n",
      "Hi there. Try this link for batch gradient descent in python\n",
      "https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f\n",
      "\n",
      "2\n",
      "4425\n",
      "5cf106a0d73408ce4fc1d8b2\n",
      "2019-06-04 15:29\n",
      "Thanks @isaacaugustus  and @gyrdym\n",
      "\n",
      "1\n",
      "4426\n",
      "5b3ed273d73408ce4f9fcb4e\n",
      "2019-06-05 04:39\n",
      "@harshchaplot this is a great resource too: https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU\n",
      "\n",
      "1\n",
      "4427\n",
      "5cf106a0d73408ce4fc1d8b2\n",
      "2019-06-05 05:48\n",
      "Thanks @srniranjan\n",
      "\n",
      "1\n",
      "4428\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 09:35\n",
      "Say you have over 9000 features which you would like to significantly reduce, what outside of PCA you can use to put down that number?\n",
      "\n",
      "1\n",
      "4429\n",
      "5c13ca6dd73408ce4fb1f2d5\n",
      "2019-06-05 09:36\n",
      "You may use lasso regression, for instance\n",
      "\n",
      "2\n",
      "4430\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-05 09:37\n",
      "https://en.wikipedia.org/wiki/Principal_component_analysis\n",
      "\n",
      "1\n",
      "4431\n",
      "5b3ed273d73408ce4f9fcb4e\n",
      "2019-06-05 09:37\n",
      "PCA is a linear way of reducing your dimensions to a few Principal components. You can use Neural Networks for a non-linear approach for the same\n",
      "Basically the last hidden layer of your neural network is a representation of your input in much the same way projecting to principal components is\n",
      "The number of dimensions of this representation will be the number of neurons in the last hidden layer\n",
      "\n",
      "4\n",
      "4432\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 09:43\n",
      "@gyrdym does it work with sparse matrices so I can use it with tfIds vectorized text?\n",
      "tfidf*\n",
      "\n",
      "2\n",
      "4433\n",
      "5c13ca6dd73408ce4fb1f2d5\n",
      "2019-06-05 09:45\n",
      "As far as I know, it works with sparse matrices, but I myself havent use it for such situations\n",
      "\n",
      "6\n",
      "4434\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-05 09:46\n",
      "SVD is not using LSA underneath: LSA is the application of (truncated) SVD to text data represented as bag of words (e.g. TF-IDF vectors).\n",
      "SVD is a generic mathematical tool, LSA is one specific application of SVD to text mining.\n",
      " 9k features features is pretty low number of features for bag of words vectors. Bag of Words is very very sparse.\n",
      "SVD on TF-IDF / bag of words is a good fast preprocessing used as baseline for text classification / clustering and information retrieval / text mining.\n",
      "\n",
      "4\n",
      "4435\n",
      "5b3ed273d73408ce4f9fcb4e\n",
      "2019-06-05 09:47\n",
      "@piotr-mamenas 9k features is huge! Are you having such a big feature set because youre considering each word in the vocabulary as a feature?\n",
      "\n",
      "6\n",
      "4436\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 09:54\n",
      "I have a set of classes with labelled data which I want to build several binary output models from, each model would just output 0, 1 to highlight whenever the article belongs to a class or not (1 class per model)\n",
      "\n",
      "1\n",
      "4437\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-05 09:56\n",
      "LogisticRegression on TF-IDF vectors should be a good and fast baseline. You can also try: TF-IDF => TruncatedSVD => LogisticRegression or RandomForestClassifier and TF-IDF => NMF => LogisticRegression / RandomForestClassifier as alternatives.\n",
      "\n",
      "1\n",
      "4438\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 09:58\n",
      "but this stays with the TfIdf approach, how would it handle the 9600 or so features?\n",
      "\n",
      "1\n",
      "4439\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-05 09:58\n",
      "You can also try to include features derived from pretrained word vectors (e.g. word2vec or glove) and some fancy neural networks with keras or pytorch but I would try the above baselines first.\n",
      "> but this stays with the TfIdf approach, how would it handle the 9600 or so features?  How is this a problem? LogisticRegression works fine on high dimensional sparse data\n",
      "About the first question: lemmatization is not always a good idea depending on what you are try to predict.\n",
      "\n",
      "3\n",
      "4440\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 10:01\n",
      "I asked that question the other day and wrote about it, lemmas might lose the context of whole sentences so in theory they will lower the accuracy but that's also a way to lower the amount of data to process\n",
      "Lets see the logistic regression approach then\n",
      "THanks\n",
      "\n",
      "3\n",
      "4441\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 10:12\n",
      "@ogrisel this looks suprisingly good, it was training just for a few seconds on a set of only 3500 articles and the confusion matrix looks like this:\n",
      "array([[1262,   19],        [   0,    0]], dtype=int64)\n",
      "Accuracy:  0.985167837626854\n",
      "if it looks to good, there must be something wrong\n",
      "\n",
      "4\n",
      "4442\n",
      "5c13ca6dd73408ce4fb1f2d5\n",
      "2019-06-05 10:30\n",
      "Is your data well balanced? Maybe you need to shuffle the original dataset before do logistic regression.\n",
      "It would be good also to use cross validation to get the answer if your model is really so good\n",
      "\n",
      "2\n",
      "4443\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 10:51\n",
      "Yeah, I am checking it.\n",
      "\n",
      "1\n",
      "4444\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-05 11:20\n",
      "array([[1262,   19],        [   0,    0]], dtype=int64) means that the mode always predict the class 0. It's the constant predictor (probably caused too much bias / regularization)\n",
      "Your test data is actually imbalanced with  19 to 1262 ratio. 1262 / (12 + 1262) == 0.99 accuracy which is weird because this does not match your reported accuracy.\n",
      "@piotr-mamenas I hope you did a proper train test split :)\n",
      "You should use a balanced accuracy or ROC AUC or precision / recall / f-beta score to evaluate your model instead of the raw accuracy.\n",
      "\n",
      "4\n",
      "4445\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 14:05\n",
      "``` UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.   'recall', 'true', average, warn_for) ```\n",
      "the train test split was 0.75:0.25 ratio\n",
      "\n",
      "2\n",
      "4446\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 14:08\n",
      "It can't predict 0.98 with constant class because you got 1200/5000 samples class 1, and the TP is 1262\n",
      "and train test split takes into account balance of classes from what I know\n",
      "@gyrdym\n",
      "\n",
      "3\n",
      "4447\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 14:12\n",
      "the overall class balance of the entire dataset is a difficult topic, the type of class I am trying to detect may appear in every 1/100 articles but as said I have 1200 samples of \"1\" and 3800 samples of \"0\" so I am guessing increasing the \"0\" sample would prove beneficial generally\n",
      "\n",
      "1\n",
      "4448\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-05 14:16\n",
      "check the content of your `y_test`: the recall warning seems to indicate that you only have negative samples (samples from the majority class) in your test set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "4449\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 14:23\n",
      "ok got it @ogrisel\n",
      "``` Accuracy:  0.8227946916471507 Precision:  0.9347826086956522 Recall:  0.28013029315960913 ```\n",
      "``` array([[968,   6],        [221,  86]], dtype=int64) ```\n",
      "looks different now\n",
      "\n",
      "4\n",
      "4450\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 14:25\n",
      "I shuffled the dataset on the train test split and it changed completely as you can see\n",
      "but the score is anyway pretty impressive considering the sample and no hyperparameter tuning\n",
      "thanks\n",
      "\n",
      "3\n",
      "4451\n",
      "5c13ca6dd73408ce4fb1f2d5\n",
      "2019-06-05 14:27\n",
      "You may also try stratified shuffled split instead of the regular one - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\n",
      "\n",
      "3\n",
      "4452\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 14:34\n",
      "Anyone has 2$ million dollars by the way? : p\n",
      "\n",
      "1\n",
      "4453\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 15:15\n",
      "oh and that's what I like, I added 6000 articles to the 0 class and I already have 0.92\n",
      "``` array([[2492,    7],        [ 235,   47]], dtype=int64) ```\n",
      "\n",
      "2\n",
      "4454\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-05 16:45\n",
      "Accuracy is rather meaningless for unbalanced problems. Look at precision and recall and plot the precision recall curve. Here a recall of 0.2 might be too bad for your classifier to be useful. It depends on the application of your classifier\n",
      "If you do parameter tuning of your text classification pipeline, use scoring=\"balanced_accuracy\" or \"f1_score\".\n",
      "See the end of this tutorial on how to build a pipeline and do parameter tuning: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#evaluation-of-the-performance-on-the-test-set\n",
      "\n",
      "3\n",
      "4455\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-05 18:38\n",
      "That will be useful, thanks @ogrisel\n",
      "\n",
      "1\n",
      "4456\n",
      "5553244215522ed4b3e05112\n",
      "2019-06-05 21:52\n",
      "hi guys does scikit sdk or articles available for go language\n",
      "\n",
      "1\n",
      "4457\n",
      "5cf106a0d73408ce4fc1d8b2\n",
      "2019-06-06 01:13\n",
      "https://www.quora.com/Go-vs-Python-which-is-better-for-AI\n",
      "\n",
      "1\n",
      "4458\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-06 17:36\n",
      "anyone on windows?\n",
      "I am trying to run a script with just \"python script.py\" but I am getting missing module errors and I am sure I should have this installed\n",
      "these*\n",
      "\n",
      "3\n",
      "4459\n",
      "5cf106a0d73408ce4fc1d8b2\n",
      "2019-06-06 18:24\n",
      "Check the spellings of the modules you have imported\n",
      "And also check whether the modules you imported contain that specific module\n",
      "And just as shortcut run pip install on all the modules to ensure everything has been properly installed\n",
      "It also depends on the version of python you are using\n",
      "\n",
      "4\n",
      "4460\n",
      "5cf106a0d73408ce4fc1d8b2\n",
      "2019-06-07 00:55\n",
      "Python 2.7 does not support many modules that are supported by python 3.6 and other newer versions\n",
      "\n",
      "1\n",
      "4461\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-07 11:34\n",
      "@piotr-mamenas on windows I would recommend you to use conda environments: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html (or alternatively Python 3 builtin venv module) to get a fine control on the list of packages installed in the currently active environment\n",
      "\n",
      "1\n",
      "4462\n",
      "5c70f4fad73408ce4fb8b1b8\n",
      "2019-06-08 08:37\n",
      "*IIT BHU* conducting Coding fest (online as well as offline also) which is *free of cost* + lots of goodies  <unconvertable>  Some information regarding codefest 2019  Just registered yourself .   https://codefest.tech/login?referral=qbK0w2UF   1.Annual coding fest of CSE Department ,IIT BHU 2. 8 events covering almost every domain of Computer Science and Engineering 3. Global participation from more than 100 countries 4. *Cash prize* of nearly *500k* ,goodies an other merchandise as well 5. No registeration cost, *certificate to all participants* 6. *Onsite events,HaXplore,accomodation,goodies and food <unconvertable> will be provided to everyone ,travel <unconvertable> reimbursement as well*  *Share with your buddies also*\n",
      "\n",
      "1\n",
      "4463\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-09 11:16\n",
      "@ogrisel that's what I am doing locally, I work with a jupyter notebook with a separate tensorflow + scikit environment installed and used with \"activate\" and conda\n",
      "I was asking because I've got a separate lightweight service running inside of container and I wanted to keep it limited to just pip and relevant libs on production\n",
      "In the container I don't want to use conda\n",
      "\n",
      "3\n",
      "4464\n",
      "5824aa0dd73408ce4f3501a2\n",
      "2019-06-09 16:06\n",
      "@piotr-mamenas have you considered Miniconda as a lightweight alternative? https://docs.conda.io/en/latest/miniconda.html\n",
      "I think there is also the odd dockerhub image to build of\n",
      "\n",
      "2\n",
      "4465\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-09 20:46\n",
      "@piotr-mamenas use a venv and pip install everything you need in it. You can also prepare a lightweight conda environment with some tricks: https://twitter.com/jiminy_crist/status/1135637901457395712\n",
      "\n",
      "1\n",
      "4466\n",
      "5b3ed273d73408ce4f9fcb4e\n",
      "2019-06-11 09:59\n",
      "I have an input sparse matrix in the csr format. Any suggestions on how to generate mini batches for training from this input?\n",
      "\n",
      "1\n",
      "4467\n",
      "5cf106a0d73408ce4fc1d8b2\n",
      "2019-06-11 17:30\n",
      "U can use the sklearn.model_selection.train_test_split\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
      "\n",
      "2\n",
      "4468\n",
      "5b3ed273d73408ce4f9fcb4e\n",
      "2019-06-12 04:02\n",
      "does it work with sparse matrices?\n",
      "Great, thanks\n",
      "\n",
      "2\n",
      "4469\n",
      "5cf106a0d73408ce4fc1d8b2\n",
      "2019-06-12 04:52\n",
      "I think it will\n",
      "\n",
      "1\n",
      "4470\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-06-12 11:12\n",
      "you can use `sklearn.utils.safe_indexing` that will return a matrix given a set of indices\n",
      "and will work with dataframe, array, sparse matrix\n",
      "depending what you are doing with mini-batch, we have some generator of mini-batch in imbalanced-learn\n",
      "http://imbalanced-learn.org/en/stable/api.html#module-imblearn.keras\n",
      "you can check the implementation if you want to bypass the resampling stage\n",
      "\n",
      "5\n",
      "4471\n",
      "5cec1aa6d73408ce4fc17534\n",
      "2019-06-12 14:13\n",
      "hi everyone\n",
      "can i ask you something\n",
      "is there autocomplete C# codes in komodo\n",
      "\n",
      "3\n",
      "4472\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-13 09:03\n",
      "@akil101 please ask on a channel related to komodo or C# development.\n",
      "\n",
      "1\n",
      "4473\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-15 16:51\n",
      "I wonder what you guys think.\n",
      "\n",
      "1\n",
      "4474\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-15 16:52\n",
      "Say you have a terrible unbalanced text data set that you vectorize, 1:50 ratio\n",
      "binary classification\n",
      "would you consider it a bad practice to lower the imbalance by duplicating proportionally the first class?\n",
      "so practically speaking, if I have 1000 articles with class 0 and 50000 with class 1, just copy each one of the class 0 to get another set of 1000 articles and push it into the training set so I have a 1:25 ratio instead?\n",
      "Similar to how its done with image classification\n",
      "\n",
      "5\n",
      "4475\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-15 18:29\n",
      "Another thing, how would you measure quality of scrapped data provided from 3rd party data science firm? Text.\n",
      "I would assume if i do cluster analysis on the data and get word frequency per cluster I should be able to see more or less but that doesn't give me a full picture\n",
      "\n",
      "2\n",
      "4476\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-06-19 22:58\n",
      "@piotr-mamenas https://www.youtube.com/watch?v=EUiIydNBIbE&list=PL_pVmAaAnxIQGzQS2oI3OWEPT-dpmwTfA&index=10 and https://www.youtube.com/watch?v=Eix70D-H5ag&list=PL_pVmAaAnxIQGzQS2oI3OWEPT-dpmwTfA&index=11 are relevant to the first question\n",
      "For imbalanced data I would worry about evaluation first\n",
      "\n",
      "2\n",
      "4477\n",
      "55f1d9790fc9f982beb04bb2\n",
      "2019-06-20 00:53\n",
      "@piotr-mamenas yeah as long as you are randomly sampling, you can either upsample from the smaller class or down sample..  or use an algorithm that can tolerate unbalanced data.. you could even turn the problem into an anomaly detection one.. if the smaller class has only very few data points.. There are other techniques like SMOTE .. that you could look into.. as well.\n",
      "\n",
      "1\n",
      "4478\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-06-20 10:06\n",
      "Thanks for the answers @amueller and @rahulunair , I wasn't aware you have models for working with imbalanced data sets specifically, how do they fare against large sparse matrices, I am talking word vectors?\n",
      "Thanks @rahulunair your response is much appreciated, I will take a look at it.\n",
      "\n",
      "2\n",
      "4479\n",
      "55f1d9790fc9f982beb04bb2\n",
      "2019-06-20 16:49\n",
      "@piotr-mamenas I would consider the word vectors as the input embeddings? and if you are looking to classify something, you can look into SVMs that deal with unbalanced classes, essentially it weights the unbalanced class differently.. scikit-learn has a section for that: https://scikit-learn.org/stable/modules/svm.html\n",
      "or try a tree based algorithms to see how your accuracy numbers and ROC curve is\n",
      "for accuracy , check out: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n",
      "\n",
      "3\n",
      "4480\n",
      "564789be16b6c7089cbab8b7\n",
      "2019-06-21 17:24\n",
      "if you are doing image classification using scikit learn, do you have to convert the images into 1d arrays first?\n",
      "@glemaitre  hmm... that seems to lose some vital information\n",
      "i..e that pixels next to each other are related\n",
      "HoG features?\n",
      "Histogram of Oriented Gradients ?\n",
      "\n",
      "15\n",
      "4481\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-06-21 17:24\n",
      "yes\n",
      "\n",
      "1\n",
      "4482\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-06-21 17:25\n",
      "you can look at an example of `load_digits` to see that the 8x8 images are transformed to 1d 64 arrays\n",
      "> you can use a pre-trained convolutional neural network to extract interesting features  which a much better approach\n",
      "https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html\n",
      "I think that I have 2 quick examples showing a bit how things can be connected:\n",
      "https://scikit-image.org/docs/dev/auto_examples/applications/plot_haar_extraction_selection_classification.html#sphx-glr-auto-examples-applications-plot-haar-extraction-selection-classification-py\n",
      "https://github.com/scikit-learn/scikit-learn/pull/6509/files\n",
      "\n",
      "6\n",
      "4483\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-21 17:25\n",
      "you can use a pre-trained convolutional neural network to extract interesting features\n",
      "or you can use scikit-image HoG features for instance. Depending on the kinds of images, it might be enough.\n",
      "\n",
      "2\n",
      "4484\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-21 17:29\n",
      "yes, you have to do feature engineering first. You can consider the 2D conv layers before the final flatten / global average pooling as a feature extractor and the last fully connected layers as a standard classifier. It's just that both the feature extraction and the classifier are trained end-to-end together\n",
      "\n",
      "9\n",
      "4485\n",
      "564789be16b6c7089cbab8b7\n",
      "2019-06-21 17:33\n",
      "@ogrisel yes . What I meant is that without any convolutions you don't get to see local patterns\n",
      "on an NN topic, is there software to give you a good guess at a reasonable architecture for a classification task? I saw autokeras but it's pretty heavy.\n",
      "@ogrisel thanks! Why won't it be implemented? Because it doesn't work or coding resources?\n",
      "@ogrisel  got you\n",
      "image classification was just interesting because the data is in 2d\n",
      "but even in 1d it seems unclear to me what the right thing to do is\n",
      "@ogrisel  I have read those guidelines! They seem very sensible to me\n",
      "I greatly admire how scikit learn is run in general\n",
      "@ogrisel  I read that second link too :)\n",
      "\n",
      "9\n",
      "4486\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-21 17:35\n",
      "I don't know what is the practical state of the art for architecture search for image classification\n",
      "\n",
      "5\n",
      "4487\n",
      "5cb5be0ed73408ce4fbddb96\n",
      "2019-06-22 06:26\n",
      "hi\n",
      "\n",
      "1\n",
      "4488\n",
      "5872a729d73408ce4f421fb8\n",
      "2019-06-22 21:03\n",
      "What version of OpenMP does scikit-learn require? Is 2.5 sufficient?\n",
      "\n",
      "1\n",
      "4489\n",
      "541a528b163965c9bc2053de\n",
      "2019-06-23 13:42\n",
      "Probably, we use OpenMP via the `prange` construct of Cython.\n",
      "\n",
      "1\n",
      "4490\n",
      "5bd32ac1d73408ce4facdf3c\n",
      "2019-07-03 19:25\n",
      "Hey guys, so I'm new to scikit, so please bear with me. I have a pandas dataframe that looks like this: [email, businessId, manager, app1, app2, app3, ... , app170] So essentially one row defines one user that has either a 1 or NaN on each of the appX columns specifying if they have that app or not.\n",
      "What I want is a classifier that given email, businessId, and manager ....would return a list of apps should have\n",
      "I've got the data in that format as i specified, what are the models do you guys think would b good for creating this type of classifier? And how would i go about this in general?\n",
      "\n",
      "3\n",
      "4491\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-07-04 20:26\n",
      "How I get the individual components from classification_report?\n",
      "I found this: https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format\n",
      "sorry, this: https://stackoverflow.com/questions/48417867/access-to-numbers-in-classification-report-sklearn\n",
      "but the output_dict=True doesn't seem to work, I am receiving an error stating this parameter does not exist on the classification_report function, I also don't trust precision_recall_fscore_support, plus it misses accuracy\n",
      "\n",
      "4\n",
      "4492\n",
      "5c77a43ed73408ce4fb93081\n",
      "2019-07-05 23:24\n",
      "@piotr-mamenas Please check the version of sklearn you are using. I believe `output_dict` was added in 0.20.\n",
      "\n",
      "1\n",
      "4493\n",
      "5bc98094d73408ce4fabf741\n",
      "2019-07-06 13:02\n",
      "@thomasjpfan yup, I figured it out yesterday and after some fight with tensorflow dependencies I got it running\n",
      "\n",
      "1\n",
      "4494\n",
      "5c49f26dd73408ce4fb5d342\n",
      "2019-07-10 16:29\n",
      "hi everyone\n",
      "\n",
      "1\n",
      "4495\n",
      "569fe132e610378809bd5552\n",
      "2019-07-13 14:56\n",
      "Hello from the SciPy sprints!\n",
      "\n",
      "1\n",
      "4496\n",
      "5b4a4878d73408ce4fa0e331\n",
      "2019-07-13 15:31\n",
      "Hi All, also from the SciPy sprints :)\n",
      "\n",
      "1\n",
      "4497\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-07-13 16:08\n",
      "Welcome everybody :)\n",
      "\n",
      "1\n",
      "4498\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-07-13 16:29\n",
      "@thomasjpfan wanna look at https://github.com/scikit-learn/scikit-learn/pull/14326 ?\n",
      "anyone wanna look at https://github.com/scikit-learn/scikit-learn/pull/14320 ?\n",
      "\n",
      "2\n",
      "4499\n",
      "5ba59281d73408ce4fa8fbb5\n",
      "2019-07-17 17:11\n",
      "how can i use yolo to detect numbers in sudoku puzzle?\n",
      "I want to read those numbers but Canny, Hough, contour aren't working any good\n",
      "\n",
      "2\n",
      "4500\n",
      "5d300aebd73408ce4fc65994\n",
      "2019-07-18 06:26\n",
      "Please let me know your opinion on this https://github.com/scikit-learn/scikit-learn/issues/4450#issuecomment-512681856\n",
      "\n",
      "1\n",
      "4501\n",
      "5bf776ebd73408ce4fafd6e3\n",
      "2019-07-24 05:18\n",
      "Hello People,\n",
      "Do we have any package like NLTK to support your languages other than English\n",
      "\n",
      "2\n",
      "4502\n",
      "54fdd51a15522ed4b3dd04b7\n",
      "2019-07-24 06:50\n",
      "@venkyyuvy this looks\n",
      "Like a good feature to have in LabelEncoder. Would this make sense as a feature @amueller\n",
      "\n",
      "2\n",
      "4503\n",
      "58f1ce53d73408ce4f588a29\n",
      "2019-07-26 12:22\n",
      "Hi\n",
      "\n",
      "1\n",
      "4504\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-07-26 12:27\n",
      "@adityap31 we choose to only officially support English in our documentation, to avoid having to maintain different versions\n",
      "\n",
      "1\n",
      "4505\n",
      "5bf776ebd73408ce4fafd6e3\n",
      "2019-07-30 12:34\n",
      "Thanks @NicolasHug\n",
      "\n",
      "1\n",
      "4506\n",
      "5d36d216d73408ce4fc6ba51\n",
      "2019-07-30 14:02\n",
      "Please the best c# tutorial online\n",
      "Give me ideas\n",
      "\n",
      "2\n",
      "4507\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-07-30 18:37\n",
      "@Emoruwa since you're not the first one asking this here: what gave you the idea of asking about C# in a channel about a Python library for machine learning?\n",
      "\n",
      "1\n",
      "4508\n",
      "5944c1e5d73408ce4f67f652\n",
      "2019-07-31 14:30\n",
      "Hi, everyone. My name is Manish and It's nice to meet you all. I used SK learn for one of my projects this summer and  I really love this library. I want to start contributing to it. I'm new to open source stuff and I don't know how to get started. I checked issues under good first issue label but I'm not able to understand anything. Can anyone plz guide me with this??\n",
      "\n",
      "1\n",
      "4509\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-07-31 16:49\n",
      "@ManishAradwad welcome! the easiest way is probably to ask directly on the issue. Have you checked out the contributors guide?\n",
      "\n",
      "1\n",
      "4510\n",
      "5944c1e5d73408ce4f67f652\n",
      "2019-08-01 03:15\n",
      "Yes, I'm now going through the repo first. I'll then go for the issues. Thanks for the reply!\n",
      "\n",
      "1\n",
      "4511\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-01 15:57\n",
      "I wouldn't try going to the repo, it's a lot. I would start with the contributor docs\n",
      "even understanding how we set up and run tests would probably take me a week to understand\n",
      "\n",
      "2\n",
      "4512\n",
      "564789be16b6c7089cbab8b7\n",
      "2019-08-01 21:21\n",
      "is there something in scikit learn for 4000 dimension regression where I know I only one or two of the coefficients to be non-zero?\n",
      "something like forward stepwise regression?\n",
      "\n",
      "2\n",
      "4513\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-02 21:03\n",
      "not yet. mlxtend has it and there's a PR\n",
      "#8684\n",
      "\n",
      "2\n",
      "4514\n",
      "564789be16b6c7089cbab8b7\n",
      "2019-08-04 09:09\n",
      "@amueller  Thanks! I will take a look at mixtend which I didn't know about\n",
      "\n",
      "1\n",
      "4515\n",
      "5bcb2df0d73408ce4fac1a51\n",
      "2019-08-04 09:10\n",
      "Can anyone please provide a good source of how to deal with categorical data? It's very helpful and thanku\n",
      "\n",
      "1\n",
      "4516\n",
      "5944c1e5d73408ce4f67f652\n",
      "2019-08-04 13:21\n",
      "@amueller  Hi!! As you said I've gone through the contributing guides and set up the development environment. Can you plz tell me what should I do next. Thanks for the help!!\n",
      "\n",
      "1\n",
      "4517\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-05 14:19\n",
      "@ManishAradwad look at things tagged as \"good first issue\" and \"help wanted\" as outlined in the contributing guide\n",
      "\n",
      "1\n",
      "4518\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-05 14:45\n",
      "Hello guys, maybe anyone can help me out here. I am running following validation code: ``` train_scores, valid_scores = validation_curve(estimator=pipeline,  # estimator (pipeline)                                               X=features,  # features matrix                                               y=target,  # target vector                                              param_name='pca__n_components',                                              param_range=range(1,50),  # test these k-values                                              cv=5,  # 5-fold cross-validation                                              scoring='neg_mean_absolute_error')  # use negative validation ```  in the same `.py` file on different machines, which I would name `#1 localhost`, `#2 staging`, `#3 live`, `#4 live`  localhost and staging have both i7 cpus, localhost needs around 40s for the validation, staging needs around 13-14 seconds  live (#3) and live (#4) need almost 10 minutes for executing the validation - both of these servers have intel cpus with 48 threads.   In order to get more \"trustworthy\" numbers I dockerized the images and run them on the servers. Anyone has an idea why the speed is so different?\n",
      "\n",
      "1\n",
      "4519\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-05 14:47\n",
      "how many cores do you have in localhost and staging?\n",
      "could be that you're overallocating processes in the estimator and parallelization actually hurts you\n",
      "what's pipeline?\n",
      "so the number of cores is the likely difference, right?\n",
      "\n",
      "4\n",
      "4520\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-05 14:50\n",
      "@amueller localhost and staging are both with i7 (4 cores and 8 threads)\n",
      "yeah, live 3 and live 4 have 48 threads, 24 cores. Pipeline: ``` from sklearn.linear_model import LinearRegression model = LinearRegression() from sklearn.preprocessing import PolynomialFeatures poly_transformer = PolynomialFeatures(degree=2, include_bias=False) from sklearn.pipeline import Pipeline pipeline = Pipeline([('poly', poly_transformer), ('reg', model)]) ```\n",
      "\n",
      "2\n",
      "4521\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-05 15:29\n",
      "After profiling, I saw this (slowest time on bottom, sorted by 3rd column): ```      4150  208.706    0.050  208.706    0.050 {built-in method numpy.dot}       245   13.112    0.054   13.360    0.055 decomp_svd.py:16(svd)      2170  142.567    0.066  143.360    0.066 decomp_lu.py:153(lu) ```  Just executed `python -m cProfiler validation.py`\n",
      "\n",
      "1\n",
      "4522\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-05 15:48\n",
      "can you try to benchmark just calling svd directly without any sklearn around it?\n",
      "if that's a pure scipy issues that would be good to isolate\n",
      "\n",
      "2\n",
      "4523\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-05 15:53\n",
      "how can I isolate it, make a separate `.py` and run `cProfiler` on it?\n",
      "\n",
      "1\n",
      "4524\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-05 16:32\n",
      "make a py file that calls scipy.linalg.svd without using sklearn\n",
      "well that should work\n",
      "\n",
      "2\n",
      "4525\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-05 17:31\n",
      "lol I am killing the sorting in the pull requests in the issue tracker with adding tags. sorry lol\n",
      "\n",
      "1\n",
      "4526\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-05 18:52\n",
      "I will try this and report here. Any ideas what could be the reason? Localhost and staging are intel i7, live3 and live4 are xeon cpus, do you think mkl would improve speed or setting up the environment in another way? (Tensorflow recommends custom compile for speed for example)\n",
      "\n",
      "2\n",
      "4527\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-05 18:59\n",
      "Using pipenv, numpy 1.16.x i think\n",
      "They are using openblas\n",
      "\n",
      "2\n",
      "4528\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-06 06:38\n",
      "@amueller I don't know if this helps: I ran ``` from scipy import linalg import numpy as np m, n = 9, 6 a = np.random.randn(m, n) + 1.j*np.random.randn(m, n) U, s, Vh = linalg.svd(a) print(U.shape,  s.shape, Vh.shape) ```  `cProfile` says: ```       394    0.004    0.000    0.017    0.000 <frozen importlib._bootstrap_external>:1233(find_spec)       900    0.004    0.000    0.004    0.000 {built-in method posix.stat}         1    0.006    0.006    0.006    0.006 lil.py:23(lil_matrix)     81/24    0.007    0.000    0.011    0.000 sre_compile.py:64(_compile)   402/399    0.011    0.000    0.022    0.000 {built-in method builtins.__build_class__}     212/1    0.023    0.000    0.222    0.222 {built-in method builtins.exec}       190    0.024    0.000    0.024    0.000 {built-in method marshal.loads}     39/37    0.038    0.001    0.043    0.001 {built-in method _imp.create_dynamic} ``` (sorted by second column)   ```         9    0.000    0.000    0.000    0.000 __future__.py:79(__init__)         9    0.000    0.000    0.000    0.000 _globals.py:77(__repr__)         9    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}         9    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}         9    0.000    0.000    0.000    0.000 os.py:742(encode)         9    0.000    0.000    0.001    0.000 abc.py:151(register)         9    0.000    0.000    0.001    0.000 datetime.py:356(__new__)       900    0.001    0.000    0.005    0.000 <frozen importlib._bootstrap_external>:75(_path_stat)       900    0.004    0.000    0.004    0.000 {built-in method posix.stat}       936    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:321(<genexpr>)        96    0.000    0.000    0.000    0.000 enum.py:630(<lambda>)     39/37    0.038    0.001    0.043    0.001 {built-in method _imp.create_dynamic}         1    0.002    0.002    0.002    0.002 __init__.py:259(_reset_cache)         1    0.006    0.006    0.006    0.006 lil.py:23(lil_matrix) ``` (sorted by third column)\n",
      "this is on the 24 core machine\n",
      "\n",
      "2\n",
      "4529\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-06 06:43\n",
      "@amueller when I run this code: ``` train_scores, valid_scores = validation_curve(estimator=pipeline,  # estimator (pipeline)                                               X=features,  # features matrix                                               y=target,  # target vector                                              param_name='pca__n_components',                                              param_range=range(1,50),  # test these k-values                                              cv=5,  # 5-fold cross-validation                                              scoring='neg_mean_absolute_error')  # use negative validation ```  directly on the host (with 24 cores) I get ~30 seconds. When I run it directly on localhost (4 cores, 8 threads) I get around 30-40 seconds as well. When I run inside docker with cpu limit of 6 cores and 6GB RAM, it needs almost 10 minutes. Inside a VirtualBox with 2 cores.. around 30 seconds, seems scikit does not play well with docker limitations which uses the CFS Scheduler: [link](https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler)\n",
      "Also found out that if I adjust `param_range` to `range(1,5)`the code runs much faster (I am no data scientist)\n",
      "this saved my life @amueller\n",
      "\n",
      "3\n",
      "4530\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-06 07:02\n",
      "It seems `validation_curve` does not really profit from multithreading/multiprocessing. I get almost same results on intel i7 (4 cores) and intel xeon (24 cores). The problem is that if the validation curve runs on the xeon machines.. it uses all cores and the machine is overloaded, which makes no sense, really :)\n",
      "`cv=3` makes it faster as well\n",
      "it reduced my validation curve\n",
      "from 500s to 15 seconds\n",
      "@amueller this is a life saver\n",
      "\n",
      "10\n",
      "4531\n",
      "5c34f182d73408ce4fb408a6\n",
      "2019-08-06 13:11\n",
      "How should I install the dependencies for local development of scikit-learn?\n",
      "\n",
      "1\n",
      "4532\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-06 14:26\n",
      "@sameshl https://scikit-learn.org/stable/developers/advanced_installation.html#install-bleeding-edge\n",
      "\n",
      "1\n",
      "4533\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-06 14:27\n",
      "I'd recommend using conda and doing ``conda install numpy scipy cython matplotlib pytest flake8 sphinx sphinx-gallery`` or something like that\n",
      "\n",
      "1\n",
      "4534\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-08-06 14:27\n",
      "@amueller by the way, numpy and scipy from conda perform somehow faster than from pip\n",
      "but I still haven't found out why\n",
      "@amueller how can I reconfigure numpy and scipy to use max threads e.g. 6?\n",
      "I have no `mkl` (from conda or pip)\n",
      "https://pypi.org/project/mkl/\n",
      "\n",
      "5\n",
      "4535\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-06 14:28\n",
      "@katsar0v thats mkl vs openblas possibly\n",
      "but could also be how they are configured by default\n",
      "i.e. how many threads they use etc\n",
      "pip has no mkl ;)\n",
      "(so far)\n",
      "https://stackoverflow.com/questions/30791550/limit-number-of-threads-in-numpy\n",
      "@katsar0v I don't think that helps given that numpy and scipy will not be linked against it\n",
      "\n",
      "7\n",
      "4536\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-06 14:34\n",
      "well in your script n and m are way too small to show anything useful\n",
      "well stackoverflow saved your live\n",
      "*life\n",
      "\n",
      "3\n",
      "4537\n",
      "5c34f182d73408ce4fb408a6\n",
      "2019-08-07 05:02\n",
      "How should I build the docs for harversine_distances in my local repo? I ran `python setup.py install` but still I can't find it under `doc/modules/`\n",
      "@lesteve Sure. Thanks for the help.\n",
      "As a beginner contributor to this organisation, the arrangements of the docs did feel a bit tough to navigate. I will put my thoughts about it more concisely and then open a issue and PR for the same\n",
      "\n",
      "3\n",
      "4538\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-08-07 08:20\n",
      "The documentation is another command line\n",
      "``` cd doc make html ``` should work all OS I think\n",
      "then it will create a `_build/html` folder and you can search for the `index.html`\n",
      "\n",
      "3\n",
      "4539\n",
      "5571fe1015522ed4b3e17d90\n",
      "2019-08-07 08:39\n",
      "@sameshl note this part of the contributing scikit-learn doc: https://scikit-learn.org/stable/developers/contributing.html#documentation\n",
      "If you see ways the contributing doc can be improved while you face this \"setup\" issues, let us know or/and open PRs to improve the contributing docs!\n",
      "\n",
      "2\n",
      "4540\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-08-07 10:10\n",
      "We're working on improving our contributing docs @sameshl, there's some discussion under #14582\n",
      "\n",
      "1\n",
      "4541\n",
      "5c34f182d73408ce4fb408a6\n",
      "2019-08-07 10:15\n",
      "Thats great. Would love to contribute on https://github.com/scikit-learn/scikit-learn/issues/14582\n",
      "\n",
      "1\n",
      "4542\n",
      "5c34f182d73408ce4fb408a6\n",
      "2019-08-07 10:19\n",
      "I am working on https://github.com/scikit-learn/scikit-learn/issues/14575. So I found the corresponding example under `sklearn/metrics/pairwise.py`.  My question is, are the examples run in the doc building process and output is generated or I am supposed to manually write the output of the example in the docstring of a function?\n",
      "\n",
      "3\n",
      "4543\n",
      "5ba59281d73408ce4fa8fbb5\n",
      "2019-08-07 19:57\n",
      "Does anyone here knows a good source to learn rnn structure ?\n",
      "Is it like replacing every hidden node with a rnn cell?\n",
      "\n",
      "2\n",
      "4544\n",
      "5c34f182d73408ce4fb408a6\n",
      "2019-08-09 19:13\n",
      "I am working on https://github.com/scikit-learn/scikit-learn/issues/14131 . So, I thought that I could append a note in the docstring of `KDTree` regarding the issue. But  I looked into `sklearn/neighbors/kd_tree.pyx ` and it looks like `KDTree` is inheriting its docstring from `BinaryTree`. So can someone tell me an elegant way to append my note docstring to the inherited docstring of `KDTree` or if I could do something else to solve this issue.\n",
      "\n",
      "1\n",
      "4545\n",
      "5944c1e5d73408ce4f67f652\n",
      "2019-08-11 14:32\n",
      "Currently working on #14081.  I am supposed to create a pitfalls section which includes practices not to be followed by users. Quite confused about how should I approach it, should I create a whole new section in documentation.html or is there another way to do this?? Thanks for the help!!!\n",
      "\n",
      "1\n",
      "4546\n",
      "5810cd4cd73408ce4f3101ce\n",
      "2019-08-15 11:12\n",
      "Hey channel, ive being working on vectorizing regression tree with Numpy, and i have achieved some speed up against the cython version of sklearn.  in case anyone is interested, here is the link https://github.com/yupbank/np_decision_tree#regression-with-mae\n",
      "on median data(10000*100), with MAE criteria, achieved 20 times speed up :)\n",
      "lol, you are right, actually with max_depth=10, i only get 5 times faster.\n",
      "\n",
      "3\n",
      "4547\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-08-15 12:52\n",
      "still haven't checked the code in depth. But it's definitely interesting @yupbank . What do you think @NicolasHug ?\n",
      "\n",
      "1\n",
      "4548\n",
      "5810cd4cd73408ce4f3101ce\n",
      "2019-08-15 12:55\n",
      "i havent clean the code yet, and also working on a blog post explainning what i did, and add some CI to it.  But i would love to have some extra inputs before i proceed, e.g. reviews.\n",
      "\n",
      "3\n",
      "4549\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-08-15 13:50\n",
      "@yupbank pretty cool stuff! I took a quick glance at the tree grower and the `greedy_split` function and it looks good as far as I can tell. I wouldn't advertise benchmarks with only `max_depth=1` though ;) Please definitely ping us when you write the blog post!!\n",
      "\n",
      "1\n",
      "4550\n",
      "5810cd4cd73408ce4f3101ce\n",
      "2019-08-17 14:47\n",
      "@NicolasHug  @adrinjalali  hey.. i have a draft version here.. comments are very welcome :) https://yupbank.github.io/learning/2019/08/08/faster-regression-tree.html\n",
      "\n",
      "1\n",
      "4551\n",
      "5810cd4cd73408ce4f3101ce\n",
      "2019-08-19 14:52\n",
      "omg omg omg, For L2 loss, if i replace `import numpy as np` with `import cupy as np`, i get another 10x Speed up for 1 split, but i would lost the edge when i have too many depth.. i need to refactor my code...\n",
      "but i really like the fact that, switching to GPU is so trivial ...\n",
      "\n",
      "2\n",
      "4552\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-08-19 14:54\n",
      "+1\n",
      "\n",
      "1\n",
      "4553\n",
      "5924c519d73408ce4f61c9c7\n",
      "2019-08-19 20:37\n",
      "Have a question maybe someone can answer. Trying to use a simple model on a set of data. About a couple thousand rows and only a dozen features, most are binary. I'm training on Logistic Regression, and found my model overfits. So when I try to tune my hyperparameters, my accuracy remains entirely unchanged. Has anyone seen this before or know why this is happening?\n",
      "\n",
      "1\n",
      "4554\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-08-20 08:17\n",
      "Do you have imbalanced classes?\n",
      "\n",
      "1\n",
      "4555\n",
      "5c34f182d73408ce4fb408a6\n",
      "2019-08-22 05:02\n",
      "I want to rebuild the 'scikit-learn' project. I tried running `pip install --editable .` as stated in the docs https://scikit-learn.org/stable/developers/advanced_installation.html#building-from-source but I am getting this error. Can someone help me out. ``` ERROR: Cannot uninstall 'scikit-learn'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall. ```\n",
      "\n",
      "1\n",
      "4556\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2019-08-22 08:03\n",
      "@sameshl See https://github.com/pypa/pip/issues/5247#issuecomment-381550610 probably best to reinstall in a new virtual environment.\n",
      "\n",
      "1\n",
      "4557\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-08-22 14:18\n",
      "@thomasjpfan you like puzzles, right? https://github.com/scikit-learn/scikit-learn/pull/14704\n",
      "\n",
      "1\n",
      "4558\n",
      "5d10a5b9d73408ce4fc459a6\n",
      "2019-08-24 14:07\n",
      "I can't seem to get to make the virtual environment with sphinxgallery  conda create -n sklearndev numpy scipy matplotlib pytest sphinx cython ipykernel sphinxgallery  or   conda create -n sklearndev numpy scipy matplotlib pytest sphinx cython ipykernel sphinx-gallery\n",
      "never mind! the solution is in the other gitter chat!\n",
      "\n",
      "2\n",
      "4559\n",
      "5a32fea2d73408ce4f836261\n",
      "2019-08-28 17:07\n",
      "Hi team, I am new to Cpython but really wants to play with the internals of sklearn. I want to test out some of the cdef classes in the pyx file but looks like the methods are inaccessible within Python. Any thought?   For example:  ```python from sklearn.tree import _utils ph = _utils.PriorityHeap(100) dir(ph) ```   And I cannot find call methods like pop, push.  Usually how does the workflow look like if I want to play with the internals of sklearn within Jupyter notebook.\n",
      "\n",
      "1\n",
      "4560\n",
      "5d677234d73408ce4fc98654\n",
      "2019-08-29 06:41\n",
      "hello everyone. I'm really new to Machine learning in general and i have been working with some sklearn Regressors. I need some help :). My question is how do i know if the RMSE i have is minimum enough for good predictions. To what do i compare this RMSE to?\n",
      "\n",
      "1\n",
      "4561\n",
      "5d68027ed73408ce4fc99289\n",
      "2019-08-29 17:27\n",
      "I was able to create a model by curve fitting a set of data that has 5 variables using GaussianProcessRegressor.    The problem is I am unable to export/load this model into an older version of python (version 2.5.2).  Is there a way to dump the equation/formula into mathematical terms in relations to these 5 variables so that I can use this prediction on the older python?  Thanks\n",
      "\n",
      "1\n",
      "4562\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-08-30 10:15\n",
      "@enoch-sun We don't really support those Python versions anymore. You can try and figure it out with some other persisting models such as ONNX or PMML, but you'll be mostly on your own\n",
      "\n",
      "1\n",
      "4563\n",
      "5c77a43ed73408ce4fb93081\n",
      "2019-09-02 03:09\n",
      "@biwa7636 The PriorityHeap functions `pop` and `push` are cdef, which means they are not available in python.\n",
      "\n",
      "1\n",
      "4564\n",
      "5d5565e0d73408ce4fc880d5\n",
      "2019-09-03 20:31\n",
      "Is there a scikit-learn preferred way to store a vector using Cython?  I've seen libcpp.vector, array.array and numpy used in the code base.  @NicolasHug @amueller\n",
      "\n",
      "1\n",
      "4565\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-09-03 20:33\n",
      "The way we do it now is to allocate numpy arrays (in python or in cython), and then use a memory view for pure cython parts. You can take a look at how we do it in e.g. `ensemble/_hist_gradient_boosting`\n",
      "\n",
      "1\n",
      "4566\n",
      "57fc82e4d73408ce4f2d5a7b\n",
      "2019-09-04 10:16\n",
      "Hi, does apply in df.apply(fun) iterate over each columns in 'df' data-frame and pass them to 'fun' function as a series?\n",
      "\n",
      "1\n",
      "4567\n",
      "5a32fea2d73408ce4f836261\n",
      "2019-09-04 17:05\n",
      "@thomasjpfan, you are right, however, I also tried to execute the above code too using `%%cython` magic also from `sklearn.tree cimport _utils` but still did not work. Was it supposed to be like that?\n",
      "```python %%cython # requires numpy headers from sklearn.tree._utils cimport Stack s = Stack(10) print(s.top) >>> AttributeError: 'sklearn.tree._utils.Stack' object has no attribute 'top' ```\n",
      "I found the source code so well written, fascinating and really want to be able to get the development environment up and running.\n",
      "\n",
      "3\n",
      "4568\n",
      "5a32fea2d73408ce4f836261\n",
      "2019-09-04 17:56\n",
      "Weird, the above code will work if I replace `s = Stack(10)` with `cdef  Stack s = Stack(10)`, I believe this must have something to do with static type declaration.\n",
      "\n",
      "1\n",
      "4569\n",
      "5d5565e0d73408ce4fc880d5\n",
      "2019-09-05 19:26\n",
      "Does anyone know why the base estimator for `ExtraTreesClassifier` is `ExtraTreeClassifier`, instead of `DecisionTreeClassifier` with splitter='random'?   I am working on adding a new type of tree. @NicolasHug @amueller\n",
      "\n",
      "1\n",
      "4570\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-09-05 19:37\n",
      "No idea. It doesn't make much sense for `ExtraTreeClassifier` to allow for a splitter that isn't 'random' IMO. Would you want to submit a PR to deprecate the parameter?\n",
      "\n",
      "1\n",
      "4571\n",
      "5d722e7fd73408ce4fca2f1f\n",
      "2019-09-06 10:06\n",
      "Hi All, I`m getting the following error while executing the python setup.py install  error: Command \"cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT -IC:\\Users\\Moti\\Anaconda3\\envs\\motidevs\\lib\\site-packages\\numpy\\core\\include /EHsc /Tpsklearn\\svm\\src\\libsvm\\libsvm_template.cpp /Fobuild\\temp.win-amd64-3.7\\sklearn\\svm\\src\\libsvm\\libsvm_template.obj\" failed with exit status 127  Do you have any idea? Thanks!\n",
      "\n",
      "1\n",
      "4572\n",
      "579618a040f3a6eec05c5e42\n",
      "2019-09-06 21:15\n",
      "Any scikit devs who can shed some light on why `calibration_curve` is only for binary estimators?\n",
      "\n",
      "1\n",
      "4573\n",
      "5d77583cd73408ce4fca7b58\n",
      "2019-09-10 08:02\n",
      "how can i start committing to the open source\n",
      "\n",
      "1\n",
      "4574\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-09-10 08:08\n",
      "@Anj-ali you can start by going through our contributing guides: https://scikit-learn.org/dev/developers/contributing.html#contributing\n",
      "\n",
      "2\n",
      "4575\n",
      "541a528b163965c9bc2053de\n",
      "2019-09-19 13:50\n",
      "Heads up: if you use conda and upgrade your env, you might get a crash when using `n_jobs>=2`. This is caused by an updated version of intel-openmp in the default channel of conda. I reported the issue upstream as https://github.com/ContinuumIO/anaconda-issues/issues/11294 and the problem is tracked in this PR on the scikit-learn side: https://github.com/scikit-learn/scikit-learn/pull/15020\n",
      "\n",
      "1\n",
      "4576\n",
      "541a528b163965c9bc2053de\n",
      "2019-09-19 13:53\n",
      "The error message is `OMP: Error #13: Assertion failure at z_Linux_util.cpp(2361)` reported by the dying worker process.\n",
      "Which in turns causes loky to raise: `TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6)}`.\n",
      "\n",
      "2\n",
      "4577\n",
      "5c34f182d73408ce4fb408a6\n",
      "2019-09-20 16:46\n",
      "If someone is free to review, please take a look at https://github.com/scikit-learn/scikit-learn/pull/14993 and https://github.com/scikit-learn/scikit-learn/pull/15045.\n",
      "\n",
      "1\n",
      "4578\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-09-27 16:28\n",
      "hm is there a pandas gitter? Or is @jorisvandenbossche around lol? For a pandas dtype, how do I get the closest numpy dtype to cast to?\n",
      "indeed\n",
      "it's for https://github.com/scikit-learn/scikit-learn/pull/15094 which is currently failing because np.result_type(pd.CategoricalDType) raises an error\n",
      "\n",
      "8\n",
      "4579\n",
      "53232ac75e986b0712efe3af\n",
      "2019-09-27 16:29\n",
      "yep\n",
      "there is pandas gitter actually (pydata/pandas)\n",
      "I don't think there is a typical way to do it\n",
      "If I remember correctly, there is an issue about it\n",
      "Basically, you would like to know the dtype of `np.asarray(obj).dtype` right? (but without needing to do the actual conversion?)\n",
      "\n",
      "5\n",
      "4580\n",
      "5d5565e0d73408ce4fc880d5\n",
      "2019-10-04 21:28\n",
      " Hello all (I am new to Cython),  I am currently working on adding an augmented version of Brieman's forest-RC (similar to RandomForest) algorithm into my fork of scikit-learn: In short, the algorithm takes linear combinations of features and projects them with weights randomly selected in {-1,1} to form a new feature to split on. The number of features combined at each split is a random variable.    The current `SplitRecord` only holds one feature, I need something to store a vector of features and a vector to hold weights.   1. I tried initializing an np.ndarray and using memoryviews, but ran into GIL issues. 1. I tried to make an `ObliqueSplitRecord` class, but that can't be passed as a pointer into functions because it is a Python object. 1. I tried to augment the `SplitRecord` struct in [_splitter.pxd](https://github.com/scikit-learn/scikit-learn/blob/a47e914163c2dbecb4a80ec40d2d8fe313a83010/sklearn/tree/_splitter.pxd#L23-L32) but that didn't seem to work because vectors would then be of fixed length. 1. I tried to use something similar to the [`tree/_utils:Stack`](https://github.com/scikit-learn/scikit-learn/blob/3046990e76c7c90a1150c26770572c8d76ee00de/sklearn/tree/_utils.pyx#L81-L157) but fell into the same problem as it was a class and couldn't be passed as a pointer into a function.  I am looking into using cppclass, but am not sure if that will fix solve my problem.   Does anyone have suggestions on how to best implement this in a Cythonic way?  i.e. storing a vector of things while avoiding the GIL and not using python objects?\n",
      "\n",
      "1\n",
      "4581\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-10-06 09:24\n",
      "@MrAE you can use a cpp vector in cython. But since you're changing the splitrecord struct, you'll need to change the code in quite a lot of places.\n",
      "\n",
      "1\n",
      "4582\n",
      "5c2cf216d73408ce4fb369e2\n",
      "2019-10-06 20:01\n",
      "Hi, I have some basic question about local docs build for scikit. I've been trying to modify docs inside API for some file in `sklearn/linear_model` and followed instructions in Contributors Guide. But after few attempts the `make` command inside `/docs` does not seem to modify local docs build inside `_build`. In the browser,  API docs didn't change although I modified the sources. Am I missing something?\n",
      "\n",
      "1\n",
      "4583\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-10-07 14:08\n",
      "@mtsokol it seems that you're doing it right... maybe double check that 1. you're actually changing the sources, i.e. not anything in the _build folder, 2. the doc that you're changing is about a public estimators/tools (private tools aren't rendered in the doc anyway) and 3. that you're looking at the generated html in `doc/_build/html/stable/`\n",
      "\n",
      "1\n",
      "4584\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-10-07 14:14\n",
      "@MrAE   re 1. you can't use (let alone allocate) numpy arrays when the GIL is released because these are Python objects. Is there a way for you to allocate the arrays somewhere where the GIL is held, and use memory views when the GIL is released? Memory views are safe to use without the GIL  re 2. is it still considered a Python object if you use a `cdef`ed class and all the attributes are `cdef`ed as well?  re 3. what vectors? can't you use a view as a field of the struct?\n",
      "Also @MrAE  I happen to have been writing about Cython over the weekend... maybe that could help http://nicolas-hug.com/blog/cython_notes\n",
      "\n",
      "2\n",
      "4585\n",
      "5a58e8b8d73408ce4f87dd73\n",
      "2019-10-12 08:30\n",
      "could somebody share a good example for class docstrings in scikit-learn that we could use as a sort of template? thanks!\n",
      "\n",
      "1\n",
      "4586\n",
      "5a7aea3dd73408ce4f8c133a\n",
      "2019-10-12 08:30\n",
      "@janjagusch https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/compose/_column_transformer.py#L37 ?\n",
      "\n",
      "1\n",
      "4587\n",
      "5da1908ed73408ce4fcda13f\n",
      "2019-10-12 08:39\n",
      "Here is the issue search string \"is:issue is:open examples class docs involves:adrinjalali\"\n",
      "https://github.com/scikit-learn/scikit-learn/issues/3846\n",
      "\n",
      "2\n",
      "4588\n",
      "5da1908ed73408ce4fcda13f\n",
      "2019-10-12 11:07\n",
      "Hey guys who is veerlosar on Githib? just want to talk about OneVsRestClassifier example\n",
      "https://github.com/scikit-learn/scikit-learn/pull/15200/\n",
      "\n",
      "2\n",
      "4589\n",
      "5da18989d73408ce4fcda083\n",
      "2019-10-13 09:54\n",
      "@zioalex I can talk to veerlosar, we're at the same sprint\n",
      "\n",
      "1\n",
      "4590\n",
      "5da2fdd6d73408ce4fcdafbd\n",
      "2019-10-13 10:37\n",
      "> Hey guys who is veerlosar on Githib? just want to talk about OneVsRestClassifier example  @zioalex  what did you want to talk about?\n",
      "\n",
      "1\n",
      "4591\n",
      "5da71b2fd73408ce4fce084d\n",
      "2019-10-16 13:33\n",
      "how to learn complete sk learn ? please give the resources?\n",
      "\n",
      "1\n",
      "4592\n",
      "5b4c9e4bd73408ce4fa10b88\n",
      "2019-10-16 13:35\n",
      "Andreas Muller's book, Introduction to Machine Learning with Python: A Guide for Data Scientists, is quite complete. You can also look at the user guides: https://scikit-learn.org/stable/user_guide.html\n",
      "\n",
      "1\n",
      "4593\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-10-17 07:10\n",
      "there's also my lecture series: https://youtube.com/AndreasMueller   The only complete resource is the user guide though\n",
      "\n",
      "1\n",
      "4594\n",
      "5a09ec4ed73408ce4f7e6c27\n",
      "2019-10-19 09:22\n",
      "Hello there!\n",
      "\n",
      "1\n",
      "4595\n",
      "5d5565e0d73408ce4fc880d5\n",
      "2019-10-20 22:05\n",
      "Hey guys, me again: Regarding me previous message :point_up: [October 4, 2019 5:28 PM](https://gitter.im/scikit-learn/scikit-learn?at=5d97b97b0e67130aae15b693) I've gone through some more attempts that don't quite work.    @NicolasHug The blog post helped a bit with my understanding of memory-views, however I still have a few questions:  Can a memory-view be initialized `with nogil`? And no, a struct member cannot be a memory view.  I tried to make my own class but then got yelled at because it's not of type `Splitter`, so that was a bust.   I augmented the `SplitRecord` with 2 cpp vectors, but that caused things to go wonky requiring cpp in files that I'm not willing to touch.   I ended up augmenting `SplitRecord` with 2 Cython vectors with hard-coded length, but then can't seem to initialize a memory-view into them inside of the `node_split`.  I'm pretty much stuck (in my current view of things), because I'm trying to do as little modification as possible, but it seems that in order to accomplish my task I'll have to re-write a big chunk of ensemble methods. I'd have to add an input argument to the `node_split` method? That doesn't sound like a good idea.  Any ideas?  Much appreciated.\n",
      "\n",
      "1\n",
      "4596\n",
      "5571fe5f15522ed4b3e17d94\n",
      "2019-10-24 18:12\n",
      "Hi all, I'm trying to help my team reduce creating new code when leveraging existing libraries might get the job done. Does anyone have thoughts on how the following can be accomplished? https://stackoverflow.com/q/58533004/1566074  Basically finding the optimal subgroups for a dataset to then feed into an estimator to reduce noise.\n",
      "\n",
      "1\n",
      "4597\n",
      "59605bcbd73408ce4f6c2b60\n",
      "2019-10-26 21:12\n",
      "Hello the scikit-learn community! I'd like to have your thoughts on what I coded. It's a way to do automatic machine learning on scikit-learn pipelines. It allows for handling hyperparameter spaces as well as hyperparameters. Example: https://www.neuraxio.com/en/neuraxle/stable/examples/hyperparams.html#sphx-glr-examples-hyperparams-py\n",
      "\n",
      "1\n",
      "4598\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-10-30 15:10\n",
      "any takers on https://github.com/scikit-learn-contrib/imbalanced-learn/issues/616? it's a good first issue.\n",
      "\n",
      "1\n",
      "4599\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-10-30 15:10\n",
      "a first good issue?\n",
      "a find it a bit harsh :)\n",
      "\n",
      "2\n",
      "4600\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-10-30 15:14\n",
      "lol, I'm just a messenger, Joel tagged it as such :D\n",
      "\n",
      "5\n",
      "4601\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-10-30 15:19\n",
      "Yep I just cross-reference my PR\n",
      "\n",
      "1\n",
      "4602\n",
      "55d21ee30fc9f982beadabb8\n",
      "2019-11-02 09:14\n",
      "For the people joining the MAN-AHL sprint, you can find the instructions to install scikit-learn from source at the following documentation page: https://scikit-learn.org/dev/developers/advanced_installation.html#building-from-source\n",
      "In addition, you can find the contributing guide as the following address: https://scikit-learn.org/dev/developers/contributing.html\n",
      "Finally, if you are searching for an issue to work on, several issues have been tagged specifically for sprints: https://github.com/scikit-learn/scikit-learn/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3ASprint You can set some other tags if you want (\"good first issues\", etc.). You also free to search any issue that you are interested in on the issue tracker.\n",
      ":)\n",
      "\n",
      "4\n",
      "4603\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2019-11-02 10:05\n",
      "One example of a \"good first issue\", particularly if you have never contributed to large open-source projects before is  https://github.com/scikit-learn/scikit-learn/issues/15440 aiming to improve docstrings. That would allow you to see how the contribution workflow works before tackling more complex issues.\n",
      "Sure, please comment about it in the issue\n",
      "\n",
      "2\n",
      "4604\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2019-11-02 10:35\n",
      "Also it's useful to read the contribution guide at https://scikit-learn.org/dev/developers/contributing.html\n",
      "\n",
      "1\n",
      "4605\n",
      "5dbd6437d73408ce4fcfbf0f\n",
      "2019-11-02 11:14\n",
      "Hi @rth I just tried to run `test_docstrings` and looks like just 13 out of 1619 tests pass. I suppose I can pick any estimator to start to improve docstrings, am I right? is there any scale of priorities?\n",
      "\n",
      "3\n",
      "4606\n",
      "5dbd6769d73408ce4fcfbf2c\n",
      "2019-11-02 11:25\n",
      "Hi @rth I'd like to pick RadiusNeighborsClassifier\n",
      "\n",
      "1\n",
      "4607\n",
      "5dbd6437d73408ce4fcfbf0f\n",
      "2019-11-02 16:13\n",
      "Would like to take care of one of the PR that has been labeled as `stalled` and `help wanted`. I suppose a new one can pick this and conclude the PR also taking into account the comments of the reviewers. What's the best here? Create a new PR that refers to the already existing one?\n",
      "\n",
      "1\n",
      "4608\n",
      "5dbd5484d73408ce4fcfbe0b\n",
      "2019-11-02 16:20\n",
      "Trying to take a look at https://github.com/scikit-learn/scikit-learn/issues/13045 -- Does this seem like a decent issue to tackle?\n",
      "I'm at the man hackathon\n",
      "\n",
      "3\n",
      "4609\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-11-02 16:21\n",
      "@norvan sure please comment there. Are you part of the wimlds sprint?\n",
      "I didn't realize there were two today lol\n",
      "\n",
      "2\n",
      "4610\n",
      "5dcb056cd73408ce4fd0cb72\n",
      "2019-11-12 19:18\n",
      "hi there any clue for identify text from one document to another?\n",
      "we are working on a prototype for fake news\n",
      "\n",
      "2\n",
      "4611\n",
      "5dd100ecd73408ce4fd140ae\n",
      "2019-11-17 08:13\n",
      "hi\n",
      "anyone there to help\n",
      "\n",
      "2\n",
      "4612\n",
      "5dbd6437d73408ce4fcfbf0f\n",
      "2019-11-17 18:22\n",
      "Hi @qazi1002 @eliseo looks like you need some NLP for this project...can you provide more info about which kind of help do you need? Are you meant to use specifically scikit-learn for this? Also, not sure that this is the proper place where to talk about this - topics should be strictly focused on scikit-learn development/bug fixing/etc.\n",
      "\n",
      "1\n",
      "4613\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-11-17 19:41\n",
      "I don't think we have a strict policy for this channel being related to the dev only. But in the interest of the rest of the community being able to use the answers we give to your questions related to the usage, posting them on stackoverflow or other related forums may be more appropriate.\n",
      "\n",
      "1\n",
      "4614\n",
      "5dd100ecd73408ce4fd140ae\n",
      "2019-11-18 12:45\n",
      "@gbroccolo  I need help regarding software development...as i am beginner so I want to get some tips for developing softwares... I want to develop software that reads the smart ID cards using card reader.\n",
      "\n",
      "1\n",
      "4615\n",
      "564e507e16b6c7089cbb6551\n",
      "2019-11-22 22:27\n",
      "Hi there, is k-means clustering stochastic even when the initial centers are given? I'm noticing different results when I run my code multiple times\n",
      "\n",
      "1\n",
      "4616\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-11-22 22:27\n",
      "@h4k1m0u I don't think it should be\n",
      "\n",
      "1\n",
      "4617\n",
      "564e507e16b6c7089cbb6551\n",
      "2019-11-22 22:32\n",
      "@amueller That's weird, I can't find out why in this short piece of code (https://bpaste.net/show/ORPVW) the centroids found by kmeans are sometimes located at the center of the 3 samples and sometimes not\n",
      "Oh sorry, I've completely forgotten the np.random above. Thanks a lot for reminding about that.\n",
      "\n",
      "2\n",
      "4618\n",
      "54d4a1d6db8155e6700f853b\n",
      "2019-11-22 22:33\n",
      "you don't fix the random seed so the data changes\n",
      "\n",
      "1\n",
      "4619\n",
      "5acfdfffd73408ce4f95738d\n",
      "2019-11-25 19:58\n",
      " When using \"random forest\" and \"gradient boosting\". I add to the main signs, a sign that in the picture. [title](https://ibb.co/997YpMK) The data is clearly not stationary. To make the series stationary, I apply a one-time difference to the data(increments) After all, I normalize the data. [title](https://ibb.co/6tbdVzJ) Why, if I don't use increments(one-time difference), then classes are separated better? [title](https://ibb.co/dBJrnDf) Although in all textbooks they write that non-stationary data should be decomposed into increments. For training, I use the first 5000 characters. If you pay attention to the data, extreme values start after 5000. That is, the model does not even see that such large values were in the training sample.\n",
      "\n",
      "1\n",
      "4620\n",
      "5671093916b6c7089cbede6a\n",
      "2019-11-28 13:53\n",
      "I have two files that contain Event Name, Event City, Event Venue, Event State but in both files it's written in different ways or you can assume both the files are from different source.  I want to create a Machine learning-based algorithm that can do the matching.  I have tried with fuzzy-wuzzy to get string similarity. Can anyone please tell me if I want to solve this with Deep Learning what would be the approach. Thanks @amueller\n",
      "\n",
      "1\n",
      "4621\n",
      "564e507e16b6c7089cbb6551\n",
      "2019-12-02 20:32\n",
      "Hi, what does it mean when `linear_model.Ridge`returns `n_iter_` = None? does it mean it didn't even perform one single iteration?\n",
      "\n",
      "1\n",
      "4622\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-12-02 21:29\n",
      "@h4k1m0u the doc says     n_iter_ : None or array of shape (n_targets,)         Actual number of iterations for each target. Available only for         sag and lsqr solvers. Other solvers will return None.  you're probably not using sag or lsqr?\n",
      "\n",
      "1\n",
      "4623\n",
      "564e507e16b6c7089cbb6551\n",
      "2019-12-03 12:33\n",
      "Thanks @NicolasHug , you  were right I was actually not even setting that parameter (`solver='auto'`). With solver=sag, it returns the # of iterations\n",
      "\n",
      "1\n",
      "4624\n",
      "56e08c0d85d51f252ab801e4\n",
      "2019-12-04 01:24\n",
      "Hi, semi random question but I can't find it in the docs - do y'all implement a consensus clustering evaluator that's not the bicluster one?\n",
      "\n",
      "1\n",
      "4625\n",
      "5684304216b6c7089cc0a229\n",
      "2019-12-05 12:57\n",
      "Hi, I am the \"maintainer\" (more like caretaker) of hmmlearn (which was split out of sklearn a couple of years ago); I tried moving the CI to azure and realized that the macOS tests were failing (previously testing was only done on linux (travis) and windows (appveyor)) but can't test locally on macOS, would anyone be willing to have a look? https://dev.azure.com/anntzer/hmmlearn/_build/results?buildId=170  Thanks!\n",
      "\n",
      "1\n",
      "4626\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-12-06 12:59\n",
      "@anntzer , running the test locally on my linux I get a bunch of  zero division warnings on the failing test, so you might be able to debug locally still\n",
      "\n",
      "1\n",
      "4627\n",
      "5684304216b6c7089cc0a229\n",
      "2019-12-06 13:31\n",
      "I get a single warning running tests locally but they still pass...\n",
      "\n",
      "1\n",
      "4628\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-12-06 15:07\n",
      "yeah they pass but they probably should not. Unless you do expect to get a zero division in the test, in which case you need to protect the call. the macOS CIs probably use different versions of numpy or scipy so that's why they fail while the others don't\n",
      "\n",
      "1\n",
      "4629\n",
      "5684304216b6c7089cc0a229\n",
      "2019-12-06 15:54\n",
      "I'll look into it but it would be strange that different versions of numpy are being used\n",
      "\n",
      "1\n",
      "4630\n",
      "5684304216b6c7089cc0a229\n",
      "2019-12-08 12:28\n",
      "wait, are you really getting zero division warnings from TestGMMHMMWithTiedCovars::test_fit_zero_variance (which is the failing test on osx)?  I get only get warnings on TestGMMHMMWithDiagCovars::test_fit_zero_variance (another test)\n",
      "\n",
      "1\n",
      "4631\n",
      "5decf929d73408ce4fd36f72\n",
      "2019-12-08 13:25\n",
      "Dear, I i tried to tune hyperparameters of scikit GradientBoostingRegressor model using the Hyperopt optimizer. I set search space for learning_rate parameter in the range [0.01, 1] by many ways (for example : \"\"'learning_rate': hp.quniform('learning_rate', 0.01, 1, 0.05)\"\" or as simple array \"\"[0.01, 0.02, 0.03, 0.1]\"\") but when I run the code hyperopt start to calculation and I get the error \" ValueError: learning_rate must be greater than 0 but was 0\".  I do not know what is problem in the code because zero value is not in the parameter's scope. How zero value come to function?  Please help me to solve this problem.\n",
      "\n",
      "1\n",
      "4632\n",
      "541a528b163965c9bc2053de\n",
      "2019-12-09 09:13\n",
      "This looks like a bug in hyperopt, no? Can you add print statements (or debugger breakpoint) in the hyperopt and scikit-learn code to check where this zero comes from?\n",
      "Actually hp.quniform is for rounding to integer values. You probably want `hp.loguniform(-3, 0)` or someting similar.\n",
      "\n",
      "2\n",
      "4633\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-12-09 12:22\n",
      "@anntzer I get an underflow for `TestGMMHMMWithTiedCovars::test_fit_sparse_data` and indeed most of the zero div warnings come from `TestGMMHMMWithDiagCovars`, not the tied version  Maybe addressing the existing warnings would fix the one that's failing?\n",
      "\n",
      "1\n",
      "4634\n",
      "5684304216b6c7089cc0a229\n",
      "2019-12-09 13:01\n",
      "that's interesting, I don't get any warning with Tied::test_fit_sparse_data and you don't see it on Azure either; what's your numpy/scipy/anythingelse relevant version?\n",
      "\n",
      "1\n",
      "4635\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2019-12-09 14:47\n",
      "``` System:     python: 3.7.4 (default, Oct  4 2019, 06:57:26)  [GCC 9.2.0] executable: /home/nico/.virtualenvs/sklearn/bin/python    machine: Linux-5.3.1-arch1-1-ARCH-x86_64-with-arch  Python dependencies:        pip: 19.0.3 setuptools: 40.8.0    sklearn: 0.23.dev0      numpy: 1.17.1      scipy: 1.3.0     Cython: 0.29.10     pandas: 0.24.2 matplotlib: 3.0.0     joblib: 0.13.2  Built with OpenMP: True  ```\n",
      "Here's my pytest output. I locally installed the master branch of hmmlearn  ``` lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithSphericalCovars::test_fit_zero_variance lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithTiedCovars::test_fit_sparse_data   /home/nico/dev/hmmlearn/lib/hmmlearn/hmm.py:849: RuntimeWarning: underflow encountered in multiply     post_comp_mix = post_comp[:, :, np.newaxis] * post_mix  lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithDiagCovars::test_fit_zero_variance   /home/nico/dev/hmmlearn/lib/hmmlearn/stats.py:47: RuntimeWarning: divide by zero encountered in log     + np.dot(X ** 2, (1.0 / covars).T))  lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithDiagCovars::test_fit_zero_variance   /home/nico/dev/hmmlearn/lib/hmmlearn/stats.py:47: RuntimeWarning: divide by zero encountered in true_divide     + np.dot(X ** 2, (1.0 / covars).T))  lib/hmmlearn/tests/test_gmm_hmm_new.py::TestGMMHMMWithDiagCovars::test_fit_zero_variance   /home/nico/dev/hmmlearn/lib/hmmlearn/stats.py:47: RuntimeWarning: invalid value encountered in add     + np.dot(X ** 2, (1.0 / covars).T))  -- Docs: https://docs.pytest.org/en/latest/warnings.html  Results (20.38s):       93 passed        3 xpassed       15 xfailed ```\n",
      "\n",
      "2\n",
      "4636\n",
      "5684304216b6c7089cc0a229\n",
      "2019-12-10 17:16\n",
      "that's... curiouser and curiouser.  I don't get the warnings with the exact same versions of everything (AFAICT, except that cpython is from conda), whether with pip-installed numpy and scipy or conda-forge ones.\n",
      "\n",
      "1\n",
      "4637\n",
      "5e094955d73408ce4fd54a90\n",
      "2019-12-30 00:50\n",
      "Hi, does anyone know about any plans that might exist involving the release cycle or timeline in moving  IterativeImpute package out of it's experimental version? Thanks, I'm hoping to use it and it looks great for my use case!\n",
      "\n",
      "1\n",
      "4638\n",
      "5e0961a4d73408ce4fd54be5\n",
      "2019-12-30 02:53\n",
      "[![Screenshot 2019-12-28 14.29.36.png](https://files.gitter.im/scikit-learn/scikit-learn/Xmzs/thumb/Screenshot-2019-12-28-14.29.36.png)](https://files.gitter.im/scikit-learn/scikit-learn/Xmzs/Screenshot-2019-12-28-14.29.36.png)\n",
      "Hi, I want to apply Multinomial Logistic Regression to compute winning probabilities for each contestant in my races. The Data I want to feed in my model look like the image above. I'm tring to understand how should I feed the target class to my model because every race can have a different number of runners, the target class for race A has 5 contestants, instead target class for race B has just 4 contestants.  Is there a way to model this using scikit-learn?\n",
      "\n",
      "2\n",
      "4639\n",
      "567f5d7716b6c7089cc043a8\n",
      "2019-12-30 16:43\n",
      "@guptane6 we hope to fix some issues by the next release. But no guarantees\n",
      "\n",
      "1\n",
      "4640\n",
      "59605bcbd73408ce4f6c2b60\n",
      "2020-01-04 16:38\n",
      "Worth reading: https://www.neuraxio.com/en/blog/scikit-learn/2020/01/03/what-is-wrong-with-scikit-learn.html\n",
      "\n",
      "1\n",
      "4641\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-05 22:14\n",
      "I would probably found cool to have an entitled the blog post with something \"Limitations and Caveats ...\" instead of \"What's wrong ...\". This said I think that there are some criticisms that should be discussed by opening issues to come up with adequate solutions.\n",
      "\n",
      "1\n",
      "4642\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-01-06 16:12\n",
      "ugh they credit me as the creator of sklearn\n",
      "\n",
      "1\n",
      "4643\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-01-06 16:16\n",
      "haha, yeah I saw :D THE creator :P to be fair, you're the sole maintainer contact on pypi (IIRC)\n",
      "\n",
      "2\n",
      "4644\n",
      "59605bcbd73408ce4f6c2b60\n",
      "2020-01-06 17:40\n",
      "@amueller Thanks for the feedback haha! I'll edit the post soon to correct what you just pointed out. I sincerely thought you were the main creator of sklearn, as you are the top contributor, and also that you are very very involved. I'd love to know if there is anything I could do to help, or if you have any idea of things you'd like to see in Neuraxle to help with making sklearn more integrated in Deep Learning projects.   For instance, I think the following code snippet is really talkative as a way to do Deep Learning pipelines using the pipe and filter design pattern: https://www.neuraxle.org/stable/Neuraxle/README.html#deep-learning-pipelines  Would you have any ideas to share, or things you'd like to point out for me to work on next with Neuraxle?\n",
      "@glemaitre \"Limitations and Caveats ...\" sounds cool! I could rename the article. I wanted it to catch the eye, seems like it worked hehe. I love sklearn tho :)   On my side, I've already fixed 95% of the issues I listed, in Neuraxle (as per the links to the Neuraxle website documentation for each problem listed).\n",
      "\n",
      "2\n",
      "4645\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-06 17:51\n",
      "Regarding deep learning pipeline, I think that we want to be conservative: https://scikit-learn.org/stable/faq.html#why-is-there-no-support-for-deep-or-reinforcement-learning-will-there-be-support-for-deep-or-reinforcement-learning-in-scikit-learn\n",
      "Issues regarding serialization and hyperparameter search could be discussed, however.\n",
      "I think that onnx-sklearn provide a nice way to deployed scikit-learn model in production\n",
      "\n",
      "3\n",
      "4646\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-01-06 17:55\n",
      "yeah, that's the goal (onnx-sklearn), but it still needs a bit of work. I'm all in favor of focusing a bit on partial_fit (mini batches) though.\n",
      "\n",
      "6\n",
      "4647\n",
      "59605bcbd73408ce4f6c2b60\n",
      "2020-01-06 17:59\n",
      "Nice to confirm that you scope scikit-learn like that. I feared I'd play a bit too much in your backyard but it seems fine, I'm glad you have this opinion. I'm totally down to make Neuraxle a way to handle all those callbacks and things required for doing deep learning, + serialization. I don't know about Onyx, but there could be a way that I adapt to that to save every neural net usign that instead of building custom savers. For now I'm doing 2 other libraries already: Neuraxle-TensorFlow and Neuraxle-PyTorch to provide default neural net savers to allow serialization and checkpointing and have those models have their special callbacks. Might also do Neuraxle-Keras and so forth.\n",
      "\n",
      "11\n",
      "4648\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-01-06 18:01\n",
      "They are for serialization and deployment, though. I think @gulliaume-chevalier wants training as well\n",
      "lol ok you beat me to it ;)\n",
      "\n",
      "2\n",
      "4649\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-07 22:28\n",
      "Does HistGradientBoostingRegressor have an equivalent of subsample and max_features in GradientBoostingRegressor? I have a GradientBoostingRegressor model with tuned hyperparameters and I want to see if HistGradientBoostingRegressor is better\n",
      "\n",
      "1\n",
      "4650\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-01-07 22:30\n",
      "@DrEhrfurchtgebietend not right now. Do you want to open an issue as a feature request? I would recommend first comparing with the default parameters of HistGradientBoosting\n",
      "\n",
      "3\n",
      "4651\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-01-07 22:34\n",
      "no, but there is min_samples_leaf (which is not the same but similar)\n",
      "\n",
      "1\n",
      "4652\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-07 22:36\n",
      "@amueller Yea, that is likely good enough. While I have you. I did send you and Gael an email about entity embedding as we discussed at NeurIPS\n",
      "\n",
      "3\n",
      "4653\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-01-07 22:38\n",
      "I'm not sure when I'll have time to look at it tbh, in particular because it's unlikely we can directly integrate with sklearn. I didn't forget about it but my list of todos is pretty long...\n",
      "maybe I should add it to my class hand have my students do it ;)\n",
      "\n",
      "2\n",
      "4654\n",
      "5c77a43ed73408ce4fb93081\n",
      "2020-01-07 23:03\n",
      "@DrEhrfurchtgebietend Are you thinking of using entity embedding for transfer learning?\n",
      "\n",
      "1\n",
      "4655\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-07 23:04\n",
      "No, categorical encoding. If you have many level it is great. I saw like a 5% RMSE improvement\n",
      "\n",
      "1\n",
      "4656\n",
      "5c77a43ed73408ce4fb93081\n",
      "2020-01-08 00:02\n",
      "For your use case, how did you learn the entity embeddings?\n",
      "\n",
      "1\n",
      "4657\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-08 00:02\n",
      "yea, this is a must otherwise they are not tied to your target\n",
      "here is the proof it works https://github.com/entron/entity-embedding-rossmann\n",
      "and the paper https://arxiv.org/abs/1604.06737\n",
      "\n",
      "3\n",
      "4658\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-08 00:35\n",
      "@amueller so I am unable to run HistGradientBoostingRegressor because you use np.isnan '''  File \"C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py\", line 151, in fit     has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)'''\n",
      "There is a \"bug\" if you have mixed types. Which is pretty common.   https://stackoverflow.com/questions/59637976/potential-bug-in-np-isnan-for-mixed-types-on-pandas-dataframe\n",
      "Normally Pandas passes through with numpy functions fine so you do not have a dependence technically. It is pretty common in practice to use pandas in this way as it makes it more simple to keep your features organized\n",
      "clf.fit(X_train.values , y_train.values) works but clf.fit(X_train, y_train) does not. The issue seems to be that I am passing a dataframe\n",
      "I can just do clf.fit(X_train.values , y_train.values)\n",
      "\n",
      "5\n",
      "4659\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-08 01:58\n",
      "Maybe switch to pd.isnull or do u npt want pandas dependance?\n",
      "\n",
      "1\n",
      "4660\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-08 12:39\n",
      "@DrEhrfurchtgebietend we don't want dependence on pandas. However, I would have expected to have `X_train` to be a NumPy array at that stage.\n",
      "Uhm we used a `check_X_y` earlier in `fit`. Could you open a bug report with a minimal example.\n",
      "nop\n",
      "But a minimal example will help :P\n",
      "normally yest\n",
      "\n",
      "5\n",
      "4661\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-08 16:55\n",
      "but `pd.isnull` will require an import of pandas?\n",
      "\n",
      "7\n",
      "4662\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-08 16:58\n",
      "OK, that is the crux of the issue. If the algorithm does not support mixed types then I will just cast it beforehand. Is there a preferred type?\n",
      "GradientBoostingRegressor supports mixed types\n",
      "Well GradientBoostingRegressor it runs with a mix of float64, int64, bool in a dataframe\n",
      "I am trying to upgrade to HistGradientBoostingRegressor and hit this error\n",
      "\n",
      "12\n",
      "4663\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-08 17:06\n",
      "X_train.values will not give a numpy array necessarly\n",
      "oh sorry this is the way it works. so it should give you a numpy array\n",
      "you can always check by hand\n",
      "```python X, y = check_X_y(X_train, y_train, dtype=[np.float64], force_all_finite=False) ```\n",
      "to check what X is looking like\n",
      "and the same by passing `X_train.values`\n",
      "to spot difference\n",
      "\n",
      "7\n",
      "4664\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-08 17:14\n",
      "yes there is a difference. For example with a date originally 2011.  ```sklearn.utils.check_X_y(X_train, y_train, dtype=[np.float64], force_all_finite=False)``` gives the original 2011\n",
      "but  ```sklearn.utils.check_X_y(X_train.values, y_train.values, dtype=[np.float64], force_all_finite=False)``` gives 2.011e+03 which I presume is how it would show it as a float\n",
      "dtype('O') yes\n",
      "in the second dtype('float64')\n",
      "I have a fix with X_train.values so I am good but I would think this issue will come up a lot as HistGradientBoostingRegressor becomes popular\n",
      "Is this then a bug in sklearn.utils.check_X_y. Was it designed to handle being passed dataframes?\n",
      "\n",
      "6\n",
      "4665\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-08 17:15\n",
      "`X.dtype` is `object` in the first case?\n",
      "Uhm this is really weird\n",
      "\n",
      "2\n",
      "4666\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-08 17:17\n",
      "dtype=[np.float64] should force the conversion\n",
      "\n",
      "1\n",
      "4667\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-08 17:19\n",
      "this would be in `check_array` in `sklearn.utils.validation.py`\n",
      "might be a side effect of this https://github.com/scikit-learn/scikit-learn/pull/15797/files\n",
      "ups\n",
      "we might have forgot to backport the fix in 0.22.1\n",
      "uhm no it is fine\n",
      "can you check the version of scikit-learn\n",
      "are you using `0.22.0` because we corrected the bug in `0.22.1`\n",
      "\n",
      "7\n",
      "4668\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-08 17:26\n",
      "yup hold on\n",
      "0.22\n",
      "0.22.1 not ready in conda yet\n",
      "This issue only happens if X_train has a float in it already\n",
      "I have a minimum example.\n",
      "\n",
      "5\n",
      "4669\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-08 17:29\n",
      "```  import pandas as pd import sklearn import numpy as np  raw_data = {'Binary 1': [True, True, False, False, True],      'Binary 2': [False, False, True, True, False],      'age': [42, 52, 36, 24, 73],      'preTestScore': [4.4, 24.1, 31.3, 2.2, 3.1],     'postTestScore': [25.7, 94.5, 57.0, 62.2, 70.9]} df = pd.DataFrame(raw_data, columns = ['Binary 1', 'Binary 2', 'age', 'preTestScore', 'postTestScore'])    X_train = df[['Binary 1', 'Binary 2', 'age', 'preTestScore']]   y_train = df['postTestScore']  print(X_train.dtypes)  X, y = sklearn.utils.check_X_y(X_train, y_train, dtype=[np.float64], force_all_finite=False)  print(X.dtype)  X, y = sklearn.utils.check_X_y(X_train.values, y_train.values, dtype=[np.float64], force_all_finite=False)  print(X.dtype)   X_train = df[['Binary 1', 'Binary 2', 'age']]   y_train = df['postTestScore']  X, y = sklearn.utils.check_X_y(X_train, y_train, dtype=[np.float64], force_all_finite=False)  print(X.dtype)  X, y = sklearn.utils.check_X_y(X_train.values, y_train.values, dtype=[np.float64], force_all_finite=False)  print(X.dtype) ```\n",
      "Sorry the markdown is not working as I would expect. Does this work for you?\n",
      "\n",
      "2\n",
      "4670\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-08 17:31\n",
      "Jump a line after the 3 quotes\n",
      "you can install from conda-forge\n",
      "we upload the packages yesterday\n",
      "or via PyPI\n",
      "yes it was the bug\n",
      "\n",
      "5\n",
      "4671\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-08 17:34\n",
      "so in 0.22.1 the last 4 print statements all give float64?\n",
      "\n",
      "4\n",
      "4672\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-08 17:35\n",
      "``` Binary 1           bool Binary 2           bool age               int64 preTestScore    float64 dtype: object float64 float64 float64 float64 ```\n",
      "\n",
      "1\n",
      "4673\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-08 18:44\n",
      "I cannot update ``` conda install scikit-learn=0.22.1 ```  does not work\n",
      "\n",
      "1\n",
      "4674\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-01-08 20:13\n",
      "`conda install scikit-learn -c conda-forge`\n",
      "the package are only upload to conda-forge\n",
      "conda is managing directly the default channel and it can take a bit more time\n",
      "\n",
      "3\n",
      "4675\n",
      "5e16d36cd73408ce4fd62604\n",
      "2020-01-09 08:09\n",
      "**Curious To Learn & Contribute To Scikit-learn At The 'Paris Scikit-learn Sprint Of The Decade' | Jan 28 - 31, 2020**  It was quite insightful listening to Reshama Shaikh's recent podcast: https://www.listennotes.com/podcasts/the-banana-data/bdn-15-finding-community-in-uK-yL2tf_S4/  It was quite helpful to broaden my horizon and perspective on open-source when I learned the challenges the organization faces in finding the sponsors and fundraisers for scikit-learn sprint events.  As an avid user of scikit-learn for my research projects in the recent past, Im excited about the potential of contributing and working alongside other attendees at the Paris scikit-learn sprint. Reshama's comments about funding & accessibility have made me even more eager to join the team.  Would you all mind letting me know if I could connect with the other participants remotely from Bengaluru, India?  Best, Sandeep Aswathnarayana\n",
      "\n",
      "1\n",
      "4676\n",
      "5c8bb176d73408ce4fbac89c\n",
      "2020-01-09 09:25\n",
      "@SandeepAswathnarayana, sprints are meant to allow people to meet in person, remote participation is not planned. There will be other sprints I'm sure you will be able to attend. In the meanwhile, thanks for your enthusiasm... if you check the contributors guidelines (https://scikit-learn.org/stable/developers/contributing.html) you could probably start helping already.\n",
      "\n",
      "1\n",
      "4677\n",
      "5e16d36cd73408ce4fd62604\n",
      "2020-01-09 13:53\n",
      "@cmarmo, Thanks for reverting to my query. I was aware of the already existing ways to contribute. I was only curious to see if I could be a part of the scikit-learn sprint which allows me to do 'Pair Programming' with individuals from diverse backgrounds attending the event.\n",
      "\n",
      "1\n",
      "4678\n",
      "5e16d36cd73408ce4fd62604\n",
      "2020-01-09 14:00\n",
      "@cmarmo, Any leads or inputs on future possibilities for remote participation are greatly appreciated. Thank you!\n",
      "\n",
      "1\n",
      "4679\n",
      "5c8bb176d73408ce4fbac89c\n",
      "2020-01-09 14:42\n",
      "@SandeepAswathnarayana  > Any leads or inputs on future possibilities for remote participation are greatly appreciated. Thank you!  noted: indeed, there is always room for improvemnts.\n",
      "\n",
      "1\n",
      "4680\n",
      "5e1765c2d73408ce4fd6356c\n",
      "2020-01-09 17:44\n",
      "hey folks. I have started my Data Science journey. In the process of completing the DataQuest online Data Science bootcamp . Is SciKit Learn & specifically Auto Sklearn a good set of tools to learn to help accelerate my journey and on the way to becoming an expert?\n",
      "\n",
      "1\n",
      "4681\n",
      "5e1765c2d73408ce4fd6356c\n",
      "2020-01-10 20:03\n",
      "@ScottHameed_twitter I know it's not a dev/Git related question, but appreciate the help\n",
      "\n",
      "1\n",
      "4682\n",
      "5e1b6aa3d73408ce4fd674ca\n",
      "2020-01-12 19:17\n",
      "@ScottHameed_twitter It's a necessary library used in machine learning. Learn it\n",
      "\n",
      "1\n",
      "4683\n",
      "59605bcbd73408ce4f6c2b60\n",
      "2020-01-13 00:12\n",
      "Hey, I opened the PR to add Neuraxle to the Related Projects page:  https://github.com/scikit-learn/scikit-learn/pull/16100  I've put it under the category for `Auto-ML` as it seems better suited here. I'm still developing the serialization plugin/extra libraries for TensorFlow and PyTorch as of right now, so those plugins could go into the `Model export for production` category later on (to allow saving / reloading / then continue training / partial_fit whenever after).\n",
      "I also corrected the thing about \"the creator of scikit-learn\" in my article :) srry again for the mistake haha\n",
      "\n",
      "2\n",
      "4684\n",
      "55a361b55e0d51bd787b3315\n",
      "2020-01-13 10:29\n",
      "I tried building scikit from source, its giving me import error for conftest.py\n",
      "``` pytest sklearn/metrics/_classification.py  ImportError while loading conftest '/media/sid21g/Dev/github-dev/scikit-learn/conftest.py'. conftest.py:15: in <module>     from sklearn import set_config sklearn/__init__.py:81: in <module>     from . import __check_build  # noqa: F401 sklearn/__check_build/__init__.py:46: in <module>     raise_build_error(e) sklearn/__check_build/__init__.py:41: in raise_build_error     %s\"\"\" % (e, local_dir, ''.join(dir_content).strip(), msg)) E   ImportError: No module named 'sklearn.__check_build._check_build' E   ___________________________________________________________________________ E   Contents of /media/sid21g/Dev/github-dev/scikit-learn/sklearn/__check_build: E   setup.py                  _check_build.c            _check_build.pyx E   __init__.py               __pycache__ E   ___________________________________________________________________________ E   It seems that scikit-learn has not been built correctly. E E   If you have installed scikit-learn from source, please do not forget E   to build the package before using it: run `python setup.py install` or E   `make` in the source directory. E E   If you have used an installer, please check that it is suited for your E   Python version, your operating system and your platform. ```\n",
      "\n",
      "2\n",
      "4685\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-01-14 15:27\n",
      "@sid21g  try maybe `make clean` and start over following the build guidelines\n",
      "\n",
      "1\n",
      "4686\n",
      "5acfdfffd73408ce4f95738d\n",
      "2020-01-16 16:33\n",
      "In order not to be verbose, I place a link to the question [stackexchange](https://ai.stackexchange.com/questions/17334/interpretation-of-feature-selection-based-on-the-model) no one answered me There, but there is a desire to understand. I apologize in advance for my poor English).\n",
      "\n",
      "1\n",
      "4687\n",
      "5dbd6437d73408ce4fcfbf0f\n",
      "2020-01-16 17:54\n",
      "@quant12345 as already replied to your post, have a read to this: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py  In RF, feature importance is affected by features able to overfit the model.\n",
      "\n",
      "1\n",
      "4688\n",
      "5acfdfffd73408ce4f95738d\n",
      "2020-01-16 22:06\n",
      "@gbroccolo Thanks!\n",
      "\n",
      "1\n",
      "4689\n",
      "5a7dae0ad73408ce4f8c6d2e\n",
      "2020-01-17 00:12\n",
      "Why does `GradientBoostingClassifier` use `DecisionTreeRegressor` instead of `DecisionTreeClassifier`?\n",
      "\n",
      "1\n",
      "4690\n",
      "55a361b55e0d51bd787b3315\n",
      "2020-01-17 06:34\n",
      "@NicolasHug Worked!\n",
      "\n",
      "1\n",
      "4691\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-01-17 12:19\n",
      "@jacobcvt12 because gradient boosting tries to predict gradients which are always continuous targets, even in the case of classification. With a log loss (as used in sklearn) these gradients are homogeneous to a log-odds ratio, and are then passed through a sigmoid function to become a probability between [0, 1]\n",
      "self plug: http://nicolas-hug.com/blog/around_gradient_boosting\n",
      "\n",
      "2\n",
      "4692\n",
      "56f3953b85d51f252aba800a\n",
      "2020-01-19 13:38\n",
      "Hello, I would be very grateful for help or hint. What I would like to do is to somehow find pattern in text. Lets say you have forum and people or posting on it. I would like to find pattern which would indicate me what are they talking about the most. Thank you for hint.\n",
      "\n",
      "1\n",
      "4693\n",
      "5a2c58c8d73408ce4f8294ba\n",
      "2020-01-20 07:04\n",
      "Try TF-IDF\n",
      "\n",
      "1\n",
      "4694\n",
      "5dce8ae9d73408ce4fd11e31\n",
      "2020-01-20 13:15\n",
      "Hello. How can I make the program better predict? When you enter the numbers 771, 322, 344, 632, 10, the program predicts 234168, but I need it to be 200000-210000. In linear regression, more than 1000 examples are already embedded.\n",
      "\n",
      "1\n",
      "4695\n",
      "5a7dae0ad73408ce4f8c6d2e\n",
      "2020-01-20 15:18\n",
      "Thanks @NicolasHug . I'm trying to figure out why sklearn's GradientBoostingClassifier gives different estimates from R's GBM. I had thought it might be the criterion for splitting, but maybe not. Any suggestions?\n",
      "\n",
      "1\n",
      "4696\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-01-20 19:25\n",
      "@jacobcvt12 I'm not familiar with R's gbm. The splitting criterion will definitely be a major factor. I'd suggest checking the parameters of each implementation and try to find equivalent settings. In a vanilla implementation of gradient boosting (ignoring the sub-estimator which is a tree in our case), the only parameters are the learning rate / shrinkage, the loss, and the number of iterations.\n",
      "\n",
      "1\n",
      "4697\n",
      "5e289bb3d73408ce4fd77f83\n",
      "2020-01-22 19:22\n",
      "Hi\n",
      "\n",
      "1\n",
      "4698\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-01-23 16:28\n",
      "@jacobcvt12 gbm supports categorical variables, I think\n",
      "\n",
      "1\n",
      "4699\n",
      "5e2d9704d73408ce4fd7ced4\n",
      "2020-01-26 13:45\n",
      "[![image.png](https://files.gitter.im/scikit-learn/scikit-learn/6t1w/thumb/image.png)](https://files.gitter.im/scikit-learn/scikit-learn/6t1w/image.png)\n",
      "\n",
      "1\n",
      "4700\n",
      "5e2d9704d73408ce4fd7ced4\n",
      "2020-01-26 13:47\n",
      "Hello, I'm new to using Pipelines and getting the above error 'Last step of Pipeline should implement fit or be the string 'passthrough'. I can't figure out how to overcome this step. Please assist if you can.\n",
      "\n",
      "1\n",
      "4701\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-01-26 13:53\n",
      "@wbadiah_gitlab you're passing a list, instead just pass the estimators directly as in the example: `make_pipeline(StandardScaler(), GaussianNB(priors=None), ...)` `\n",
      "i.e. remove the brackets. you can also remove the redundant parenthesis around the estimators\n",
      "\n",
      "2\n",
      "4702\n",
      "5e2d9704d73408ce4fd7ced4\n",
      "2020-01-26 14:01\n",
      "Thanks @NicolasHug It worked!\n",
      "\n",
      "1\n",
      "4703\n",
      "54f621ca15522ed4b3dcc151\n",
      "2020-01-29 09:09\n",
      "This PR by me is also with the Paris Sprint https://github.com/scikit-learn/scikit-learn/pull/16256\n",
      "\n",
      "1\n",
      "4704\n",
      "54e07d1515522ed4b3dc0852\n",
      "2020-01-29 09:12\n",
      "I'll start by completing https://github.com/scikit-learn/scikit-learn/pull/11296/files at Paris Sprint\n",
      "\n",
      "1\n",
      "4705\n",
      "593bb72fd73408ce4f663d44\n",
      "2020-01-29 09:23\n",
      "I am working on this Issue https://github.com/scikit-learn/scikit-learn/issues/12542 as part of the Paris Sprint\n",
      "\n",
      "1\n",
      "4706\n",
      "5d43be47d73408ce4fc78bde\n",
      "2020-01-29 09:36\n",
      "I'm working on the issue scikit-learn/scikit-learn#12730 at the Paris Sprint\n",
      "\n",
      "1\n",
      "4707\n",
      "54f621ca15522ed4b3dcc151\n",
      "2020-01-29 09:59\n",
      "I'm working on GenericUnivariateSelect of https://github.com/scikit-learn/scikit-learn/issues/11000\n",
      "\n",
      "1\n",
      "4708\n",
      "5e314a9cd73408ce4fd81532\n",
      "2020-01-29 10:03\n",
      "I'm working on random_state descriptions for _weight_boosting of  scikit-learn/scikit-learn#16264\n",
      "\n",
      "1\n",
      "4709\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2020-01-29 10:51\n",
      "Please also comment in the issues to indicate the topic you are working on  @DatenBiene @ksslng @martinagvilas @maskani-moh so that other people don't work on the same things.\n",
      "\n",
      "1\n",
      "4710\n",
      "5e314ae8d73408ce4fd81550\n",
      "2020-01-29 15:29\n",
      "I'm working on a documentation for _coordinate_descent scikit-learn/scikit-learn#16285 related to the issue scikit-learn/scikit-learn#15761 at the Paris Sprint\n",
      "\n",
      "1\n",
      "4711\n",
      "5911fe0fd73408ce4f5e38ec\n",
      "2020-02-07 00:44\n",
      "Is there like a standard file format for storing the output of roc_curve? Maybe something that is easy to read, analyze, visualize, etc?\n",
      "\n",
      "1\n",
      "4712\n",
      "5d8b71ded73408ce4fcc0322\n",
      "2020-02-08 20:09\n",
      "Can you help me please !!\n",
      "\n",
      "1\n",
      "4713\n",
      "5d8b71ded73408ce4fcc0322\n",
      "2020-02-08 20:10\n",
      "How can i reach to each tree in random forest algorithm\n",
      "\n",
      "1\n",
      "4714\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-02-08 20:10\n",
      "the estimators_ attribute, check the docs\n",
      "\n",
      "1\n",
      "4715\n",
      "5d8b71ded73408ce4fcc0322\n",
      "2020-02-08 20:11\n",
      "I want to extract the rules for every tree i get the following code put there is an error in it\n",
      "model=RandomForestClassifier(n_estamator=10)\n",
      "model.fit(iris.data,iris.target)\n",
      "estamator =model.n_estamator_[5]\n",
      "\n",
      "4\n",
      "4716\n",
      "5d8b71ded73408ce4fcc0322\n",
      "2020-02-08 20:18\n",
      "The error is ''RandomForestClassifier object has no attribute 'n_estimators_'\n",
      "how can i write this line please\n",
      "estimator=model.n_estimators_[5]\n",
      "\n",
      "3\n",
      "4717\n",
      "5d7aee71d73408ce4fcacd7e\n",
      "2020-02-09 06:59\n",
      "Hello. I'm new to this community and I have got no prerequisites to get started, could someone please help me getting started. Thank you! :)\n",
      "\n",
      "1\n",
      "4718\n",
      "5d837603d73408ce4fcb7845\n",
      "2020-02-12 09:48\n",
      "Hey guys! New here. Got a question: why oob_score computing isn't done in parallel like the predict method from RandomForestRegressor?\n",
      "\n",
      "1\n",
      "4719\n",
      "5e352127d73408ce4fd863d1\n",
      "2020-02-12 10:55\n",
      "Hi there,  I'm new here... quick question, I'm recently using MLFLOW to manage model lifecycle... I've used it with some scikit-learn models and TensorFlow. Any opinion about mlflow framework? just wanted to know other experiences\n",
      "\n",
      "1\n",
      "4720\n",
      "5634e8e116b6c7089cb8fa99\n",
      "2020-02-20 00:47\n",
      "can GaussianProcessRegressor fail in subtle and obvious ways? I might have uncovered a bug.\n",
      "\n",
      "1\n",
      "4721\n",
      "5634e8e116b6c7089cb8fa99\n",
      "2020-02-20 00:53\n",
      "demonstration here: https://github.com/tanimislam/sharing-github/blob/master/DEMO%20GPR%20MISUNDERSTANDING.ipynb\n",
      "never mind, found the subtlety in scikit-learns GPR and possibly other regressors <unconvertable> scaling the variables made the problem disappear.\n",
      "\n",
      "2\n",
      "4722\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-02-20 20:07\n",
      "@tanimislam hi any body Please, I am beginner ,how can i update 3.6 to 3.8 python version on window 10 , 64 bit ? _\n",
      "\n",
      "1\n",
      "4723\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-02-21 15:55\n",
      "Hi any body , I am beginner , can body can give me some basic assignments for python , just for beginners to learn Please? I am using PyCharm . _\n",
      "\n",
      "1\n",
      "4724\n",
      "57e9f34a40f3a6eec06789ad\n",
      "2020-02-22 06:37\n",
      "@anjumuaf123_twitter  sir you can join cs50 or EDX Mit course on INTRODUCTION TO PROGRAMMING USING PYTHON\n",
      "\n",
      "1\n",
      "4725\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-02-22 14:28\n",
      "@jhamlal Dear Sir, how to join this , is it free?  Please send me link  @jhamlal  @jhamlal  @jhamlal\n",
      "\n",
      "1\n",
      "4726\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-02-22 14:36\n",
      "@jhamlal Sir, when will you online  acc. to your indian time ?? I need to discuss few things\n",
      "please\n",
      "@jhamlal I can explain  or tell you in details\n",
      "\n",
      "3\n",
      "4727\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-02-22 14:46\n",
      "@anshu_bansal280_twitter  I am also new like for learning python\n",
      "\n",
      "1\n",
      "4728\n",
      "57e9f34a40f3a6eec06789ad\n",
      "2020-02-25 03:30\n",
      "> [![perceptron.png](https://files.gitter.im/scikit-learn/scikit-learn/Tyle/thumb/perceptron.png)](https://files.gitter.im/scikit-learn/scikit-learn/Tyle/perceptron.png)  can anyone explain line 6-7 update work , if example that would be great\n",
      "\n",
      "1\n",
      "4729\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-02-25 03:51\n",
      "@jhamlal  hi sir\n",
      "google colab mutb??\n",
      "\n",
      "2\n",
      "4730\n",
      "5a32d2b0d73408ce4f835d1a\n",
      "2020-02-25 14:37\n",
      "Hello, why does `cross_val_predict` \"eats\" prints from `Pipeline`? When i set verbose on Pipeline i dont see any verbosity and when i have debug step in the pipeline that prints shape of the data it also doesnt print anything but when i execute the pipeline manualy it prints all the thing above\n",
      "\n",
      "1\n",
      "4731\n",
      "5e2072c5d73408ce4fd6daef\n",
      "2020-02-25 14:56\n",
      "Hello! new, hopefully soon to be contributor here, I am working with a team of fellow data science majors who have recently submitted a pull request for documentation. Would love some guidance or suggestions as we haven't heard anything back yet and are working on more as we speak! Thanks in advance!\n",
      "thank you so much, understandably so! we are PR #16417 and really just added a couple lines of documentation\n",
      "@adrinjalali thanks again for responding, even if someone has a minute to let us know if we are on the right track that would be super helpful!\n",
      "\n",
      "3\n",
      "4732\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-02-25 14:57\n",
      "nice to have you on board @hansenallison . Review time is our main bottleneck, and therefore it may take some time before a reviewer can get to check your PR. What's your PR number?\n",
      "\n",
      "1\n",
      "4733\n",
      "5a32d2b0d73408ce4f835d1a\n",
      "2020-02-25 15:15\n",
      "I am running the same version of sklearn but once the pipeline outputs the verbose when in cross_val_predict and the other time it doesnt, could it be something conserning jupyter?\n",
      "\n",
      "1\n",
      "4734\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-02-25 15:47\n",
      "@EnyMan probably it'd be easier to track the problem if you open an issue with a reproducible bit of code\n",
      "\n",
      "1\n",
      "4735\n",
      "5e540a2fd73408ce4fda9945\n",
      "2020-03-02 08:38\n",
      "Hey everyone I recently discovered the partial_fit method for certain regressors such as SGDRegressor, I wondered if I can partial_fit the model with a decreasing learning rate, save it, then reload it and further train with partial_fit but with a different learning rate schedule as before? Thank you in advance\n",
      "\n",
      "1\n",
      "4736\n",
      "5d1b262ed73408ce4fc50b53\n",
      "2020-03-03 11:05\n",
      "Can we use an unsupervised algorithm to perform sentiment analysis? If no , how to extract dataset on the customer support conversation transcripts ? as I dont' have any dataset I looking for unsupervised model Please let me know how to proceed ?\n",
      "\n",
      "1\n",
      "4737\n",
      "5dbd6437d73408ce4fcfbf0f\n",
      "2020-03-03 12:55\n",
      "Hi @vishu_rj_twitter it depends from what you need...which kind of sentiment analysis would you like to perform? Through unsupervised models you could in theory clusterise the conversations, and for this I'd suggest to have a look to bag of words techniques, word2vec, doc2vec, etc.  But I think you need to classify the conversation following some criteria that exploits the sentiment, right? In this case I don't see any alternative to a supervised classification. And like any supervised problem, find a godd training dataset it's not trivial at all.  But anyway, I'd be open to any further suggestion that could come from this chat. Hope it helped anyway.\n",
      "\n",
      "1\n",
      "4738\n",
      "5d1b262ed73408ce4fc50b53\n",
      "2020-03-03 13:02\n",
      "@gbroccolo  I am looking for happy / unhappy ( or positive /negative) criteria sentiments. I have searched for the  customer support conversation transcripts , but I did not got any datasets . I have searched in kaggle and google . let me know if any other repository for such datasets\n",
      "\n",
      "1\n",
      "4739\n",
      "5dbd6437d73408ce4fcfbf0f\n",
      "2020-03-03 14:25\n",
      "@vishu_rj_twitter have a look on NLTK package: there should be some basic implementation based on word's semantics (i.e. it extracts single words and check the presence of generally negative terms like \"isn't\", \"aren't\", etc.).\n",
      "but, again, it strongly depends from what you define \"positive\" or \"negative\" in your sentiment analysis. In most cases, you need supervised approaches, and generally you need your own labeled datasets. Machine learning stops to be nice at the moment you realise  you need to label your datasets by your own :)\n",
      "\n",
      "2\n",
      "4740\n",
      "5e614c10d73408ce4fdbbb92\n",
      "2020-03-05 19:01\n",
      "Hello, i'm ML student and i'm loving scikit-learn. Altough, i'm struggling using AdaboostClassifier. I'd like to set MLPClassifier as base_estimator but it says \"MLPClassifier does not implement sample_weight function\". Is there anyway i can customize this to accept this learner and others?\n",
      "I also tried to create a VotingClassifier of MLPClassifiers, and then using AdaboostClassifier with VotingClassifier as base_estimator, but no success\n",
      "oh\n",
      "@NicolasHug do you recommend another option to achieve this? I really need an Adaboost of MLP classifiers, even if it's not scikit-learn\n",
      "\n",
      "4\n",
      "4741\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-03-05 22:17\n",
      "@vendrafilm you can't use AdaBoost with MLP because MLP does not support sample weights and AdaBoost requires the the `base_estimator` parameter to support sample weights (it's in the docstring). AdaBoost works by re-weighting some of the samples, so support for SW is mandatory. There's no way around it unfortunately.\n",
      "\n",
      "1\n",
      "4742\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-03-06 19:25\n",
      "Hi all, I am working on revamping the [Keras](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras) [Scikit-Learn wrappers](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/wrappers/scikit_learn.py). This essentially requires implementing the entire Scikit-Learn API supporting multi-outputs, etc. I think I got everything working just by reading the API reference, but I would like to see if any of the Scikit-Learn developers are willing to take a look at the implementation and give me any pointers on things that might be issues. For example, it is not clear to me if ensembles of multi-output estimators are supported, and other edge cases of that nature. The PR is [here](https://github.com/tensorflow/tensorflow/pull/37201) if anyone wants to take a look. Thank you!\n",
      "\n",
      "1\n",
      "4743\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-03-07 01:39\n",
      "@vendrafilm you should actually check out that PR I just linked! It should be easy to make an MLP in Keras that supports sample_weight and then wrap it to be Scikit-Learn compatible\n",
      "\n",
      "1\n",
      "4744\n",
      "5e633ca9d73408ce4fdbe6d6\n",
      "2020-03-07 06:41\n",
      "Hi, I've recently cloned the scikit-learn repository. But I'm having difficulty debugging the code in my Pycharm editor. I keep running into the infamous relative import error. Can someone pass me a reference that guides me on how to do it?  Also, please take a look at my debug configurations and let me know if something's wrong: Working directory: S:\\Scikit-learn\\scikit-learn\\sklearn (I already marked sklearn as the source root) Python interpreter is the virtual environment I created for this. Running \"pip show scikit-learn\" in this env correctly shows 'Version: 0.23.dev0'. Script path: S:\\Scikit-learn\\scikit-learn\\sklearn\\ensemble\\_iforest.py I even tried using Module instead of Script for debugging: \"sklearn.ensemble._forest\".  This just throws another runtime warning and fails. Pycharm is using \"pydev debugger (build 193.6494.30)\".  Am I wrong to try and debug this file? How can I debug the module as a whole? Some stack overflow discussions answer this but I couldn't figure it out for sklearn. Any help here would be greatly appreciated. Thanks in advance!\n",
      "\n",
      "1\n",
      "4745\n",
      "5e614c10d73408ce4fdbbb92\n",
      "2020-03-07 21:58\n",
      "@adriangb thank you, i will take a look. In fact, i need not only Adaboost to support MLP but also support different base estimators in one AdaboostClassifier.. not quite sure if it's possible\n",
      "\n",
      "1\n",
      "4746\n",
      "5e6b96b3d73408ce4fdca2f0\n",
      "2020-03-13 14:31\n",
      "Hello, I wanted to write a usage-question on github, but the issue tracker brought me here. I have a multilabel problem which i want to solve with a RandomForestClassifier. But I have so much data that I use a dataloader and a random forest with warmstart: https://stats.stackexchange.com/questions/327335/batch-learning-w-random-forest-sklearn . But I get an error after I fitted the model multiple times and try to predict testdata with multilabels. I broke it down to a minimal example (which fails too), but when I fit data, which i created with make_multilabel_classification, in one go the prediction of the random forest works. Any ideas?\n",
      "\n",
      "1\n",
      "4747\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2020-03-15 15:25\n",
      "Hi, I want to write a new scikit-learn compatible estimator where the behaviour of fit/predict depends on a \"strategy\" hyper-parameter (similar to DummyClassifier but the behaviour is more complicated). Is there any other way/design pattern to implement this apart from making case distinctions within fit/predict?\n",
      "\n",
      "1\n",
      "4748\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-03-15 19:41\n",
      "@mloning if the different \"strategies\" share common code, the conveniently named strategy design pattern might be worth using\n",
      "\n",
      "1\n",
      "4749\n",
      "59f3eda9d73408ce4f7c14ba\n",
      "2020-03-16 08:52\n",
      "Hi,In the Scikit learn Website: https://scikit-learn.org/stable/testimonials/testimonials.html#who-is-using-scikit-learn. I know many other companies use scikit-learn package but do not see in the list, what is the process to list the testimonials in the website.\n",
      "\n",
      "1\n",
      "4750\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2020-03-16 13:00\n",
      "@NicolasHug well I guess scikit-learn's Estimator class itself follows the strategy design pattern, but no, the fit/predict behaviour does not share code. I could encapsulate them into their respective classes (`StrategyARegressor`, `StrategyBRegressor`, etc) but then I could tune over the strategy parameter ... any other idea or example in scikit-learn that comes to mind? Thanks for the help!\n",
      "\n",
      "1\n",
      "4751\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-03-16 13:17\n",
      "@mloning you can run parameter search over multiple estimators by wrapping them in a Pipeline object (slightly hacky, but it works: https://stackoverflow.com/questions/38555650/try-multiple-estimator-in-one-grid-search). Otherwise,  you can have both StrategyARegressor and StrategyBRegressor, and use these as instances in a MyRegressorClass.\n",
      "\n",
      "1\n",
      "4752\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2020-03-16 14:35\n",
      "@NicolasHug \"Otherwise, you can have both StrategyARegressor and StrategyBRegressor, and use these as instances in a MyRegressorClass.\" so how does this last suggestion work? Like a composition? So I'd have something like `if strategey=\"a\": self.estimator = StrategyARegressor()` and then call `fit` by calling `self.estimator.fit()`\n",
      "\n",
      "2\n",
      "4753\n",
      "57b364d540f3a6eec05fce2b\n",
      "2020-03-16 17:45\n",
      "Hello all, i'd be grateful if anyone could take some time to see my question:  https://datascience.stackexchange.com/questions/69788/valueerror-the-estimator-should-be-a-classifier\n",
      "Briefly, i need ELMClassifier from sklearn-extensions running with VotingClassifier and AdaboostClassifier. After some customizations, i am able to pass the ELM directly to the Adaboost, but i also need to pass a VotingClassifier (of ELMs) to the Adaboost as well. And that's where i'm struggling\n",
      "\n",
      "2\n",
      "4754\n",
      "5df656e9d73408ce4fd42524\n",
      "2020-03-23 04:27\n",
      "Hi everyone , Im a begginer and i want to contribute to scikit learn. I have worked on few Deep Learning projects in pytorch . I cant find a good issue to work as someone is already assigned to it or somebody has already fixed it . Can someone guide me to contributing in this repo\n",
      "\n",
      "1\n",
      "4755\n",
      "56bb7a56e610378809c0cb2c\n",
      "2020-03-23 08:43\n",
      "`jonpsy` I can see the chat's packed already,anyway here's my two cents  I was going through the implementation of RandomFourierFeatures implemented in sklearn, under kernel_approximation.py. I noticed a part where you put  ``` class RBFSampler(..):     . . .. def transform(..): .. ..  np.cos(projection,projection) ..  return projection ``` Now np.cos doesn't take two arguments, so what's happening? I tried replicating this in my console and I got error, also why is no variable storing that information ?. I've created the corresponding [issue](https://github.com/scikit-learn/scikit-learn/issues/16746).   Thanks for the help :)\n",
      "`jonpsy` Nvm Issue solved thanks !!\n",
      "`jonpsy`  * Nvm Issue solved thanks Nicolas Hug (Gitter) .\n",
      "\n",
      "3\n",
      "4756\n",
      "5e7988c7d73408ce4fddc8ec\n",
      "2020-03-24 04:16\n",
      "greetings!\n",
      "\n",
      "1\n",
      "4757\n",
      "5e7988c7d73408ce4fddc8ec\n",
      "2020-03-24 04:21\n",
      "I have an assignment in which I am supposed to implement a neural network from scratch. My NN outputs a number between 0 and 1 to classify the input to either +1 or -1. so if the output if less than 0.5 then it belongs to -1, else +1. Unfortunately, my code only returns either +1 or -1, depending on how I initialize the weight matrix. I THINK I'm facing vanishing gradient problem. But I need an expert's opinion to make sure. anyone can please help me??\n",
      "link to my code: https://www.kaggle.com/mowhamadrexa/kernel2ee3f07315\n",
      "\n",
      "2\n",
      "4758\n",
      "5decf929d73408ce4fd36f72\n",
      "2020-03-28 12:27\n",
      "I made some machine learning models using Python scikitlearn library and I found some strange situation for me regarding real importance of some variables (features) to ML model. I found that variable which has smaller Pearson coefficient has higher importance on ML model (when exclude variable from model using backward elimination principle) than variables which has higher Pearson.  Below I send the real results of three models where first model includes all three variables and another two models excludes some variables (- means variable is excludes). Iuse Random Forest method.  Model Name     MAE  ModelV1V2V3 0.92 ModelV1V2- 3.86 ModelV1-V3 2.96  PearsonV1=0.99, PearsonV1=0.82, PearsonV3=0.02  When I exclude variables which has no importance based on Pearson (0.02) I got model with better performance comparing to model which includes another variable (V2) whic has far higher Pearson (0.82). Please, help me to explain this situation.  Please, answer me as soon as possible.  Thank You in advance.  Dusko Tovilovic\n",
      "\n",
      "1\n",
      "4759\n",
      "5e8208c9d73408ce4fde8bc4\n",
      "2020-03-30 14:59\n",
      "Is it possible to create an adjacency matrix  with LDA?\n",
      "\n",
      "1\n",
      "4760\n",
      "5799a0a940f3a6eec05cd618\n",
      "2020-04-03 11:32\n",
      "Hosting TFUG Mysore first meetup : TensorFlow JS - Show and Tell by Jason Mayes  - Senior Developer Advocate at Google. 8 presenters showing what they have #MadeWithTFJS with epic demos lined up + more. RSVP now  http://meetu.ps/e/HTwBV/jYwqF/a\n",
      "\n",
      "1\n",
      "4761\n",
      "54db2b1015522ed4b3dbe1bc\n",
      "2020-04-08 01:12\n",
      "greetings\n",
      "\n",
      "1\n",
      "4762\n",
      "5e925802d73408ce4fe01b38\n",
      "2020-04-11 23:52\n",
      "Hello, I was using sklearn.svm.SVC and it took five hours.  I did get good results, but I am wondering if scikit-learn has an ETA (Estimated time to Arrival) setting, which shows the estimated time for the program to run.\n",
      "\n",
      "1\n",
      "4763\n",
      "5668c71116b6c7089cbe1ea3\n",
      "2020-04-12 08:20\n",
      "@luishrd  There is a verbose setting and a max_iter.\n",
      "\n",
      "1\n",
      "4764\n",
      "5e860fd3d73408ce4fdefde9\n",
      "2020-04-12 22:18\n",
      "@JohnPaulMSU15_twitter There are 2 interesting things I found after a bit of research . There is a parameter you can set for your SVM called `verbose` .  As such you can write: ``` cf = svc.SVM(verbose=2) cf.fit(X,Y) ``` Now , according to [this](https://stackoverflow.com/questions/22443041/predicting-how-long-an-scikit-learn-classification-will-take-to-run) stackoverflow answer, setting this parameter will only output the number of iterations required for optimization as they finish, so they may only give you a hint.  The second and more interesting thing I found is this library : [scitime](https://github.com/scitime/scitime)  It does exactly what you need, trying to provide an estimate for your system. More info at the docs they provide :)\n",
      "\n",
      "1\n",
      "4765\n",
      "5e9b457bd73408ce4fe0e90c\n",
      "2020-04-18 18:28\n",
      "I need to write a custom random_selection(for random selection of feature i.e \"max_feature\" and subset of train data i.e. \"subsample\") module in scikit-learn to be used with sklearn.ensemble.RandomForestClassifier and GradientBoostingClassifier. Can someone point to some example/documentation/discussion etc.?\n",
      "\n",
      "1\n",
      "4766\n",
      "5dc8e3cfd73408ce4fd09e96\n",
      "2020-04-22 01:01\n",
      "What is the best score for the Iris Dataset?\n",
      "\n",
      "1\n",
      "4767\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-04-22 07:48\n",
      "@Dgomzi you probably need to look at the source code for that.\n",
      "\n",
      "1\n",
      "4768\n",
      "537bc9bd048862e761fa2239\n",
      "2020-04-22 15:42\n",
      "Hey! Is there a way of computing the dimensionality of a given dataset in scikit-learn? I have some high-dimensional data, which I believe reside on a lower-dimension manifold, the dimension of which is unknown. What I am looking for is some way of measuring the number of dimension of the manifold.\n",
      "\n",
      "1\n",
      "4769\n",
      "5e540a2fd73408ce4fda9945\n",
      "2020-04-22 16:21\n",
      "Hey everyone! I wanted to know why sci-kit learn provides the quantile loss for GBRT but not for the SGDRegressor? I am aware that the quantile loss is non-smooth, however GBRT seems to calculate the subgradient at zero for that reason. Wouldn't this be applicable for SGD?\n",
      "\n",
      "1\n",
      "4770\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-04-22 18:23\n",
      "@arturgvieira_twitter  hi sir\n",
      "\n",
      "1\n",
      "4771\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-04-23 12:16\n",
      "@cphyc the PCA estimator has a `mle` option to automatically choose the number of components to project on. That's not exactly what you want but I think it's related\n",
      "\n",
      "1\n",
      "4772\n",
      "581f1608d73408ce4f33fcaf\n",
      "2020-04-24 10:07\n",
      "hello, would adding the `return_std` argument to the `predict` method of the lightgbm regressor class be enough to plug it into scikit optimize? Also what kind of stdev is it referencing to by `std(Y | X)` ? I tried to look how it is added to the estimators provided by the skopt but it appears it is done in different ways for all of them so it is not clear to me..\n",
      "\n",
      "1\n",
      "4773\n",
      "5ad33bbad73408ce4f95b5df\n",
      "2020-04-25 13:30\n",
      "Hi Everyone. I would like to contribute to this open-source project. May I know the process, please.\n",
      "\n",
      "1\n",
      "4774\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-04-25 13:30\n",
      "@Akram1234 hi sir\n",
      "\n",
      "1\n",
      "4775\n",
      "5ad33bbad73408ce4f95b5df\n",
      "2020-04-25 13:32\n",
      "Hello @anjumuaf123_twitter  sir\n",
      "\n",
      "1\n",
      "4776\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-04-25 13:33\n",
      "I  am  beginner   for python\n",
      "I want to  go for time series analysis\n",
      "  @Akram1234  I have my own data\n",
      "@Akram1234 Would you like to help me?\n",
      "Please\n",
      "\n",
      "5\n",
      "4777\n",
      "5ad33bbad73408ce4f95b5df\n",
      "2020-04-25 17:12\n",
      "sure  @anjumuaf123_twitter\n",
      "\n",
      "1\n",
      "4778\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2020-04-27 08:34\n",
      "Hi all, what's the reason why one shouldn't set other attributes during construction in sklearn estimators apart from those in the `__init__` signature (see estimator check: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/utils/estimator_checks.py#L2390)? If I understand it correctly, `clone` calls the constructor and so all other attributes will also be re-set.\n",
      "\n",
      "1\n",
      "4779\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-04-27 08:37\n",
      "`clone` doesn't call the constructor with the expected parameters. The parameters are set in `set_params`. That means if setting some other attributes depends on some given attribute to `__init__`, and that logic is not duplicated in `set_params`, the constructor would be in an invalid state. Putting those in `fit` makes them all consistent.\n",
      "But there are ways where you can handle it with setting other parameters in `__init__`, and the estimator not going into an invalid state, if you know what you're doing, then you can ignore that test.\n",
      "\n",
      "2\n",
      "4780\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2020-04-28 09:54\n",
      "Thanks for the quick reply, yes, so, as long as there is no logic or interaction of attributes in `__init__`, I can initialise other attributes in `__init__`. We are developing a toolbox that extends sklearn to time series and currently have an `is_fitted` attribute in our base estimator which is initialised to `False`, any reason why this may be a bad idea?\n",
      "\n",
      "1\n",
      "4781\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-04-28 10:04\n",
      "That's just a bad idea cause we have a `check_is_fitted` utility function which should be used ;)\n",
      "\n",
      "1\n",
      "4782\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2020-04-28 11:10\n",
      "I know, I find having an `is_fitted` state cleaner than following the trailing-underscore convention :)  thanks for the help\n",
      "\n",
      "1\n",
      "4783\n",
      "567363d316b6c7089cbf1f13\n",
      "2020-04-29 22:27\n",
      "Hey all, currently doing some dev loop on the pyx in the neighbors package\n",
      "any way to get a faster dev loop than `pip install --editable .`\n",
      "on ubuntu 16.04\n",
      "\n",
      "3\n",
      "4784\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-04-30 07:05\n",
      "You probably wanna pass `-no-build-isolation` as well @mhamilton723\n",
      "\n",
      "1\n",
      "4785\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-04-30 11:56\n",
      "@mhamilton723  `make inplace` will only recompile the files that you changed Also if you do lots of back and forth, https://ccache.dev/ might help\n",
      "\n",
      "1\n",
      "4786\n",
      "5af2b904d73408ce4f98a88d\n",
      "2020-05-01 13:33\n",
      "Hi everyone!\n",
      "\n",
      "1\n",
      "4787\n",
      "5af2b904d73408ce4f98a88d\n",
      "2020-05-01 13:34\n",
      "I've got a few questions considering lin regression. These are mostly focussed on using scikit learn, but I guess the questions are also applicable to all kinds of ML tools :)\n",
      "[More general formulation of the questions: standardization, onehotencoding/dummy coding with mixed var. types for poly. reg](https://stats.stackexchange.com/questions/463894/feature-standardization-for-polynomial-regression-with-categorical-data)\n",
      "[and here the python/sklearn specific formulation of the problem with a focus on onehotencoding](https://stats.stackexchange.com/questions/463690/multiple-regression-with-mixed-continuous-categorical-variables-dummy-coding-s)\n",
      "any help is welcome and appreciated. :)\n",
      "Since I didn't find any information on this topic anywhere in the user guide, I could also help with improving the user guide once I have more information :)\n",
      "\n",
      "5\n",
      "4788\n",
      "567363d316b6c7089cbf1f13\n",
      "2020-05-01 17:26\n",
      "Thank you @adrinjalali  and @NicolasHug :)\n",
      "\n",
      "1\n",
      "4789\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-05-02 06:29\n",
      "hi guys, I wrote some codes  to get Latitude  and longitude of some districts  but error , Please help me\n",
      "\n",
      "1\n",
      "4790\n",
      "5d1b67bcd73408ce4fc51274\n",
      "2020-05-03 07:05\n",
      "Hi everyone I have a quick question about the math behind the last step of Linear Discriminant Analysis (LDA) for dimensionality reduction. So I understand for the algorithm to calculate for k projection vector(s) you need to determine the eigenvector(s) that corresponds to the top k eigenvalue(s). But does anyone know what you do with those eigenvectors after you have calculated them? My guess is to multiply all of the eigenvectors (projection vectors) together and then multiply that with each point, x, in the original dataset to produce a new point y. Does this seem right? Thank you in advance\n",
      "\n",
      "1\n",
      "4791\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-05-03 07:05\n",
      "@nadimk1 hi sir\n",
      "@nadimk1 I am new  in python . I want to ask that Lubuntu and window 10  can be run in  one machine\n",
      "?\n",
      "\n",
      "3\n",
      "4792\n",
      "5d1b67bcd73408ce4fc51274\n",
      "2020-05-03 17:32\n",
      "@anjumuaf123_twitter don't quote me on this, but I think you could probably run it in a virtual machine on your computer using virtualbox\n",
      "\n",
      "1\n",
      "4793\n",
      "5d1b67bcd73408ce4fc51274\n",
      "2020-05-04 17:50\n",
      "okay super basic question, but does anyone know how to determine if a KNN model (Sklearn-KNN) is overfitting the data\n",
      "\n",
      "1\n",
      "4794\n",
      "5d1b67bcd73408ce4fc51274\n",
      "2020-05-04 17:51\n",
      "I am trying to run it on the MNIST digits dataset, but the accuracy seems way too high\n",
      "\n",
      "1\n",
      "4795\n",
      "5446d339db8155e6700cd529\n",
      "2020-05-06 19:40\n",
      "Hi, anyone with Cython knowledge who would have an idea what is going on in here: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L817  To me, it seems like a node is being subtracted by an array of nodes, which is given as a pointer index. Also, it looks like the index arithmetic is done on the smaller value, causing supposedly a negative index.\n",
      "Is it just the index being subtracted here, or is there something else happening here?\n",
      "\n",
      "2\n",
      "4796\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-05-07 13:36\n",
      "@toldjuuso from what I understand `self.nodes` is both the array of nodes *and* the address of the root node (In C and thus in Cython, an array is just a constant pointer on the first value of that array) so `node - self.nodes` will be positive and it correspond to the offset between the final node `node` and the root i.e. it corresponds to its index in the `self.nodes` array\n",
      "\n",
      "1\n",
      "4797\n",
      "5446d339db8155e6700cd529\n",
      "2020-05-07 17:47\n",
      "@NicolasHug Ooh now I see, makes much more sense. I don't program in C, so this has been quite cryptic. I appreciate the help, really!\n",
      "\n",
      "1\n",
      "4798\n",
      "5eb542add73408ce4fe31aad\n",
      "2020-05-08 11:31\n",
      "Hi, I'm trying to use Pipeline for combining feature extraction and clustering. I see that there are ways in sklearn for transforming a single column into one or more columns. What I would like to do is specify a new feature as a function of a number of features, inside a pipeline.  I looked at ColumnTransformer, FeatureUnion, and Pipeline, but neither of these enable what I am looking for. Do I have to create these new *features* before I put them in the pipeline? Or am I missing something int he documentation?\n",
      "\n",
      "1\n",
      "4799\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-05-08 16:02\n",
      "@Bri9k_gitlab maybe the FunctionTransformer is what you're looking for?\n",
      "\n",
      "1\n",
      "4800\n",
      "5eb542add73408ce4fe31aad\n",
      "2020-05-09 11:10\n",
      "@NicolasHug Thanks! I think that will work.\n",
      "\n",
      "1\n",
      "4801\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-05-19 19:08\n",
      "does someone remember what the \"s\" in pandas stands for?\n",
      "\n",
      "1\n",
      "4802\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-05-19 19:33\n",
      "@amueller  I would have guessed it comes from \"Shit, `panda` is already taken\" but according to PyPI `pandas` predates `panda` so IDK\n",
      "\n",
      "2\n",
      "4803\n",
      "5ec49d18d73408ce4fe44c2c\n",
      "2020-05-20 03:06\n",
      "Hi, I am trying to create anisotropic exponential and anisotropic gaussian kernel function for Sklearns' Gaussian Process Regressor. Any idea how I can do this with sklearns' inbuilt kernels? See attached image. ![alt](https://i.stack.imgur.com/DRKAE.png)\n",
      "I am new to this and I have been looking for help since a long time. Any help will very appreciated. Thanks.\n",
      "\n",
      "2\n",
      "4804\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2020-05-22 13:22\n",
      "At the bottom of the scikit-learn website it says \" <unconvertable> 2007 - 2019, scikit-learn developers (BSD License)\". I assume it should be 2020 instead of 2019, right? If yes, I'm happy to make an issue and fix it.\n",
      "\n",
      "1\n",
      "4805\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-05-22 14:12\n",
      "Thanks for noticing @marenwestermann , this is already fixed in the dev version https://scikit-learn.org/dev/user_guide.html\n",
      "\n",
      "1\n",
      "4806\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2020-05-22 14:45\n",
      "Thanks for pointing it out! I'll have a look there first next time.\n",
      "\n",
      "1\n",
      "4807\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2020-05-22 16:53\n",
      "Hi, do you have any useful resources for thinking about legal issues in open source (e.g. governance, sponsoring, contributor license agreements)? Would be much appreciated, thanks! :)\n",
      "\n",
      "1\n",
      "4808\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-05-22 17:44\n",
      "@mloning I think this book https://producingoss.com/ covers some of these topics\n",
      "\n",
      "1\n",
      "4809\n",
      "5ec8df4dd73408ce4fe4a46b\n",
      "2020-05-23 08:40\n",
      "want a project partner for kaggleing\n",
      "\n",
      "1\n",
      "4810\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2020-05-24 13:04\n",
      "Thanks @NicolasHug\n",
      "\n",
      "1\n",
      "4811\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-05-26 14:19\n",
      "FYI in case anyone is interested here's a quick intro video to contributing to sklearn: https://www.youtube.com/watch?v=5OL8XoMMOfA&feature=youtu.be I probably forgot a couple of things, this one is made with the upcoming data umbrella sprint in mind\n",
      "\n",
      "1\n",
      "4812\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-05-28 08:45\n",
      "Hey guys, I'm trying to use Agglomerative Clustering. When applying the distance threshold, does it have a specific range? E.g. 0-1? or 0-10? or 0-1000? When I tried 1000, it gave me all the same cluster. When trying 500 it gave me clusters 0 1 2 3. I wanted to do something like: \"All vectors above 70% similarity should merge\" but I'm not sure how to implement that with distance threshold.\n",
      "\n",
      "1\n",
      "4813\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-05-28 08:53\n",
      "what does 70% similarity mean?\n",
      "the range is applied on the output of your distance metric\n",
      "if you want .7 to mean 70% similarity, then you should give a metric which results in 0.7 when they're 70% similar, with whatever definition you have\n",
      "but the distances are not bounded, and therefore usually you need to sample from your distances, look at the distribution, and decide on what the threshold should be\n",
      "\n",
      "4\n",
      "4814\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-05-28 08:56\n",
      "Yes I do mean something like: if I tried to do cosine distance with 2 vectors and then subtract that by 1 and get the absolute value, I get the similarity.\n",
      "Sorry I'm not familiar with what distance metric refers to\n",
      "Ah okay I'm currently trying to test with cosine, will report back. Thank you!\n",
      "The confusing part for me is the linkage choice. I'm not sure which would give the desired effect. I'll try to search for tutorials online for that though.\n",
      "Okay doing this: AgglomerativeClustering(distance_threshold=1,n_clusters=None, affinity='cosine', linkage='complete').fit(sentence_embeddings)\n",
      "Gives me 147 clusters\n",
      "Hey @anjumuaf123_twitter\n",
      "\n",
      "7\n",
      "4815\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-05-28 08:58\n",
      "When you say the distances are not bounded, do you mean that, for every dataset I apply this to, the clustering behavior would change? Basically my situation is I have a bunch of sentence embeddings and I want to cluster the sentences. I found that with my sentence embedding model, sentences with a similarity above 0.7 tend to be actually similar.\n",
      "\n",
      "1\n",
      "4816\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-05-28 08:59\n",
      "\"sentences with a similarity above 0.7 tend to be actually similar. \" is not welldefined unless you define you metric, if it's cosine, then your statement may be correct, in which case you should set the `affinity` parameter to cosine\n",
      "the default is euclidean\n",
      "https://scikit-learn.org/dev/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\n",
      "\n",
      "3\n",
      "4817\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-05-28 09:05\n",
      "@youssefabdelm_twitter Hi Sir,\n",
      "\n",
      "1\n",
      "4818\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-05-28 09:07\n",
      "sir,  are you familiar with python GIS?\n",
      "\n",
      "4\n",
      "4819\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-05-28 09:09\n",
      "I expected something like 1 cluster for that last line: AgglomerativeClustering(distance_threshold=1,n_clusters=None, affinity='cosine', linkage='complete').fit(sentence_embeddings)\"\n",
      "\n",
      "1\n",
      "4820\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-05-28 09:16\n",
      "Just realized where I might've gone wrong in my thinking there. My dataset probably has a bunch of sentences which are in fact that similar.\n",
      "\n",
      "2\n",
      "4821\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-05-28 09:19\n",
      "Ah yes. 1 would refer to more dissimilarity per cluster and 0 more similarity per cluster I think.\n",
      "\n",
      "2\n",
      "4822\n",
      "5e148b88d73408ce4fd5f28a\n",
      "2020-06-03 02:07\n",
      "Can anyone help with the sprint event? I already filled the form.\n",
      "\n",
      "1\n",
      "4823\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-06-03 06:29\n",
      "@reshama should know better about the participants and RSVPs\n",
      "\n",
      "1\n",
      "4824\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-04 00:43\n",
      "Hey guys, is the agglomerative clustering in scikit learn soft or hard? Or is there a setting somewhere we could use to set that?\n",
      "Also, while using agglomerative clustering on 100K data points, my computer crashed. I assume this is because I didn't use the 'memory' parameter for caching? Or did I misunderstand something there?\n",
      "\n",
      "2\n",
      "4825\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-06-04 19:02\n",
      "the memory parameter will not help you here. What do you mean by soft or hard?\n",
      "it might be useful to precomputed the distance matrix in a chunked way\n",
      "how much RAM do you have?\n",
      "and which linkage mode are you using?\n",
      "\n",
      "4\n",
      "4826\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-05 00:04\n",
      "Hey @amueller ! By soft or hard I mean to refer to soft clustering or hard clustering - which if I understand correctly, soft clustering refers to when datapoints can co-exist in different clusters whereas with hard clustering they can only exist in one cluster.\n",
      "I have 16 GB of RAM\n",
      "Using complete linkage\n",
      "How could I go about precomputing the distance matrix in a chunked way?\n",
      "I don't know if I got this right so far:\n",
      "@rth Do you mean using this in place of agglomerative clustering or for chunking / precomputing the distance matrix?\n",
      "Oops! Now I see where I was wrong\n",
      "And why those 100K data points only exported 1 cluster\n",
      "In the toy example, I did this:  chunked = pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1)  for chunk in chunked: clustering = AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(chunk)\n",
      "I set the parameter metric to 'cosine' in pairwise distances\n",
      "and I set affinity to 'precomputed' in AgglomerativeClustering\n",
      "When I then tried to do this with the 100K datapoints\n",
      "I forgot to change 'cosine' to 'precomputed' in AgglomerativeClustering\n",
      "\n",
      "13\n",
      "4827\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-05 00:41\n",
      "from sklearn.metrics import pairwise_distances_chunked from sklearn.cluster import AgglomerativeClustering  chunked = pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1)  for chunk in chunked:        clustering = AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(chunk)\n",
      "print(clustering.labels_)\n",
      "\n",
      "2\n",
      "4828\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2020-06-05 13:41\n",
      "@youssefabdelm_twitter You could try https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer\n",
      "\n",
      "1\n",
      "4829\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-06-05 14:14\n",
      "@youssefabdelm_twitter you should call pairwise_distances directly I think as it does chunking internally. Though I now realized agglomerativeclustering might already be doing. Also agglomerative clustering is hard clustering @rth I was thinking about that, but shouldn't agglomerativeclustering be already chunked using the working_memory parameter? cc @jnothman\n",
      "\n",
      "1\n",
      "4830\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-05 16:15\n",
      "Interestingly I actually tried the code I shared above on a small example and it worked, but on the 100K embeddings it just exported 1 cluster, and the amount of embeddings in that was around 750 which really confused me. I still have no idea what happened.\n",
      "By calling it directly, do you mean something like this:  \"for chunk in pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1):\"\n",
      "Or this: AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1))\n",
      "I hope I'm making some other stupid mistake rather than this being a consequence of a large input\n",
      "So far I don't see any difference between the toy example and the code I'm using for the 100K embeddings\n",
      "I might try the 'meta-clustering' approach I talked about where I just compare centroids (or try to apply complete linkage) of clusters from different chunks and then merge, and sort of testing that against what agglomerative clustering would normally do and see how the results vary. Do you think this would yield expected results (All points above 70% similarity should be grouped) or should I instead go with HDBSCAN? My one need is not specifying the number of clusters, but instead a distance threshold of some kind.\n",
      "\n",
      "6\n",
      "4831\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-05 16:28\n",
      "@amueller  I tried both, I assume you mean the first as that one works. The second gives me an error. \"ValueError: Expected 2D array, got scalar array instead: array=<generator object pairwise_distances_chunked at 0x11009fc78>. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\"\n",
      "\n",
      "1\n",
      "4832\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-05 17:07\n",
      "Now I'm getting this new error: \"ValueError: Distance matrix should be square, Got matrix of shape {X.shape}\"\n",
      "After making that change\n",
      "\n",
      "2\n",
      "4833\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-05 17:39\n",
      "This is what I'm using for the 100K embeddings:\n",
      "chunked_distances = pairwise_distances_chunked(sentence_embeddings, metric='cosine', n_jobs=-1, working_memory=3072) for chunk in tqdm(chunked_distances, total=22):    with io.capture_output() as captured:     clustering = AgglomerativeClustering(distance_threshold=0.5,n_clusters=None, affinity='precomputed', linkage='complete').fit(chunk)\n",
      "@rth Very useful, thank you!\n",
      "\n",
      "3\n",
      "4834\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2020-06-05 20:49\n",
      "@youssefabdelm_twitter I don't think that using AgglomerativeClustering on chunks of the distance matrix would give you anything meaningful. I meant using `KNeighborsRegressor`to precompute a sparse distance matrix, but then I'm not sure if `AgglomerativeClustering` actually supports sparse distance matrices. You won't be able to compute a dense as for 100k samples that would be ~80GB.  Generally AgglomerativeClustering doesn't scale well with default options (https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html) so I would suggest starting with a smaller dataset and progressively increase the number of samples to see how it scales. You may run into performance issues before memory ones. @amueller I haven't looked at the code in detail, but internally it never uses `pairwise_distances` on the full dataset.\n",
      "\n",
      "1\n",
      "4835\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-06-05 20:49\n",
      "hm. I was wondering if we need to add an example of doing some chunked clustering\n",
      "\n",
      "1\n",
      "4836\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-05 21:09\n",
      "@rth Does this mean that besides a sparse distance matrix (I'll give it a shot soon), there's really no way to do this all at the same time without a more powerful computer? So far what I've been doing is doing the clustering in chunks, meaning slicing the data (38K data points at a time) and then creating clusters from those. However, to me this is undesirable because of course (in my case) I get sentences which are in separate clusters which should more preferably be in one. I thought one idea to mitigate this is to calculate the centroid of each cluster, and then after all the 38K chunks are done, to compare centroids and then merge if they're below a certain distance threshold. Of course this loses the benefit of having the leaves & children info though.\n",
      "I ask because eventually, I want to do this with around 1 million vectors\n",
      "Also curious why you say that using AgglomerativeClustering on chunks of the distance matrix wouldn't give any meaningful results. Is it for the same reason I said above on getting different clusters which should really be one cluster?\n",
      "\n",
      "3\n",
      "4837\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2020-06-05 21:14\n",
      "@youssefabdelm_twitter Looks like single linkage should scale better for agglomeration clustering https://github.com/scikit-learn/scikit-learn/pull/11514#issuecomment-557349961  Well I mean it's just awkward to work with N separate clustering, but you can if you are comfortable with it.\n",
      "Also I think think Birch or DBSCAN might scale better if want a hierarchy of clusters. If you have 1M samples that certainly going to put constraints on the algorithms you can use (cf above linked HDBSCAN docs for a comparison) unless you can accept working on a subsampled dataset.\n",
      "\n",
      "2\n",
      "4838\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-05 21:20\n",
      "Awesome! Helpful charts. Wanted to ask what you meant by \"N separate clustering\"\n",
      "I assume you mean what I described above with splitting the data?\n",
      "\n",
      "2\n",
      "4839\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2020-06-05 21:24\n",
      "yes, I mean that if you do `AgglomerativeClustering().fit(X_chunk)` in a loop, as mentionned in your above code, at each iteration you are going to get a new clustering. All previous information is erased when you call fit, it's not a `partial_fit`. So that would be just equivalent to running the this clustering on N subsets of the full dataset I think.\n",
      "\n",
      "2\n",
      "4840\n",
      "5ecf797dd73408ce4fe523d7\n",
      "2020-06-05 21:55\n",
      "Sorry never mind, I didn't know about epsilon! I gotta do more research on HDBSCAN as I'm not that familiar with it.\n",
      "\n",
      "1\n",
      "4841\n",
      "5bac969fd73408ce4fa98ba2\n",
      "2020-06-06 09:39\n",
      "@adrinjalali https://github.com/scikit-learn/scikit-learn/issues/16951\n",
      "\n",
      "1\n",
      "4842\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-06-06 10:14\n",
      "I think the \"go wanted\" tag was removed from the issue cause we already have at least a half solution to the issue. Not sure if we want to work on it during the sprint @Mariam-ke\n",
      "*\"help wanted\"\n",
      "Is it on the issue list for the sprint @Mariam-ke ?\n",
      "\n",
      "3\n",
      "4843\n",
      "5a9c7b4dd73408ce4f8fd92e\n",
      "2020-06-06 20:52\n",
      "I have a small concern.  I am wondering if matplotlib should also be installed in setup.py along with NumPy and scipy. Is there is a reason it is not?\n",
      "\n",
      "1\n",
      "4844\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-06-06 21:33\n",
      "@Sahanave there is no reason to. It's a soft dependency and many people run scikit-learn in a server setup where there's not even a screen attached\n",
      "\n",
      "1\n",
      "4845\n",
      "5a9c7b4dd73408ce4f8fd92e\n",
      "2020-06-07 00:19\n",
      "That makes sense. Thansk @amueller\n",
      "\n",
      "1\n",
      "4846\n",
      "5a3b81c4d73408ce4f84514c\n",
      "2020-06-08 11:12\n",
      "Hello ! I have a question about the use of GutHub. I have started working on a pull request with Joseph Lucas (https://github.com/JosephTLucas) during the Data Umbrella sprint this week-end (https://github.com/scikit-learn/scikit-learn/pull/17504). I am trying to add a commit to this pull request (while I have not created it, Joseph has). In order to do so, I want to send a pull request to Joseph Lucas on his branch. However, I do not understand why I the github interface does not propose his repo as base when I create the pull request (while he has his own scikit-learn repo https://github.com/JosephTLucas/scikit-learn). What do you recommend I do ? I would prefer not to open a new pull request.\n",
      "\n",
      "1\n",
      "4847\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-06-08 11:19\n",
      "I think when you open a PR there's a checkbox which says \"show all forks\" or something, and then it'd show you Joseph's fork as well.\n",
      "\n",
      "1\n",
      "4848\n",
      "5a3b81c4d73408ce4f84514c\n",
      "2020-06-08 11:24\n",
      "Github displays the following :  Open a pull request Create a new pull request by comparing changes across two branches. If you need to, you can also compare across forks. When I click on \"compare across forks\", it displays a list of forks (base repository), but Joseph's fork does not appear. (the list seems too short to contain all forks of scikit-learn). Github does not manage to find his fork even when I enter the repo name :  JosephTLucas/scikit-learn.\n",
      "Thanks ! That is exactly what I wanted : )\n",
      "\n",
      "2\n",
      "4849\n",
      "5571fe1015522ed4b3e17d90\n",
      "2020-06-08 11:25\n",
      "Yeah this seems be a github interface issue (not 100% sure more of a wild guess).\n",
      "\n",
      "2\n",
      "4850\n",
      "5571fe1015522ed4b3e17d90\n",
      "2020-06-08 11:26\n",
      "I think if you go to a URL like this: https://github.com/JosephTLucas/scikit-learn/compare/master...ab-anssi:master\n",
      "You should be able to create a PR to JosephTLucas repo\n",
      "There is this weird thing that if you use the \"natural way\" (at least to me) of creating a PR to JosephTLucas fork it insists on creating a PR to the main repo (scikit-learn/scikit-learn)\n",
      "If you go there: https://github.com/JosephTLucas/scikit-learn/pulls and click \"New Pull Request\" it creates the PR against scikit-learn/scikit-learn from JosephTLucas fork and as you noticed I was not able to change that to use JosephTLucas fork as the target (**edit** it turns out clicking on the blue arrow between base and head fork is another work-around)...\n",
      "\n",
      "4\n",
      "4851\n",
      "5a3b81c4d73408ce4f84514c\n",
      "2020-06-08 11:40\n",
      "I have sent the pull request to Joseph. Thanks for help. It is the first time I send a pull request to a contributor to update his pull request to the main repo. If he accepts my pull request (to the branch of the pull request), will it automatically update the pull request (to the main repo) with my commit ?\n",
      "\n",
      "1\n",
      "4852\n",
      "5571fe1015522ed4b3e17d90\n",
      "2020-06-08 11:52\n",
      "Short answer: yes. Longer answer: his PR is tied to his branch so as soon he merges your PR on his fork, his branch will be updated and your changes will appear on the PR to scikit-learn/scikit-learn.\n",
      "Side-comment: if you collaborate from time to time with JosephTLucas a reasonable way to make that easier is that JosephTLucas gives you write access to his fork. This way you can push directly to his branch without doing a PR on his fork.\n",
      "\n",
      "2\n",
      "4853\n",
      "5a3b81c4d73408ce4f84514c\n",
      "2020-06-08 12:01\n",
      "@lesteve Thanks for the \"side-comment\". I will ask him if he agrees to do so. It would be much easier : )\n",
      "\n",
      "1\n",
      "4854\n",
      "5ec571c9d73408ce4fe461b0\n",
      "2020-06-08 12:21\n",
      "Sounds like a plan!\n",
      "\n",
      "1\n",
      "4855\n",
      "5ee0be30d73408ce4fe67f61\n",
      "2020-06-10 11:05\n",
      "Hello, i hope this is the right place for my question\n",
      "\n",
      "1\n",
      "4856\n",
      "5ee0be30d73408ce4fe67f61\n",
      "2020-06-10 11:07\n",
      "i have a problem with a 1D-Classification. I simplified my problem to the following: There are 2 classes +1 and -1 and i have a trainvalue for each of the classes 0.0293294646367189  (class -1) and  0.025545042466768184 (class 1) Now that i trained a LinearSVC with those values i throw some random values to the classifier to predict\n",
      "i expect the decision limit to be in the center of the two example values\n",
      "but even 0.025545042466768184, the train data for class 1 is predicted as -1\n",
      "i even tried to move this to a 2D Problem adding a 2nd feature to the values [0.0293294646367189, 0] and [0.025545042466768184, 0] but this didn't worked either\n",
      "\n",
      "4\n",
      "4857\n",
      "564789be16b6c7089cbab8b7\n",
      "2020-06-10 15:49\n",
      "I have some data I want to so linear regression on. When I use LinearRegression().fit(X_scaled, y[policy]) I get a score of over 0.96\n",
      "when I use LassoLarsCV(cv=5).fit(X_scaled, y[policy]) I get a score of 0\n",
      "what am I doing wrong?\n",
      "\n",
      "3\n",
      "4858\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-06-10 19:43\n",
      "what's the complete code? Training set score or test set score?\n",
      "\n",
      "1\n",
      "4859\n",
      "5c3cec52d73408ce4fb4aea1\n",
      "2020-06-12 09:13\n",
      "Hi, I have a large dataset, 600K rows and 2 columns of target. Multioutput xgboost works well, but Random Forest is so slow. How Can I perform it ?\n",
      "Its a regression problem\n",
      "\n",
      "2\n",
      "4860\n",
      "5ee529b7d73408ce4fe6ceb3\n",
      "2020-06-13 19:36\n",
      "i am writing a neural network without any external libraries for MNIST with only 4 labels.In the train _labels.csv  file i have one hot encoded data(1000) for all samples, my doubt is how to call the data directly to the function in the code. iam using softmax as my activation function in output layer\n",
      "\n",
      "1\n",
      "4861\n",
      "5dbd6437d73408ce4fcfbf0f\n",
      "2020-06-14 18:18\n",
      "Hi @FranciscoPalomares Random Forest is a collection of models which can be trained independently and can be parallelised: I guess for the classification you are using the RandomForestClassifier, you can scale the training through the n_jobs parameter when you instantiate it, something like  ``` RandomForestClassifier(n_estimators=100, n_jobs=-1) ```\n",
      "\n",
      "1\n",
      "4862\n",
      "564789be16b6c7089cbab8b7\n",
      "2020-06-15 14:32\n",
      "does the default FeatureAgglomeration just leave you with 2 features?\n",
      "\n",
      "1\n",
      "4863\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-06-15 16:51\n",
      "as stated in the [docs](https://scikit-learn.org/dev/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration), `n_clusters=2` is the default value.\n",
      "\n",
      "7\n",
      "4864\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-06-15 16:56\n",
      "I think we could add it to the docstring as well. I think it's clear from the user guide but why not add it to the docstring?\n",
      "\n",
      "1\n",
      "4865\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-06-15 16:57\n",
      "I agree that its docstring can be substantially improved :D the examples can also see some love\n",
      "\n",
      "1\n",
      "4866\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-06-19 20:18\n",
      "Hullo! I took on #9602 as a first issue for the MLH Fellowship. Looking into it further, it seems like the scope of the issue is much larger than clarifying a single docstring. I'm unsure of whether to start poking away at it, or whether to start a larger discussion on the direction of multiclass/multilabel learning in sklearn. Would love to hear thoughts. :)\n",
      "\n",
      "1\n",
      "4867\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-06-19 21:29\n",
      "hm honestly I think we're pretty consistent with the terms in the glossary\n",
      ">  In some places, multiclass/multilabel functionality is explicit and indicated by the name of the function.\n",
      "can you give an example of that?\n",
      "that's for meta-estimators, I guess?\n",
      "\n",
      "4\n",
      "4868\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-06-19 21:58\n",
      "Ah, I guess I went a bit overboard with my comment, seeing an issue where there was none.\n",
      "I'll focus on clarifying the OVR docstring, then.\n",
      "\n",
      "2\n",
      "4869\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-06-23 18:49\n",
      "Does anyone have any tips for making an API proposal go smoothly? (e.g. the structure of a good proposal, good examples of existing proposals)  Context: I was hoping to start working on a proposal for adding Gibbs sampling to LatentDirichletAllocation, going off of Thomas's recommendations.\n",
      "Ah, just found the SLEP template.\n",
      "Ah, thanks for the clarification, @adrinjalali!\n",
      "\n",
      "3\n",
      "4870\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-06-23 18:52\n",
      "I think adding features like that is usually discussed in the issues @joshuacwnewton SLEPs have been a bit more on the general API rather than specific features\n",
      "it can be just an issue like this one, for example https://github.com/scikit-learn/scikit-learn/issues/15346\n",
      "\n",
      "2\n",
      "4871\n",
      "5ef58225d73408ce4fe7f0dc\n",
      "2020-06-26 05:17\n",
      "Hello, the question I meet is the socres returned from the function 'cross_val_score'  seem to be very different from the result of actual fitting process. This is the last training process where most of the valid loss are about 0.1. However the score I get from the 'cross_val_score' is about 0.49. The score I use is the 'neg mse' which is the similar to the loss function of the network  'mse'. I want to know why it happens and how to fix it. Thanks a lot.\n",
      "[![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/rNGu/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/rNGu/image.png)\n",
      "\n",
      "2\n",
      "4872\n",
      "582376efd73408ce4f34cba5\n",
      "2020-06-27 16:34\n",
      "hola\n",
      "\n",
      "1\n",
      "4873\n",
      "5eeb998cd73408ce4fe74924\n",
      "2020-06-29 23:10\n",
      "Just submitted a new PR for the cross-validation documentation #17781\n",
      "\n",
      "1\n",
      "4874\n",
      "5e7c6de2d73408ce4fde10ba\n",
      "2020-06-30 19:01\n",
      "Hello, guys. How can I use a custom Distance Function for OPTICS clustering algorithm?\n",
      "\n",
      "1\n",
      "4875\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-01 16:43\n",
      "@GF-Huang you can use the metric parameter\n",
      "\n",
      "1\n",
      "4876\n",
      "5efcc3d8d73408ce4fe8707d\n",
      "2020-07-01 17:13\n",
      "Upvote vscode issue to allow colorfull output when fitting with Sklearn. https://github.com/microsoft/vscode-python/issues/12615\n",
      "\n",
      "1\n",
      "4877\n",
      "5de821c2d73408ce4fd31a52\n",
      "2020-07-02 20:54\n",
      "Can I request I/we/someone works on getting this merged https://github.com/scikit-learn/scikit-learn/pull/15007 as part of the SciPy sprint\n",
      "\n",
      "1\n",
      "4878\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2020-07-04 11:26\n",
      "@raybellwaves That PR is now merged.\n",
      "\n",
      "1\n",
      "4879\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2020-07-06 14:22\n",
      "I came across this on Twitter today and it made me think that scikit-learn also uses the word \"dummy\": https://twitter.com/wimlds/status/1279635475825754112 It would probably be good to move away from using this word (Google is trying to do the same btw: https://developers.google.com/style/word-list#dummy-variable), however it would be a big undertaking. A search for this word in scikit-learn gives me 479 results. I just wanted to point this out and ask what you think about it.\n",
      "\n",
      "1\n",
      "4880\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-07-06 15:30\n",
      "I guess we could call the `Dummy{Classifier/Regressor}` a `Trivial{Classifier/Regressor}`?\n",
      "\n",
      "1\n",
      "4881\n",
      "5eeb997bd73408ce4fe7491d\n",
      "2020-07-06 15:47\n",
      "in some testing files like `sklearn/cluster/tests/test_hierarchical.py` and `sklearn/gaussian_process/tests/test_gpr.py` the comments refer to creating dummy data and dummy optimizers but don't use the dummy classifier or variables. Maybe a good first step would be to change the wording in these comments as it doesn't conflict with the code?\n",
      "\n",
      "1\n",
      "4882\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-07-06 19:12\n",
      "is dummy offensive?\n",
      "\n",
      "1\n",
      "4883\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-07-06 19:34\n",
      "I looked up why it's offensive, and it seems most people think it is or may be offensive because of the way it's use as a derogatory term to call somebody stupid or an idiot. Now if we think of the people whom unfortunately are called often dumb or stupid by their friends or colleagues while many of them being very talented, we realize why the term may be a trigger for them. I personally don't find it offensive in the context which is used in our library, but general rule of thumb is that if there are many reasonable people out there who thing something is offensive, then it's offensive to them and I should avoid using the term, if that makes sense.\n",
      "\n",
      "1\n",
      "4884\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-07-07 13:33\n",
      "Could be wrong but my impression is that we mostly use the word dummy to denote \"something designed to resemble and serve as a substitute for the real or usual thing; a counterfeit or sham\" (taken from google definition). That might not be true in the case of DummyClassifier/Regressor, where the name might come from the fact that these estimators are over simplistic. Though the previous definition could also hold for them. IDK. Changing their names will force lots of users to change their code though,  so it's not just going to be an internal change.  As a side note we also have a bunch of \"sanity checks\", deemed non-inclusive by twitter, and I'll admit I'm quite puzzled by this one\n",
      "\n",
      "1\n",
      "4885\n",
      "5f05657ed73408ce4fe9033c\n",
      "2020-07-08 06:23\n",
      "thank you\n",
      "\n",
      "1\n",
      "4886\n",
      "5a25274ed73408ce4f819b55\n",
      "2020-07-08 08:30\n",
      "I have a question regarding the usage of the FeatureUnion weights.  I recently had a typo in a key of the weights dictionary, which does not lead to a warning or an error. Suppose:  ```python features = FeatureUnion([   ('f0', my_fancy_extractor_0),   ('f1', my_fanyc_extractor_1), ], transformer_weights={   'f1': 1.5,   'f2': 1.0, }) ```  I understand that tranformers without any weights (e.g., `f0`) will implicitly have assigned a weight of 1.0 (i.e., it is not multiplied with anything), which seems intuitive, but trying to assign weights to a transformer that is not part of the FeatureUnion is simply ignored.  I would argue that it would be beneficial if a warning would be shown instead, as this is a very tedious problem to debug in my opinion. What do you think?\n",
      "\n",
      "1\n",
      "4887\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-07-08 09:47\n",
      "I think this deserves an issue on the issue tracker with a small reproducible bit of code using the latest release. Input feature name validations are improving on our side, but there's still work to do.\n",
      "\n",
      "1\n",
      "4888\n",
      "5a25274ed73408ce4f819b55\n",
      "2020-07-08 11:41\n",
      "Thank you @adrinjalali , i have submitted an issue: #17863\n",
      "\n",
      "1\n",
      "4889\n",
      "5c3cec52d73408ce4fb4aea1\n",
      "2020-07-09 11:32\n",
      "I dump with joblib a model with all cpus (njobs = -1), when I load the model and predict, not use all cpus\n",
      "\n",
      "1\n",
      "4890\n",
      "5dbe0c1fd73408ce4fcfc84a\n",
      "2020-07-09 17:00\n",
      "hi sir  how to  ignore negative floating values in a data set ?\n",
      "\n",
      "1\n",
      "4891\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2020-07-09 18:56\n",
      "@FranciscoPalomares Use `estimator.set_params(n_jobs=1)` after unpickling\n",
      "\n",
      "1\n",
      "4892\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-10 18:40\n",
      "Hello! I've got one approval on an open PR -- if anyone has time, may I have a second reviewer? #17662  No rush, of course. I understand how busy things are. Thanks much! :)\n",
      "\n",
      "1\n",
      "4893\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-11 00:36\n",
      "My above request has been addressed by @NicolasHug. Thanks! :D\n",
      "\n",
      "1\n",
      "4894\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-11 13:29\n",
      "Good morning :)\n",
      "\n",
      "1\n",
      "4895\n",
      "5f09bedad73408ce4fe951c0\n",
      "2020-07-11 13:36\n",
      "Hello!  Is there anyone who can assist me in using scikit's normalized mutual information for real, continuous climate data?\n",
      "\n",
      "1\n",
      "4896\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-11 13:37\n",
      "@bfiranski as far as I know, it's only implemented for discrete data. computing mutual information for continuous data requires making distributional assumptions\n",
      "We actually have a non-parametric mutual information for feature selection (between a continuous and a discrete distribution). Are both your distributions discrete?\n",
      "\n",
      "2\n",
      "4897\n",
      "5f09bedad73408ce4fe951c0\n",
      "2020-07-11 13:38\n",
      "I was wondering about that because I was getting non-nonsensical results\n",
      "many thanks!\n",
      "no, i just kept on getting near unity results no matter what i tried - related data, unrelated data, two noise arrays\n",
      "i tried that one as well, it seemed to behave a bit better, but as you know, is not normalized to a metric.  my problem is that, as you also likely know, estimates of entropy to normalize it are highly dependent on resolution\n",
      "\n",
      "4\n",
      "4898\n",
      "5f09bedad73408ce4fe951c0\n",
      "2020-07-11 13:39\n",
      "both are real i.e. temperature and pollution concentration\n",
      "\n",
      "2\n",
      "4899\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-11 13:40\n",
      "can you please open an issue on the issue tracker for that? I think we should provide an error or at least a warning\n",
      "\n",
      "7\n",
      "4900\n",
      "5f09bedad73408ce4fe951c0\n",
      "2020-07-11 13:44\n",
      "it's not an easy thing to do it seems, thanks for your input!\n",
      "\n",
      "1\n",
      "4901\n",
      "5f09bedad73408ce4fe951c0\n",
      "2020-07-11 13:45\n",
      "hahaha!  you are talking to an atmospheric scientist who has, sadly, no stats and is way in over their head :)\n",
      "\n",
      "2\n",
      "4902\n",
      "5f09bedad73408ce4fe951c0\n",
      "2020-07-11 13:47\n",
      "there are some papers on using k-nearest neighbour for entropy which supposedly works well for continuous data, but i wanted to confirm that scikit wasn't appropriate before going down that rabbit hole\n",
      "\n",
      "3\n",
      "4903\n",
      "5f09bedad73408ce4fe951c0\n",
      "2020-07-11 13:53\n",
      "that is good news!  many thanks for providing a direction. for now i am going to report the non-warning issue you requested\n",
      "\n",
      "2\n",
      "4904\n",
      "5f09bedad73408ce4fe951c0\n",
      "2020-07-11 14:08\n",
      "Just to be sure we are on the same page, here is what my results are:\n",
      "```noise=np.arange(500) wavelength=np.linspace(0.01,1,500)*1e-6  # testing the normalized MI score between wavelength and noise - should return near zero for totally unrelated data sets constant_normalized_mi=normalized_mutual_info_score(wavelength,noise) ``` output: 1.0\n",
      "\n",
      "2\n",
      "4905\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-11 14:09\n",
      "yeah it should just error on floats, I think\n",
      "\n",
      "1\n",
      "4906\n",
      "5f0ac63ad73408ce4fe95ce0\n",
      "2020-07-12 08:20\n",
      "hey, I'm new to sklearn opensource community can anyone please help me for my first PR.\n",
      "\n",
      "1\n",
      "4907\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-13 01:28\n",
      "Hey @chaitanyamogal , have you already picked an issue to work on?\n",
      "\n",
      "1\n",
      "4908\n",
      "5f0ac63ad73408ce4fe95ce0\n",
      "2020-07-13 07:14\n",
      "No, but I am looking for a good first issue.\n",
      "\n",
      "1\n",
      "4909\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-16 03:54\n",
      "I have question/curiosity: generally, for new algorithms, are there any reasons that scikit-learn would prefer a Python/NumPy implementation over a Cython implementation?   The only thing I can think of is if the efficiency gain for the Cython version so minimal that it's not worth the added complexity. But, are there any other reasons?\n",
      "\n",
      "1\n",
      "4910\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-16 03:55\n",
      "@joshuacwnewton yes, mostly readability and maintainability\n",
      "\n",
      "1\n",
      "4911\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-16 03:56\n",
      "there is another potential reason in the future: using the __array_function__ protocol and possibly NEP 37, pure numpy algorithm could directly be ported to GPU or distributed dask datastructures, while that's not possible for Cython implementations.\n",
      "(this doesn't say anything about how efficient that would be though)\n",
      "There is even a (very hypothetical) future even where we might add both Cython and Numpy for some algorithms so we have a fallback in case the array is not a numpy array (this sounds weird; we need a numpy implementation if the array is not native numpy, but that's the way it would be lol)\n",
      "\n",
      "3\n",
      "4912\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-16 04:03\n",
      "Goodness, thank you. This is very valuable!\n",
      "I'm writing a uni report for my co-op term comparing Numpy to Cython (specifically in the context of #9661), and it felt a little too one-sided of me to say \"Cython faster, Cython better\" (lol) I figured there was more nuance there, so thank you.\n",
      "I've got a tab open with Numba too, ehe.\n",
      "Too many tabs, really!\n",
      "If I may pick your brain a little more, what about Cython do you feel makes it more difficult to maintain?\n",
      "\n",
      "5\n",
      "4913\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-16 04:07\n",
      "the main thing is really maintainability. In general, there's also the fact that you have to compile Cython, so distribution becomes much harder. But in sklearn we already made that investment so the additional burden is relatively small.\n",
      "Also, new contributors might not be familiar with Cython.\n",
      "A comparison with Numba might also be interesting if you want to go all out ;)\n",
      "   @joshuacwnewton no he means writing code that is closer to C than Python\n",
      "you can use pointers etc\n",
      "in Cython\n",
      "in which case you have to do manual memory allocation, which Python programmers might not be familiar with\n",
      "https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_criterion.pyx#L260\n",
      "\n",
      "8\n",
      "4914\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-07-16 09:27\n",
      "maintainability has a lot to do with code readability and people who are comfortable with that language. Cython is both less readable (especially when you do C instead of Python in it) and far fewer people know it than Python. It's more of a people issue than a language itself issue. If you go to Shogun's community, they're very comfortable with C++ and in that community C++ is very maintainable.\n",
      "\n",
      "1\n",
      "4915\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-07-16 10:13\n",
      "another point is that while numpy is reasonably straightforward to use and understand, Cython can act in magical ways that are not necessarily intuitive even with some experience (this is even more true for Numba BTW). https://github.com/scikit-learn/scikit-learn/issues/17299 is an example\n",
      "\n",
      "1\n",
      "4916\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-16 14:48\n",
      "@amueller, @adrinjalali, and @NicolasHug: I appreciate that you've taken the time to share your thoughts! I now have many great starting points for further reading, so thank you. :)\n",
      "> another point is that while numpy is reasonably straightforward to use and understand, Cython can act in magical ways that are not necessarily intuitive even with some experience  I guess this might be a natural consequence of Numpy being designed for array operations and used as the backbone for Python DS/ML, while Cython can be applied more generally. So, I would imagine that Numpy has stress-tested common use-cases, but that someone might have to recreate themselves using Cython...\n",
      "> (especially when you do C instead of Python in it)  @adrinjalali Just to clarify, is this referring to wrapping external C code? i.e. https://cython.readthedocs.io/en/latest/src/userguide/external_C_code.html\n",
      "Ah! Good to know. Thanks. :)\n",
      "\n",
      "4\n",
      "4917\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-16 15:02\n",
      "Ah! This actually came up in the extension I was writing... I had been allocating space using `np.empty` and was curious about how I might do that without Python. Malloc came up in search results but I left it at the time.   Thanks for linking that line, @amueller. :D\n",
      "\n",
      "3\n",
      "4918\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-16 15:07\n",
      "Hehehe exactly. I'm having a \"don't know what I don't know\" moment here, where I realize there's much more to Cython than I had thought.\n",
      "\n",
      "1\n",
      "4919\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-16 15:08\n",
      "you can look at the history of the tree cython files and you'll probably find some fun bugs we fixed over the years\n",
      "https://github.com/scikit-learn/scikit-learn/pull/8002/files\n",
      "\n",
      "2\n",
      "4920\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-16 15:12\n",
      "So, if I'm understanding correctly, the \"Python version\" (so to speak) of Cython memory management would be to create a memview into a Python object that already has its memory managed automatically. But, that Cython LOC you linked up there doesn't seem like it uses memviews at all. Just, dealing directly with a C array?  Although, IIRC from reading, memviews are relatively new to Cython, and it seems like you can use them with C arrays too.\n",
      "> https://github.com/scikit-learn/scikit-learn/pull/8002/files  Oh ho ho, interesting. Thanks for finding that link. :D\n",
      "\n",
      "2\n",
      "4921\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-16 15:14\n",
      "yes and we use both ways, usually for historical reasons. I think there used to be a speed difference between raw pointers and memory views but I don't think there is any more\n",
      "\n",
      "3\n",
      "4922\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-16 15:30\n",
      "So, I guess one last question I have is, it seems like for contributors, it might be worth exploring Numpy optimizations just as much as it is Cython optimizations? e.g. replacing for loops in an algorithm with vectorized operations when possible to try and push the pure Python implementation closer to Cython\n",
      "Although, I guess that sometimes leads to less readable Python code, e.g. with fancy np indexing\n",
      "So many tradeoffs... this is all very interesting, hehe.\n",
      "\n",
      "3\n",
      "4923\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-07-16 15:33\n",
      "We do prefer numpy if it's possible, and we prefer like numpy, but if we have to pick, we pick fast numpy over readable numpy, I think\n",
      "\n",
      "1\n",
      "4924\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-16 15:35\n",
      "That's good to know... I'll have to revisit the slower python/numpy implementation that I had converted to cython, then, before I can really put this to rest. :)\n",
      "\n",
      "1\n",
      "4925\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2020-07-16 21:39\n",
      "In the past it happened to me to spend time on a Cython implementation, then realize that it could be as fast or faster with just numpy with a better algorithmic approach. So spending time on numpy optimization as a first step is useful in any case. And of course profiling code (with e.g. snakeviz) is very helpful to make sure that the optimized code is actually the performance bottleneck..\n",
      "\n",
      "1\n",
      "4926\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-17 01:52\n",
      "Ah, I had never heard of snakeviz! I was just using cProfile + pStats. Thank you, @rth. :)\n",
      "\n",
      "1\n",
      "4927\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-07-17 02:05\n",
      "I appreciate the advice, too. I did jump right into Cython, so some planning ahead of time might have saved some trouble!\n",
      "\n",
      "1\n",
      "4928\n",
      "5b965bf1d73408ce4fa77f24\n",
      "2020-07-17 04:02\n",
      "checkout computer vision pre-trained model   https://github.com/balavenkatesh3322/CV-pretrained-model\n",
      "\n",
      "1\n",
      "4929\n",
      "5eeb997bd73408ce4fe7491d\n",
      "2020-07-23 21:21\n",
      "Hi! Does anyone have any advice for tackling errors in the azure pipeline? We're failing for linux and windows and I can't find a descriptive error message as to why it's failing other than \"Bash exited with code '1'.\" I'm on a mac. https://github.com/scikit-learn/scikit-learn/pull/17877/checks\n",
      "\n",
      "1\n",
      "4930\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-07-24 08:45\n",
      "It is bit tricky but you need to follow the links to get to this page: https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=20118&view=logs&j=13650d7c-49e8-54f3-0598-c3480d1c1e4f&t=97f2162d-a0c4-54dc-9fce-2fceef7172e3\n",
      "So to do so: click on \"details\" on checks frame in the PR page. It will lead you to a page where you have only the \"Bash exited with code'1'\" most of the time. In this page you can go on the bottom and click on \"View more details on Azure Pipelines \". It will redirect you on the azure where you can see the terminal output\n",
      "and there you will get which test is failing and what are the reason\n",
      "usually you can reproduce those locally afterwards\n",
      "\n",
      "4\n",
      "4931\n",
      "5d285286d73408ce4fc5e475\n",
      "2020-07-28 14:26\n",
      "Hello, I understand this is a long shot, but didn't know where else to ask. I am running scikit learn in an online judge environment (DMOJ). Such environment disallows syscalls (for security reasons). The sklearn is installed in conda environment. If I use LogisticRegression().fit(X,y) everything works fine.  However, if I use LogisticRegression(multi_class='multinomial').fit(X,y), the judge stops the process as the LR was apperently trying to make some disallowed syscall. What is the low-level difference between LR and multinomial LR? Is there a workaround for this?\n",
      "\n",
      "1\n",
      "4932\n",
      "5d285286d73408ce4fc5e475\n",
      "2020-07-28 14:40\n",
      "could th eproblem be that the multinomial version tries to spawn another processes even with n_jobs=1??\n",
      "\n",
      "1\n",
      "4933\n",
      "5f203be0d73408ce4feabfb8\n",
      "2020-07-28 14:56\n",
      "could someone help me understand how the regression tree works in sklearn, is it created in the cart, using the standard deviation to create the leaves? I did not find any manual implementation of the cart for regression.\n",
      "\n",
      "1\n",
      "4934\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-07-28 15:52\n",
      "@oplatek it's probably due to some multithreading, probably in BLAS, and I think nowadays you can control that through `threadpoolctl`. @ogrisel or @jeremiedbb know probably much better\n",
      "\n",
      "1\n",
      "4935\n",
      "5d285286d73408ce4fc5e475\n",
      "2020-07-28 15:58\n",
      "@adrinjalali  thats other Ondrej :]. thank you, I already tried   os.environ['MKL_NUM_THREADS'] = '1' os.environ['NUMEXPR_NUM_THREADS'] = '1' os.environ['OMP_NUM_THREADS'] = '1' os.environ['OPENBLAS_NUM_THREADS'] = '1'  or eporting these variables, but with no luck so far\n",
      "\n",
      "1\n",
      "4936\n",
      "5d285286d73408ce4fc5e475\n",
      "2020-07-29 14:12\n",
      "the syscall is sched_setaffinity. is there any way hot to prevent it?\n",
      "\n",
      "1\n",
      "4937\n",
      "564b231816b6c7089cbb06f6\n",
      "2020-07-30 08:28\n",
      "Hi. If I want to do multi-label classifications based on keywords that come from a large-ish vocabulary (~5.000), how do I need to encode the keywords so that an arbitrary amount of keywords can be used as predictors?\n",
      "If I one-hot encode them, how can I pass an arbitrary amount of one-hot encoded vectors to a classifier?\n",
      "Or, is it possible to use the outputs of a multilabelbinarizer as inputs to a classifier?\n",
      "\n",
      "3\n",
      "4938\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-07-31 16:06\n",
      "Hi all, I'm developing a wrapper that wraps Keras models with the Scikit-Learn API. I'd like some input from Scikit-Learn developers on what direction I should take the interface, I'd like to make it as aligned as possible with Scikit-Learn.\n",
      "The package is here: https://github.com/adriangb/scikeras\n",
      "Would any of you able to chat and give some input?\n",
      "\n",
      "3\n",
      "4939\n",
      "589b9e0fd73408ce4f490ba4\n",
      "2020-07-31 18:19\n",
      "@ldorigo You could  probably`' '.join(keywords)` and then use `CountVectorizer`?\n",
      "@adriangb The design choices in https://github.com/skorch-dev/skorch might be helpful, if you have specific question don't hesitate to ask.\n",
      "\n",
      "2\n",
      "4940\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-08-01 23:28\n",
      "Thanks @rth. I saw how Skorch does things, it's somewhat similar to how SciKeras is doing things now.  An alternative I thought of that I wanted to get feedback on is to have the user define (1) `__init__` and parameters for this model and (2) a `build_keras_model_` function that is expected to return an instance of a Keras `Model`. So a user defined model would look something like: ```python3 class MyModel(KerasClassifier):     def __init__(         self,         random_state=None,     ):         self.random_state = random_state      def build_keras_model(self, X, n_classes_):         model = keras.models.Model         ...         return model ```\n",
      "Where `build_keras_model` would have access to all of the estimator attributes in addition to variables like `n_classes_` and `X/y` which `KerasClassifier.fit` passes if they exist in the signature of `build_keras_model`.\n",
      "\n",
      "2\n",
      "4941\n",
      "5ac29bedd73408ce4f9419a3\n",
      "2020-08-06 23:13\n",
      "Hi, everybody. Does this channel have a rule of conduct that I should read before commenting?\n",
      "\n",
      "1\n",
      "4942\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-08-07 07:07\n",
      "The code of conduct (https://www.python.org/psf/conduct/) applies here, and we don't love spam or ads or promotions. Otherwise all good. Happy to answer any specific questions :)\n",
      "\n",
      "1\n",
      "4943\n",
      "5eeb9b35d73408ce4fe749bc\n",
      "2020-08-09 01:20\n",
      "Hello! For the last 2 weeks of the MLH program, I was thinking of helping to address sklearn's backlog of PRs (e.g. by doing code reviews). Does anyone have any recommendations for how best to approach that? (searching through the backlog, triaging PRs, leaving some types of PRs for core maintainers, etc.)\n",
      "\n",
      "1\n",
      "4944\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-08-09 16:11\n",
      "@joshuacwnewton reviewing is always welcome and it's a great way to learn! The things I personally look for during a review are: - backward compatibility and consistency with the current library / ecosystem. - test coverage. Ideally we want tests to be fast as well. - docs: every new feature should be documented at least in the UG. Big ones ideally come with a new example - code clarity and comments: you want your future self to be able to understand why something was done the way it was done  In terms of where to find PRs to review, we have a \"waiting for reviewer\" tag. Also you can try filtering by PRs which already have one approval: these might be easier to review. Feel free to review old PRs but note that there's a chance these will never be addressed.\n",
      "\n",
      "1\n",
      "4945\n",
      "5ac29bedd73408ce4f9419a3\n",
      "2020-08-10 01:48\n",
      "By seeing the sgd and sag code in https://github.com/scikit-learn/scikit-learn/sklearn/linear_model I wonder why the solvers do not use the mini-batch implementation?\n",
      "\n",
      "1\n",
      "4946\n",
      "5f3125ecd73408ce4febd212\n",
      "2020-08-10 10:53\n",
      "Hey, everyone, I am new to Scikit-Learn and after reading this [Security & maintainability limitations](https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations) I want to know is their other safer ways to save and load sklearn models.\n",
      "\n",
      "1\n",
      "4947\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-08-10 11:31\n",
      "@kaelgabriel  SGD processes each sample individually so by definition it cannot be a mini-batch approach\n",
      "\n",
      "1\n",
      "4948\n",
      "5ac29bedd73408ce4f9419a3\n",
      "2020-08-10 12:04\n",
      "@NicolasHug yes, you are right, I was wondering why people chosen SGD over mini-batch SGD (the one that is more common in DL). Just trying to understand.\n",
      "\n",
      "1\n",
      "4949\n",
      "5e614c10d73408ce4fdbbb92\n",
      "2020-08-10 13:58\n",
      "Hello, everybody. I hope you all's been fine. Has anyone already created a custom classifier with `sample_weight` that can share with me? I'm building my own estimator and i need `sample_weight` to make it work with AdaboostClassifier, tough i have no clue what it is\n",
      "\n",
      "1\n",
      "4950\n",
      "57b364d540f3a6eec05fce2b\n",
      "2020-08-13 20:23\n",
      "Hello, everyone. Can anyone clarify to me how `OneVsRestClassifier` performs the classification to decide the output of each input? I am coding a RBF network for classification and the output layer is trained with `n_classes` linear regressors, then the final output is given by applying a softmax function. But `OneVsRestClassifier` could be useful, i just can't confirm if it uses a similar approach.\n",
      "\n",
      "1\n",
      "4951\n",
      "5ee61de0d73408ce4fe6d981\n",
      "2020-08-15 05:44\n",
      "@rth I have addressed your review comments in #18124\n",
      "\n",
      "1\n",
      "4952\n",
      "5f0ac63ad73408ce4fe95ce0\n",
      "2020-08-17 11:37\n",
      "is it possible to directly assign: VotingClassifier.fit_transform.__doc__ = \"Return class labels or probabilities for X for each estimator.\"  #18150\n",
      "\n",
      "1\n",
      "4953\n",
      "5f0ac63ad73408ce4fe95ce0\n",
      "2020-08-23 12:09\n",
      "hey, someone please review my PR #18185\n",
      "\n",
      "1\n",
      "4954\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-08-23 13:23\n",
      "The tests are still not passing. You should solve this.\n",
      "\n",
      "1\n",
      "4955\n",
      "5be4801ed73408ce4fae4cc6\n",
      "2020-09-04 08:44\n",
      "Hello, anyone can explain what the format of the patches in https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html is? https://www.reddit.com/r/scikit_learn/comments/iinxo7/whats_the_format_of_the_patches_returned_by/\n",
      "are the returned values corner points or what? Why are there duplicates?\n",
      "\n",
      "2\n",
      "4956\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-09-04 09:47\n",
      "The documentation said: `array of shape (n_patches, patch_height, patch_width) or (n_patches, patch_height, patch_width, n_channels)`\n",
      "n_patches is the number of patches\n",
      "so `patches[0]` will be the small patch\n",
      "on the image\n",
      "2d if this a gray image\n",
      "3d if this is a color image\n",
      "\n",
      "6\n",
      "4957\n",
      "5be4801ed73408ce4fae4cc6\n",
      "2020-09-04 11:13\n",
      "That doesn't explain what the contents of the arrays are.\n",
      "\n",
      "1\n",
      "4958\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-09-04 12:28\n",
      "The pixel values\n",
      "\n",
      "1\n",
      "4959\n",
      "5be4801ed73408ce4fae4cc6\n",
      "2020-09-04 12:31\n",
      "RGB colors? Why are there two of the same?\n",
      "\n",
      "13\n",
      "4960\n",
      "5be4801ed73408ce4fae4cc6\n",
      "2020-09-04 12:33\n",
      "each patch is a [ [[RGB_1] [RGB_1]] [[RGB_2] [RGB_2]]]\n",
      "\n",
      "1\n",
      "4961\n",
      "5be4801ed73408ce4fae4cc6\n",
      "2020-09-04 12:36\n",
      "if one takes a 2x2 sample from the image then it should have 4 pixels?\n",
      "so 4 values\n",
      "\n",
      "20\n",
      "4962\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-09-04 12:39\n",
      "`[[RGB_1] [RGB_1]] [[RGB_2] [RGB_2]]` This is a (2, 2) numpy array\n",
      "and this is is not RGB\n",
      "they are only gray scale (because you have only one channel)\n",
      "(2, 2, 3) would be 3-channel\n",
      "> but I don't understand why (0,0)=(0,1) and (1,0)=(1,1)\n",
      "it does not\n",
      "it is a coincidence\n",
      "for your specific image\n",
      "\n",
      "8\n",
      "4963\n",
      "5be4801ed73408ce4fae4cc6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-04 12:42\n",
      "if one uses the python list index convention, then one finds (0,0), (0,1), (1,0), (1,1)\n",
      "\n",
      "11\n",
      "4964\n",
      "5be4801ed73408ce4fae4cc6\n",
      "2020-09-04 12:48\n",
      "because those symmetries got me thinking about whether I'm even looking at a 2x2 patch in the actual image\n",
      "or some sklearn abstraction about it\n",
      "\n",
      "16\n",
      "4965\n",
      "5be4801ed73408ce4fae4cc6\n",
      "2020-09-04 12:56\n",
      "extract_2d_patches that returns [[[corner1 corner2 length] ...\n",
      "so one knows what area the pixel covers\n",
      "\n",
      "2\n",
      "4966\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-09-04 12:58\n",
      "You will not find in scikit-learn\n",
      "you might want to look at scikit-image thought\n",
      "\n",
      "2\n",
      "4967\n",
      "5f52759ed73408ce4fee177a\n",
      "2020-09-04 17:14\n",
      "Hi, does anyone know more about this efficiency warning?\n",
      "scikit-learn/sklearn/neighbors/_base.py:167: EfficiencyWarning: Precomputed sparse input was not sorted by data.   warnings.warn('Precomputed sparse input was not sorted by data.'\n",
      "so im passing in a csr_matrix, but the csr_matrix is not sorted by data  (it's sorted by indices). i looked at  the code, and it seems this warning is thrown whenever each row in the csr_matrix is not sorted, low->high\n",
      "however,  i can't figure out how to sort the matrix. no matter how i initialize it, the matrix sorts it by index, even if i pass  it in sorted by data\n",
      "does anyone know how to pass in the right data to get rid of this efficiency warning?\n",
      "\n",
      "5\n",
      "4968\n",
      "5ee61de0d73408ce4fe6d981\n",
      "2020-09-05 04:20\n",
      "Would be nice to have scikit at @pyconindia sprints this year! Link to submit https://t.co/LZ0Hz0fa2I?amp=1\n",
      "\n",
      "1\n",
      "4969\n",
      "5f5369edd73408ce4fee2138\n",
      "2020-09-05 10:41\n",
      "Suppose,i want to run a grid search  using KNN as te estimator with 5 to 6 k values and i want to see \"accuracy\" for each K values,is there any way?\n",
      "\n",
      "1\n",
      "4970\n",
      "5f5369edd73408ce4fee2138\n",
      "2020-09-05 17:20\n",
      "Anyone?\n",
      "\n",
      "1\n",
      "4971\n",
      "56bb7a56e610378809c0cb2c\n",
      "2020-09-12 07:05\n",
      "`algo_max` Mohammad Masudul Alam (Gitter): have you take results = GridSearchCV, results has a cv_results_ variable.. it should contain what you're looking for.\n",
      "\n",
      "1\n",
      "4972\n",
      "57b364d540f3a6eec05fce2b\n",
      "2020-09-14 19:33\n",
      "Hello, can anyone explain why my custom estimator fails when used with `cross_val_score`? The error message is not clear: `ValueError: output_type='binary', but y.shape = (30, 3)`\n",
      "My estimator has its own `score()` function which computes the accuracy\n",
      "\n",
      "2\n",
      "4973\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-09-16 20:01\n",
      "@henrique-voni can you try running check_estimator on your estimator?\n",
      "\n",
      "1\n",
      "4974\n",
      "551710ad15522ed4b3ddfadb\n",
      "2020-09-18 03:31\n",
      "Hi all, I was wondering, does scikit support GPU accelerating?\n",
      "\n",
      "1\n",
      "4975\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-09-18 12:16\n",
      "no https://scikit-learn.org/stable/faq.html#will-you-add-gpu-support\n",
      "\n",
      "1\n",
      "4976\n",
      "5f64da0cd73408ce4fef4a7c\n",
      "2020-09-18 16:30\n",
      "Hi all, Ive been wanting to make use of the varimax rotation argument for sklearn.decomposition.FactorAnalysis (version 0.24dev0). So I created a conda environment with this version of sklearn (I have checked conda list, so definitely have the correct version installed), alongside hyperspy.  However when calling FactorAnalysis as follows: skl.decomposition.FactorAnalysis(rotation='varimax') I receive the following error: TypeError: __init__() got an unexpected keyword argument 'rotation'  The documentation for version 0.24 states:  <unconvertable> decomposition.FactorAnalysis now supports the optional argument rotation, which can take the value None, 'varimax' or 'quartimax'. <unconvertable>  Any ideas for what the problem might be?\n",
      "\n",
      "1\n",
      "4977\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-09-18 16:35\n",
      "what does `sklearn.__version__` say?\n",
      "(It works on `master` so you likely don't have the very latest version)\n",
      "\n",
      "2\n",
      "4978\n",
      "5f64da0cd73408ce4fef4a7c\n",
      "2020-09-18 16:38\n",
      "Oh that's odd, it says my current version is 0.23.2  although conda says version is 0.24dev0\n",
      "\n",
      "1\n",
      "4979\n",
      "57330f91c43b8c6019723dab\n",
      "2020-09-20 11:07\n",
      "Hi, everybody.  I wouild like to make a system to recognize coins. I have made database with coins text information and a couple images for every coin. My hypothesis is to use dectloscopic approach to recognize coins. May be someone know componen or library that I can use?  Thanks.\n",
      "\n",
      "1\n",
      "4980\n",
      "56bb7a56e610378809c0cb2c\n",
      "2020-09-22 08:37\n",
      "`algo_max` Will coins be always nicely centered and rotated in the same direction? Or scattered around the whole image and randomly rotated? Also one coin per image or multiple?\n",
      "`algo_max` Found it.. there was a template matching example for coins in skimage. Is this of any help? https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_template.html\n",
      "\n",
      "2\n",
      "4981\n",
      "56a7cd59e610378809be543d\n",
      "2020-10-02 15:05\n",
      "Hello, I would like to talk to the maintainers of scikit-learn about their infrastructure needs: https://scikit-learn.org/dev/about.html#infrastructure-support are you currently all set? Or are you looking for a cloud service provider to offer cloud infrastructures?\n",
      "Ok. Thank you very much :)\n",
      "\n",
      "2\n",
      "4982\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-10-02 16:19\n",
      "We are currently using Microsoft Azure/ CircleCI / Travis for our continuous integration and I think that we are pretty happy with the current services.\n",
      "\n",
      "1\n",
      "4983\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-10-02 16:25\n",
      "we might actually improve the testing for ARM but I think that some work has been done using Travis recently\n",
      "\n",
      "1\n",
      "4984\n",
      "541a528b163965c9bc2053de\n",
      "2020-10-02 16:30\n",
      "@remyleone I wouldn't mind having ssh access to ARM nodes from time to time to be able to interactively debug stuff. I the moment we use https://scikit-learn.org/dev/developers/tips.html#building-and-testing-for-the-arm64-platform-on-a-x86-64-machine which is kind of slow.\n",
      "hum I just read: https://www.theregister.com/2020/04/21/scaleway_arm64_cloud_end_of_life/\n",
      "so probably not interested in this :)\n",
      "\n",
      "3\n",
      "4985\n",
      "5dd4e82bd73408ce4fd18d22\n",
      "2020-10-04 11:18\n",
      "Hi Guys\n",
      "\n",
      "1\n",
      "4986\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-10-07 19:01\n",
      "@ogrisel but Amazon did a huge investment in arm, rihgt?\n",
      "\n",
      "1\n",
      "4987\n",
      "5f7f7040d73408ce4ff1073b\n",
      "2020-10-08 20:19\n",
      "I just joined open source community and wanted to contribute in some open source project please suggest me some I have hands on experience on python and Machine learning\n",
      "\n",
      "1\n",
      "4988\n",
      "541a528b163965c9bc2053de\n",
      "2020-10-09 07:41\n",
      "@anantgna here is the place to start: https://scikit-learn.org/dev/developers/contributing.html . This doc presents how to browse the issue tracker to find issues to tackle. Ideally, it's best to start with a small issue or improvement to an estimator class or function that you use yourself on a regular basis.\n",
      "@amueller  yes but Remy does not work at Amazon :) I remembered that scaleway had an ARM-based cloud and it could have been useful to better support scikit-learn on ARM but it's no longer the case apparently.\n",
      "\n",
      "2\n",
      "4989\n",
      "5f7f7040d73408ce4ff1073b\n",
      "2020-10-09 09:25\n",
      "@ogrisel thank you\n",
      "\n",
      "1\n",
      "4990\n",
      "5770e32dc2f0db084a201d09\n",
      "2020-10-12 14:42\n",
      "Hello everyone, is there a known dependency issue with Numpy? I am trying to install Scikit-learn in editable mode following the guide on the `contributing to scikit-learn` page but I get a `ModuleNotFoundError` for Numpy during the  build process. The call that triggers this error is `from numpy.distutils.compiler import new_compiler`. I have gotten this on multiple versions of Python 3.8.x and 3.9 virtual environments on a machine running Ubuntu 20.04. Any help trouble-shooting this will be much appreciated.\n",
      "\n",
      "1\n",
      "4991\n",
      "5f7ac1dcd73408ce4ff0af34\n",
      "2020-10-17 07:20\n",
      "Hello, everyone! I aim to implement an experimental Artificial Neuron-Glia Network (ANGN) for a  research project. I am not sure why (despite being 13 years in research community) I was not able to find any code for it. Any insights would be highly appreciated.\n",
      "\n",
      "1\n",
      "4992\n",
      "5f7ac1dcd73408ce4ff0af34\n",
      "2020-10-17 07:26\n",
      "For those of you unfamiliar with ANGN. Following image is the proposed architecture of a single unit of the network. [![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/geth/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/geth/image.png)\n",
      "\n",
      "1\n",
      "4993\n",
      "5dbd6437d73408ce4fcfbf0f\n",
      "2020-10-19 20:58\n",
      "@kushaangupta have you tried with Tensorflow or PyTorch? You should be more able to implement customised topologies for network even at level of single neuron.\n",
      "\n",
      "1\n",
      "4994\n",
      "5f8e7722d73408ce4ff1f8ea\n",
      "2020-10-20 06:34\n",
      "HI Team,  I am trying to make a recommendation engine for a food delivery app. I was going through different things like Market Basket Analyisis etc.. Company have huge amount of data for orders from different customers. Could you please help me on where to start?\n",
      "\n",
      "1\n",
      "4995\n",
      "5c790377d73408ce4fb94f40\n",
      "2020-10-20 12:59\n",
      "hello, I am trying to use sklearn linear regression to predict bonuses from enron dataset, and when chaching the score of prediction, I am getting a negative answer, can anyone please tell me the meaning of this\n",
      "\n",
      "1\n",
      "4996\n",
      "5f8fac00d73408ce4ff2116d\n",
      "2020-10-21 03:34\n",
      "Hello\n",
      "\n",
      "1\n",
      "4997\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2020-10-22 07:31\n",
      "Hi everyone, does anyone know any good references that discuss the importance of framework toolboxes like scikit-learn for reproducible data science?\n",
      "\n",
      "1\n",
      "4998\n",
      "5f7ae264d73408ce4ff0b263\n",
      "2020-10-27 20:24\n",
      "Hi! Ive built an interactive git cli - igit. Check it out: https://github.com/kobibarhanin/igit to install: pip install igit\n",
      "\n",
      "1\n",
      "4999\n",
      "5f9c412fd73408ce4ff2ed47\n",
      "2020-10-30 16:40\n",
      "Hello everyone, does anyone know how to solve nonlinear equations of a trained sklearn model?? I mean I have a trained sklearn model `f(x)` and I want to solve equations like `f(x)=c` where c is a constant?\n",
      "\n",
      "1\n",
      "5000\n",
      "569f975ce610378809bd48e8\n",
      "2020-10-31 19:29\n",
      "I am trying to use sklearn.decomposition.FastICA on the  audio files from https://cnl.salk.edu/~tewon/Blind/blind_audio.html  by Te-Won Lee. These have  been referenced in Prof. Andrew Ng's old lectures  on Independent component analysis. The unmixing does not seem to work with FastICA. All the code examples that I could find mix simple waveforms like sine, sawtooth with a simple mixing matrix and try to recover it back.   Does any one  have any  insights on this  ?\n",
      "\n",
      "1\n",
      "5001\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2020-11-06 10:42\n",
      "Hi scikit-learn team! I'll start a new job on 16 November and have some free time until then. I just wanted to let you know that I'll try to add all missing documentation for attributes by then (see #14312). There are only a few undocumented attributes left. So if anyone has time to review my PRs you (hopefully) could have all attributes documented by the end of next week. :) Nicolas and Adrin have already kindly started reviewing the PRs I sent in for this in the last few days.\n",
      "\n",
      "1\n",
      "5002\n",
      "5c8bb176d73408ce4fbac89c\n",
      "2020-11-06 11:42\n",
      "Thanks @marenwestermann right in time for 0.24!\n",
      "\n",
      "1\n",
      "5003\n",
      "5c62c8f4d73408ce4fb791fb\n",
      "2020-11-10 09:00\n",
      "Hi everyone, I need some help in implementing sklearn.decomposition.SparseCoder in c++. I've been searching it for a while but couldn't find good resource/library so far. Is there any library out there which can solve sparse decomposition in c++?\n",
      "\n",
      "1\n",
      "5004\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-11-10 09:06\n",
      "Have you checked Shogun? They might have something\n",
      "\n",
      "1\n",
      "5005\n",
      "5c62c8f4d73408ce4fb791fb\n",
      "2020-11-10 09:34\n",
      "Not yet, will check\n",
      "\n",
      "1\n",
      "5006\n",
      "5da2a33dd73408ce4fcdacee\n",
      "2020-11-10 11:30\n",
      "@KartikShrivastava , there is MLPack, https://mlpack.org/doc/mlpack-3.1.0/doxygen/cftutorial.html, I haven't read the complete article, but it does have the phrase \"Sparse Matrix\" in it.\n",
      "\n",
      "1\n",
      "5007\n",
      "5c62c8f4d73408ce4fb791fb\n",
      "2020-11-10 13:53\n",
      "Yup, I've tried the sparse coding module of that but couldn't get the right results from as, not as good as sklearn, so I thought that it might not be the way to go\n",
      "\n",
      "1\n",
      "5008\n",
      "5da2a33dd73408ce4fcdacee\n",
      "2020-11-10 15:11\n",
      "Ah, okay! What other solutions have you tried? Would like to know.\n",
      "\n",
      "1\n",
      "5009\n",
      "5c62c8f4d73408ce4fb791fb\n",
      "2020-11-10 16:57\n",
      "I'm also trying nnls solver of eigen library. I've used it in a similar application\n",
      "\n",
      "2\n",
      "5010\n",
      "5c62c8f4d73408ce4fb791fb\n",
      "2020-11-10 17:22\n",
      "Took help of mlpack people and it worked using the sparse coder interface :) Just to mention it requires initializing dictionary and a call to encode. I realized that it's very similar to sklearn sparse coder\n",
      "\n",
      "1\n",
      "5011\n",
      "5fa72f4dd73408ce4ff383e4\n",
      "2020-11-10 21:22\n",
      "Hi! I am new\n",
      "\n",
      "1\n",
      "5012\n",
      "5fb02cd9d73408ce4ff4111a\n",
      "2020-11-14 19:36\n",
      "hey\n",
      "\n",
      "1\n",
      "5013\n",
      "5f2fbda7d73408ce4febbf62\n",
      "2020-11-16 13:06\n",
      "Hi! I want to ask here before bothering people on GitHub: Is there a reason for the submoduled import structure of sklearn? I often find myself importing only to have to search for the submodule name of a thing I already know. For example, I might need \"PCA\", but I won't remember that it is under \"decomposition\". Is it worth having each collection of functions and classes under their own module? Granted, there are many submodules, each containing many more things to import, but surely maintaining a list of imports at the top level of sklearn wouldn't be too big of a hurdle, so that `from sklearn import PCA` would be possible. It would be a huge usability boost in my humble opinion. But I might also be dumb. Am I missing something obvious here? Cheers!\n",
      "\n",
      "1\n",
      "5014\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-11-16 13:11\n",
      "I'm not a big fan of heavy import statements, and having all those classes and functions as a top level import would make \"import sklearn\" very very heavy (since it would load every user-facing API immediately). Also, sometimes there may be conflicts between modules. Other than that, I find the code more readable and understandable when  imports are from the sub-modules instead of a top level import.\n",
      "\n",
      "1\n",
      "5015\n",
      "5f2fbda7d73408ce4febbf62\n",
      "2020-11-16 13:15\n",
      "Oh that's true! I don't really have a grasp on the full scope of sklearn, so I can imagine the import could be huge. I'm quite satisfied with this being the main explanation. Thanks a bunch!\n",
      "\n",
      "1\n",
      "5016\n",
      "5da2a33dd73408ce4fcdacee\n",
      "2020-11-18 12:37\n",
      "Hi, guys. Just a quick question.\n",
      "\n",
      "1\n",
      "5017\n",
      "5da2a33dd73408ce4fcdacee\n",
      "2020-11-18 12:37\n",
      "I'm seriously considering investing my time to solving issues on Scikit, so as to work my way towards becoming a maintainer of the project itself.\n",
      "I really think I would learn a lot in the process,  and something I really like doing - contributing. For now, I think the best way for me is to send PRs and try to solve issues as much as I can.\n",
      "should I keep something specific in mind while doing so, is what I wanted to ask. thanks. :D\n",
      "\n",
      "3\n",
      "5018\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-11-18 13:09\n",
      "> For now, I think the best way for me is to send PRs and try to solve issues as much as I can.\n",
      "yes. Just be sure to read the contributing guideline to be sure to not miss anything :)\n",
      "\n",
      "2\n",
      "5019\n",
      "5da2a33dd73408ce4fcdacee\n",
      "2020-11-18 13:13\n",
      "I've gone through it 5-6 times now. xD Even went ahead with the asv thing, but still have to really understand how to really use it.\n",
      "\n",
      "1\n",
      "5020\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-11-18 19:30\n",
      "Hi, we're trying to decide how to handle a Keras / sklearn API integration issue over at https://github.com/adriangb/scikeras/issues/131, I wanted to see if anyone in this group can chime in. The jist of it is how to support passing validation data to the estimators. Skorch solves this by having a an `__init__` parameter, but as is pointed out in that issue that means that calling with a different `X, y` would give you different results (since the validation losses would be totally off).\n",
      "\n",
      "1\n",
      "5021\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-11-18 20:03\n",
      "I think that right now we cannot do that. I vaguely recall a discussion where we discuss something linked with something close to the callback mechanism\n",
      "\n",
      "1\n",
      "5022\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-11-18 20:03\n",
      "Yeah I could not find any sklearn estimator that accepts extra data as a fit kwarg\n",
      "Or as an `__init__` param for that matter\n",
      "\n",
      "7\n",
      "5023\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-11-18 20:10\n",
      "Yep. If I had to pick between the options that I currently see available, my gut feeling would be to make it a fit kwarg, but that's just a knee jerk reaction\n",
      "which is why I'm asking here :laughing:\n",
      "\n",
      "2\n",
      "5024\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2020-11-18 20:13\n",
      "We don't have that in scikit-learn and our validation data is always a subset of X-y as passed to fit. IIRC LightGBM adds new arguments to `fit`\n",
      "\n",
      "1\n",
      "5025\n",
      "5fb60715d73408ce4ff46856\n",
      "2020-11-19 05:58\n",
      "hey, I need help with plotting rbf kernel's output in svm.SVC. Where do I go for help?\n",
      "\n",
      "1\n",
      "5026\n",
      "5fb60715d73408ce4ff46856\n",
      "2020-11-19 06:35\n",
      "[![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/Y9W4/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/Y9W4/image.png)  Any material on rbf kernel will help. I am looking for the above function.\n",
      "\n",
      "1\n",
      "5027\n",
      "5da2a33dd73408ce4fcdacee\n",
      "2020-11-19 13:25\n",
      "@salih.four_gitlab are you looking for this, https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html?\n",
      "\n",
      "1\n",
      "5028\n",
      "5fb60715d73408ce4ff46856\n",
      "2020-11-20 02:02\n",
      "Actually, I tried that module yesterday. For a 100x1 array X as input, the RBF.__call__(X) is returning a 100x100 array. I was looking for a 100x1 array that can be used to create a new axis to the existing 1d data resulting in 2d data. But, by reading the docs I could not make sense of the returned 100x100 array. That is why I came here. > @salih.four_gitlab are you looking for this, https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html?\n",
      "@Rubix982\n",
      "Do well :thumbsup: .\n",
      "\n",
      "3\n",
      "5029\n",
      "5da2a33dd73408ce4fcdacee\n",
      "2020-11-20 04:05\n",
      "Oh, okay, got your problem.  May or may not be able to check this out. Busy with University right now. Exams on the way next week. @salih.four_gitlab\n",
      "\n",
      "1\n",
      "5030\n",
      "5da2a33dd73408ce4fcdacee\n",
      "2020-11-20 09:42\n",
      "Thank you.\n",
      "\n",
      "1\n",
      "5031\n",
      "54d4a1d6db8155e6700f853b\n",
      "2020-11-20 22:44\n",
      "@salih.four_gitlab for the RBF(X) you will always get a square matrix because it's the formula above between all rows in X. If you have a single vector, it should be 1x100. If you want to have the kernels with an existing set, it should be RBF(X, Y)\n",
      "though I'm not sure what you're trying to achieve. That is indeed for the GP. we also have a rbf_kernel function (which does the same thing) if you just want to compute the kernel values\n",
      "\n",
      "2\n",
      "5032\n",
      "5f9fce72d73408ce4ff3159b\n",
      "2020-11-23 09:19\n",
      "Hi. I am looking at the [Classification example](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html). Do we only support `Dense` layer for neural networks for now? I was hoping to try some `AutoML` features of `scikit-learn` to create some convolution networks.\n",
      "\n",
      "1\n",
      "5033\n",
      "541a528b163965c9bc2053de\n",
      "2020-11-23 09:22\n",
      "convolution neural networks are indeed not supported and out of the scope of scikit-learn. Please use a library dedicated to deep learning for this. If you need scikit-learn API compatibility, you can try: https://github.com/skorch-dev/skorch or https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn .\n",
      "\n",
      "1\n",
      "5034\n",
      "5f9fce72d73408ce4ff3159b\n",
      "2020-11-23 09:25\n",
      "Thanks. I think I will stick with tensorflow for now.\n",
      "\n",
      "1\n",
      "5035\n",
      "5fbd959ad73408ce4ff4db63\n",
      "2020-11-24 23:34\n",
      "hi guys i'm  new to the contributors community and i want to start contributing to scikit-learn, but really i don't know how to start, and to be more specific when i check an issue and feel like i want to start working on it i don't know how to communicate with the developers there, like what should i say or shouldn't and what are the best practices or the ethics i should know before start working, any advices please ?. thank you\n",
      "\n",
      "1\n",
      "5036\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-11-25 01:39\n",
      "@ichisadashioko if you want to use Keras + sklearn check out SciKeras: https://github.com/adriangb/scikeras\n",
      "\n",
      "1\n",
      "5037\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-11-25 07:49\n",
      "@adriangb I was wondering if you could mention what is the benefit of scikeras compared to the KerasClassifier and KerasRegressor?\n",
      "\n",
      "1\n",
      "5038\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-11-25 14:20\n",
      "It's a pretty long list. Some Scikit-Learn specific ones are:\n",
      "\n",
      "1\n",
      "5039\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-11-25 14:47\n",
      "(was on mobile) * Allows pickling of estimators, allow ensembles and gridsearch to work properly * Allows you to use multi-input/output models to sklearn (especially important since some pre-trained models have \"extra\" outputs) * Working random_state (random state in Keras is pretty complicated) * Adds `partial_fit` and `predict_proba` * Your models can dynamically adjust the Keras input/output shape to the data (previously, that had to be hardcoded) * Support for `class_weights` in the same format as other sklearn estimators (including `class_weights=\"auto\"`). * A lot of compatibility improvements, stuff like implementing `n_features_in_` and `_estimator_type` * Allows you to grid-search parameters for optimizers and losses via [routed params](https://scikeras.readthedocs.io/en/latest/advanced.html#routed-parameters).\n",
      "\n",
      "1\n",
      "5040\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-11-25 14:55\n",
      "Some not Sklearn specific advantages: * Is actively maintained (the TF team may [deprecate](https://github.com/tensorflow/tensorflow/pull/37201#pullrequestreview-391650001) the wrappers in TF) * Has actual [documentation](https://scikeras.readthedocs.io/en/latest/index.html)\n",
      "I should write this stuff down somewhere in a docs page :sweat_smile:\n",
      "\n",
      "2\n",
      "5041\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-11-25 14:57\n",
      "OK so this us super useful :). Thanks for maintaining such wrapper\n",
      "Basically, it would be great to have the info (regarding the deprecation and full-compatibility) in the landing page.  When reading it I was kinda currious about it :)\n",
      "\n",
      "2\n",
      "5042\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-11-25 15:00\n",
      "I hope it's useful! And yep totally agreed, I opened myself an issue to add it to our docs / README.md\n",
      "\n",
      "1\n",
      "5043\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-11-25 15:24\n",
      "we could also add this to our \"related projects\"\n",
      "\n",
      "1\n",
      "5044\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-11-25 15:54\n",
      "It would be great if you did! Probably could also update `keras Deep Learning library capable of running on top of either TensorFlow or Theano` -> `Keras, the official high-level Deep Learning API for TensorFlow`\n",
      "\n",
      "1\n",
      "5045\n",
      "5977a991d73408ce4f6ebde7\n",
      "2020-12-03 01:32\n",
      "hello, I'm wondering if anyone has ever used ElasticNet with both a `sample_weight` argument and a precomputed gram matrix before. I'm trying a simple experiment to run a fit with and without the gram matrix passed in and they end up with different coefficients. I'm using the default setting of 'cyclic' for the coordinate descent so there shouldn't be any randomness.  without gram: ``` en = ElasticNet(alpha=0.001) en.fit(X, y, sample_weight=swgt) ```  with gram: ``` # pre-center data to avoid warning about gram matrix being tossed away # when data is centered in _pre_fit X_cent = X - np.average(X, axis=0, weights=swgt) gram_mat = X_cent.T @ X_cent  en_gram = ElasticNet(alpha=0.001, precompute=gram_mat) en_gram.fit(X_cent, y, sample_weight=swgt) ```  am I doing something dumb here?\n",
      "\n",
      "1\n",
      "5046\n",
      "541a528b163965c9bc2053de\n",
      "2020-12-03 10:56\n",
      "maybe this is related to data normalization but I am not sure. Could you please open and issue with a full minimal reproducer (e.g. using data generated on the fly with `numpy.random` for instance?\n",
      "\n",
      "1\n",
      "5047\n",
      "5977a991d73408ce4f6ebde7\n",
      "2020-12-03 14:19\n",
      "Will do.\n",
      "\n",
      "1\n",
      "5048\n",
      "541a528b163965c9bc2053de\n",
      "2020-12-03 18:17\n",
      "Please help test scikit-learn 0.24.0rc1 and help us spread the news: https://twitter.com/scikit_learn/status/1334562221498753026\n",
      "\n",
      "1\n",
      "5049\n",
      "5977a991d73408ce4f6ebde7\n",
      "2020-12-04 00:01\n",
      "regarding my earlier question, i managed to figure it out, a bit tricky but makes sense after closely reading through some of the preprocessing code in _base.py. See example below, perhaps it could be adapted in to a unit test?  ``` from sklearn.linear_model import ElasticNet from sklearn.datasets import make_regression from numpy.testing import assert_almost_equal import numpy as np  X, y = make_regression(n_samples=int(1e5), noise=0.5)  # random lognormal weight vector. weights = np.random.lognormal(size=y.shape)  en = ElasticNet(alpha=0.01, fit_intercept=True, normalize=False, precompute=False) en.fit(X, y, sample_weight=weights)  X_c = (X - np.average(X, axis=0, weights=weights)) # row wise multiply X_r = X_c * np.sqrt(weights)[:, np.newaxis]  en_precompute = ElasticNet(alpha=0.01, fit_intercept=True, normalize=False, precompute=X_r.T@X_r) en_precompute.fit(X_c, y, sample_weight=weights)  assert_almost_equal(en.coef_, en_precompute.coef_) ```\n",
      "\n",
      "1\n",
      "5050\n",
      "541a528b163965c9bc2053de\n",
      "2020-12-05 23:15\n",
      "Thanks for the follow-up, indeed is not very intuitive. Maybe it's normal, maybe the documentation of the precompute argument could explain better how to deal with the sample_weight. Maybe this could be part of a new tests / doc improvements related to https://github.com/scikit-learn/scikit-learn/pull/17785.\n",
      "\n",
      "1\n",
      "5051\n",
      "5977a991d73408ce4f6ebde7\n",
      "2020-12-05 23:54\n",
      "@ogrisel I'm happy to try to make the change myself - do you think it could be done as a PR to the docs for ElasticNet (and the other linear models that take precompute as an argument to their constructors and accept a sample_weight arg to `fit`)\n",
      "\n",
      "1\n",
      "5052\n",
      "541a528b163965c9bc2053de\n",
      "2020-12-06 18:15\n",
      "@amidvidy maybe open an issue that states that the documentation is not clear enough on how to use `sample_weight` an `precompute` together. Maybe we could add a new dedicated section to the user guide and reference that section from the docstring for the precompute parameters of those models.\n",
      "And we probably also need a new test to check that this will never be unintentionally broken by other changes in the code base.\n",
      "\n",
      "2\n",
      "5053\n",
      "5977a991d73408ce4f6ebde7\n",
      "2020-12-06 20:11\n",
      "sounds good - will do!\n",
      "\n",
      "1\n",
      "5054\n",
      "5fa98f30d73408ce4ff3a641\n",
      "2020-12-14 09:17\n",
      "Byo.ai an intelligent assistant to make people carbon neutral/positive. Anyone with experience with one or more general purpose programming languages including but not limited to: Python, Java, C/C++ (also Pytorch,) feel free to send your CV to work@byo.ai (passion for the environment, clean technologies and artificial intelligence is a plus!)\n",
      "\n",
      "1\n",
      "5055\n",
      "5977a991d73408ce4f6ebde7\n",
      "2020-12-15 02:58\n",
      "hey @ogrisel , I put up a PR for the issue we discussed. thanks for the help.\n",
      "\n",
      "1\n",
      "5056\n",
      "5fdb56f7d73408ce4ff6c2f5\n",
      "2020-12-17 13:23\n",
      "Hi all, I implemented a new feature for gaussian mixture models and I'm wondering whether it would be useful to have it integrated in scikit-learn. Should I open an issue to discuss this or can it be discussed here? In brief, I implemented the mixture entropy estimators (lower and upper bound) introduced in this paper https://arxiv.org/pdf/1706.02419.pdf\n",
      "\n",
      "1\n",
      "5057\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-12-17 13:58\n",
      "hi @giuliolovisotto , please have a look at our inclusion criteria: https://scikit-learn.org/dev/faq.html?highlight=inclusion%20criteria#what-are-the-inclusion-criteria-for-new-algorithms\n",
      "\n",
      "1\n",
      "5058\n",
      "5fdb56f7d73408ce4ff6c2f5\n",
      "2020-12-17 14:14\n",
      "Hi Adrin, thanks for that, so if I check, I get:     * [x] 3 years since publication: (2017)   * [ ] 200+ citations: no, 57 at the moment.   * [?] wide use and usefulness: this is a bit arbitrary.   * [?] clear cut improvement: for some settings (in particular with larger no. of features and no. of GMM components) using those bounds gives more efficient and more accurate entropy estimation than using a Monte Carlo sampling approach (which can be done with the GMM.score method).  By seeing this would you say I should open an issue to discuss this on github or just leave it?\n",
      "\n",
      "1\n",
      "5059\n",
      "5547dd8a15522ed4b3dfed5a\n",
      "2020-12-17 15:44\n",
      "Colleagues: looking for best practice tips to get logging output from sklearn, in particular from KNN clustering where Im having trouble figuring out how to use `verbose=1` (parameter doesnt seem to exist on `sklearn.neighbors.KNeighborsRegressor` nor on the `.fit()` method)  Have reviewed lots of issues on the subject but unclear current status of logging context managers or similar.\n",
      "@glemaitre thank you for the prompt response. I dont see a way to get *any* output from KNeighborsRegressor.  Am I missing something?\n",
      "\n",
      "4\n",
      "5060\n",
      "55d21ee30fc9f982beadabb8\n",
      "2020-12-17 16:36\n",
      "@ijstokes We don't have any logging. The verbose is only printing on the stdout.\n",
      "We are currently looking at improving this part with a real logging system\n",
      "\n",
      "2\n",
      "5061\n",
      "567f5d7716b6c7089cc043a8\n",
      "2020-12-17 17:02\n",
      "@giuliolovisotto I'd probably open an issue to discuss it. My gut feeling is that it doesn't pass the inclusion criteria, but it'd be nice to also here what other maintainers think on the issue tracker\n",
      "\n",
      "1\n",
      "5062\n",
      "5fdbaa88d73408ce4ff6cb87\n",
      "2020-12-17 19:09\n",
      "Hey folks <unconvertable> Want to talk with you about pipelines <unconvertable>\n",
      "What is the use case they were created to cover?\n",
      "In most of the examples, people groups processing \"branches\" by feature types (num/cat)\n",
      "Is this the main use case pipelines were designed for?\n",
      "\n",
      "4\n",
      "5063\n",
      "5fdbaa88d73408ce4ff6cb87\n",
      "2020-12-17 19:12\n",
      "What I was trying to do, but got frustrated is convert my Pandas-based data preprocessing into sklearn pipelines:\n",
      "\n",
      "1\n",
      "5064\n",
      "5fdbaa88d73408ce4ff6cb87\n",
      "2020-12-17 19:19\n",
      "``` # just copied some parts of the notebook to illustrate   # 1. data cleaning like this for feature in (     'PoolQC',      'FireplaceQu',      'Alley',      'Fence',      'MiscFeature',      'BsmtQual',      'BsmtCond',      'BsmtExposure',      'BsmtFinType1',      'BsmtFinType2',     'GarageType',      'GarageFinish',      'GarageQual',      'GarageCond',     'BsmtQual',      'BsmtCond',      'BsmtExposure',      'BsmtFinType1',      'BsmtFinType2',     'MasVnrType', ):     train_df[feature] = train_df[feature].fillna('None')     test_df[feature] = test_df[feature].fillna('None')     full_df[feature] = full_df[feature].fillna('None')  for dataframe in [train_df, test_df, full_df]:     dataframe['MSZoning'] = dataframe.groupby(['Neighborhood'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))     dataframe['MSSubClass'] = dataframe.groupby(['HouseStyle'])['MSSubClass'].transform(lambda x: x.fillna(x.mode()[0]))     dataframe['LotFrontage'] = dataframe.groupby(['Neighborhood', 'MSSubClass'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))     dataframe['Functional'] = dataframe['Functional'].fillna('Typ')  # 2. Some ordinal encoding   ordinal_feature_mapping = {     'ExterQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},      'ExterCond': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},     'BsmtQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'BsmtCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},     'BsmtFinType2': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},     'HeatingQC': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},     'KitchenQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},     'FireplaceQu': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},     'GarageQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'GarageCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'PoolQC': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},     'Fence': {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4},     'PavedDrive': {'N': 0, 'P': 1, 'Y': 2},     'CentralAir': {'N': 0, 'Y': 1},     'Alley': {'None': 0, 'Pave': 1, 'Grvl': 2},     'Street': {'Pave': 0, 'Grvl': 1},     'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},     'Functional': {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7} }  non_ordinal_cat_features = list(set(cat_features) - set(ordinal_feature_mapping.keys()))  for cat_feature in non_ordinal_cat_features:     train_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(train_df[cat_feature])     test_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(test_df[cat_feature])     full_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(full_df[cat_feature])  for ordinal_feature, feature_mapping in ordinal_feature_mapping.items():     train_df[ordinal_feature + 'Enc'] = train_df[ordinal_feature].map(feature_mapping)     test_df[ordinal_feature + 'Enc'] = test_df[ordinal_feature].map(feature_mapping)     full_df[ordinal_feature + 'Enc'] = full_df[ordinal_feature].map(feature_mapping)  # 3. Excessive feature engineering  for dataframe in [train_df, test_df, full_df]:     dataframe['HasFireplace'] = dataframe['Fireplaces'].apply(lambda x: int(x > 0))     dataframe['HouseAge'] = dataframe['YrSold'].astype('int') - dataframe['YearBuilt'].astype('int')      dataframe['TotalBathrooms'] = (dataframe['FullBath'] + (0.5 * dataframe['HalfBath']) +                                 dataframe['BsmtFullBath'] + (0.5 * dataframe['BsmtHalfBath']))      dataframe['OverallHouseQC'] = dataframe['OverallQual'] + dataframe['OverallCond']     dataframe['IsPavedDrive'] = (dataframe['PavedDrive'] == 'Y') * 1      dataframe['IsNeighborhoodElite'] = (dataframe['Neighborhood'].isin(['NridgHt', 'CollgeCr', 'Crawfor', 'StoreBr', 'Timber'])) * 1    # bunch of other features ```\n",
      "These three stages are the main in my data processing flow.\n",
      "So ideally I would like to process data in the same order in the pipeline as well. Then it would be converted naturally to pipeline definition\n",
      "So I would imagine a pipeline definition like this: ``` Pipeline([     ('missing_value_imputing', ColumnTransformer([...])),     ('feature_engineering', FeatureUnion([...])),     ('feature_transforming', ColumnTransformer([...])), ]) ```\n",
      "\n",
      "4\n",
      "5065\n",
      "5fdbaa88d73408ce4ff6cb87\n",
      "2020-12-17 19:28\n",
      "However, this doesn't work, because missing_value_imputing step would return me numpy array which is hard to work with on the following stages\n",
      "\n",
      "1\n",
      "5066\n",
      "5fdbaa88d73408ce4ff6cb87\n",
      "2020-12-17 19:33\n",
      "In the same time, my data processing has  constrains (feature_engineering step requires all values in place (missing_value_imputing) and feature_transforming requires all set of features (feature_engineering)). There are also operations I could apply on a multiple columns (and would love to do) like \"None\" constant imputing or ordinal encoding  and single column specific actions like MSZoning imputing\n",
      "Please let me know if all of this makes any sense\n",
      "\n",
      "2\n",
      "5067\n",
      "5fdbaa88d73408ce4ff6cb87\n",
      "2020-12-17 19:36\n",
      "With that being said, I'm wondering what is the cleanest way to define sklearn pipeline for this task?\n",
      "\n",
      "1\n",
      "5068\n",
      "5de821c2d73408ce4fd31a52\n",
      "2020-12-19 03:05\n",
      "Any thoughts on this? Just added an MCVE example to an old Q on SO. https://stackoverflow.com/a/65366293/6046019\n",
      "[![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/Peus/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/Peus/image.png)\n",
      "\n",
      "2\n",
      "5069\n",
      "541a528b163965c9bc2053de\n",
      "2020-12-22 17:07\n",
      "scikit-learn 0.24.0 is out! https://twitter.com/scikit_learn/status/1341429250696630276\n",
      "\n",
      "1\n",
      "5070\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2020-12-27 19:51\n",
      "Are there any plans to allow estimators to customize their expected precision for your estimator checks? The checks are great, but I've found that estimators I have fail because of precision issues (like a single element of a 60 element array being off by 5e-8). It would be nice to be able to at least have some rough measure of precision as an estimator attribute that the tests could then use to adapt their behavior.\n",
      "\n",
      "1\n",
      "5071\n",
      "5fec660ed73408ce4ff7a933\n",
      "2020-12-30 11:37\n",
      "Hey people\n",
      "Is this place active?\n",
      "Is there a real time communication platform for scikit learn?\n",
      "Thanks!!\n",
      "\n",
      "4\n",
      "5072\n",
      "5dbd6437d73408ce4fcfbf0f\n",
      "2020-12-31 12:38\n",
      "@AdityaPujara23 ask here, somebody will reply you if possible.\n",
      "\n",
      "1\n",
      "5073\n",
      "5fec660ed73408ce4ff7a933\n",
      "2021-01-01 07:21\n",
      "Cool thanks!\n",
      "\n",
      "1\n",
      "5074\n",
      "584177bed73408ce4f3a3d87\n",
      "2021-01-06 12:43\n",
      "Hi there! I'm developing an online ensemble learning thingy, and I just wanted to praise you guys for the quality of your code. It's always a pleasure to read! (Even though I always weep looking at `BaseEstimator` and its cool `get/set_params`, wondering whether I should rewrite something similar or re-use your code -- and *wanting* to do neither :P )\n",
      "\n",
      "1\n",
      "5075\n",
      "541a528b163965c9bc2053de\n",
      "2021-01-06 13:29\n",
      "Thanks :)\n",
      "\n",
      "1\n",
      "5076\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-08 12:23\n",
      "do any classifiers in scikit-learn handle categorical features directly? I feel there were some PRs about this a long time ago\n",
      "\n",
      "1\n",
      "5077\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-01-08 13:37\n",
      "You're in luck @lesshaste https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_24_0.html#native-support-for-categorical-features-in-histgradientboosting-estimators\n",
      "\n",
      "1\n",
      "5078\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-08 13:50\n",
      "@NicolasHug  that's great. I will try it in a few minutes\n",
      "\n",
      "1\n",
      "5079\n",
      "5ff86ab4d73408ce4ff853dc\n",
      "2021-01-08 14:26\n",
      "Hi guys! I wrote a document which is \"Hello Kaggle\". I hope to get some feedback about the document such as a typo, grammar error, wrong information, etc. https://github.com/stevekwon211/Hello-Kaggle thank you :)\n",
      "\n",
      "1\n",
      "5080\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-08 14:38\n",
      "@NicolasHug  it works which is great.\n",
      "hmm. except I can't get it to work with categorical features\n",
      "\n",
      "2\n",
      "5081\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-08 14:44\n",
      "what am I doing wrong? https://bpa.st/IXRQ\n",
      "why is it trying to convert a string to a float?\n",
      "\n",
      "2\n",
      "5082\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-01-08 15:19\n",
      "@lesshaste categorical features are supported but the estimators themselves only understand integer values in [0, 255]. You'll need to encode hte categorical features with an OrdinalEncoder before passing them to the predictor. You can take a look at https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_categorical.html#gradient-boosting-estimator-with-native-categorical-support for an example\n",
      "In your case since it seems that you only have categorical features, you can bypass the ColumnTransformer and just  do `clf  = make_pipeline(OrdinalEncoder(), HistGradientBoostingClassifier())`\n",
      "\n",
      "2\n",
      "5083\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-08 15:46\n",
      "@NicolasHug  ah ok. And then I list which features are categorical using categorical_features=cat_features? As in clf = make_pipeline(OrdinalEncoder(), HistGradientBoostingClassifier(categorical_features=cat_features)) ?\n",
      "@NicolasHug  I should say that catboost has some very clever tricks for categorical features that would be awesome if included in scikit-learn\n",
      "the default for HistGradientBoostingClassifier seems to  be no categorical features unless I misunderstood it\n",
      "@NicolasHug  thanks.. they actually have two tricks\n",
      "https://arxiv.org/pdf/1706.09516.pdf  explains it better than I could\n",
      "but essentially the first and more most important trick is Ordered boosting   (section 4.2) and the second is Feature combinations\n",
      "\n",
      "6\n",
      "5084\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-01-08 17:15\n",
      "> @NicolasHug  ah ok. And then I list which features are categorical using categorical_features=cat_features? As in clf = make_pipeline(OrdinalEncoder(), HistGradientBoostingClassifier(categorical_features=cat_features)) ?  Yes, or you can use `clf.set_params(histgradientboostingclassifier__categorical_features=...)` after `clf` is defined. And yes, by default all features are treated as continuous, as is the case for the overwhelming majority of estimators (only a few tranformers like OneHotEncoder expect categorical features by default). Regarding CatBoost:  I think  they do target encoding for high-cardinality categorical features. This is in the works https://github.com/scikit-learn/scikit-learn/pull/17323\n",
      "\n",
      "1\n",
      "5085\n",
      "5e532f4fd73408ce4fda84b2\n",
      "2021-01-11 05:56\n",
      "https://github.com/scikit-learn/scikit-learn/issues/19092\n",
      "Hey,\n",
      "\n",
      "2\n",
      "5086\n",
      "5e532f4fd73408ce4fda84b2\n",
      "2021-01-11 05:57\n",
      "Can someone help me understand if I can start working on this?\n",
      "\n",
      "1\n",
      "5087\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-11 11:18\n",
      "how exactly does prediction work for gradient boosted trees? In a random forest you just get a prob from every tree and average them I think. Is this different for gradient boosted trees?\n",
      "\n",
      "1\n",
      "5088\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-01-11 13:29\n",
      "@lesshaste in gradient boosting, one directly optimizes a loss function. For binary classification in sickit-learn this is the log loss, much like logistic regression. What the trees predict is the log-odds ratio (decision_function()) and it's passed into a sigmoid to get a probability (predict_proba())\n",
      "this is a self plug but this might help: http://nicolas-hug.com/blog/around_gradient_boosting\n",
      "\n",
      "2\n",
      "5089\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-11 13:33\n",
      "@NicolasHug  thank you. In low level terms, what happens at prediction time? Is something computed for every tree separately and then averaged?\n",
      "\n",
      "1\n",
      "5090\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-01-11 13:56\n",
      "decision_function is the sum of all the tree values, not the average\n",
      "see https://nbviewer.jupyter.org/github/NicolasHug/nicolashug.github.io/blob/master/assets/gradient_boosting_descent/GradientBoosting.ipynb\n",
      "\n",
      "2\n",
      "5091\n",
      "59bc1baad73408ce4f75eec5\n",
      "2021-01-11 14:27\n",
      "I've cross-compiled scikit-learn for an armv7h system and I'm receiving the following error when testing, does anyone have any suggestions for where the issue may lie?\n",
      "``` File \"sklearn/cluster/_dbscan_inner.pyx\", line 40, in sklearn.cluster._dbscan_inner.dbscan_inner ValueError: Buffer dtype mismatch, expected 'npy_intp' but got 'long long'  ```\n",
      "It looks like it could be something 32bit vs 64bit related but from what I can see the `setup.py` build system just imports from `numpy` which I would expect to handle this all properly\n",
      "so I'm wondering if this isn't a cross compile issue and is maybe just an assumption made in the code that the argument will be 64bit?\n",
      "\n",
      "4\n",
      "5092\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-11 14:51\n",
      "@NicolasHug thanks\n",
      "\n",
      "1\n",
      "5093\n",
      "541a528b163965c9bc2053de\n",
      "2021-01-14 17:18\n",
      "@jackmitch this looks like a bug indeed. I don't have a quick fix from the tip of my head but could you please open an issue on github to avoid forgetting about it? It's probably a temporary buffer that is assigned with an `np.int` instead of `np.intp` in our Cython code or wrapping Python code.\n",
      "Here is a similar bug we had in the past (a long time ago): https://github.com/scikit-learn/scikit-learn/commit/627c564faec36c783788b7488b4cf19a2535916c\n",
      "Ideally a PR would be appreciated because none of us will be able to reproduce it because we do not have access to such hardware.\n",
      "\n",
      "3\n",
      "5094\n",
      "541a528b163965c9bc2053de\n",
      "2021-01-14 17:22\n",
      "Also if you want to submit a PR for our doc that documents how you cross-compile scikit-learn for armv7h, that could be useful knowledge to share. For instance it could be a new section at the end of this page: https://scikit-learn.org/dev/developers/advanced_installation.html\n",
      "\n",
      "1\n",
      "5095\n",
      "54a6f79fdb8155e6700e5114\n",
      "2021-01-15 15:55\n",
      "How to train neural network and what i the best design of it if I have discrete/continual input and continual output?\n",
      "\n",
      "1\n",
      "5096\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2021-01-16 06:55\n",
      "Hi, I'm wondering why this test is checking for float64 specifically (as opposed `dtype.kind == \"f\"`)? https://github.com/scikit-learn/scikit-learn/blob/2218ec46227c92301ac6837c4a8ae9b8dc5d3960/sklearn/utils/estimator_checks.py#L1735\n",
      "\n",
      "1\n",
      "5097\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-01-16 19:39\n",
      "@adriangb no real reason. This is typically the kind of check that's probably a bit too strict for non-internal estimators. We're working on making the `check_estimator` suite less restrictive (https://github.com/scikit-learn/scikit-learn/issues/13969). BTW @glemaitre , any progress on that? (I don't mean to rush you by any means, just wondering  :) )\n",
      "\n",
      "1\n",
      "5098\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2021-01-16 21:10\n",
      "Great, I left a comment on that PR for the record.\n",
      "Would there be any chance that tolerances might be relaxed as well? There are several checks that check rtol/atol to <1e-7, which makes it very flaky when the estimator internally uses <=32 bit precision.\n",
      "\n",
      "2\n",
      "5099\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2021-01-16 21:16\n",
      "@urosn do you mean like this: ```python estimator.fit(X, [1, 2, 3, 4.5]) estimator.predict(X)  # [1.1, 2.3, 2.7, 4.4] ``` ?\n",
      "\n",
      "1\n",
      "5100\n",
      "59076a40d73408ce4f5c34b4\n",
      "2021-01-17 08:35\n",
      "Hi. I want to learn scikit-learn from scratch. Do we have an official book or guide?\n",
      "If not, I will be making notes. Can I contribute one?\n",
      "Thank you\n",
      "\n",
      "3\n",
      "5101\n",
      "58de4778d73408ce4f551e04\n",
      "2021-01-17 12:28\n",
      "Hi @D3V4N5H, there is plenty on resources in the docs. You can start with the [\"Getting Started\" pages](https://scikit-learn.org/stable/getting_started.html).\n",
      "\n",
      "1\n",
      "5102\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-18 08:52\n",
      "@NicolasHug I was a focus on the release recently but I think that we are going to branch today so it would be one of the task pretty soon.\n",
      "\n",
      "1\n",
      "5103\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-19 19:00\n",
      "I am building a classifier and have 10,000 examples of the positive class. I can generate any amount from the negative class. Is there a standard way to take advantage of the unlimited amount of data from the negative class?\n",
      "\n",
      "1\n",
      "5104\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 08:53\n",
      "@byo-ai  pay?\n",
      "@lesteve  :)\n",
      "\n",
      "2\n",
      "5105\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 08:54\n",
      "a really simple question. Currently I do clf.score(Xordinal_test, y_test). If I want to use balanced accuracy instead, is there a similar one line solution?\n",
      "\n",
      "1\n",
      "5106\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 09:28\n",
      "`balanced_accuracy_score(clf.predict(X_ordinal_test), y_test)`\n",
      "This is a one liner solution if you omit the import :)\n",
      "We don't allow to switch the default score in `clf.score` to be more explicit\n",
      "so you need to get the prediction and call the score function\n",
      "\n",
      "4\n",
      "5107\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 09:31\n",
      "You also have the possibility to use the scorer API but this is not a one liner\n",
      "\n",
      "3\n",
      "5108\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 09:33\n",
      "Actually you could `get_scorer(\"balanced_acccuracy\")(clf, X, y)` but I think that we don't head toward readable code :)\n",
      "\n",
      "1\n",
      "5109\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 09:42\n",
      "@glemaitre I like it. Thank you\n",
      "I have a different more general question. I am doing binary classification. I would like to maximize the number of items in the positive class that get a probability higher than any probability from the negative class. Does this correspond to a known loss function?\n",
      "let me edit it to get rid of the word score...\n",
      "\n",
      "3\n",
      "5110\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 09:49\n",
      "https://github.com/scikit-learn/scikit-learn/pull/16525\n",
      "You might want this things maybe\n",
      "\n",
      "2\n",
      "5111\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 09:50\n",
      "Basically, this is tuning the threshold of the argmax when doing the predict from the predict_proba\n",
      "Otherwise, `sample_weigth` or `class_weigth` will allow you to play on the inner loss\n",
      "while training\n",
      "\n",
      "3\n",
      "5112\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 09:55\n",
      "@glemaitre thank you. I haven't fully understood how to use your suggestions for my problem but I will have a think\n",
      "maybe it could go on scikit-learn discussions as well :)\n",
      "\n",
      "4\n",
      "5113\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 10:00\n",
      "I can give one in about 90 minutes\n",
      "\n",
      "1\n",
      "5114\n",
      "5571fe1015522ed4b3e17d90\n",
      "2021-01-22 10:12\n",
      "> maybe it could go on scikit-learn discussions as well :)  +1. As mentioned in https://github.com/scikit-learn/scikit-learn/discussions/19220#discussioncomment-298015 my feeling (and probably others feeling) is that gitter is not the best place for Q&A. I guess a reasonable approach is to create a discussion and then ping on gitter if you feel you have not received an answer after some time\n",
      "\n",
      "1\n",
      "5115\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 10:16\n",
      "@lesteve thanks. I do like the interactive nature of gitter to a) improve the question and/or b) realise I shouldn't have asked it in the first place :)\n",
      "\n",
      "1\n",
      "5116\n",
      "5571fe1015522ed4b3e17d90\n",
      "2021-01-22 10:29\n",
      "Yeah I agree the threshold about \"what is OK to ask on gitter\" is not very clear. I would favour an approach as I mention above discussion + ping on gitter after some time. It is not as much interactive but it is a better investment of answerer time since the question + answer will be findable by googling (contrary to gitter)\n",
      "\n",
      "2\n",
      "5117\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 10:31\n",
      "Gitter should come with a feature that you cannot scroll-up in your discussion feed\n",
      "because this is a bit what happens in reality :)\n",
      "\n",
      "3\n",
      "5118\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 10:35\n",
      "This isn't a full example but hopefully it will help clarify. Say my positive class items get 0.1, 0.3, 0.7, 0.9 from predict_proba and my negative class items get 0.01, 0.2, 0.2, 0.5. Then two of the positive class items get a prob (0.7, 0.9) larger than the largest prob (0.5) from the negative class.\n",
      "@glemaitre does that make it any clearer?\n",
      "\n",
      "2\n",
      "5119\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 10:39\n",
      "So the cutoff classifier intend to change the probability from 0.5 to another threshold\n",
      "such that you can for instance the maximum number of predictions of the positive label\n",
      "\n",
      "2\n",
      "5120\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 10:41\n",
      "@glemaitre  yes. But the cutoff is a function of the probs that the negative class items are given\n",
      "my example of 0.5 above wasn't a great choice :)\n",
      "argh... I hate how easy it is to be confusing.\n",
      "@lesteve do you think my post is clear now?\n",
      "\n",
      "4\n",
      "5121\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 10:43\n",
      "Oh you want to reinforce your learning step\n",
      "I see\n",
      "\n",
      "2\n",
      "5122\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 10:45\n",
      "In some way, I could think about a boosting strategy as AdaBoost, but instead of learning new learner favoring misclassified samples, you want to favor specific samples from the positive class.\n",
      "\n",
      "1\n",
      "5123\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 10:51\n",
      "I don't know if there is something in active learning allowing such stuff\n",
      "But I am not knowing so much in this area\n",
      "\n",
      "2\n",
      "5124\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 10:59\n",
      "thanks. I was going to post on discussions but I can't think of a suitable title :)\n",
      "\n",
      "1\n",
      "5125\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-22 11:00\n",
      "\"Reinforce sample weight for online learning\"\n",
      "\n",
      "1\n",
      "5126\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 11:21\n",
      "posted\n",
      "\n",
      "1\n",
      "5127\n",
      "5571fe1015522ed4b3e17d90\n",
      "2021-01-22 11:34\n",
      "with the link there is even more chances that someone answers :wink: https://github.com/scikit-learn/scikit-learn/discussions/19239\n",
      "\n",
      "2\n",
      "5128\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 12:43\n",
      "I guess it's equivalent to maximizing true positives when you have 0 false positives...?\n",
      "now I am tempted to try one of the options mentioned in the discussions. Now really sure which one though\n",
      "\n",
      "2\n",
      "5129\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-22 16:46\n",
      "*Not\n",
      "\n",
      "1\n",
      "5130\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-23 15:59\n",
      "Can any of the classifiers in scikit learn directly optimize auc as the loss function?\n",
      "\n",
      "1\n",
      "5131\n",
      "6008674dd73408ce4ff94a9b\n",
      "2021-01-23 23:01\n",
      "Hello all! I'm not a library developer, I'm a student developer and user of scikit-learn. Is there any work going on for scikit-learn to use Apple's ML Compute frameworks so that ML calculations can be accelerated by the 16-core neural engine in the recent Apple Silicon macs?\n",
      "\n",
      "1\n",
      "5132\n",
      "52f100f45e986b0712ef4def\n",
      "2021-01-25 11:54\n",
      "Hi All. Had a question about a possible discrepancy between user guide and autogenerated docs for LASSO Linear model. Auto-gen docs (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) says that:  ``` (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 ```  Is minimised.  However, the user guide seems to to imply that it's `||Xw - y||` rather than `||y-Xw||` (https://scikit-learn.org/stable/modules/linear_model.html#lasso).  `y-Xw` makes more sense to me. Am I reading something incorrectly, or is the user guide wrong?\n",
      "\n",
      "1\n",
      "5133\n",
      "5b4c9e4bd73408ce4fa10b88\n",
      "2021-01-25 12:57\n",
      "@JosephRedfern both are the same. ||-x|| = ||x||\n",
      "\n",
      "1\n",
      "5134\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-25 12:57\n",
      "I think the common way would be `y - y_hat = y - Xw`\n",
      "but they lead to the same\n",
      "as @jeremiedbb just mentioned :)\n",
      "\n",
      "3\n",
      "5135\n",
      "52f100f45e986b0712ef4def\n",
      "2021-01-25 16:04\n",
      "oh boy, what a brain fart! apologies, I should have thought before posting.\n",
      "\n",
      "1\n",
      "5136\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-27 14:23\n",
      "I am trying HistGradientBoostingClassifier for multiclass classification. It seems to stop too early. I.e. I get [22/200] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 4.36169, val loss: 2.27454, in 0.114s [23/200] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 5.42246, val loss: 2.60417, in 0.113s [24/200] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 5.19449, val loss: 3.30153, in 0.113s Fit 624 trees in 2.951 s, (19300 total leaves)\n",
      "you can get a much smaller loss using catboost for example\n",
      "but it is much faster than catboost for multiclass classification\n",
      "\n",
      "3\n",
      "5137\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-01-27 14:32\n",
      "hmm.. is multiclass classification meant to work yet with HistGradientBoostingClassifier ?\n",
      "\n",
      "1\n",
      "5138\n",
      "5e3f3d7cd73408ce4fd915a4\n",
      "2021-01-27 22:24\n",
      "I'm trying to modify the sklearn transformer interface for transformers that need to transform `X`, `y` and `sample_weight` together, i.e. the entire dataset. This is the signature I came up with that allows chaining these transformers with a `Pipeline`, I'm wondering if anyone has any better ideas? I really don't like having a `dummy` parameter.  ```python3     class DatasetTransformer(BaseEstimator, TransformerMixin):         def fit(self, data, dummy=None) -> \"DatasetTransformer\":             X, y, sample_weight = data             ...             return self          def transform(self, data):             X, y, sample_weight = data             ...             return (X, y, sample_weight) ```\n",
      "One alternative I see is to not have the `dummy` parameter, and instead specify `\"passthrough\"` as the last estimator in pipelines. I think this may be better? `dummy` is only there because it would be confusing to have a `y` parameter when `y` is part of `data`.\n",
      "\n",
      "2\n",
      "5139\n",
      "5fa98f30d73408ce4ff3a641\n",
      "2021-01-28 17:26\n",
      "Byo.ai an intelligent assistant to make people carbon neutral/positive. Anyone with experience with one or more general purpose programming languages including but not limited to: Python, Java, C/C++ (also Pytorch,) feel free to send your CV to work@byo.ai (equity only) - passion for the environment, clean technologies and artificial intelligence is a plus!\n",
      "\n",
      "1\n",
      "5140\n",
      "601604b6d73408ce4ffa348b\n",
      "2021-01-31 01:17\n",
      "Hi, is there any way to make it so that sklearn doesn't normalize the columns in PCA?\n",
      "\n",
      "1\n",
      "5141\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-31 21:13\n",
      "I don't think that we have a parameter to do that.\n",
      "\n",
      "1\n",
      "5142\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-31 21:16\n",
      "It would be weird to not normalize the columns since it is an assumption of PCA if I am not wrong\n",
      "\n",
      "1\n",
      "5143\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-31 21:28\n",
      "Not centering will make that you will get an intercept while transforming\n",
      "\n",
      "1\n",
      "5144\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-01-31 21:33\n",
      "Uhm now that I think about it, there is the TruncatedSVD\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
      "This should do the job\n",
      "\n",
      "3\n",
      "5145\n",
      "60190f016da037398460db58\n",
      "2021-02-02 10:06\n",
      "Hi! I have a quick question about the capabilities of Scikit-Learn's Pipeline (also asked in the other channel): Currently my data looks like the following 0 20 1 23 2 25 3 29 4 24 ... Where the index is a step in time, and the value is the value associated with that timestep. I am transforming it into 0 [20, 23, 25] [29, 24] 1 [29, 24, 24] [23, 22] 2 [23, 22, 26] [23, 25] ... By running a sort of window function over it. The index then is sample number, the first array is the input features, and the second array is the target.  If I input my data values as X into a pipeline, how to address the following: my custom transformer generates y from the input X, but the pipeline requires a y to begin with my custom transformer changes the length of X, which I think the pipeline also complains about.  Especially the latter is really important to me, as I want to be able to change the amount of features in X. Other options for example would be: 0 [20, 23] [25, 29] 1 [25, 29] [24, 29] ... or even 0 [20, 23, 25, 29] [24, 29] 1 [24, 29, 24, 24] [23, 22] etc.  Is there a standard solution for this, or should I write a wrapper for this? Thanks in advance!\n",
      "\n",
      "1\n",
      "5146\n",
      "541a528b163965c9bc2053de\n",
      "2021-02-02 10:10\n",
      "Our pipelines are unfortunately no meant to change the number of samples or the target. So I guess this kind of preprocessing will have to happen outside of the pipeline it-self...\n",
      "If the goal of the pipeline is to do parameter tuning for size of the time-window of the feature extraction preprocessor in a Grid/RandomizedSearchCV (for instance), then I think it's better to switch to a parameter tuner with a more flexible / less opinionated API such as https://optuna.org/ for instance.\n",
      "> Is there a specific reason why that is prevented from being changed?  Our transformers where never meant to change y. This is an early design decision that is really hard to change now (without breaking users code). And changing the number of samples without changing y is meaningless.\n",
      "\n",
      "3\n",
      "5147\n",
      "60190f016da037398460db58\n",
      "2021-02-02 10:13\n",
      "Is there a specific reason why that is prevented from being changed?\n",
      "Thank you!\n",
      "\n",
      "2\n",
      "5148\n",
      "60190f016da037398460db58\n",
      "2021-02-02 10:19\n",
      "That's a shame, because I can imagine resampling / different sampling strategies / generating additional samples with differing noise levels would be a very useful thing to put in a pipeline so it can be gridsearched.\n",
      "I'll have a look at optuna, thanks for your answers!\n",
      "\n",
      "2\n",
      "5149\n",
      "541a528b163965c9bc2053de\n",
      "2021-02-02 10:47\n",
      "But's really trick to get right, especially with the metrics. For instance when dealing with imbalanced classification problems, you want to resample at training time but not at test  validation / prediction / score time.  Imbalanced Learn has a custom pipeline and a custom API for resampling transformers with the fit_resample method: https://imbalanced-learn.org/  However this is not what you want for your use case: you want to perform the same transformation both at fit and predict time. It's hard to express all those use cases in a simple and intuitive unified API that would not led to users to shoot themselves in the foot. And there the backward compat constraints to take into account which makes it really hard to make our API evolve in that regard.\n",
      "@KylevdLangemheen would you mind reposting this question to github discussions: https://github.com/scikit-learn/scikit-learn/discussions   I think this would make it more googleable and linkable for others that have related issues.\n",
      "\n",
      "2\n",
      "5150\n",
      "60190f016da037398460db58\n",
      "2021-02-02 11:33\n",
      "Very understandable! I'm just sad I couldn't get an easy way out :P  I'll repost it there when I can, likely later today.\n",
      "\n",
      "1\n",
      "5151\n",
      "5d6e5defd73408ce4fc9e1a2\n",
      "2021-02-04 09:45\n",
      "Hello all\n",
      "I am new to this chatroom\n",
      " i have a question regarding dataset imbalance, can someone help me out ?\n",
      "\n",
      "3\n",
      "5152\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-04 10:11\n",
      "@SyedMuhamadYasir  You can freely ask your question and you might get an answer\n",
      "\n",
      "1\n",
      "5153\n",
      "5d6e5defd73408ce4fc9e1a2\n",
      "2021-02-04 10:12\n",
      "@glemaitre  thank you, i am going to ask right now\n",
      "\n",
      "1\n",
      "5154\n",
      "5d6e5defd73408ce4fc9e1a2\n",
      "2021-02-04 10:12\n",
      "I have a dataset which is for binary classification ( or at least we are approaching it from a binary classification perspective )  There are a total of 2.5 million rows, with label 0 belonging to around 220000 (2.2 million) rows and label 1 belonging to around 321000 (0.3 million) rows , there are around 45 features.  The imbalance approaches a ratio of around 1 : 7  My problem is very straightforward, even WITHOUT any data preprocessing if i try to classify the data  the classification algorithms, no matter what parameters are set, give around 99% in ALL performance metrics ( accuracy, precision, recall, f1 score etc )  This would probably suggest a bad case of overfitting but i am not sure, feel free to explain and add your opinion regarding what could be the reason  I tried to visualize the graph using TSNE and saw that the entire data is shaped like an ellipse and there is heavy overlap between both the labels. This means that (1) data is badly imbalanced (2) data is badly overlapped , i highly doubt i can use anomaly detection there as all the 'anomalies' (label 1) are sitting close with the 'normal' (label 0) data  any suggestions on how i should proceed ?\n",
      "\n",
      "1\n",
      "5155\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-04 10:18\n",
      "I am not sure that I would give to much weight with what you observe with TSNE.\n",
      "While accuracy will be boost for sure, the precision and recall will be good measure with imbalance problem.\n",
      "You can use the `balanced_accuracy_score` instead of `accuracy_score` as a baseline.\n",
      "I would say that one potential error would be to forget to set `pos_label` in precision/recall if it is not `1` (that does not seem to be the case).\n",
      "You can probably look at the entire confusion matrix to be sure that the stats seem correct\n",
      "Then if you still get good results by properly cross-validated your experiment, everything should be fine.\n",
      "\n",
      "6\n",
      "5156\n",
      "5d6e5defd73408ce4fc9e1a2\n",
      "2021-02-04 10:25\n",
      "thank you, i see some really good points in your answer than i can experiment with\n",
      "however, it is important to tell you the context of the problem\n",
      "so you can understand what i am trying to do exactly\n",
      "I am trying to do a Feature Selection / Feature Reduction task\n",
      "\n",
      "4\n",
      "5157\n",
      "5d6e5defd73408ce4fc9e1a2\n",
      "2021-02-04 10:27\n",
      "since the original dataset, without any preprocessing and with ALL features, gives near perfect results, it will annul the validity of using any type of Feature Reduction techniques\n",
      "i will try out what you said in your answer, but i thought that it would be better to let you know the entire context and the actual reason for why exactly we need 'bad' results before applying any feature reduction\n",
      "> It might still allow you to fit and predict faster and this probably what feature selection is best at.  that .. is actually a very good point, thanks !\n",
      "@glemaitre  Je vous remercie  :)\n",
      "\n",
      "4\n",
      "5158\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-04 10:28\n",
      "It might still allow you to fit and predict faster and this probably what feature selection is best at.\n",
      "\n",
      "1\n",
      "5159\n",
      "5b444ca2d73408ce4fa03e12\n",
      "2021-02-05 17:57\n",
      "hi, quick question, is there a preprocessor in sci-kit that allows me to split a feature into two?\n",
      "\n",
      "1\n",
      "5160\n",
      "5b444ca2d73408ce4fa03e12\n",
      "2021-02-05 18:18\n",
      "ok, there's column transformer, I can use that\n",
      "\n",
      "1\n",
      "5161\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-02-06 11:28\n",
      "has anyone started a PR on dirichlet calibration of probabilities?\n",
      "I couldn't find it if they have\n",
      "as in this paper https://arxiv.org/abs/1910.12656\n",
      "\n",
      "3\n",
      "5162\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-02-06 11:50\n",
      "That's rather a new paper, and I'm not sure if it passes our inclusion criteria.\n",
      "\n",
      "1\n",
      "5163\n",
      "5ee61de0d73408ce4fe6d981\n",
      "2021-02-06 17:18\n",
      "I am getting some unrelated errors (azure pipeline stack trace) for a PR I just pushed. Anyone who can check and let me know how I can fix? https://github.com/scikit-learn/scikit-learn/pull/19387\n",
      "\n",
      "1\n",
      "5164\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-02-07 08:09\n",
      "@adrinjalali  yes. I was just wondering if anyone thought it was interesting.\n",
      "\n",
      "1\n",
      "5165\n",
      "5e43c3d3d73408ce4fd969da\n",
      "2021-02-07 17:25\n",
      "Why column transformer convert datatype to objects after calling fit_transform?\n",
      "\n",
      "1\n",
      "5166\n",
      "matrix-benny:michael-enders.com\n",
      "2021-02-09 15:57\n",
      "I have a general question: If for my dataset a kneighbor classifier works well (compared to e.g. SVC and Random Forest),  are there other classifiers that might also work equally well?\n",
      "\n",
      "1\n",
      "5167\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-09 17:04\n",
      "@benny Stuff based on distances then\n",
      "\n",
      "1\n",
      "5168\n",
      "matrix-benny:michael-enders.com\n",
      "2021-02-09 18:33\n",
      "can you give some examples?\n",
      "\n",
      "1\n",
      "5169\n",
      "60223b726da0373984618209\n",
      "2021-02-09 23:20\n",
      "Hi everyone, I am not certain if this is the right place to ask. I am a first-time contributor. I love the library and it has helped me immensely in my studies so far. I was hoping to work on this issue as my first issue: https://github.com/scikit-learn/scikit-learn/issues/18338  As far as I can understand, this issue requires that the documentation be updated, does that indicate the docstring within the function definition only, or is that referring to another piece of documentation?  One of the commentators on the issue also mentions ensuring there are tests that break if this documentation doesn't exist, how do I go about doing that effectively?\n",
      "\n",
      "1\n",
      "5170\n",
      "60223b726da0373984618209\n",
      "2021-02-10 00:21\n",
      "> I have a general question: If for my dataset a kneighbor classifier works well (compared to e.g. SVC and Random Forest),  are there other classifiers that might also work equally well?  I think it will depend on the data set.  It also depends on how you are pre-processing your data. So kinda hard to say without knowing more.\n",
      "\n",
      "1\n",
      "5171\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-02-11 10:07\n",
      "when I do OrdinalEncoder on my matrix X how can I make the mapping the same for each column?\n",
      "currently it is different if there is one new value in a column that doesn't occur in another column\n",
      "\n",
      "2\n",
      "5172\n",
      "5479972adb8155e6700d9370\n",
      "2021-02-11 11:03\n",
      "Hello Happy scikit-learners ! I need some help please. I want to serve an onnx model.  Input = 144 columns ( medical records, some categoricals, some  not ).  Output = classification.  Pipeline = StandardScaler + LabelEncoder + LightGBM.  I am stuck with the LabelEncoder. Any example of such configuration somewhere ? Google was not my friend. I was able to produce an onnx model when bypassing the LabelEncoder... but I need it and want to avoid 1HE because LightGBM performs much better without 1HE.  Anyone ?\n",
      "\n",
      "1\n",
      "5173\n",
      "matrix-rthy:matrix.org\n",
      "2021-02-11 13:22\n",
      " @citron You probably want OneHotEncoder not the LabelEncoder\n",
      "Also tree based models it's better to use OrdinalEncoder instead for categorical features\n",
      "\n",
      "2\n",
      "5174\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-02-11 15:14\n",
      "> Also tree based models it's better to use OrdinalEncoder instead for categorical features  I'm not sure that's true, using OE will make the trees treat categories as ordered values, but they're not. Native categorical support (as in LightGBM) properly treats categories as un-ordered and can yield the same splits with less tree depth\n",
      "\n",
      "1\n",
      "5175\n",
      "matrix-rthy:matrix.org\n",
      "2021-02-11 15:28\n",
      "Yes, you are right. I guess I'm too used to scikit-learn tree based models not having native categorical support )\n",
      "\n",
      "1\n",
      "5176\n",
      "541a528b163965c9bc2053de\n",
      "2021-02-11 16:38\n",
      "I agree with @NicolasHug in theory, but in practice the difference with `OrdinalEncoder` (with tuned hyperparams) is typically negligible ;)\n",
      "\n",
      "1\n",
      "5177\n",
      "541a528b163965c9bc2053de\n",
      "2021-02-11 16:39\n",
      "@citron Using `OrdinalEncoder` is probably the pragmatic solution. `OneHotEncoder` is only efficient if you use sparse output which are currently not supported by ONNX as far as I know.\n",
      "\n",
      "1\n",
      "5178\n",
      "5de8bd5ad73408ce4fd32399\n",
      "2021-02-11 16:46\n",
      "@citron: what's the issue with LabelEncoder and ONNX? (I'm the main author of sklearn-onnx).\n",
      "\n",
      "1\n",
      "5179\n",
      "541a528b163965c9bc2053de\n",
      "2021-02-11 16:46\n",
      "@citron also you said \"Pipeline = StandardScaler + LabelEncoder + LightGBM.\" but I assume you use  a column transformer to separate to only scale the numerical features and encode the categorical feature separately: https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data  BTW, StandardScaling the numerical features if often useless for tree-based models in general, and even more so for implementations such as LightGBM than bin the features.\n",
      "\n",
      "1\n",
      "5180\n",
      "5479972adb8155e6700d9370\n",
      "2021-02-11 17:02\n",
      "Bonjour @xadupre , @ogrisel, @rthy:matrix.org, @NicolasHug . Yes I do use a ColumnTransformer. Maybe I should better express my needs.  The training set is made of 300000 rows.  Colums types are either floating points, integers ( and sadly Pandas does not provide the R Dataframe handling of N/A ), booleans, categories or list of categories. For instance, some category columns may have 2 or 10 numerical categories, some only have \"string\" categories, some have a list of medicaments or a list of pathologies.  I have tried plenty of frameworks and among them, lightGBM was the best. Now, as I need to export the model and the pipeline in ONNX/ONNX-ML format, I need to wrap lightGBM in something to keep the pipeline around.\n",
      "@ogrisel Yes, no problem with pure int columns.\n",
      "\n",
      "2\n",
      "5181\n",
      "541a528b163965c9bc2053de\n",
      "2021-02-11 17:05\n",
      "pandas 1.0 and later has support for explicit missing values in integer columns: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
      "scikit-learn however will convert this to a float anyway (but no big deal).\n",
      "\n",
      "2\n",
      "5182\n",
      "541a528b163965c9bc2053de\n",
      "2021-02-11 17:08\n",
      "For the categorical columns, try to use OrdinalEncoder. In 0.24+ we have better support for unknown categories at test time:  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html  Although I am not sure that sklearn-onnx has replicated that feature yet.\n",
      "\n",
      "2\n",
      "5183\n",
      "541a528b163965c9bc2053de\n",
      "2021-02-11 17:09\n",
      "If you have specific problems exporting a pipeline with OrdinalEncoder to onnx, better report the exact error message with a simple reproduction case to https://github.com/onnx/sklearn-onnx\n",
      "\n",
      "1\n",
      "5184\n",
      "5de8bd5ad73408ce4fd32399\n",
      "2021-02-11 17:22\n",
      "I wrote this example about converting a pipeline including a lightgbm model in a scikit-learn pipeline: http://onnx.ai/sklearn-onnx/auto_tutorial/plot_gexternal_lightgbm.html.\n",
      "\n",
      "1\n",
      "5185\n",
      "5479972adb8155e6700d9370\n",
      "2021-02-11 17:36\n",
      "@xadupre Thanks! The binder link at the end of the page has a problem. In fact, that's the example I started with. Works fine without labelEncoder.\n",
      "I forgot to tell an important thing : I do use FLAML to select the best hyperparameters and thus the best model.\n",
      "\n",
      "2\n",
      "5186\n",
      "5de8bd5ad73408ce4fd32399\n",
      "2021-02-11 18:11\n",
      "I'll investigate the issue with LabelEncoder then. What is the error you get?\n",
      "\n",
      "1\n",
      "5187\n",
      "5571fe1015522ed4b3e17d90\n",
      "2021-02-12 09:01\n",
      "I think it would be a good idea to encourage creating a Github Discussion (rather than gitter) for anything else than simple questions/answers: https://github.com/scikit-learn/scikit-learn/discussions/new. gitter is not properly indexed by search engines so it is not a great use of time for people who answer questions.\n",
      "I agree that \"simple qestion/answer\" does not have a very-well defined boundary but in the case of @citron's questions I think we have crossed this boundary a long time ago ...\n",
      "\n",
      "2\n",
      "5188\n",
      "5479972adb8155e6700d9370\n",
      "2021-02-12 09:16\n",
      "@lesteve I understand and agree.\n",
      "\n",
      "1\n",
      "5189\n",
      "5571fe1015522ed4b3e17d90\n",
      "2021-02-12 10:40\n",
      "@citron then if you find the time maybe create a Github Discussion and post the link in the gitter so that the discussion can continue in the Github Discussion?\n",
      "\n",
      "1\n",
      "5190\n",
      "6026fa066da037398461e161\n",
      "2021-02-12 22:04\n",
      "Hello guys! Me and my friends are looking to tackle some open issues on scikit-learn soon. We're very new so I would love a high level overview of the architecture.\n",
      "Can anyone help or point to some resources?\n",
      "\n",
      "2\n",
      "5191\n",
      "matrix-rthy:matrix.org\n",
      "2021-02-13 16:29\n",
      "Have a look at https://scikit-learn.org/stable/developers/contributing.html for a getting starting guide.\n",
      "\n",
      "1\n",
      "5192\n",
      "5479972adb8155e6700d9370\n",
      "2021-02-16 18:17\n",
      "Hello ( I am back and I will try not to flood the your screen )\n",
      "\n",
      "1\n",
      "5193\n",
      "5479972adb8155e6700d9370\n",
      "2021-02-16 18:25\n",
      "Using Scikit-learn 0.24.1 and sklearn-onnx 1.7.0, I try to export a pipeline embedding an HistGradientBoostingClassifier. The data contains only StandardScaled floating point features.  convert_sklearn prompts an error : 'numpy.bool' object has no attribute 'encode'StringTensorTypeStringTensorType Any idea please ?\n",
      "\n",
      "1\n",
      "5194\n",
      "5cdeaebed73408ce4fc08df3\n",
      "2021-02-22 08:35\n",
      "Hello,  I'm trygin to use `SimpleImputer(strategy='most_frequent')` in Pipeline on dataframe with ~ 1.5 M samples but I take a lot of time  Is it normal ? If so, are there some alternatives to solve this issue ?  ``` def vectorizer_df(input_data, categorical_cols, numerical_cols):  \tcategorical_pipe = Pipeline([ \t    ('imputer', SimpleImputer(strategy='most_frequent')) \t])  \tnumerical_pipe = Pipeline([ \t    ('imputer', SimpleImputer(strategy='median')), \t    ('bucketizer', KBinsDiscretizer(n_bins=10, strategy='uniform', encode='ordinal'))  # ordinal \t])  \tpreprocessing = ColumnTransformer( \t    [('cat', categorical_pipe, categorical_cols), \t     ('num', numerical_pipe, numerical_cols) \t     ])  \tvectorizer_pipeline = Pipeline([ \t    ('vectorize', preprocessing) \t])  \treturn vectorizer_pipeline.fit_transform(input_data) ```  Thanks\n",
      "\n",
      "1\n",
      "5195\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-22 10:42\n",
      "@razou which version of scikit-learn are you using?\n",
      "We merged the following improvement in 0.24 -> https://github.com/scikit-learn/scikit-learn/pull/18987\n",
      "that make it efficient to work with string while it was not really possible before\n",
      "because it was too slow\n",
      "\n",
      "4\n",
      "5196\n",
      "5cdeaebed73408ce4fc08df3\n",
      "2021-02-22 11:00\n",
      "Thanks @glemaitre  I'm using ``` scikit-learn==0.22.2.post1 sklearn-crfsuite==0.3.6 ```\n",
      "\n",
      "1\n",
      "5197\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-22 11:03\n",
      "yep so this should be the reason. You can update to 0.24 via conda-forge or PyPI and it should work better\n",
      "\n",
      "1\n",
      "5198\n",
      "5cdeaebed73408ce4fc08df3\n",
      "2021-02-22 11:20\n",
      "Thanks @glemaitre  for your answers (y)\n",
      "\n",
      "1\n",
      "5199\n",
      "5f7ac1dcd73408ce4ff0af34\n",
      "2021-02-23 07:46\n",
      "Could anyone help me with this error? https://www.reddit.com/r/learnpython/comments/ibk04a/linalgerror_svd_did_not_converge_in_linear_least/\n",
      "\n",
      "1\n",
      "5200\n",
      "matrix-benny:michael-enders.com\n",
      "2021-02-24 09:24\n",
      "Hello, can you tell me why GridSearchCV .best_score_ is worse than when I evaluate the same dataset with .score() ?\n",
      "shouldn't those be the same?\n",
      "\n",
      "2\n",
      "5201\n",
      "matrix-benny:michael-enders.com\n",
      "2021-02-24 09:32\n",
      "or is it because .best_score_ is only evaluated on the test-cross validation split?\n",
      "\n",
      "1\n",
      "5202\n",
      "6018a9a26da037398460d5ca\n",
      "2021-02-25 13:08\n",
      "Hello, when I run pytest -Werror::RuntimeWarning  sklearn/ensemble/tests/test_iforest.py I get an 'ImportError while loading confest' error.  How can I bypass this?\n",
      "\n",
      "1\n",
      "5203\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-25 14:12\n",
      "Could you provide the traceback @icky254\n",
      "\n",
      "1\n",
      "5204\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-25 14:15\n",
      "@benny:michael-enders.com By checking the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), `grid_search.best_score_` is mean cross-validated score of the best_estimator.\n",
      "So the `best_estimator.score` is called on each fold (supposing that you do `KFold`) and averaged.\n",
      "\n",
      "2\n",
      "5205\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-25 14:20\n",
      "> when I evaluate the same dataset with .score()  It depends what you mean. You might do something wrong here. Calling a single time `score` will not give you an estimate of the variability of your model. If you reuse `best_estimator_` (with best parameter set), you need to evaluate on some left-out data . You might want to look the following example regarding why you need to nest grid-search and cross-validation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py\n",
      "\n",
      "1\n",
      "5206\n",
      "6018a9a26da037398460d5ca\n",
      "2021-02-25 16:12\n",
      "[![Screenshot from 2021-02-25 19-06-35.png](https://files.gitter.im/541a528c163965c9bc2053e1/3Pzi/thumb/Screenshot-from-2021-02-25-19-06-35.png)](https://files.gitter.im/541a528c163965c9bc2053e1/3Pzi/Screenshot-from-2021-02-25-19-06-35.png)\n",
      "@glemaitre\n",
      "\n",
      "2\n",
      "5207\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-02-25 18:38\n",
      "As the message mentioned, it seems that scikit-learn was not build\n",
      "Be sure to have activated the environment where you installed and built scikit-learn\n",
      "FYI: https://scikit-learn.org/stable/developers/advanced_installation.html#building-from-source\n",
      "basically to install and build: `pip install --verbose --no-build-isolation --editable .`\n",
      "could work if the previous message steps where already done\n",
      "\n",
      "5\n",
      "5208\n",
      "53d67ca7107e137846ba83ca\n",
      "2021-02-25 22:51\n",
      "Apologies for the PSA style post, but in a collaboration with Microsoft, we just published a blog post on using compilation techniques to accelerate SKLearn models for production, it's also another route to run models on GPUs (and potentially other accelerators) as well! Check it out if interested: https://medium.com/octoml/compiling-classical-ml-for-up-to-30x-performance-gains-and-hardware-portability-2aef760af694\n",
      "\n",
      "1\n",
      "5209\n",
      "6018a9a26da037398460d5ca\n",
      "2021-02-26 04:50\n",
      "@glemaitre, solved. Thank you.\n",
      "\n",
      "1\n",
      "5210\n",
      "matrix-rthy:matrix.org\n",
      "2021-02-26 08:58\n",
      "@binarybana: Interesting, you may want to send it on the mailing list (or in github discussions). I don't think that many people people check gitter..\n",
      "\n",
      "1\n",
      "5211\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-02-26 10:40\n",
      "@binarybana really nice!\n",
      "\n",
      "1\n",
      "5212\n",
      "6030f5b06da0373984627f56\n",
      "2021-02-27 05:05\n",
      "Hey there!  I am trying to develop a new python module for video chatting with a bot. (You may also collaborate)  Repo: https://github.com/avaish1409/VideoChatBot/ Gitter: https://gitter.im/VideoChatBot/community  Downloads: 429 (in first 2 days of launch)  ''' pip install VideoChatBot ''' or ''' pip install https://files.pythonhosted.org/packages/5b/cc/9dbb790525fe3daa8f0822e60eec38dfea8af5e33af0334dc66b4a022ac4/VideoChatBot-0.0.2.tar.gz '''  Do contribute on github, let's build it together!  Plz star the repository if you like it .. you can also contribute on github <unconvertable>\n",
      "\n",
      "1\n",
      "5213\n",
      "6043eb366da037398465c260\n",
      "2021-03-06 20:52\n",
      "Hello, everyone. I am new here. I have found that the user guide of sklearn 0.24 is well-structured. Is there any way for me to download a pdf version of it? I could only find those of the older versions. Thank you.\n",
      "\n",
      "1\n",
      "5214\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-03-07 10:00\n",
      "@lester1027 I think we stopped generating pdf versions and simply provide the html files, so the docs look like just as on the website. Generating pdfs involved Latex and it was difficult to maintain (random failures every now and then, etc)\n",
      "if you really want pdfs you can try converting the docs to pdf with pandoc\n",
      "\n",
      "2\n",
      "5215\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-03-15 09:35\n",
      "I have one hot encoded feature vectors which I am using for multiclass classification.  If in my training set there is a feature which is always 0, what happens in testing when it comes across one that is a 1 for the feature?\n",
      "\n",
      "1\n",
      "5216\n",
      "matrix-rthy:matrix.org\n",
      "2021-03-15 11:33\n",
      "@lesshaste: By default OHE would error. You would need to set `handle_unknown='ignore'` to ignore it.\n",
      "\n",
      "1\n",
      "5217\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-03-15 13:58\n",
      "@rthy:matrix.org thanks.  Do you know if that would be the same for logistic regression for example?\n",
      "can sklearn.metrics.pairwise be made to work for hamming or levenshtein distance?\n",
      "\n",
      "2\n",
      "5218\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-03-24 19:46\n",
      "what are good options for supervised categorical encoding when the target is also categorical?  target encoding looks attractive but doesn't really make sense when the target is categorical\n",
      "\n",
      "1\n",
      "5219\n",
      "5afe987cd73408ce4f99ce3b\n",
      "2021-03-29 16:35\n",
      "Hi everyone, I am a developer who's trying to test multiple models in different frameworks. I just want to know if this idea makes sense - I want to create a single sklearn pipeline script for testing various models (all mapped to the sklearn-keras interface or using skorch). But from what I understand is that models are not just plain classifiers or regressors. The models are a combination and sklearn pipeline doesn't apparently support it. Is my understanding correct? Is this a futile effort? Can someone please help me out with this issue. Thank you. If there's an alternative, please let me know. I'm talking about object detection models. I really like the pipeline method/interface and would like to extend my models to match the same .fit, .predict interface\n",
      "\n",
      "1\n",
      "5220\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-03-31 09:53\n",
      "HistGradientBoostingClassifier seems to have no n_jobs argument.  Is there any way set the number of threads/cores?\n",
      "\n",
      "1\n",
      "5221\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-03-31 16:31\n",
      "You should use the  OMP_NUM_THREADS env variable https://scikit-learn.org/stable/computing/parallelism.html#openmp-based-parallelism\n",
      "\n",
      "1\n",
      "5222\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-03-31 16:35\n",
      "@NicolasHug  thank you. Is anyone working on adding n_jobs for this classifier?\n",
      "it would make it inline with the other classifiers\n",
      "and can it be done in the script itself?\n",
      "\n",
      "3\n",
      "5223\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-03-31 18:01\n",
      "it's been discussed but we ended up staying with the status quo https://github.com/scikit-learn/scikit-learn/issues/14265\n",
      "\n",
      "1\n",
      "5224\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-04-01 07:15\n",
      "@NicolasHug  that is surprising. I normally agree with all the decisions of scikit learn devs\n",
      "\n",
      "1\n",
      "5225\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-04-01 07:17\n",
      "I have two questions about HistGradientBoostingClassifier.  a) When using early stopping do you end up with the \"best model\" according to the validation loss or the most recent one after it stops?\n",
      "b) Is the validation set chosen by  HistGradientBoostingClassifier chosen at random and is it the same set for every iteration of the training?\n",
      "maybe these should be asked on github as an issue?\n",
      "\n",
      "3\n",
      "5226\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-04-01 08:33\n",
      "a) there's no notion of best model. early stopping stops the training process  if the score hasn't improved by more than `tol` in the last  `n_iter_no_change` iterations. The score can be the loss or an arbitrary scorer and it can be computed on the training set or on the validation set b) yes and yes\n",
      "\n",
      "1\n",
      "5227\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-04-03 07:08\n",
      "Thank you.  Maybe best model couid be a good addition?\n",
      "\n",
      "1\n",
      "5228\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-04-03 08:46\n",
      "I'm not sure what you mean by best model. There's no notion of best model, only one model is built. If you mean \"model with the lowest training loss\" that's basically the model at the last iteration,  under the assumption that the training loss is always supposed to decrease (unless your learning rate becomes too high). If you mean \"model with the lowest training loss that doesn't make the validation loss go up\", that's what early stopping is supposed to give you (and it's preferable to the former)\n",
      "\n",
      "1\n",
      "5229\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-04-10 13:05\n",
      "@NicolasHug  let's say the latter example you gave. The problem is that with early stopping you wait some number of iteration before deciding to stop. So the final iteration is not the best. That's why catboost for example has a use_best parameter.\n",
      "It is common in early stopping for the final valudation loss to be higher than the loss a few epochs before.  How long you wait to see if the loss will start going down again is sometimes called \"patience\" . I think that's what pytorch lightning calls it\n",
      "\n",
      "2\n",
      "5230\n",
      "6075a37e6da03739847a0d9b\n",
      "2021-04-13 13:59\n",
      "does using the fit function on a fitted model replace the fitted model, or update it?\n",
      "\n",
      "1\n",
      "5231\n",
      "6075a37e6da03739847a0d9b\n",
      "2021-04-13 14:01\n",
      "I'm trying to use a Lasso in a machine learning context, and I want to keep updating it with each test run I do\n",
      "obviously, I could in theory, take the model, and train it with the results of the particular test run, then merge the coefficents with the last model myself, but it would be better if I could avoid that\n",
      "I will use that in the project my team is doing. Thanks for helping\n",
      "\n",
      "3\n",
      "5232\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-04-13 15:05\n",
      "`fit` make a full training from scratch\n",
      "`partial_fit` is doing an update\n",
      "\n",
      "2\n",
      "5233\n",
      "6075a37e6da03739847a0d9b\n",
      "2021-04-13 15:10\n",
      "@glemaitre thank you.  what kinds of models is partial_fit available for?\n",
      "hrm... it seems like all of the ones with that method only are for classification, not for regressed output.\n",
      "\n",
      "2\n",
      "5234\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-04-13 15:16\n",
      "SGD estimator is one of them\n",
      "\n",
      "2\n",
      "5235\n",
      "603cd4cc6da0373984649165\n",
      "2021-04-19 07:03\n",
      "I wrote a [short blog post](https://www.bodyworkml.com/posts/scikit-learn-meet-production) that might be of interest to the community - deploying Scikit-Learn models to Kuberentes using [Bodywork](https://github.com/bodywork-ml/bodywork-core) (an open source deployment tool that I have developed).\n",
      "\n",
      "1\n",
      "5236\n",
      "54a6f79fdb8155e6700e5114\n",
      "2021-04-28 10:54\n",
      "I would like to ask how to join two pereprocessors I did saved in two separate files. I have one file with a model and another with prerpocessor (doing average and filling NaN boxes)  then another file with a model (same estimator) and forth file is preprocessor for second model? I would like to merge these four files into two (one joint model and one joint estimator).\n",
      "\n",
      "1\n",
      "5237\n",
      "matrix-urosn:matrix.org\n",
      "2021-04-28 16:15\n",
      "I have transformer1.file, model1.file, transformer2.file and model2.file (same estimator in model1 and model2). I would like to have tranformer_composite.file and model_composite.file.\n",
      "\n",
      "1\n",
      "5238\n",
      "608ace656da03739847b4a14\n",
      "2021-04-29 18:30\n",
      "Yo\n",
      "\n",
      "1\n",
      "5239\n",
      "6094a87f6da03739847c04de\n",
      "2021-05-07 02:45\n",
      "Hi everyone, I'm a graduate student at Cornell and I had a paper (https://dl.acm.org/doi/abs/10.1145/3429445) published a while ago in correcting the bias of feature importance in tree-based methods. Impurity-based feature importances can be misleading for high cardinality features (many unique values), which is already noted in the docstring of feature_importances_ in RandomForest. I just opened a new pull request #20058 to implement a new feature importance measurement based on out-of-bag samples, which is guaranteed to remove this bias.  I think this feature is going to be useful for scikit-learn users. Any comments or suggestions will be helpful!\n",
      "\n",
      "1\n",
      "5240\n",
      "56d028dbe610378809c39bd5\n",
      "2021-05-11 12:00\n",
      "Hi! What week scheduled for release scikit-learn 1.0?\n",
      "\n",
      "1\n",
      "5241\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-05-11 13:09\n",
      "There is no specific week scheduled @PetrovKP , but we try to release every 6 months  and the previous one was released in december\n",
      "\n",
      "1\n",
      "5242\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-05-11 13:11\n",
      "So perfectly June but I think that for 1.0 we want a couple of feature to be inside the release so we might be delayed.\n",
      "\n",
      "1\n",
      "5243\n",
      "609c828b6da03739847c7c73\n",
      "2021-05-13 17:01\n",
      "Hi, my name is Zoe Prieto. I am currently working on neutron and photon transport problems. I have some questions and maybe one of you can help me.\n",
      "\n",
      "1\n",
      "5244\n",
      "matrix-rthy:matrix.org\n",
      "2021-05-13 17:04\n",
      "Sure, don't hesitate to write them here.\n",
      "\n",
      "1\n",
      "5245\n",
      "609c828b6da03739847c7c73\n",
      "2021-05-13 17:12\n",
      "Thanks! I have a list of particles with their caracteristics (position, direction, energy and stadistic weight). This variables are correlated. I want to fit that curves and later sample new particles. I want to know how scikit-learn keep the correlation. I'm sorry if it is a beginner question. And thanks again.\n",
      "\n",
      "1\n",
      "5246\n",
      "matrix-rthy:matrix.org\n",
      "2021-05-14 13:19\n",
      "Well you need to define what are your feature variables and the target variable. So for instance you could try to predict the position from all the other variables. Correlations would be taken into account depending on the model. So for instance if your model is linear the target would be a linear combination of the features. If you do have a known analytical relation between your variables it might be easier and more reliable to use scipy.optimize or scipy.odr to find the coefficients you would like to learn though.\n",
      "\n",
      "1\n",
      "5247\n",
      "609c828b6da03739847c7c73\n",
      "2021-05-14 20:31\n",
      "Thanks for your answer, I forgot to mention that I fit my data with KDE and I sample new particles from this model. Is this model keeping the correlation between the different variables? Or it assumes the variables are independent from each other? Thanks again!\n",
      "\n",
      "1\n",
      "5248\n",
      "6018a9a26da037398460d5ca\n",
      "2021-05-16 10:46\n",
      "Hi everyone, I'm trying to create a streamlit app but face the following message when I run `streamlit run <file.py>`: `Make this Notebook Trusted to load map: File -> Trust Notebook`. I Googled this issue, and even after making Chrome my default browser, nothing changes. Please help.\n",
      "\n",
      "1\n",
      "5249\n",
      "matrix-nyanpasu:matrix.org\n",
      "2021-05-17 11:37\n",
      "Hello! in `sklearn.decomposition.PCA`, how do I tell it which column represents the label?  For example, I have a dataframe with the following columns:  `feature_0 feature_1 feature_2 label`  How do I tell PCA that `label` is the dependent variable?\n",
      "\n",
      "1\n",
      "5250\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-05-17 14:32\n",
      "@nyanpasu:matrix.org you don't, PCA is unsupervised and doesn't take the labels as input, only the features.\n",
      "\n",
      "1\n",
      "5251\n",
      "5cdeaebed73408ce4fc08df3\n",
      "2021-05-19 10:04\n",
      "Hello Is the `accuracy_at_k` (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html#sklearn.metrics.top_k_accuracy_score) an implementation of the `hit ratio at k`(https://www.researchgate.net/publication/344486356_Hit_ratio_An_Evaluation_Metric_for_Hashtag_Recommendation) Thanks\n",
      "\n",
      "1\n",
      "5252\n",
      "55cbc84e0fc9f982bead32a4\n",
      "2021-06-03 14:43\n",
      "Hi folks, I am a master's student in CS and I have a question for you.  I am working on a multi-class text classification problem, and I am using scikit-learn to implement my solution. I want to predict for a paragraph x if x belongs to one out of seven categories of information. I already implemented my solution using your library, but I am not confident if the steps I am following are correct or not, or if I am missing something. Could you please take a look at the image below and give your opinion? If this is not the right place for this kind of question, please let me know. Thank you in advance for your contribution!  ![Image](https://i.imgur.com/ZSr7jAT.png)\n",
      "\n",
      "1\n",
      "5253\n",
      "60be9c186da03739847e6279\n",
      "2021-06-07 23:45\n",
      "Hi, I want to start working on the Sci-kit learn bug fixes. Anyone who is already working can I team up with you?\n",
      "\n",
      "1\n",
      "5254\n",
      "60c1c1146da03739847e93c1\n",
      "2021-06-11 07:27\n",
      "Hi all! We're working on a generic implementation of a discrete time survival model for random forests. Similar to [this](https://link.springer.com/article/10.1007/s10618-020-00682-z) and [this](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0863-0). Basically, the idea is to split on hazard curves which are a bit like the class probabilities of regular classification random forests but then stratified per duration since inception of an observation. We want to use scikit-learn for a base. Is anyone here familiar with the random forest code? Also tips for a good PR are very welcome.\n",
      "\n",
      "1\n",
      "5255\n",
      "matrix-um_duaa:matrix.org\n",
      "2021-06-13 17:31\n",
      "hi\n",
      "\n",
      "1\n",
      "5256\n",
      "60c644cd6da03739847ecce2\n",
      "2021-06-13 17:49\n",
      "I have only one question, please!!!\n",
      "\n",
      "1\n",
      "5257\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-06-14 10:49\n",
      "What would people recommend for clustering strings (e.g. english words) of the same length?\n",
      "\n",
      "1\n",
      "5258\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-06-14 10:57\n",
      "or is this better off at github discuss?\n",
      "\n",
      "1\n",
      "5259\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-06-14 11:05\n",
      "It really depends on the kind of data that you have. If you have a corpus of documents LDA would be one way to get cluster/topics https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html You could also try pre-trained embeddings like word2vec and the likes  why do they have to be of the same length?\n",
      "\n",
      "1\n",
      "5260\n",
      "5f1bb183d73408ce4fea7ec7\n",
      "2021-06-17 01:45\n",
      "Hello ... Greetings to all..!!! I will participate in the Sprint on Saturday June 26..!!!\n",
      "\n",
      "1\n",
      "5261\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-06-17 10:28\n",
      "Welcome @asnramos !\n",
      "\n",
      "1\n",
      "5262\n",
      "60ce79cf6da03739847f4458\n",
      "2021-06-19 23:15\n",
      "Hello all . I'm also participating in the sprint next saturday,i'm  excited to be able to help checking and fixing an issue!\n",
      "\n",
      "1\n",
      "5263\n",
      "608f617d6da03739847b9aac\n",
      "2021-06-20 08:23\n",
      "Hello, how can I join the sprint?\n",
      "\n",
      "1\n",
      "5264\n",
      "60d1a8256da03739847f6fe8\n",
      "2021-06-22 09:18\n",
      "@um_duaa123_twitter sure ask\n",
      "\n",
      "1\n",
      "5265\n",
      "608f617d6da03739847b9aac\n",
      "2021-06-22 13:15\n",
      "Thanks\n",
      "\n",
      "1\n",
      "5266\n",
      "575b0eccc2f0db084a1d3a41\n",
      "2021-07-02 12:51\n",
      "why isn't the website working?\n",
      "\n",
      "1\n",
      "5267\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-07-02 12:59\n",
      "works for me\n",
      "\n",
      "1\n",
      "5268\n",
      "575b0eccc2f0db084a1d3a41\n",
      "2021-07-03 05:00\n",
      "Now it also works for me. Had tried with two different networks yesterday... didn't work that time..\n",
      "anyway, I wanted to ask what version of LAPACK (libblas.so) does sklearn use (assuming it uses it. If not, what blas library is used)?\n",
      "\n",
      "2\n",
      "5269\n",
      "564789be16b6c7089cbab8b7\n",
      "2021-07-03 16:41\n",
      "If I have a neural network classifier I can easily simulate data from the probability distribution implied by the classifier. Can this be done for any of the classifiers in scikit learn?\n",
      "\n",
      "1\n",
      "5270\n",
      "575b0eccc2f0db084a1d3a41\n",
      "2021-07-04 04:05\n",
      "scikit-learn custom compilation: Is it possible to pass custom gcc flags during scikit-learn build as described here  https://scikit-learn.org/stable/developers/advanced_installation.html\n",
      "the documentation uses pip install. But since the pyx files get compiled in a C file first before finally compiled into a SO file, I wondered if it was possible to pass custom gcc flags in the intermediate stage\n",
      "\n",
      "2\n",
      "5271\n",
      "575b0eccc2f0db084a1d3a41\n",
      "2021-07-05 06:59\n",
      "Is there any option to build scikit-learn with DEBUG symbols?\n",
      "\n",
      "1\n",
      "5272\n",
      "matrix-rthy:matrix.org\n",
      "2021-07-05 11:55\n",
      "Yes, you can pass `CFLAGS` env variable https://stackoverflow.com/a/10867041/1791279\n",
      "\n",
      "1\n",
      "5273\n",
      "575b0eccc2f0db084a1d3a41\n",
      "2021-07-05 12:23\n",
      "thanks @rthy:matrix.org\n",
      "\n",
      "1\n",
      "5274\n",
      "5baebfaed73408ce4fa9bd25\n",
      "2021-07-07 19:37\n",
      "Hello, I am wondering why this PR (https://github.com/scikit-learn/scikit-learn/pull/18758) doesn't show up at the top here: https://github.com/scikit-learn/scikit-learn/pulls  Is it because I had submitted it a long time ago, but my recent changes are considered updates?\n",
      "\n",
      "1\n",
      "5275\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-07-07 19:38\n",
      "I think that PRs are ordered by PR numbers by default in github\n",
      "\n",
      "2\n",
      "5276\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-07-07 19:44\n",
      "However, you have the option of \"Sort by recently updated\". This would actually be a good default while reviewing :)\n",
      "\n",
      "1\n",
      "5277\n",
      "5baebfaed73408ce4fa9bd25\n",
      "2021-07-07 20:11\n",
      "Yes, that's what I was looking for (and expecting as a default). Thanks!\n",
      "\n",
      "1\n",
      "5278\n",
      "matrix-rthy:matrix.org\n",
      "2021-07-08 20:31\n",
      "The \"Refined GitHub\" browser extension makes it the default among other improvements.\n",
      "\n",
      "1\n",
      "5279\n",
      "575b0eccc2f0db084a1d3a41\n",
      "2021-07-09 11:58\n",
      "I see that the binary_tree.pxi passes the value num_samples in the _recursive_build() procedure as an argument. The num_samples is calculated using self.data_arr.shape[0] whereas _recursive_build() expects an ITYPE_t argument. ITYPE_t is defined as np.intp_t which I assume is only 32 bit signed integer. So how is the binary tree built in cases when n_samples is greater than this value - let's say 10million data points?\n",
      "In default python, this would've been handled by increasing the data size to long long implicitly. Does cython take care of it? I don't see any methods to take care of such scenario in the scikit-learn implementation code.\n",
      "@rthy:matrix.org  thanks for pointing it out. A silly mistake on my part.\n",
      "\n",
      "3\n",
      "5280\n",
      "575b0eccc2f0db084a1d3a41\n",
      "2021-07-09 12:07\n",
      "Except for that idx_end - idx_start < 2 will be true in this case (due to signed integer overflow?) and the node 0 will be made a leaf node. But this is an unexpected behaviour, right?\n",
      "\n",
      "1\n",
      "5281\n",
      "matrix-rthy:matrix.org\n",
      "2021-07-09 12:59\n",
      "@HarshVardhanKumar: maximum value for int32 is ~2e9 not 2e6. So probably no one has tried using it with more than 2 billion samples.  Not sure it's really an issue for the near future.\n",
      "+1 to check for that overflow though.\n",
      "\n",
      "2\n",
      "5282\n",
      "5baebfaed73408ce4fa9bd25\n",
      "2021-07-14 18:02\n",
      "Hello, I ran `pytest sklearn` and see the following.  Is this ok, or is there something wrong with my build: ``` SKIPPED [16] sklearn/utils/tests/test_validation.py:1374: could not import 'pandas': No module named 'pandas' ==== 355 failed, 19625 passed, 1443 skipped, 117 xfailed, 37 xpassed, 3371 warnings in 2380.84s (0:39:40) ==== (sklearn-dev)  ```\n",
      "OK, it works now.  Thanks Thomas Fan.\n",
      "\n",
      "2\n",
      "5283\n",
      "5f7499a2d73408ce4ff04fe5\n",
      "2021-07-19 03:36\n",
      "Hello, I was working on this issue https://github.com/scikit-learn/scikit-learn/issues/20435\n",
      "However I was not able to find the file to contribute the documentation into, can someone please help me with that\n",
      "\n",
      "2\n",
      "5284\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-07-19 13:42\n",
      "@yashasvimisra2798 the documentation is generated from the docstrings in the `.py` files where those classes are implemented. You should look for classes which inherit that class, and find the relevant part of the docstring there.\n",
      "\n",
      "1\n",
      "5285\n",
      "5f7499a2d73408ce4ff04fe5\n",
      "2021-07-19 18:49\n",
      "Thanks, @adrinjalali I will look into it.\n",
      "\n",
      "1\n",
      "5286\n",
      "matrix-bmoroz82:matrix.org\n",
      "2021-08-07 16:30\n",
      "Does Scikit support regression parameters with multidimensional data structure, e.g., 3-dimensional point data? I would like to perform a regression to predict the position of a 3-dimensional point (x,y,z) using other known 3-dimensional points while weighting by inverse-distance. I have a small sample of dependents Y comprised (xi, yi, zi) and a complete set of independents X1, X2, X3, ... each comprised of (xi, yi, zi). I would like to test a simple model Y = X1 + X2 + ...\n",
      "<unconvertable> appreciated\n",
      "\n",
      "2\n",
      "5287\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-08-07 17:59\n",
      "you probably want to look at the following: https://scikit-learn.org/stable/modules/multiclass.html#multioutput-regression\n",
      "\n",
      "1\n",
      "5288\n",
      "610a83356da03739848276fc\n",
      "2021-08-09 08:05\n",
      "Can I use a confusion  matrix to see the accuracy of SVR(support vector regression) ?\n",
      "or is it only for classification ?\n",
      "\n",
      "2\n",
      "5289\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-08-09 09:12\n",
      "confusion matrix and derived metrics are only for classification\n",
      "look at the regression metrics instead\n",
      "\n",
      "2\n",
      "5290\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2021-08-16 18:36\n",
      "Hi all, how do you manage your changelog? Any useful tools or files that you could point me too?\n",
      "\n",
      "1\n",
      "5291\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-08-17 09:24\n",
      "@mloning most PRs come with a changelog entry, e.g. https://github.com/scikit-learn/scikit-learn/pull/20727/files Then we have scripts to process / check it which are described in https://scikit-learn.org/stable/developers/maintainer.html (we call the change log a \"What's new\")\n",
      "\n",
      "1\n",
      "5292\n",
      "5b58594ed73408ce4fa23ee3\n",
      "2021-08-17 12:01\n",
      "@NicolasHug thanks - that's very helpful!\n",
      "\n",
      "1\n",
      "5293\n",
      "5fd8fff2d73408ce4ff69a39\n",
      "2021-08-17 17:32\n",
      "A very good evening everyone!! I am trying to setup scikit-learn project into my development env but encountered with an error when I try to run \"pip install --verbose --no-build-isolation --editable .\"\n",
      "Can someone help me to fix this error?\n",
      "\n",
      "2\n",
      "5294\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-08-18 08:51\n",
      "@RAVANv2 can only help if you tell us what the issue is :)\n",
      "\n",
      "1\n",
      "5295\n",
      "5fd8fff2d73408ce4ff69a39\n",
      "2021-08-18 13:02\n",
      "Hi @adrinjalali, Actually, I solve it on my own :)\n",
      "\n",
      "1\n",
      "5296\n",
      "5fd8fff2d73408ce4ff69a39\n",
      "2021-08-18 20:02\n",
      "Hi all, I made one PR on issue #20754 but it failing the linting job of black and showing the error \"##[error]Bash exited with code '1'\". Can anyone tell me where I am doing wrong? I am totally a newbie in open-source but I m really enjoying :)\n",
      "\n",
      "1\n",
      "5297\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-08-18 21:31\n",
      "You can see how to fix that here: https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist You need to run black on your code\n",
      "\n",
      "1\n",
      "5298\n",
      "611f80036da03739848392b1\n",
      "2021-08-20 10:38\n",
      "Hello Everyone, I am a newbie to open source with basic knowledge about scikit-learn and want to contribute to this repository. Can anyone tell me how should I start? I have basic knowledge about generating my first PR and solving good first issues. Now I want to fix bugs and do some code contributions but I am unable to understand the issues. Can anyone guide me?\n",
      "\n",
      "1\n",
      "5299\n",
      "54d4a1d6db8155e6700f853b\n",
      "2021-08-20 18:04\n",
      "Hi @KiranHipparagi , thanks for wanting to contribute! Have you read the contributor's guide? I would suggest you look for issues tagged \"good first issue\". Many of those are multi-part issues where you can pick just a part to work on\n",
      "see https://scikit-learn.org/dev/developers/contributing.html and https://github.com/scikit-learn/scikit-learn/labels/good%20first%20issue for good first issues\n",
      "this one might be good to get started: https://github.com/scikit-learn/scikit-learn/issues/20308\n",
      "\n",
      "3\n",
      "5300\n",
      "6102db756da0373984821b80\n",
      "2021-08-23 14:29\n",
      "Hello i am starting out this one project and some of the parts are really complicated . And i am happy to learn during the complicated parts. But i feel i need more people. If anyone is interested in working together and learning together feel free to dm me\n",
      "my discord is Aura#5549\n",
      "\n",
      "2\n",
      "5301\n",
      "5baebfaed73408ce4fa9bd25\n",
      "2021-09-01 14:11\n",
      "Deleted this message from the /dev/ channel.  Copying and pasting here: >I am Bhavya Bhardwaj (https://github.com/Bhavya1705). I am a student of Electronics and Communication at Amrita Vishwa Vidyapeetham, India. My thanks to you and the team for sklearn. I have been try to make some contributions to the scikit-learn library - scikit-learn/scikit-learn#5516. I have made the code, and the necessary changes to the init file and test files, in addition to the _classification file. This is the links to my commits - scikit-learn/scikit-learn#20861, as you will see, there are many mistakes, that I have made, Any help that you can render to me would be much appreciated and would be a wonderful learning experience. Thank You\n",
      "\n",
      "1\n",
      "5302\n",
      "61291ef16da03739848428f6\n",
      "2021-09-01 15:35\n",
      "@reshamas Thank You, I have managed to solve the issue.\n",
      "\n",
      "1\n",
      "5303\n",
      "5cb5d739d73408ce4fbdddff\n",
      "2021-09-07 16:57\n",
      "Hi. I am trying to develop my own Estimator based on TransformerMixin and BaseEstimator. To make sure I am doing things right I have added a test to my project : ``` import MyEstimator from sklearn.utils.estimator_checks import check_estimator def test () :      me = MyEstimator(**params)     check_estimator(me) ``` If I run the test, I get the following error message :  ``` AssertionError: The error message should contain one of the following patterns:                0 feature\\(s\\) \\(shape=\\(\\d*, 0\\)\\) while a minimum of \\d* is required. ```  I don't understand how I am supposed to take care of that. I am even more surprise because my fit_transorm method uses self._validate_data at the beginning. I would expect that function to take care of case like these. Could someone help me with that issue ?\n",
      "\n",
      "1\n",
      "5304\n",
      "matrix-rthy:matrix.org\n",
      "2021-09-07 22:26\n",
      "@adriente: Could you please open a Github issue with full traceback and tag me (@rth) in?\n",
      "\n",
      "1\n",
      "5305\n",
      "5f31510fd73408ce4febd603\n",
      "2021-09-08 18:59\n",
      "Hello! I opened this feature request a week ago. Just bumping it here in case it got lost: https://github.com/scikit-learn/scikit-learn/issues/20890\n",
      "\n",
      "1\n",
      "5306\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-09-09 07:05\n",
      "@freddyaboulton I can assure you it is not lost :) I saw it but I did not look at it yet because we are kind of working on releasing 1.0. Once the release done, you might get some attention from core-devs\n",
      "\n",
      "1\n",
      "5307\n",
      "613f27626da0373984858a1f\n",
      "2021-09-13 10:28\n",
      "Morning all\n",
      "\n",
      "1\n",
      "5308\n",
      "613f27626da0373984858a1f\n",
      "2021-09-13 10:30\n",
      "I was kind of curious . Ive been dragging my feet using torch and I was wondering does this lib offer anything over torch ? Maybe this is better suited for the low level scientist trying to learn theory ? Or is it just an alternative ?\n",
      "Omg Im trapped\n",
      "\n",
      "2\n",
      "5309\n",
      "5baf7d9ad73408ce4fa9c9b2\n",
      "2021-09-13 16:15\n",
      "@makingglitches pytorch and scikit-learn operate at different levels of abstractions but simply put in terms of scope, pytorch is for deep-learning while scikit-learn is for the rest of ML that's *not* deep learning. So one might be better suited than the other, depending on the theory that you're interested in.\n",
      "\n",
      "1\n",
      "5310\n",
      "5e1a8a04d73408ce4fd66bde\n",
      "2021-09-15 12:36\n",
      "[FEATURE REQUEST] Add GitHub Organisation README profile  Just found out this new GitHub feature on GitHub org.  Like this: https://twitter.com/vinzvinci/status/1438033675313025024\n",
      "\n",
      "1\n",
      "5311\n",
      "614fb4c06da03739848687d4\n",
      "2021-09-25 23:47\n",
      "When a PR generates html doc, where to see it?\n",
      "\n",
      "1\n",
      "5312\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-09-27 07:53\n",
      "@lobpcg do you have a PR number?\n",
      "\n",
      "1\n",
      "5313\n",
      "614fb4c06da03739848687d4\n",
      "2021-09-28 18:06\n",
      " @adrinjalali #21148\n",
      "\n",
      "1\n",
      "5314\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-09-28 19:28\n",
      "When the documentation CIs are finished\n",
      "there will be a \"ci/circleci: doc artifact <unconvertable> Link to 0/doc/_changed.html \" line\n",
      "You can clicked on \"Details\"\n",
      "It will redirect to an HTML page where you will have hyperlinks to each documentation page that has been generated\n",
      "in PRs we only generate documentation pages where there is a modification\n",
      "\n",
      "5\n",
      "5315\n",
      "614fb4c06da03739848687d4\n",
      "2021-09-28 21:35\n",
      "@glemaitre great, found it - thanks!\n",
      "\n",
      "1\n",
      "5316\n",
      "5cafacbfd73408ce4fbd7ec2\n",
      "2021-10-04 08:50\n",
      "Morning all I want to know if scikit-learn 1.1 will be released in late 2021?\n",
      "\n",
      "1\n",
      "5317\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-10-04 09:17\n",
      "There will be minor releases this year (1.0.1 for instance), but the next major release will be next year.\n",
      "\n",
      "1\n",
      "5318\n",
      "541a528b163965c9bc2053de\n",
      "2021-10-08 15:09\n",
      " Hello everyone, we are having a live community office hour on discord. Feel free to join to discuss your PRs!\n",
      "https://discord.gg/YBdN45kD\n",
      "\n",
      "2\n",
      "5319\n",
      "58de4778d73408ce4f551e04\n",
      "2021-10-08 15:14\n",
      ":boom: :+1:\n",
      "\n",
      "1\n",
      "5320\n",
      "6069ea306da0373984793e79\n",
      "2021-10-11 19:12\n",
      "has anyone tried to implement GAM's via sklearn pipeline's before?\n",
      "\n",
      "1\n",
      "5321\n",
      "5cdeaebed73408ce4fc08df3\n",
      "2021-10-12 09:45\n",
      "Hello I'm using scikit-learn `0.22.2.post1` and getting the following error  ` AttributeError: '_CalibratedClassifier' object has no attribute 'classes_' `when I try to use `predict_proba`on calibrated classifier  Do you you know if this issue is related to the scikit-learn's version ? Thanks\n",
      "\n",
      "1\n",
      "5322\n",
      "613b2abc6da0373984853e44\n",
      "2021-10-12 10:27\n",
      "Hi, can I post a call for participants in an interview study on open source projects here? If any mod wants more details via DM first, then I'm happy to oblige :)\n",
      "\n",
      "1\n",
      "5323\n",
      "5c9a8be3d73408ce4fbbe82f\n",
      "2021-10-12 15:50\n",
      "Is cohen kappa score and balanced accuracy score supposed to work w/ multiclass labels?  I have a 3-class classification and I'm trying to use `cross_validate`, but it returns nans for all my scores. I tested the problem by running `cross_val_score` on all scores individually and isolated it to those 2 metrics.  X = (100, 5) y = (100, 3) clf is a Random Forest Classifier ``` from sklearn.model_selection import cross_val_score  cross_val_score(clf, X, y, cv=5, scoring='balanced_accuracy') ```\n",
      "\n",
      "1\n",
      "5324\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-10-13 08:05\n",
      "@razou could you please paste a fully reproducible piece of code?\n",
      "\n",
      "1\n",
      "5325\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-10-13 08:07\n",
      "@NoahWoehler_twitter we get quite a bit of these requests these days (which is a good thing, shows people are looking into issues). But it would help people decide if they want to spend time on it, if you give a tiny bit of intro on what it is. Also, feel free to send an email to the mailing list with that information if you want to reach more people.\n",
      "\n",
      "1\n",
      "5326\n",
      "613b2abc6da0373984853e44\n",
      "2021-10-13 08:13\n",
      "Sure, I wasn't sure whether this falls under advertising. We are looking for open source contributors who are willing to talk to us about how security and trust are handled within their projects' communities. This is the landing page with more info: https://research.teamusec.de/2021-interviews-oss/\n",
      "\n",
      "2\n",
      "5327\n",
      "5cdeaebed73408ce4fc08df3\n",
      "2021-10-13 08:17\n",
      "> @razou could you please paste a fully reproducible piece of code?  ``` from sklearn.calibration import CalibratedClassifierCV from sklearn.multioutput import ClassifierChain from lightgbm import LGBMClassifier  base_estimator = LGBMClassifier() calibrator = CalibratedClassifierCV(base_estimator=base_estimator) clf = ClassifierChain(base_estimator=calibrator, order='random', random_state=20) clf.fit(X=train_x, Y=train_y)  y_pred_proba = clf.predict_proba(validation_x) ```\n",
      "The aim was to perform multi-label classifier and retourn probability scores for each (label, profile) pair. NB: y was encoded wit h MultiLabelBinarizer\n",
      "\n",
      "2\n",
      "5328\n",
      "541a528b163965c9bc2053de\n",
      "2021-10-13 08:27\n",
      "@razou please provide a minimal reproducible piece of code, that is a piece a piece of code that we can just copy and paste in a python shell or python script and run to trigger the problem. Here the code you provide does not include the definition of `train_x` and `train_y` which is probably the core of the problem. Using minimal random data from np.random.normal(size=(n_samples, n_features) or np.random.randint(low=0, high=10, size=n_samples)\n",
      "and also add the necessary code to preprocess train_y and the code that computes the cross validation with the score you want.\n",
      "Minimal stands for removing anything that is not necessary. For instance are CalibratedClassifierCV ClassifierChain necessary to reproduce the problem? Or can you just reproduce the problem by cross validating the base estatimtor directly? If so simplify the code snippet.\n",
      "https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n",
      "\n",
      "4\n",
      "5329\n",
      "5cdeaebed73408ce4fc08df3\n",
      "2021-10-13 09:04\n",
      "Thanks you guys for your answers  1. libraries ``` pip install lightgbm==3.2.1 pip install scikit-learn==0.22.2.post1 ```  2. Code snipet  ``` from sklearn.datasets import make_multilabel_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import MultiLabelBinarizer from sklearn.calibration import CalibratedClassifierCV from sklearn.multioutput import ClassifierChain  from lightgbm import LGBMClassifier  X, y = make_multilabel_classification(n_samples=2000, n_classes=10, n_labels=2, allow_unlabeled=True) train_x, validation_x, train_y, validation_y = train_test_split(X, y, test_size=0.25)  mlb = MultiLabelBinarizer() train_y_encoded = mlb.fit_transform(train_y) validation_y_encoded = mlb.transform(validation_y)  base_estimator = LGBMClassifier() calibrator = CalibratedClassifierCV(base_estimator=base_estimator) clf = ClassifierChain(base_estimator=calibrator, order='random', random_state=20) clf.fit(X=train_x, Y=train_y_encoded)  y_pred_proba = clf.predict_proba(validation_x) print(y_pred_proba[:3]) ```\n",
      "\n",
      "1\n",
      "5330\n",
      "541a528b163965c9bc2053de\n",
      "2021-10-13 14:09\n",
      "I don't understand why you are using `MultiLabelBinarizer` here because `y` is already a binary representation of the target variable since in this snippet you used `make_multilabel_classification`. Please provide a snippet that causes the same error message as the problem you observe with cross-validation cohen kappa score.  Anyways by reading the scikit-learn documentation https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa I don't see how this would work for binary encoded multilabeled data.\n",
      "The scikit-learn error message is actually quite explicit:  ```python >>> from sklearn.metrics import cohen_kappa_score >>> cohen_kappa_score([[0, 1], [1, 1]], [[0, 0], [1, 0]]) Traceback (most recent call last):   File \"<ipython-input-19-2a87559cbf88>\", line 1, in <module>     cohen_kappa_score([[0, 1], [1, 1]], [[0, 0], [1, 0]])   File \"/Users/ogrisel/code/scikit-learn/sklearn/metrics/_classification.py\", line 639, in cohen_kappa_score     confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)   File \"/Users/ogrisel/code/scikit-learn/sklearn/metrics/_classification.py\", line 304, in confusion_matrix     raise ValueError(\"%s is not supported\" % y_type) ValueError: multilabel-indicator is not supported ```\n",
      "\n",
      "2\n",
      "5331\n",
      "5cdeaebed73408ce4fc08df3\n",
      "2021-10-14 09:22\n",
      "Where Kappa metric cames from ? I did not used it ...\n",
      "\n",
      "1\n",
      "5332\n",
      "541a528b163965c9bc2053de\n",
      "2021-10-18 10:23\n",
      "@razou sorry I mixed 2 conversations. Ignore the bit on Cohen's Kappa then.\n",
      "@razou your code snippet works with `clf.fit(X=train_x, Y=train_y)` instead of `clf.fit(X=train_x, Y=train_y_encoded)`.\n",
      "\n",
      "2\n",
      "5333\n",
      "5cdeaebed73408ce4fc08df3\n",
      "2021-10-18 16:47\n",
      "Thanks @ogrisel  for answers (y)\n",
      "\n",
      "1\n",
      "5334\n",
      "5acfdfffd73408ce4f95738d\n",
      "2021-10-20 17:14\n",
      "how to get precision and recall from function 'precision_recall_curve' for class 0. I posted a question with a code on this topic [here](https://ai.stackexchange.com/questions/32127/how-to-get-precision-and-recall-from-function-precision-recall-curve-for-class)\n",
      "\n",
      "1\n",
      "5335\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-10-20 17:34\n",
      "You should use either\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html#sklearn.metrics.PrecisionRecallDisplay.from_estimator\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html#sklearn.metrics.PrecisionRecallDisplay.from_predictions\n",
      "with `pos_label=0`\n",
      "\n",
      "4\n",
      "5336\n",
      "5acfdfffd73408ce4f95738d\n",
      "2021-10-21 12:43\n",
      "@glemaitre Solved the problem by turning class 0 into 1.                                                              But it's still not clear what kind of data I get by setting label=0. Updated the code and added two videos with label=0 and label=1. I put the code and videos [here](https://ai.stackexchange.com/questions/32127/how-to-get-precision-and-recall-from-function-precision-recall-curve-for-class/)  It is quite possible that I am difficult to understand, since English is not my native language. There is no opportunity to practice in English.\n",
      "\n",
      "2\n",
      "5337\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-10-21 13:24\n",
      "turning class 0 to 1 is equivalent to change `pos_label=0` without changing the label.\n",
      "\n",
      "1\n",
      "5338\n",
      "5acfdfffd73408ce4f95738d\n",
      "2021-10-21 13:28\n",
      "Thanks!\n",
      "\n",
      "1\n",
      "5339\n",
      "5d7033aad73408ce4fca0665\n",
      "2021-10-30 13:33\n",
      "If we run this command after setup `pytest maint_tools/test_docstrings.py -k sklearn.utils.extmath.cartesian`, we got  platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 rootdir: /workspaces/scikit-learn, configfile: setup.cfg plugins: cov-3.0.0 collected 0 items / 2 skipped                                                                                                   =================================================== short test summary info =================================================== SKIPPED [2] maint_tools/test_docstrings.py:12: could not import 'numpydoc.validate': No module named 'numpydoc' ===================================================== 2 skipped in 0.47s ======================================================\n",
      "\n",
      "1\n",
      "5340\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-10-30 13:36\n",
      "you need to install numpydoc via pip or conda\n",
      "\n",
      "4\n",
      "5341\n",
      "5d7033aad73408ce4fca0665\n",
      "2021-10-30 13:37\n",
      "Working on #21350 issue\n",
      "\n",
      "1\n",
      "5342\n",
      "5d7033aad73408ce4fca0665\n",
      "2021-10-30 13:43\n",
      "===================================================== test session starts ===================================================== platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 rootdir: /workspaces/scikit-learn, configfile: setup.cfg plugins: cov-3.0.0 collected 2110 items / 2109 deselected / 1 selected                                                                             maint_tools/test_docstrings.py .                                                                                        [100%]  ============================================= 1 passed, 2109 deselected in 0.98s ==============================================\n",
      "now, we got passed, then we make a PR for it ?\n",
      "\n",
      "2\n",
      "5343\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-10-30 14:04\n",
      "yes\n",
      "\n",
      "1\n",
      "5344\n",
      "5d7033aad73408ce4fca0665\n",
      "2021-10-30 14:29\n",
      "> yes  Done!\n",
      "\n",
      "1\n",
      "5345\n",
      "matrix-zacchiro:matrix.org\n",
      "2021-10-31 13:20\n",
      "hey, i'm unclear on whether this channel is for user- or developer-related questions (or both)? can someone clarify? i don't want to bother others with off-topic questions :-)\n",
      "> hey, i'm unclear on whether this channel is for user- or developer-related questions (or both)? can someone clarify? i don't want to bother others with off-topic questions :-)  oh, i guess the topic answers that (thanks)\n",
      "so, for the actual Q:\n",
      "\n",
      "3\n",
      "5346\n",
      "matrix-zacchiro:matrix.org\n",
      "2021-10-31 13:39\n",
      "i'm trying to use sklearn.clustering.DBSCAN on a large document corpus (coming from gensim, converted to numpy sparse matrix)\n",
      "the matrix is this:\n",
      "<6748785x4974743 sparse matrix of type '<class 'numpy.float64'>'         with 677079990 stored elements in Compressed Sparse Column format>\n",
      "that's ~5M documents with ~7M features (TFIDF-weighted words)\n",
      "i'm trying to cluster this using DBSCAN(n_jobs=64).fit(corpus_csc)\n",
      "how do I know if it will ever terminate? there seems to be no way of having a progress indication\n",
      "(RAM doesn't seem to be an issue for now, it's running on a machine with 1 TiB RAM, but it's sitting at ~70 GiB for now)\n",
      "it's been going on for ~3 days now, any guess on whether it has any chances of terminating in reasonable time?\n",
      "(or how to enable progress logging, if that's possible)\n",
      "\n",
      "9\n",
      "5347\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-11-02 19:20\n",
      "my guess is most of the time is spent on ball_tree or kd_tree in nearestneighbors. If I were to investigate, I'd add a few logging info in those areas to see what's happening, but probably most of them are in cython space. We don't really have logging in those areas, if you want to figure it out, you should add it to your copy of scikit-learn and see where the time is spent.\n",
      "\n",
      "1\n",
      "5348\n",
      "matrix-zacchiro:matrix.org\n",
      "2021-11-03 13:38\n",
      "@adrinjalali: thanks for your feedback. Aside from that (which I'm gonna try) any reference number about the performances of sklearn's DBSCAN implementation on large datasets?\n",
      "it'd be very useful to have an idea if I'm \"almost there\" or, like, only 1e-6 % done\n",
      "\n",
      "2\n",
      "5349\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-11-03 14:40\n",
      "I personally haven't worked with very large datasets and DBSCAN, so I can't really help you there unfortunately.\n",
      "\n",
      "1\n",
      "5350\n",
      "5c9a8be3d73408ce4fbbe82f\n",
      "2021-11-04 20:34\n",
      "Does logistic regression support pandas dataframe as input for the X matrix?  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit  Does this then allow us to make use of the new attribute name: `feature_names_in`?\n",
      "\n",
      "1\n",
      "5351\n",
      "55d21ee30fc9f982beadabb8\n",
      "2021-11-04 20:44\n",
      "you can pass a daframe\n",
      "currently `feature_names_in`would be useless in classifier and regressor\n",
      "because we will convert into a numpy array to make the optimisation\n",
      "the idea for the moment is to propagate feature names in the preprocessing steps\n",
      "such that you for instance know the name of the columns of the numpy array that will be passed to the logistic regression (and potentially build the dataframe)\n",
      "The work that could be done for classifier and regressor is to make that the fitted attribute could use the feature names to decorate themselves\n",
      "but it can be still be easily done by getting the `feature_names_in` from the last preprocessing stage and create for instance a pandas series using the coefficients\n",
      "but for the moment we try that we can move the feature names in the preprocessing\n",
      "\n",
      "8\n",
      "5352\n",
      "61856d766da037398489e910\n",
      "2021-11-05 17:54\n",
      "Hi folks! I have a very simple fix for https://github.com/scikit-learn/scikit-learn/issues/12052 that only addresses this issue for CalibratedClassifierCV. I know SLEP006 seeks to fix this for the general case, but it appears to still be a WIP. Is it worth me throwing together a PR to simply add the groups parameter to CalibratedClassifierCV with checks to ensure the cv supports it? I've tested this out on my own by subclassing CalibratedClassifierCV and writing a modified fit method. It's essentially a 2 line code change. I'm happy to submit a PR if it seems likely this will be accepted?\n",
      "\n",
      "1\n",
      "5353\n",
      "matrix-sumitdatascience:matrix.org\n",
      "2021-11-06 02:26\n",
      "Hi everyone. I was wondering if anyone could get older versions of scikit-learn that are below 1.0.0 installed in a Python 3.9 virtual environment. I think there is no wheel for scikit-learn 0.21.0 or 0.21.1 that works with Python 3.9, so it has to be built from source, which doesn't work very well on a lot of systems. I know installing with Anaconda works, but I was curious if there are some solutions to get it to work with Python 3.9's typical pip install command.\n",
      "\n",
      "1\n",
      "5354\n",
      "616083aa6da0373984877911\n",
      "2021-11-11 06:51\n",
      "Hello!\n",
      "\n",
      "1\n",
      "5355\n",
      "61a11a356da03739848b5ea2\n",
      "2021-12-08 20:11\n",
      "Hello, Scikit devs and contributors. I have a question regarding one of the examples left in PR #21958. `/examples/linear_model/plot_sparse_logistic_regression_mnist.py`. I guess this one is left because is not as straightforward to accelerate. This example involves a logistic regression using the saga algorithm and l1 penalty. In my attempt to optimize it, I found out that most of the running time is spent fetching the data instead of running the regression itself. The most I could do was 5% faster, which translates to 2 seconds difference. Unfortunately, by reducing `train_samples` the running time is not meaningfully decreased.  Another option I considered was to run LogisticRegression with `tol=0.9` and `n_jobs=2`, and `max_iter=40`. Unfortunately, can't get more than 5%.  One of the tweaks I managed to do was in the plot itself, by running the reshape outside the plot and using a list comprehension itself. Overall it makes the plot faster.  Checking other PRs, it seems that the acceleration obtained in this example is quite low. I am not sure even if this is an example that can be further improved considering the most expensive operation here is `fetch_openml`. For the same reason, not sure submitting a PR with such a low improvement is even a good idea.  I appreciate your thoughts on it. Maybe I'm missing something relevant. Thanks!\n",
      "\n",
      "1\n",
      "5356\n",
      "6186f8e46da037398489fa20\n",
      "2021-12-10 19:24\n",
      "Hi guys, I want to install the scikit-learn package in an environment that previously TensorFlow, NumPy, pandas, scipy, and matplotlib were installed. I installed the scikit-learn package in Windows 7 64-bit. When, in activated enviroment I write: conda install scikit-learn; it shows: ERROR conda.core.link:_execute(699): An error occurred while installing  package 'defaults::scikit-learn-1.0.1-py38hf11a4ad_0'. Rolling back  transaction: done  LinkError: post-link script failed for package  defaults::scikit-learn-1.0.1-py38 hf11a4ad_0 location of failed  script: G:\\programfile\\anaconda3\\envs\\tf\\Scripts\\.scikit-lear n-post-link.bat  ==> script messages <==  <None> ==> script output <==  stdout:  stderr:  return code: 1   ()\n",
      "\n",
      "1\n",
      "5357\n",
      "567f5d7716b6c7089cc043a8\n",
      "2021-12-10 20:56\n",
      "@ojeda-e thanks for investigating those ones. Please leave the same comment on the issue, and I'll mark them as \"won't change\"\n",
      "\n",
      "1\n",
      "5358\n",
      "matrix-djmvicente:matrix.org\n",
      "2022-01-25 02:29\n",
      "Hi\n",
      "\n",
      "1\n",
      "5359\n",
      "61f03ddb6da03739848f1c1e\n",
      "2022-01-26 13:03\n",
      "hello guys i am getting this error can anybody please tell\n",
      "ImportError while loading conftest 'E:\\scikit\\scikit-learn\\sklearn\\conftest.py'. sklearn\\__init__.py:81: in <module>     from . import __check_build  # noqa: F401 sklearn\\__check_build\\__init__.py:50: in <module>     raise_build_error(e) sklearn\\__check_build\\__init__.py:31: in raise_build_error     raise ImportError( E   ImportError: No module named 'sklearn.__check_build._check_build' E   ___________________________________________________________________________ E   Contents of E:\\scikit\\scikit-learn\\sklearn\\__check_build: E   setup.py                  _check_build.pyx          __init__.py E   __pycache__ E   ___________________________________________________________________________ E   It seems that scikit-learn has not been built correctly. E E   If you have installed scikit-learn from source, please do not forget E   to build the package before using it: run `python setup.py install` or E   `make` in the source directory. E E   If you have used an installer, please check that it is suited for your E   Python version, your operating system and your platform.\n",
      "\n",
      "2\n",
      "5360\n",
      "61f03ddb6da03739848f1c1e\n",
      "2022-01-26 13:08\n",
      "i have tried puthon setup.py install too\n",
      "\n",
      "1\n",
      "5361\n",
      "matrix-ogrisel:matrix.org\n",
      "2022-01-26 16:41\n",
      "which error? Please copy the command you used and the error message.\n",
      "\n",
      "1\n",
      "5362\n",
      "57b364d540f3a6eec05fce2b\n",
      "2022-01-26 19:05\n",
      "Hello guys, i am having a lot of trouble creating a dummy classifier with scikit-learn. I asked a question with all the details in stackoverflow, can you please check what am i doing wrong? Link: https://www.stackoverflow.com/questions/70866945/assertionerror-not-equal-to-tolerance\n",
      "\n",
      "1\n",
      "5363\n",
      "567f5d7716b6c7089cc043a8\n",
      "2022-01-27 13:13\n",
      "@henrique-voni I've answered on SO\n",
      "\n",
      "1\n",
      "5364\n",
      "61f5898d6da03739848f5c77\n",
      "2022-01-29 18:45\n",
      "Hello guys while testing  the file my test are getting skipped how to fix it  '''  ==================================================================== test session starts ==================================================================== platform win32 -- Python 3.9.10, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 rootdir: D:\\OpenSource\\sci-kit\\scikit-learn, configfile: setup.cfg plugins: cov-3.0.0 collected 0 items / 2 skipped ''''\n",
      "\n",
      "1\n",
      "5365\n",
      "61cc86716da03739848d405f\n",
      "2022-01-29 18:52\n",
      "what is the command you are using ?\n",
      "\n",
      "1\n",
      "5366\n",
      "61f5898d6da03739848f5c77\n",
      "2022-01-29 18:57\n",
      "pytest sklearn/tests/test_docstrings.py -k sklearn.datasets._california_housing.fetch_california_housing\n",
      "\n",
      "1\n",
      "5367\n",
      "61cc86716da03739848d405f\n",
      "2022-01-29 18:59\n",
      "I think you forgot to install documentation dependencies https://scikit-learn.org/stable/developers/contributing.html#building-the-documentation\n",
      "\n",
      "1\n",
      "5368\n",
      "61f5898d6da03739848f5c77\n",
      "2022-01-29 19:06\n",
      "@PurnaChandraMansingh  thanks ,Now its working\n",
      "\n",
      "1\n",
      "5369\n",
      "5ee61de0d73408ce4fe6d981\n",
      "2022-01-30 05:50\n",
      "I get an error trying to use a callable for metric with KNN. What is the correct way to use it?  ``` clf = KNeighborsClassifier(metric=dm.get_metric(\"minkowski\")) clf.fit(X,y)  ValueError: Metric '<sklearn.metrics._dist_metrics.EuclideanDistance object at 0x000001ECEC658DD0>' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['brute']) to get valid options. Metric can also be a callable function. ```\n",
      "good point :-)\n",
      "\n",
      "2\n",
      "5370\n",
      "matrix-hansuke:matrix.org\n",
      "2022-01-30 17:30\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
      "\n",
      "1\n",
      "5371\n",
      "567f5d7716b6c7089cc043a8\n",
      "2022-01-30 18:32\n",
      "@amy12xx it's always easier for people to give you a meaningful answer if you paste a minimally reproducible code, which can in its entirety be copy/pasted to produce your issue.\n",
      "\n",
      "1\n",
      "5372\n",
      "5ee61de0d73408ce4fe6d981\n",
      "2022-01-30 20:08\n",
      "``` from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import DistanceMetric as dm from sklearn.datasets import load_iris iris = load_iris() X = iris.data y = iris.target clf = KNeighborsClassifier(metric=\"euclidean\") clf.fit(X,y) clf = KNeighborsClassifier(metric=dm.get_metric(\"euclidean\")) clf.fit(X,y)  Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"C:\\Users\\Amanda\\Miniconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 198, in fit     return self._fit(X, y)   File \"C:\\Users\\Amanda\\Miniconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 437, in _fit     self._check_algorithm_metric()   File \"C:\\Users\\Amanda\\Miniconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 374, in _check_algorithm_metric     raise ValueError( ValueError: Metric '<sklearn.metrics._dist_metrics.EuclideanDistance object at 0x0000018099BB9780>' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['brute']) to get valid options. Metric can also be a callable function.  ```\n",
      "\n",
      "1\n",
      "5373\n",
      "567f5d7716b6c7089cc043a8\n",
      "2022-01-31 15:48\n",
      "That's indeed curious @amy12xx , I've opened and issue, and you can follow the discussion there: https://github.com/scikit-learn/scikit-learn/issues/22348\n",
      "\n",
      "1\n",
      "5374\n",
      "5a0d62aed73408ce4f7ed9b2\n",
      "2022-02-11 18:41\n",
      "Hi, I am using TransformedTargetRegressor with KNeighborsRegressor for precomputed metric, however when I do cross validation with GridSearchCV, an error is raised saying that the dimension of the metric is not correct. The code is like this: ``` from sklearn.preprocessing import MinMaxScaler target_scaler = MinMaxScaler() estimator = Pipeline([         ('scaler', MinMaxScaler()),         ('model', TransformedTargetRegressor(           KNeighborsRegressor(metric='precomputed'),           transformer=target_scaler         ))])  clf = GridSearchCV(estimator, param_grid=grid_params, \t\t\t\t\t   scoring=scoring, \t\t\t\t\t   cv=cv, return_train_score=True, refit=True, \t\t\t\t\t   error_score='raise') clf.fit(D_app, y_app) ... ``` May I ask what may be the problem? In case it is not supported, is there other ways to corrected scale targets in GridSearchCV (as well as HalvingGridSearchCV, etc.). Thank you very much!\n",
      "\n",
      "1\n",
      "5375\n",
      "6208ba1a6da037398490413f\n",
      "2022-02-13 11:29\n",
      "I am getting this error-:       Could not find conda environment: sklearn-env. You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "1\n",
      "5376\n",
      "6208ba1a6da037398490413f\n",
      "2022-02-14 07:29\n",
      " E   ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "1\n",
      "5377\n",
      "matrix-ogrisel:matrix.org\n",
      "2022-02-14 08:48\n",
      "@Shubham1450: can you please open an issue with this error message and the full output of `conda list`?\n",
      "\n",
      "1\n",
      "5378\n",
      "matrix-RebelCoder:matrix.org\n",
      "2022-02-19 12:34\n",
      "Hey smart people. I am trying to figure out/understand the warning. Solution I found just tell you to disable to warning. Maybe someone give a hint why I am seen the following warning is this super simple Multiple Regression example? ``` data_file = pd.read_csv(\"FuelConsumption.csv\")  data_frame = data_file[     [         'ENGINESIZE',         'CYLINDERS',         'FUELCONSUMPTION_CITY',         'FUELCONSUMPTION_HWY',         'FUELCONSUMPTION_COMB',         'CO2EMISSIONS'     ] ]  data_set_x = ['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB'] data_set_y = ['CO2EMISSIONS']  mask = np.random.rand(len(data_frame)) < 0.8 train = data_frame[mask] test = data_frame[~mask]  lr_regression = linear_model.LinearRegression() train_x = np.asanyarray(train[data_set_x]) train_y = np.asanyarray(train[data_set_y]) lr_regression.fit(train_x, train_y)  y_hat = lr_regression.predict(test[data_set_x]) test_x = np.asanyarray(test[data_set_x]) test_y = np.asanyarray(test[data_set_y]) ```  The line 26: ``` y_hat = lr_regression.predict(test[data_set_x]) ```  Produces this warning: ``` sklearn/base.py:443: UserWarning: X has feature names, but LinearRegression was fitted without feature names   warnings.warn( ```\n",
      "\n",
      "1\n",
      "5379\n",
      "5c77a43ed73408ce4fb93081\n",
      "2022-02-19 13:44\n",
      "In your example, `lr_regression.fit` was called with an ndarray, while `lr_regression.predict` was called with a DataFrame. To prevent the warning, you can `fit` with the DataFrame directly:  ```python lr_regression.fit(train[data_set_x], train[data_set_y]) ```  without casting to a ndarray.\n",
      "\n",
      "1\n",
      "5380\n",
      "matrix-RebelCoder:matrix.org\n",
      "2022-02-19 15:11\n",
      "Interesting! Thanks. I tried that, and it still has a warning though. I think I figured it out. I use the `np array` on all of them now. ``` lr_regression = linear_model.LinearRegression() train_x = np.asanyarray(train[data_set_x]) train_y = np.asanyarray(train[data_set_y]) lr_regression.fit(train_x, train_y)  test_x = np.asanyarray(test[data_set_x]) test_y = np.asanyarray(test[data_set_y]) y_hat = lr_regression.predict(test_x) ```  This works. I am also wondering, (yet to look into it) why `np arrays` are used if just the data frame can be used, as you have suggested?e\n",
      "\n",
      "1\n",
      "5381\n",
      "62137b006da037398490b938\n",
      "2022-02-21 11:57\n",
      "I am a question with sklearn.PCA. Whether data needs to be standardized? eg.  the variance may be used before create the pca object?\n",
      "[![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/rsRh/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/rsRh/image.png)\n",
      "Thank you for answering\n",
      "\n",
      "3\n",
      "5382\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2022-02-22 15:17\n",
      "Hi scikit-learn team! I've got a new computer (MacBookPro, chip: Apple M1 Pro) on which I installed the development version of scikit-learn. When I ran pytest I encountered the following:  ``` (sklearn-dev) <unconvertable>  scikit-learn git:(main) pytest =========================================================================================== test session starts ============================================================================================ platform darwin -- Python 3.9.10, pytest-7.0.1, pluggy-1.0.0 rootdir: /Users/maren/Documents/scikit-learn, configfile: setup.cfg, testpaths: sklearn plugins: xdist-2.5.0, forked-1.4.0, cov-3.0.0 collecting ... [1]    54294 killed     pytest ````\n",
      "\n",
      "1\n",
      "5383\n",
      "55d21ee30fc9f982beadabb8\n",
      "2022-02-22 15:19\n",
      "this does not look good :)\n",
      "\n",
      "1\n",
      "5384\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2022-02-22 15:19\n",
      "I then tried to check what's going on and found the following. Do you have an idea of what I need to do?  ``` (sklearn-dev) <unconvertable>  scikit-learn git:(main) python -vvv -c \"import sklearn\" import _frozen_importlib # frozen import _imp # builtin import '_thread' # <class '_frozen_importlib.BuiltinImporter'> import '_warnings' # <class '_frozen_importlib.BuiltinImporter'> import '_weakref' # <class '_frozen_importlib.BuiltinImporter'> import '_io' # <class '_frozen_importlib.BuiltinImporter'> import 'marshal' # <class '_frozen_importlib.BuiltinImporter'> import 'posix' # <class '_frozen_importlib.BuiltinImporter'> import '_frozen_importlib_external' # <class '_frozen_importlib.FrozenImporter'> # installing zipimport hook import 'time' # <class '_frozen_importlib.BuiltinImporter'> import 'zipimport' # <class '_frozen_importlib.FrozenImporter'> # installed zipimport hook # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/__init__.cpython-39.pyc matches /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__init__.py # code object from '/Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/__init__.cpython-39.pyc' # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.cpython-39-darwin.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.abi3.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.py # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/__pycache__/codecs.cpython-39.pyc matches /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/codecs.py # code object from '/Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/__pycache__/codecs.cpython-39.pyc' import '_codecs' # <class '_frozen_importlib.BuiltinImporter'> import 'codecs' # <_frozen_importlib_external.SourceFileLoader object at 0x101613be0> # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.cpython-39-darwin.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.abi3.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.py # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/aliases.cpython-39.pyc matches /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/aliases.py # code object from '/Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/aliases.cpython-39.pyc' import 'encodings.aliases' # <_frozen_importlib_external.SourceFileLoader object at 0x101643190> import 'encodings' # <_frozen_importlib_external.SourceFileLoader object at 0x1016139d0> # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.cpython-39-darwin.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.abi3.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.py # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/utf_8.cpython-39.pyc matches /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/utf_8.py # code object from '/Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/utf_8.cpython-39.pyc' import 'encodings.utf_8' # <_frozen_importlib_external.SourceFileLoader object at 0x1016138b0> import '_signal' # <class '_frozen_importlib.BuiltinImporter'> # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/latin_1.cpython-39-darwin.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/latin_1.abi3.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/latin_1.so # trying /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/latin_1.py # /Users/maren/mambaforge/envs/sklearn-dev/lib/python3.9/encodings/__pycache__/latin_1.cpython-39.pyc matches /Users/maren/mambaforg ```\n",
      "\n",
      "2\n",
      "5385\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2022-02-22 15:22\n",
      "It looks like I can't post the full error message because it's too long. I followed the installation here: https://scikit-learn.org/stable/developers/advanced_installation.html#macos-compilers-from-conda-forge So I installed `compilers` and `llvm-openmp`.\n",
      "And I used the `Miniforge3-MacOSX-arm64` download from here: https://github.com/conda-forge/miniforge#miniforge\n",
      "\n",
      "2\n",
      "5386\n",
      "55d21ee30fc9f982beadabb8\n",
      "2022-02-22 15:24\n",
      "let me try with the last version of comilers on my M1 machine\n",
      "I assume that you forced installing python 3.9 and not 3.10?\n",
      "\n",
      "2\n",
      "5387\n",
      "matrix-ogrisel:matrix.org\n",
      "2022-02-22 15:27\n",
      "you can use https://gist.github.com to post the full error log and give a link here\n",
      "hopefully you will be able to find a workaround if it proves too complex to fix\n",
      "\n",
      "2\n",
      "5388\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2022-02-22 15:30\n",
      "Thank you! Here is the link: https://gist.github.com/marenwestermann/9ffddb7a2f0ef6798d350f3595997ed1\n",
      "\n",
      "2\n",
      "5389\n",
      "55d21ee30fc9f982beadabb8\n",
      "2022-02-22 15:35\n",
      "clang and llvm have been updated\n",
      "\n",
      "1\n",
      "5390\n",
      "55d21ee30fc9f982beadabb8\n",
      "2022-02-22 15:39\n",
      "so temporary I think that installing `compilers=1.3` should fix the problem. I will give it a try.\n",
      "Then we need to understand why the new compilers are failing. But I can see that clang and llvm have been updated\n",
      "uhm it is not the compilers :(\n",
      "\n",
      "3\n",
      "5391\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2022-02-22 15:44\n",
      "I just tried using `compilers=1.3` but it didn't solve the problem\n",
      "\n",
      "3\n",
      "5392\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2022-02-22 15:47\n",
      "I'm about to head off to a PyLadies Berlin open source hack night that I'm hosting, so will do it then. This is actually a good example case that I can show. :)\n",
      "\n",
      "2\n",
      "5393\n",
      "5e1185ead73408ce4fd5bf77\n",
      "2022-02-28 09:25\n",
      "Hello all. I'm new to contributing here. Can anybody guide me how should I start to contribute in sklearn!\n",
      "\n",
      "1\n",
      "5394\n",
      "614fb4c06da03739848687d4\n",
      "2022-03-02 18:31\n",
      "I am trying to rejuvenate scikit-learn/scikit-learn#14636 which would then close scikit-learn/scikit-learn#8834 and scikit-learn/scikit-learn#8842. That requires merging https://github.com/scipy/scipy/pull/15391 Could someone please help by reviewing? Even though it's SciPy PR, it is a must to merge for sklearn spectral embedding and clustering that relies on SciPy to construct the graph Laplacian.\n",
      "\n",
      "1\n",
      "5395\n",
      "5a0d62aed73408ce4f7ed9b2\n",
      "2022-03-06 16:11\n",
      "@adrinjalali Hi, thanks for your advice. Here is my code: ``` import numpy as np from sklearn.pipeline import Pipeline from sklearn.preprocessing import MinMaxScaler from sklearn.compose import TransformedTargetRegressor from sklearn.neighbors import KNeighborsRegressor from sklearn.model_selection import GridSearchCV  target_scaler = MinMaxScaler() estimator = Pipeline([         ('scaler', MinMaxScaler()),         ('model', TransformedTargetRegressor(           KNeighborsRegressor(metric='precomputed'),           transformer=target_scaler         ))])  grid_params = {'model__regressor__n_neighbors': [3, 5, 7]} scoring = 'accuracy' clf = GridSearchCV(estimator, param_grid=grid_params,                        scoring=scoring,                        cv=5, return_train_score=True, refit=True,                        error_score='raise')  D_app = np.random.rand(10, 10) y_app = np.random.rand(10) clf.fit(D_app, y_app) ``` The following error is raised: ``` File \"/media/ljia/DATA/research-repo/projects/202110 Redox/codes/Redox/issues/target_scaling.py\", line 34, in <module>     clf.fit(D_app, y_app)    File \"/home/ljia/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\", line 891, in fit     self._run_search(evaluate_candidates)    File \"/home/ljia/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\", line 1392, in _run_search     evaluate_candidates(ParameterGrid(self.param_grid))    File \"/home/ljia/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\", line 838, in evaluate_candidates     out = parallel(    File \"/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 1043, in __call__     if self.dispatch_one_batch(iterator):    File \"/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch     self._dispatch(tasks)    File \"/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch     job = self._backend.apply_async(batch, callback=cb)    File \"/home/ljia/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async     result = ImmediateResult(func)    File \"/home/ljia/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__     self.results = batch()    File \"/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__     return [func(*args, **kwargs)    File \"/home/ljia/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>     return [func(*args, **kwargs)    File \"/home/ljia/.local/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 211, in __call__     return self.function(*args, **kwargs)    File \"/home/ljia/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score     estimator.fit(X_train, y_train, **fit_params)    File \"/home/ljia/.local/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit     self._final_estimator.fit(Xt, y, **fit_params_last_step)    File \"/home/ljia/.local/lib/python3.8/site-packages/sklearn/compose/_target.py\", line 246, in fit     self.regressor_.fit(X, y_trans, **fit_params)    File \"/home/ljia/.local/lib/python3.8/site-packages/sklearn/neighbors/_regression.py\", line 213, in fit     return self._fit(X, y)    File \"/home/ljia/.local/lib/python3.8/site-packages/sklearn/neighbors/_base.py\", line 489, in _fit     raise ValueError(  ValueError: Precomputed matrix must be square. Input is a 8x10 matrix. ``` May I ask what may be the problem? In case it is not supported, are there other ways to correctly scale targets in GridSearchCV (as well as HalvingGridSearchCV, etc.)? Thank you very much!\n",
      "\n",
      "1\n",
      "5396\n",
      "5f31510fd73408ce4febd603\n",
      "2022-03-14 17:36\n",
      "Hello! I hope everyone is well. Just wondering, when is the next sklearn release happening, `1.0.3`?\n",
      "\n",
      "1\n",
      "5397\n",
      "592e4901d73408ce4f638303\n",
      "2022-03-15 11:24\n",
      "Hello Everyone, I have a very specific use case designed around scikit-learn and wanted to see if it was possible to code it up. The overall idea is to train a 1D embedding using a pretrained GLM. For this, I take a pretrained Poisson Regressor model trained using scikit-learn(this is GIVEN and persay has been trained on 50 features), I want to add a new feature to it i.e a random variable X ~ N(0, 1) and retrain the model to get a 51 parameter model. Once this is done, I treat the input X as a parameter, freeze the model and get optimal value for X_i using gradient based approaches. Finally, upon having the optimal set of X_i, I want to retrain end to end using all 51 features. This is the gist of the algorithm that is designed. Any leads towards APIs or whether this is achievable or not would be really helpful. Thanks!\n",
      "\n",
      "1\n",
      "5398\n",
      "564789be16b6c7089cbab8b7\n",
      "2022-03-23 19:53\n",
      "in sklearn.inspection.permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0 what is the default for scoring?\n",
      "A \" baseline metric, defined by scoring, is evaluated on a (potentially different) dataset defined by the x\" so it has to be something\n",
      "\n",
      "2\n",
      "5399\n",
      "matrix-ogrisel:matrix.org\n",
      "2022-03-24 17:33\n",
      "I think it's using `estimator.score(X_test, y_test)` by default.\n",
      "So accuracy for classifier and R2 for regressors.\n",
      "\n",
      "2\n",
      "5400\n",
      "564789be16b6c7089cbab8b7\n",
      "2022-03-25 14:11\n",
      "@ogrisel:matrix.org  thank you\n",
      "I am really confused though...\n",
      "\n",
      "2\n",
      "5401\n",
      "564789be16b6c7089cbab8b7\n",
      "2022-03-25 14:12\n",
      "model = xgbr.fit(X_train, y_train) print(model.score(X_test, y_test)) r = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=0) for i in r.importances_mean.argsort()[::-1]:     print(f\"{r.importances_mean[i]:.3f}\" f\" +/- {r.importances_std[i]:.3f}\")\n",
      "\n",
      "1\n",
      "5402\n",
      "564789be16b6c7089cbab8b7\n",
      "2022-03-25 14:13\n",
      "that simple code that give feature importance gives me a value over 1.20 for the top one. But how can you have a permutation feature importance higher than 1?\n",
      "\n",
      "1\n",
      "5403\n",
      "564789be16b6c7089cbab8b7\n",
      "2022-03-27 05:45\n",
      "At least a partial answer is that r2 can be arbitrarily negative\n",
      "\n",
      "1\n",
      "5404\n",
      "5e7de2b6d73408ce4fde36e6\n",
      "2022-04-29 10:38\n",
      "Hi! I promoted the scikit-learn office hours on PyLadies slack and WiMLDS slack because many people don't seem to be aware of them. I also did a tweet via PyLadies Berlin (https://twitter.com/PyLadiesBer/status/1519981569343164417). I think that the biweekly office hours is a great initiative that is especially helpful for folks belonging to groups which are underrepresented in tech and open source. I hope that the office hours and also spreading the word about them in these communities will help with contributor retention.\n",
      "\n",
      "1\n",
      "5405\n",
      "55d21ee30fc9f982beadabb8\n",
      "2022-04-29 11:57\n",
      "Thanks Maren. Indeed, I assume that it could be motivating to have a closer follow-up on some PR.\n",
      "\n",
      "1\n",
      "5406\n",
      "62828ace6da0373984968dd7\n",
      "2022-05-16 18:31\n",
      "Hi everyone, i'm new in this community and i want to apologize in advance for any errors i might make in asking the following question. So, i have to implement a classification task using scikit-multiflow for a big dataset (84 feature x 2,5 milion of exemples), processed like a stream. After many and many attempts my code finally run without warnings or errors but there is a problem: i am using the class Evaluate Prequential and its methods for the classification and, by setting adquate metrics to evaluate the goodness of this classification, i obtain very high values for each metric used. This is \"strange\" considering the dataset i am working on, reason why i want to generate the confusion matrix in order to understand on wich classes my classification algorithm works better and on wich classes it makes more misclassification. Generating confusion matrix is very easy using scikit-learn, but this method needs to have as input parameter true labels and predicted labels and here is the problem: i cannot isolate from Evaluate Prequential, in particular from the method \"evaluate\", predicted labels, consequently i have no way to generate the confusion matrix because i have not predicted labels to make a comparison with true labels. For sure there is trick to get around this problem but all of my attempts since two days failed and i have no more ideas on how i could do it. Please, do you have an idea on how to solve this problem? Thank you a lot.\n",
      "\n",
      "1\n",
      "5407\n",
      "matrix-auw9da:matrix.org\n",
      "2022-05-27 22:53\n",
      "ok I have a set of market returns. But I want to explain the returns using a set of features, then find the hierarchy and find an equal weighted portfolio that minimizes variance. How can this be done?\n",
      "``ok I have a set of market returns. But I want to explain the returns using a set of features, then find the hierarchy``\n",
      "this much Idk how to do, I can figure out the rest.\n",
      "\n",
      "3\n",
      "5408\n",
      "579618a040f3a6eec05c5e42\n",
      "2022-06-01 14:06\n",
      "Hey folks, I have maybe a silly question. But is there any fundamental difference between `model_selection.cross_validate` and doing it manually?  For instance, take a look at the example below, I can't figure out why the roc scores in red are different while the logloss scores in green are the same?  I expect that the roc scores should match, unless I'm doing sth stupid and haven't noticed yet?\n",
      "[![image.png](https://files.gitter.im/541a528c163965c9bc2053e1/s4Ci/thumb/image.png)](https://files.gitter.im/541a528c163965c9bc2053e1/s4Ci/image.png)\n",
      "\n",
      "2\n",
      "5409\n",
      "579618a040f3a6eec05c5e42\n",
      "2022-06-01 15:08\n",
      "I seems that `cross_validate` uses silently a different splitting mechanism depending on label `y`. For int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, Fold is used.  If that's true then this is an undesired behaviour for me as a user. I want things to be explicit and not implicit like this spending hours searching to find what's going on?  On an additional note I don't understand the phrase `In all other cases, Fold is used.`, so in the case of my example instead of using `StratifiedGroupKFold` is `cross_validate` implicitly using just `Fold`?\n",
      "\n",
      "1\n",
      "5410\n",
      "55d21ee30fc9f982beadabb8\n",
      "2022-06-02 08:42\n",
      "> If that's true then this is an undesired behaviour for me as a user. I want things to be explicit and not implicit like this spending hours searching to find what's going on?  Stratification will always be the best but this is not possible with regression. So we stated in the `cv` argument doc and the user guide as well:  > When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.  One should improve the documentation to make it explicit for the default `None` and add `cross_validate` together with `cross_val_score`.\n",
      "> On an additional note I don't understand the phrase In all other cases, Fold is used., so in the case of my example instead of using StratifiedGroupKFold is cross_validate implicitly using just Fold?  Oh actually this is already documented properly. `Fold` is a typo, it should be replace with `KFold` and link to the cv splitter.\n",
      "\n",
      "2\n",
      "5411\n",
      "56805cf416b6c7089cc051b2\n",
      "2022-06-05 04:41\n",
      "Any suggestions on how to get my PR reviewed?\n",
      "\n",
      "1\n",
      "5412\n",
      "628dfd066da03739849737d8\n",
      "2022-06-05 06:11\n",
      "If your PR is ready, you can add [MRG] to the title of PR. You can check the PR checklist: https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist. And I found the wiki have some reviewers you can ping: https://github.com/scikit-learn/scikit-learn/wiki/Available-reviewers.\n",
      "\n",
      "1\n",
      "5413\n",
      "56805cf416b6c7089cc051b2\n",
      "2022-06-05 06:15\n",
      "Thanks! I didn't know about that checklist\n",
      "\n",
      "1\n",
      "5414\n",
      "5f31510fd73408ce4febd603\n",
      "2022-06-09 14:20\n",
      "Hello, I see `ElasticNet` does not have an `n_jobs` parameter. Does that mean it uses all available resources on the machine to train? https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "scikitlearn_results,scikitlearn_profile = read_logs(utterance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5414"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scikitlearn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(save_path+channel+'/Scikitlearn_issue.json', 'w') as outfile:\n",
    "    json.dump(scikitlearn_results, outfile)\n",
    "with open(save_path+channel+'/Scikitlearn_profile.json', 'w') as outfile:\n",
    "    json.dump(scikitlearn_profile, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strToDatetime(str):\n",
    "    strptime = datetime.datetime.strptime(str, \"%Y-%m-%d %H:%M\")\n",
    "    return strptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_dateframe(profile):\n",
    "    user_Id = []\n",
    "    cluster_Id = []\n",
    "    answer_date = []\n",
    "    answers = []\n",
    "\n",
    "    for user in profile:\n",
    "        for cluster in profile[user]:\n",
    "            #print(profile[user][cluster][\"Time\"])\n",
    "            time = strToDatetime(profile[user][cluster][\"Time\"])\n",
    "            user_Id.append(user)\n",
    "            cluster_Id.append(cluster)\n",
    "            answer_date.append(time)\n",
    "            answers.append(profile[user][cluster][\"Answer\"])\n",
    "    \n",
    "    df_user = pd.DataFrame()\n",
    "    df_user[\"user_Id\"] =user_Id\n",
    "    df_user[\"cluster_Id\"] =cluster_Id\n",
    "    df_user[\"answer_date\"] =answer_date\n",
    "    df_user[\"answers\"] =answers\n",
    "    \n",
    "    return df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path+channel+'/'+channel+'_profile.json', 'r') as f:\n",
    "    profile = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_scikitlearn_profile = conv_to_dateframe(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_Id</th>\n",
       "      <th>cluster_Id</th>\n",
       "      <th>answer_date</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54d4a1d6db8155e6700f853b</td>\n",
       "      <td>12</td>\n",
       "      <td>2015-02-06 11:14:00</td>\n",
       "      <td>I'm with family this week and so not super act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54d4a1d6db8155e6700f853b</td>\n",
       "      <td>61</td>\n",
       "      <td>2015-02-24 22:46:00</td>\n",
       "      <td>cool\\nok\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54d4a1d6db8155e6700f853b</td>\n",
       "      <td>69</td>\n",
       "      <td>2015-02-25 18:59:00</td>\n",
       "      <td>yeah that sounds like a good idea\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54d4a1d6db8155e6700f853b</td>\n",
       "      <td>72</td>\n",
       "      <td>2015-02-25 19:14:00</td>\n",
       "      <td>you mean == 1\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54d4a1d6db8155e6700f853b</td>\n",
       "      <td>75</td>\n",
       "      <td>2015-02-25 19:19:00</td>\n",
       "      <td>that is also weird ;)\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    user_Id cluster_Id         answer_date  \\\n",
       "0  54d4a1d6db8155e6700f853b         12 2015-02-06 11:14:00   \n",
       "1  54d4a1d6db8155e6700f853b         61 2015-02-24 22:46:00   \n",
       "2  54d4a1d6db8155e6700f853b         69 2015-02-25 18:59:00   \n",
       "3  54d4a1d6db8155e6700f853b         72 2015-02-25 19:14:00   \n",
       "4  54d4a1d6db8155e6700f853b         75 2015-02-25 19:19:00   \n",
       "\n",
       "                                             answers  \n",
       "0  I'm with family this week and so not super act...  \n",
       "1                                         cool\\nok\\n  \n",
       "2                yeah that sounds like a good idea\\n  \n",
       "3                                    you mean == 1\\n  \n",
       "4                            that is also weird ;)\\n  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scikitlearn_profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_scikitlearn_profile.to_csv(save_path+channel+'/'+channel+'_profile.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
