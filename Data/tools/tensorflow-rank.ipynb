{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de0cd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:33:38.214257: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 19:33:40.042273: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/simin/miniconda3/envs/tf-rank/lib/:/home/simin/miniconda3/envs/tf-rank/lib/python3.10/site-packages/nvidia/cudnn/lib:/home/simin/miniconda3/envs/tf-rank/lib/:/home/simin/miniconda3/envs/tf-rank/lib/python3.10/site-packages/nvidia/cudnn/lib\n",
      "2023-04-07 19:33:40.042492: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/simin/miniconda3/envs/tf-rank/lib/:/home/simin/miniconda3/envs/tf-rank/lib/python3.10/site-packages/nvidia/cudnn/lib:/home/simin/miniconda3/envs/tf-rank/lib/:/home/simin/miniconda3/envs/tf-rank/lib/python3.10/site-packages/nvidia/cudnn/lib\n",
      "2023-04-07 19:33:40.042501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow_serving.apis import input_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "feb97a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the paths to files containing training and test instances.\n",
    "_TRAIN_DATA_PATH = \"/tmp/train.tfrecords\"\n",
    "_TEST_DATA_PATH = \"/tmp/test.tfrecords\"\n",
    "\n",
    "# Store the vocabulary path for query and document tokens.\n",
    "_VOCAB_PATH = \"/tmp/vocab.txt\"\n",
    "\n",
    "# The maximum number of documents per query in the dataset.\n",
    "# Document lists are padded or truncated to this size.\n",
    "_LIST_SIZE = 50\n",
    "\n",
    "# The document relevance label.\n",
    "_LABEL_FEATURE = \"relevance\"\n",
    "\n",
    "# Padding labels are set negative so that the corresponding examples can be\n",
    "# ignored in loss and metrics.\n",
    "_PADDING_LABEL = -1\n",
    "\n",
    "# Learning rate for optimizer.\n",
    "_LEARNING_RATE = 0.05\n",
    "\n",
    "# Parameters to the scoring function.\n",
    "_BATCH_SIZE = 32\n",
    "_HIDDEN_LAYER_DIMS = [\"64\", \"32\", \"16\"]\n",
    "_DROPOUT_RATE = 0.8\n",
    "_GROUP_SIZE = 1  # Pointwise scoring.\n",
    "\n",
    "# Location of model directory and number of training steps.\n",
    "_MODEL_DIR = \"/tmp/ranking_model_dir\"\n",
    "_NUM_TRAIN_STEPS = 5 * 1000\n",
    "_EMBEDDING_DIMENSION = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa013ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_feature_columns():\n",
    "    \"\"\"Returns context feature names to column definitions.\"\"\"\n",
    "    sparse_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        key=\"query_tokens\",\n",
    "        vocabulary_file=_VOCAB_PATH)\n",
    "    query_embedding_column = tf.feature_column.embedding_column(\n",
    "        sparse_column, _EMBEDDING_DIMENSION)\n",
    "    return {\"query_tokens\": query_embedding_column}\n",
    "\n",
    "\n",
    "def example_feature_columns():\n",
    "    \"\"\"Returns the example feature columns.\"\"\"\n",
    "    sparse_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        key=\"document_tokens\",\n",
    "        vocabulary_file=_VOCAB_PATH)\n",
    "    document_embedding_column = tf.feature_column.embedding_column(\n",
    "        sparse_column, _EMBEDDING_DIMENSION)\n",
    "    return {\"document_tokens\": document_embedding_column}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a606be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(path, num_epochs=None):\n",
    "    context_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "        context_feature_columns().values())\n",
    "    label_column = tf.feature_column.numeric_column(\n",
    "        _LABEL_FEATURE, dtype=tf.int64, default_value=_PADDING_LABEL)\n",
    "    example_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "        list(example_feature_columns().values()) + [label_column])\n",
    "    dataset = tfr.data.build_ranking_dataset(\n",
    "        file_pattern=path,\n",
    "        data_format=tfr.data.ELWC,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        list_size=_LIST_SIZE,\n",
    "        context_feature_spec=context_feature_spec,\n",
    "        example_feature_spec=example_feature_spec,\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        shuffle=False,\n",
    "        num_epochs=num_epochs)\n",
    "    features = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "    label = tf.squeeze(features.pop(_LABEL_FEATURE), axis=2)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0c6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform_fn():\n",
    "    def _transform_fn(features, mode):\n",
    "        \"\"\"Defines transform_fn.\"\"\"\n",
    "        context_features, example_features = tfr.feature.encode_listwise_features(\n",
    "            features=features,\n",
    "            context_feature_columns=context_feature_columns(),\n",
    "            example_feature_columns=example_feature_columns(),\n",
    "            mode=mode,\n",
    "            scope=\"transform_layer\")\n",
    "\n",
    "        return context_features, example_features\n",
    "    return _transform_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae9bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score_fn():\n",
    "    \"\"\"Returns a scoring function to build `EstimatorSpec`.\"\"\"\n",
    "\n",
    "    def _score_fn(context_features, group_features, mode, params, config):\n",
    "        \"\"\"Defines the network to score a group of documents.\"\"\"\n",
    "        with tf.compat.v1.name_scope(\"input_layer\"):\n",
    "            context_input = [\n",
    "                tf.compat.v1.layers.flatten(context_features[name])\n",
    "                for name in sorted(context_feature_columns())\n",
    "            ]\n",
    "            group_input = [\n",
    "                tf.compat.v1.layers.flatten(group_features[name])\n",
    "                for name in sorted(example_feature_columns())\n",
    "            ]\n",
    "            input_layer = tf.concat(context_input + group_input, 1)\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        cur_layer = input_layer\n",
    "        cur_layer = tf.compat.v1.layers.batch_normalization(\n",
    "            cur_layer,\n",
    "            training=is_training,\n",
    "            momentum=0.99)\n",
    "\n",
    "        for i, layer_width in enumerate(int(d) for d in _HIDDEN_LAYER_DIMS):\n",
    "            cur_layer = tf.compat.v1.layers.dense(cur_layer, units=layer_width)\n",
    "            cur_layer = tf.compat.v1.layers.batch_normalization(\n",
    "                cur_layer,\n",
    "                training=is_training,\n",
    "                momentum=0.99)\n",
    "            cur_layer = tf.nn.relu(cur_layer)\n",
    "            cur_layer = tf.compat.v1.layers.dropout(\n",
    "                inputs=cur_layer, rate=_DROPOUT_RATE, training=is_training)\n",
    "        logits = tf.compat.v1.layers.dense(cur_layer, units=_GROUP_SIZE)\n",
    "        return logits\n",
    "\n",
    "    return _score_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b78ffb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric_fns():\n",
    "    \"\"\"Returns a dict from name to metric functions.\n",
    "\n",
    "    This can be customized as follows. Care must be taken when handling padded\n",
    "    lists.\n",
    "\n",
    "    def _auc(labels, predictions, features):\n",
    "        is_label_valid = tf_reshape(tf.greater_equal(labels, 0.), [-1, 1])\n",
    "        clean_labels = tf.boolean_mask(tf.reshape(labels, [-1, 1], is_label_valid)\n",
    "        clean_pred = tf.boolean_maks(tf.reshape(predictions, [-1, 1], is_label_valid)\n",
    "        return tf.metrics.auc(clean_labels, tf.sigmoid(clean_pred), ...)\n",
    "    metric_fns[\"auc\"] = _auc\n",
    "\n",
    "    Returns:\n",
    "        A dict mapping from metric name to a metric function with above signature.\n",
    "    \"\"\"\n",
    "    metric_fns = {}\n",
    "    metric_fns.update({\n",
    "        f\"metric/ndcg@{topn}\": tfr.metrics.make_ranking_metric_fn(\n",
    "            tfr.metrics.RankingMetricKey.NDCG, topn=topn)\n",
    "        for topn in [1, 3, 5, 10]\n",
    "    })\n",
    "\n",
    "    return metric_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e46627",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c54b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    tokens = value.split()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[token.encode() for token in tokens]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99848c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfexample(feature0, feature1):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "        'document_tokens': _bytes_feature(feature0),\n",
    "        'relevance': _int64_feature(feature1), \n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e000589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfquery(feature0):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "        'query_tokens': _bytes_feature(feature0),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c479dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_elwc(elwc,num_example):\n",
    "    return tfr.data.parse_from_example_list(\n",
    "        [elwc],\n",
    "        list_size=num_example,\n",
    "        context_feature_spec={\"query_tokens\": tf.io.RaggedFeature(dtype=tf.string)},\n",
    "        example_feature_spec={\n",
    "            \"document_tokens\":\n",
    "                tf.io.RaggedFeature(dtype=tf.string),\n",
    "            \"relevance\":\n",
    "                tf.io.FixedLenFeature(shape=[], dtype=tf.int64, default_value=0)\n",
    "        },\n",
    "        size_feature_name=\"_list_size_\",\n",
    "        mask_feature_name=\"_mask_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "700a767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_serialized_elwc(tf_query, tf_examples):\n",
    "    ELWC = input_pb2.ExampleListWithContext()\n",
    "    ELWC.context.CopyFrom(tf_query)\n",
    "\n",
    "    for example in tf_examples:\n",
    "        example_features = ELWC.examples.add()\n",
    "        example_features.CopyFrom(example)\n",
    "    \n",
    "    return ELWC.SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218a0f7",
   "metadata": {},
   "source": [
    "### sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [\"this\",\"is\",\"a\",\"relevant\",\"answer\",\"irrelevant\",\"data\",\"query\"]\n",
    "with open(\"vocab.txt\", \"w\") as file1:\n",
    "    for token in vocab_list:\n",
    "        file1.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_example = create_tfexample(\"this is a relevant answer\",1)\n",
    "tf_example2 = create_tfexample(\"irrelevant data\",0)\n",
    "tf_example3 = create_tfexample(\"relevant data\",1)\n",
    "tf_example4 = create_tfexample(\"irrelevant data\",0)\n",
    "tf_example4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b108dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES = [tf_example, tf_example2, tf_example3, tf_example4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697603f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_query = create_tfquery(\"this is a query\")\n",
    "tf_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fa371",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELWC = input_pb2.ExampleListWithContext()\n",
    "ELWC.context.CopyFrom(tf_query)\n",
    "\n",
    "for example in EXAMPLES:\n",
    "    example_features = ELWC.examples.add()\n",
    "    example_features.CopyFrom(example)\n",
    "\n",
    "print(ELWC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef9ba93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"\\nL\\nJ\\n4\\n\\x0fdocument_tokens\\x12!\\n\\x1f\\n\\x04this\\n\\x02is\\n\\x01a\\n\\x08relevant\\n\\x06answer\\n\\x12\\n\\trelevance\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n?\\n=\\n\\x12\\n\\trelevance\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n'\\n\\x0fdocument_tokens\\x12\\x14\\n\\x12\\n\\nirrelevant\\n\\x04data\\n=\\n;\\n\\x12\\n\\trelevance\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n%\\n\\x0fdocument_tokens\\x12\\x12\\n\\x10\\n\\x08relevant\\n\\x04data\\n?\\n=\\n\\x12\\n\\trelevance\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n'\\n\\x0fdocument_tokens\\x12\\x14\\n\\x12\\n\\nirrelevant\\n\\x04data\\x12*\\n(\\n&\\n\\x0cquery_tokens\\x12\\x16\\n\\x14\\n\\x04this\\n\\x02is\\n\\x01a\\n\\x05query\"\n"
     ]
    }
   ],
   "source": [
    "serialized_elwc = ELWC.SerializeToString()\n",
    "print(serialized_elwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1372b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_elwc = create_serialized_elwc(tf_query, EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b72ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_elwc2 = create_serialized_elwc(tf_query, EXAMPLES)\n",
    "serialized_elwc3 = create_serialized_elwc(tf_query, EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff787b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_elwc_list = [serialized_elwc, serialized_elwc2, serialized_elwc3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5b80a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data = parse_elwc(serialized_elwc, len(EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter('test.tfrecords') as writer:\n",
    "    writer.write(serialized_elwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c25c0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter('train.tfrecords') as writer:\n",
    "    for objs in serialized_elwc_list:\n",
    "        writer.write(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0a3dc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(\"train.tfrecords\")\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_record in raw_dataset.take(10):\n",
    "    print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca296840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = tf.data.TFRecordDataset(_TRAIN_DATA_PATH)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e21275",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_record in train_data.take(1):\n",
    "    print(repr(train_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5883f4a",
   "metadata": {},
   "source": [
    "### stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a2e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16903645",
   "metadata": {},
   "source": [
    "## Losses, Metrics and Ranking Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f848f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function. To find a complete list of available\n",
    "# loss functions or to learn how to add your own custom function\n",
    "# please refer to the tensorflow_ranking.losses module.\n",
    "\n",
    "_LOSS = tfr.losses.RankingLossKey.APPROX_NDCG_LOSS\n",
    "loss_fn = tfr.losses.make_loss_fn(_LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d259a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdagradOptimizer(\n",
    "    learning_rate=_LEARNING_RATE)\n",
    "\n",
    "\n",
    "def _train_op_fn(loss):\n",
    "    \"\"\"Defines train op used in ranking head.\"\"\"\n",
    "    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "    minimize_op = optimizer.minimize(\n",
    "        loss=loss, global_step=tf.compat.v1.train.get_global_step())\n",
    "    train_op = tf.group([update_ops, minimize_op])\n",
    "    return train_op\n",
    "\n",
    "\n",
    "ranking_head = tfr.head.create_ranking_head(\n",
    "      loss_fn=loss_fn,\n",
    "      eval_metric_fns=eval_metric_fns(),\n",
    "      train_op_fn=_train_op_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ba1333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building groupwise ranking model.\n"
     ]
    }
   ],
   "source": [
    "model_fn = tfr.model.make_groupwise_ranking_fn(\n",
    "          group_score_fn=make_score_fn(),\n",
    "          transform_fn=make_transform_fn(),\n",
    "          group_size=_GROUP_SIZE,\n",
    "          ranking_head=ranking_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8533672",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab2efe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_fn():\n",
    "    \"\"\"Train and eval function used by `tf.estimator.train_and_evaluate`.\"\"\"\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps=1000)\n",
    "    ranker = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        model_dir=_MODEL_DIR,\n",
    "        config=run_config)\n",
    "\n",
    "    train_input_fn = lambda: input_fn(_TRAIN_DATA_PATH)\n",
    "    eval_input_fn = lambda: input_fn(_TEST_DATA_PATH, num_epochs=1)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=train_input_fn, max_steps=_NUM_TRAIN_STEPS)\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        name=\"eval\",\n",
    "        input_fn=eval_input_fn,\n",
    "        throttle_secs=15)\n",
    "    return (ranker, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d68e76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/simin/miniconda3/envs/tf-rank/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/ranking_model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function _make_model_fn.<locals>._model_fn at 0x7fc323d4c9d0>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From /home/simin/miniconda3/envs/tf-rank/lib/python3.10/site-packages/tensorflow/python/training/training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "WARNING:tensorflow:From /home/simin/miniconda3/envs/tf-rank/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "WARNING:tensorflow:From /home/simin/miniconda3/envs/tf-rank/lib/python3.10/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9176/2110212537.py:8: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  tf.compat.v1.layers.flatten(context_features[name])\n",
      "/tmp/ipykernel_9176/2110212537.py:12: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  tf.compat.v1.layers.flatten(group_features[name])\n",
      "/tmp/ipykernel_9176/2110212537.py:19: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  cur_layer = tf.compat.v1.layers.batch_normalization(\n",
      "/tmp/ipykernel_9176/2110212537.py:25: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  cur_layer = tf.compat.v1.layers.dense(cur_layer, units=layer_width)\n",
      "/tmp/ipykernel_9176/2110212537.py:26: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  cur_layer = tf.compat.v1.layers.batch_normalization(\n",
      "/tmp/ipykernel_9176/2110212537.py:31: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
      "  cur_layer = tf.compat.v1.layers.dropout(\n",
      "/tmp/ipykernel_9176/2110212537.py:33: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  logits = tf.compat.v1.layers.dense(cur_layer, units=_GROUP_SIZE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/simin/miniconda3/envs/tf-rank/lib/python3.10/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:36:58.496604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:36:58.497399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:36:58.497956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:36:58.498779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:36:58.498814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-07 19:36:58.499274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:36:58.499355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0b:00.0, compute capability: 8.6\n",
      "2023-04-07 19:36:58.521827: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = -0.85622907, step = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:37:01.688919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 36.5057\n",
      "INFO:tensorflow:loss = -0.86425054, step = 100 (2.740 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.3126\n",
      "INFO:tensorflow:loss = -0.832706, step = 200 (2.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1262\n",
      "INFO:tensorflow:loss = -0.83526194, step = 300 (2.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.4205\n",
      "INFO:tensorflow:loss = -0.78599846, step = 400 (2.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.2698\n",
      "INFO:tensorflow:loss = -0.854683, step = 500 (2.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.4441\n",
      "INFO:tensorflow:loss = -0.86314833, step = 600 (2.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.6228\n",
      "INFO:tensorflow:loss = -0.8332889, step = 700 (2.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.4663\n",
      "INFO:tensorflow:loss = -0.85919327, step = 800 (2.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.0756\n",
      "INFO:tensorflow:loss = -0.8144463, step = 900 (2.269 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1000...\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2023-04-07T19:37:26\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ranking_model_dir/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:37:26.634419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:26.635029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:26.635536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:26.636380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:26.636416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-07 19:37:26.636863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:26.636916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 1.19033s\n",
      "INFO:tensorflow:Finished evaluation at 2023-04-07-19:37:27\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, labels_mean = 1.9630322, logits_mean = 1.2490524, loss = -0.77059895, metric/ndcg@1 = 0.5878571, metric/ndcg@10 = 0.7951875, metric/ndcg@3 = 0.67231107, metric/ndcg@5 = 0.72104204\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/ranking_model_dir/model.ckpt-1000\n",
      "INFO:tensorflow:global_step/sec: 20.6239\n",
      "INFO:tensorflow:loss = -0.8284209, step = 1000 (4.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.1428\n",
      "INFO:tensorflow:loss = -0.87761724, step = 1100 (2.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.0684\n",
      "INFO:tensorflow:loss = -0.8619217, step = 1200 (2.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.7552\n",
      "INFO:tensorflow:loss = -0.8232709, step = 1300 (2.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.3003\n",
      "INFO:tensorflow:loss = -0.845335, step = 1400 (2.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.3536\n",
      "INFO:tensorflow:loss = -0.8592038, step = 1500 (2.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.7943\n",
      "INFO:tensorflow:loss = -0.8595308, step = 1600 (2.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.6488\n",
      "INFO:tensorflow:loss = -0.8319205, step = 1700 (2.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9417\n",
      "INFO:tensorflow:loss = -0.85033673, step = 1800 (2.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.5752\n",
      "INFO:tensorflow:loss = -0.85954875, step = 1900 (2.349 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2000...\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2023-04-07T19:37:52\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ranking_model_dir/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:37:52.683988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:52.684547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:52.685388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:52.686351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:52.686393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-07 19:37:52.686889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:37:52.686947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.92012s\n",
      "INFO:tensorflow:Finished evaluation at 2023-04-07-19:37:53\n",
      "INFO:tensorflow:Saving dict for global step 2000: global_step = 2000, labels_mean = 1.9630322, logits_mean = 2.127494, loss = -0.805644, metric/ndcg@1 = 0.665, metric/ndcg@10 = 0.8230577, metric/ndcg@3 = 0.7217461, metric/ndcg@5 = 0.76058114\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /tmp/ranking_model_dir/model.ckpt-2000\n",
      "INFO:tensorflow:global_step/sec: 22.6789\n",
      "INFO:tensorflow:loss = -0.8421985, step = 2000 (4.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.7648\n",
      "INFO:tensorflow:loss = -0.853926, step = 2100 (2.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.5663\n",
      "INFO:tensorflow:loss = -0.85652995, step = 2200 (2.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.0595\n",
      "INFO:tensorflow:loss = -0.8402942, step = 2300 (2.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.0409\n",
      "INFO:tensorflow:loss = -0.8823969, step = 2400 (2.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.3201\n",
      "INFO:tensorflow:loss = -0.8506988, step = 2500 (2.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.0519\n",
      "INFO:tensorflow:loss = -0.82811606, step = 2600 (2.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.6231\n",
      "INFO:tensorflow:loss = -0.82908285, step = 2700 (2.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9173\n",
      "INFO:tensorflow:loss = -0.88832897, step = 2800 (2.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.3933\n",
      "INFO:tensorflow:loss = -0.8520845, step = 2900 (2.358 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3000...\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /tmp/ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2023-04-07T19:38:18\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ranking_model_dir/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:38:18.403126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:18.403773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:18.404280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:18.405043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:18.405078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-07 19:38:18.405535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:18.405586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.90692s\n",
      "INFO:tensorflow:Finished evaluation at 2023-04-07-19:38:19\n",
      "INFO:tensorflow:Saving dict for global step 3000: global_step = 3000, labels_mean = 1.9630322, logits_mean = 2.6156185, loss = -0.8224302, metric/ndcg@1 = 0.69714284, metric/ndcg@10 = 0.8326173, metric/ndcg@3 = 0.7329309, metric/ndcg@5 = 0.769486\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3000: /tmp/ranking_model_dir/model.ckpt-3000\n",
      "INFO:tensorflow:global_step/sec: 22.8967\n",
      "INFO:tensorflow:loss = -0.8494569, step = 3000 (4.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.9177\n",
      "INFO:tensorflow:loss = -0.8190213, step = 3100 (2.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.4934\n",
      "INFO:tensorflow:loss = -0.8363153, step = 3200 (2.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.6793\n",
      "INFO:tensorflow:loss = -0.83196455, step = 3300 (2.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.2026\n",
      "INFO:tensorflow:loss = -0.8564136, step = 3400 (2.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9313\n",
      "INFO:tensorflow:loss = -0.86584544, step = 3500 (2.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1161\n",
      "INFO:tensorflow:loss = -0.85607684, step = 3600 (2.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1676\n",
      "INFO:tensorflow:loss = -0.877599, step = 3700 (2.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.7133\n",
      "INFO:tensorflow:loss = -0.87709713, step = 3800 (2.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.0818\n",
      "INFO:tensorflow:loss = -0.8536861, step = 3900 (2.434 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4000...\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /tmp/ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2023-04-07T19:38:44\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ranking_model_dir/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:38:44.445214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:44.446039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:44.446613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:44.447382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:44.447416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-07 19:38:44.447884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:38:44.447934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.93187s\n",
      "INFO:tensorflow:Finished evaluation at 2023-04-07-19:38:45\n",
      "INFO:tensorflow:Saving dict for global step 4000: global_step = 4000, labels_mean = 1.9630322, logits_mean = 2.9765208, loss = -0.82578903, metric/ndcg@1 = 0.68357146, metric/ndcg@10 = 0.83014214, metric/ndcg@3 = 0.73244303, metric/ndcg@5 = 0.7694423\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4000: /tmp/ranking_model_dir/model.ckpt-4000\n",
      "INFO:tensorflow:global_step/sec: 21.7926\n",
      "INFO:tensorflow:loss = -0.85554034, step = 4000 (4.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.7767\n",
      "INFO:tensorflow:loss = -0.8523695, step = 4100 (2.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.668\n",
      "INFO:tensorflow:loss = -0.8554511, step = 4200 (2.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6598\n",
      "INFO:tensorflow:loss = -0.8843602, step = 4300 (2.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.4199\n",
      "INFO:tensorflow:loss = -0.86835945, step = 4400 (2.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.8781\n",
      "INFO:tensorflow:loss = -0.8627713, step = 4500 (2.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1387\n",
      "INFO:tensorflow:loss = -0.86049235, step = 4600 (2.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.1524\n",
      "INFO:tensorflow:loss = -0.86649776, step = 4700 (2.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.755\n",
      "INFO:tensorflow:loss = -0.8962395, step = 4800 (2.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.6345\n",
      "INFO:tensorflow:loss = -0.87036884, step = 4900 (2.346 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5000...\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/ranking_model_dir/model.ckpt.\n",
      "WARNING:tensorflow:From /home/simin/miniconda3/envs/tf-rank/lib/python3.10/site-packages/tensorflow/python/training/saver.py:1064: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2023-04-07T19:39:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ranking_model_dir/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:39:11.090744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:39:11.091560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:39:11.092133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:39:11.092809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:39:11.092843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-07 19:39:11.093253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 19:39:11.093319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.90976s\n",
      "INFO:tensorflow:Finished evaluation at 2023-04-07-19:39:11\n",
      "INFO:tensorflow:Saving dict for global step 5000: global_step = 5000, labels_mean = 1.9630322, logits_mean = 3.1429155, loss = -0.8247399, metric/ndcg@1 = 0.6685715, metric/ndcg@10 = 0.8292692, metric/ndcg@3 = 0.7333425, metric/ndcg@5 = 0.76657265\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/ranking_model_dir/model.ckpt-5000\n",
      "INFO:tensorflow:Loss for final step: -0.87629384.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'labels_mean': 1.9630322,\n",
       "  'logits_mean': 3.1429155,\n",
       "  'loss': -0.8247399,\n",
       "  'metric/ndcg@1': 0.6685715,\n",
       "  'metric/ndcg@10': 0.8292692,\n",
       "  'metric/ndcg@3': 0.7333425,\n",
       "  'metric/ndcg@5': 0.76657265,\n",
       "  'global_step': 5000},\n",
       " [])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! rm -rf \"/tmp/ranking_model_dir\"  # Clean up the model directory.\n",
    "ranker, train_spec, eval_spec = train_and_eval_fn()\n",
    "tf.estimator.train_and_evaluate(ranker, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f380b9",
   "metadata": {},
   "source": [
    "## Inference and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea85824",
   "metadata": {},
   "source": [
    "We show how to generate predictions over the features of a dataset. We assume that the label is not present and needs to be inferred using the ranking model.\n",
    "\n",
    "Similar to the `input_fn` used for training and evaluation,  `predict_input_fn` reads in data in ELWC format and stored as TFRecords to generate features. We set number of epochs to be 1, so that the generator stops iterating when it reaches the end of the dataset. Also the datapoints are not shuffled while reading, so that the behavior of the `predict()` function is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab9ced1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input_fn(path):\n",
    "    context_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "        context_feature_columns().values())\n",
    "    example_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "        list(example_feature_columns().values()))\n",
    "    dataset = tfr.data.build_ranking_dataset(\n",
    "        file_pattern=path,\n",
    "        data_format=tfr.data.ELWC,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        list_size=_LIST_SIZE,\n",
    "        context_feature_spec=context_feature_spec,\n",
    "        example_feature_spec=example_feature_spec,\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        shuffle=False,\n",
    "        num_epochs=1)\n",
    "    features = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cef1b3",
   "metadata": {},
   "source": [
    "We generate predictions on the test dataset, where we only consider context and example features and predict the labels. The `predict_input_fn` generates predictions on a batch of datapoints. Batching allows us to iterate over large datasets which cannot be loaded in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2c43668",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ranker.predict(input_fn=lambda: predict_input_fn(\"/tmp/test.tfrecords\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7799940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Estimator.predict at 0x7f4939cc5c40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09a7de",
   "metadata": {},
   "source": [
    "`ranker.predict` returns a generator, which we can iterate over to create predictions, till the generator is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74afe2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26705/3212595179.py:8: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  tf.compat.v1.layers.flatten(context_features[name])\n",
      "/tmp/ipykernel_26705/3212595179.py:12: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  tf.compat.v1.layers.flatten(group_features[name])\n",
      "/tmp/ipykernel_26705/3212595179.py:19: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  cur_layer = tf.compat.v1.layers.batch_normalization(\n",
      "/tmp/ipykernel_26705/3212595179.py:25: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  cur_layer = tf.compat.v1.layers.dense(cur_layer, units=layer_width)\n",
      "/tmp/ipykernel_26705/3212595179.py:26: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  cur_layer = tf.compat.v1.layers.batch_normalization(\n",
      "/tmp/ipykernel_26705/3212595179.py:31: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
      "  cur_layer = tf.compat.v1.layers.dropout(\n",
      "/tmp/ipykernel_26705/3212595179.py:33: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  logits = tf.compat.v1.layers.dense(cur_layer, units=_GROUP_SIZE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ranking_model_dir/model.ckpt-15000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 18:36:08.220005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 18:36:08.220645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 18:36:08.221137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 18:36:08.222343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 18:36:08.222379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-07 18:36:08.222839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 18:36:08.222893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2459 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "x = next(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "010a01fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.6875005 , -3.5406964 ,  4.5525036 ,  2.8311815 , -0.0627908 ,\n",
       "        2.1847188 , -1.1926783 , -0.5721942 ,  4.5686088 ,  4.643297  ,\n",
       "        2.4712627 ,  3.3735383 , -6.5405955 ,  4.679387  ,  4.195493  ,\n",
       "        4.3931046 ,  4.6696076 ,  4.586497  ,  3.0385761 ,  2.1049716 ,\n",
       "        4.4866858 ,  4.3540955 ,  2.8567202 ,  1.7653334 ,  2.465593  ,\n",
       "        1.7653961 ,  4.6871815 ,  4.621498  , -0.04419075,  4.322982  ,\n",
       "        4.322982  ,  4.322982  ,  4.322982  ,  4.322982  ,  4.322982  ,\n",
       "        4.322982  ,  4.322982  ,  4.322982  ,  4.322982  ,  4.322982  ,\n",
       "        4.322982  ,  4.322982  ,  4.322982  ,  4.322982  ,  4.322982  ,\n",
       "        4.322982  ,  4.322982  ,  4.322982  ,  4.322982  ,  4.322982  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f6c541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2880760c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.6372685 ,   0.21779408,   1.5418385 ,   3.3559597 ,\n",
       "         4.663595  , -13.910055  ,   2.180758  ,   4.4911757 ,\n",
       "         4.4917107 ,  -6.385597  ,   4.5188103 ,  -4.326332  ,\n",
       "         4.189927  ,   4.3294077 ,   4.2710466 ,   4.5378838 ,\n",
       "         4.5378838 ,   4.5378838 ,   4.5378838 ,   4.5378838 ,\n",
       "         4.5378838 ,   4.5378838 ,   4.5378838 ,   4.5378838 ,\n",
       "         4.5378838 ,   4.5378838 ,   4.5378838 ,   4.5378838 ,\n",
       "         4.5378838 ,   4.5378838 ,   4.5378838 ,   4.5378838 ,\n",
       "         4.5378838 ,   4.5378838 ,   4.5378838 ,   4.5378838 ,\n",
       "         4.5378838 ,   4.5378838 ,   4.5378838 ,   4.5378838 ,\n",
       "         4.5378838 ,   4.5378838 ,   4.5378838 ,   4.5378838 ,\n",
       "         4.5378838 ,   4.5378838 ,   4.5378838 ,   4.5378838 ,\n",
       "         4.5378838 ,   4.5378838 ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4911ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95062315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.8983476 , -2.6677127 ,  3.4520075 ,  0.21621923,  4.6609907 ,\n",
       "       -3.2321227 , -6.6224174 ,  2.70285   , -2.7546382 ,  4.6620975 ,\n",
       "        1.0986121 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,\n",
       "        4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,\n",
       "        4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,\n",
       "        4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,\n",
       "        4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,\n",
       "        4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,\n",
       "        4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,\n",
       "        4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ,  4.1087656 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea13d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-rank",
   "language": "python",
   "name": "tf-rank"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
